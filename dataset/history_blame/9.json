{
  "id": "9",
  "blame_commit": {
    "commit": {
      "commit_id": "5fac0c642592dabe1885054806f428f493bd7032",
      "commit_message": "More robust exists for hive.\n\nInstead of trying to scrape the output for a \"table not found\" we can just look at the database and see if the table exists by querying for all the tables with the exact name.",
      "commit_author": "Taylor Perkins",
      "commit_date": "2014-09-20 10:29:41",
      "commit_parent": "8ba2001d61d5e6208698b306496f96f38f4b6540"
    },
    "function": {
      "function_name": "table_exists",
      "function_code_before": "",
      "function_code_after": "",
      "function_before_start_line": "",
      "function_before_end_line": "",
      "function_after_start_line": "",
      "function_after_end_line": "",
      "function_before_token_count": 0,
      "function_after_token_count": 0,
      "functions_name_modified_file": [
        "run_hive_script",
        "job_runner",
        "run_hive_cmd",
        "output",
        "table_location",
        "open",
        "__enter__",
        "_existing_partitions",
        "load_hive_cmd",
        "table_schema",
        "hiverc",
        "path",
        "query",
        "exists",
        "partition_spec",
        "hiveconfs",
        "table_exists",
        "run_job",
        "__init__",
        "prepare_outputs",
        "get_hive_syntax",
        "run_hive",
        "__exit__"
      ],
      "functions_name_all_files": [
        "run_hive_script",
        "job_runner",
        "tearDown",
        "run_hive_cmd",
        "output",
        "test_run_hive_script_exists",
        "test_apacheclient_table_schema",
        "table_location",
        "test_client_def",
        "open",
        "test_default_table_location",
        "test_table_exists",
        "test_metastoreclient_partition_existence_regardless_of_order",
        "__enter__",
        "_existing_partitions",
        "load_hive_cmd",
        "table_schema",
        "hiverc",
        "path",
        "test_apacheclient_table_exists",
        "query",
        "exists",
        "partition_spec",
        "hiveconfs",
        "table_exists",
        "run_job",
        "mock_hive_cmd",
        "test_partition_spec",
        "setUp",
        "__init__",
        "test_metastore_partition_spec_has_the_same_order",
        "test_create_parent_dirs",
        "prepare_outputs",
        "get_hive_syntax",
        "run_hive",
        "test_run_hive_command",
        "test_run_hive_script_not_exists",
        "test_table_schema",
        "__exit__"
      ],
      "functions_name_co_evolved_modified_file": [],
      "functions_name_co_evolved_all_files": [
        "test_apacheclient_table_exists",
        "test_table_exists"
      ]
    },
    "file": {
      "file_name": "hive.py",
      "file_nloc": 257,
      "file_complexity": 83,
      "file_token_count": 2021,
      "file_before": "# Licensed under the Apache License, Version 2.0 (the \"License\"); you may not\n# use this file except in compliance with the License. You may obtain a copy of\n# the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n# License for the specific language governing permissions and limitations under\n# the License.\n\nimport abc\nimport logging\nimport operator\nimport luigi\nimport luigi.hadoop\nfrom luigi.target import FileSystemTarget, FileAlreadyExists\nimport os\nimport subprocess\nimport tempfile\nfrom luigi.task import flatten\n\nlogger = logging.getLogger('luigi-interface')\n\n\nclass HiveCommandError(RuntimeError):\n    def __init__(self, message, out=None, err=None):\n        super(HiveCommandError, self).__init__(message, out, err)\n        self.message = message\n        self.out = out\n        self.err = err\n\n\ndef load_hive_cmd():\n    return luigi.configuration.get_config().get('hive', 'command', 'hive')\n\n\ndef get_hive_syntax():\n    return luigi.configuration.get_config().get('hive', 'release', 'cdh4')\n\n\ndef run_hive(args, check_return_code=True):\n    \"\"\"Runs the `hive` from the command line, passing in the given args, and\n       returning stdout.\n\n       With the apache release of Hive, so of the table existence checks\n       (which are done using DESCRIBE do not exit with a return code of 0\n       so we need an option to ignore the return code and just return stdout for parsing\n    \"\"\"\n    cmd = [load_hive_cmd()] + args\n    p = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n    stdout, stderr = p.communicate()\n    if check_return_code and p.returncode != 0:\n        raise HiveCommandError(\"Hive command: {0} failed with error code: {1}\".format(\" \".join(cmd), p.returncode),\n                               stdout, stderr)\n    return stdout\n\n\ndef run_hive_cmd(hivecmd, check_return_code=True):\n    \"\"\"Runs the given hive query and returns stdout\"\"\"\n    return run_hive(['-e', hivecmd], check_return_code)\n\n\ndef run_hive_script(script):\n    \"\"\"Runs the contents of the given script in hive and returns stdout\"\"\"\n    if not os.path.isfile(script):\n        raise RuntimeError(\"Hive script: {0} does not exist.\".format(script))\n    return run_hive(['-f', script])\n\n\nclass HiveClient(object):  # interface\n    __metaclass__ = abc.ABCMeta\n\n    @abc.abstractmethod\n    def table_location(self, table, database='default', partition={}):\n        \"\"\"\n        Returns location of db.table (or db.table.partition). partition is a dict of partition key to\n        value.\n        \"\"\"\n        pass\n\n    @abc.abstractmethod\n    def table_schema(self, table, database='default'):\n        \"\"\" Returns list of [(name, type)] for each column in database.table \"\"\"\n        pass\n\n    @abc.abstractmethod\n    def table_exists(self, table, database='default', partition={}):\n        \"\"\"\n        Returns true iff db.table (or db.table.partition) exists. partition is a dict of partition key to\n        value.\n        \"\"\"\n        pass\n\n    @abc.abstractmethod\n    def partition_spec(self, partition):\n        \"\"\" Turn a dict into a string partition specification \"\"\"\n        pass\n\n\nclass HiveCommandClient(HiveClient):\n    \"\"\" Uses `hive` invocations to find information \"\"\"\n    def table_location(self, table, database='default', partition={}):\n        cmd = \"use {0}; describe formatted {1}\".format(database, table)\n        if partition:\n            cmd += \" PARTITION ({0})\".format(self.partition_spec(partition))\n\n        stdout = run_hive_cmd(cmd)\n\n        for line in stdout.split(\"\\n\"):\n            if \"Location:\" in line:\n                return line.split(\"\\t\")[1]\n\n    def table_exists(self, table, database='default', partition={}):\n        if not partition:\n            stdout = run_hive_cmd('use {0}; describe {1}'.format(database, table))\n\n            return not \"does not exist\" in stdout\n        else:\n            stdout = run_hive_cmd(\"\"\"use %s; show partitions %s partition\n                                (%s)\"\"\" % (database, table, self.partition_spec(partition)))\n\n            if stdout:\n                return True\n            else:\n                return False\n\n    def table_schema(self, table, database='default'):\n        describe = run_hive_cmd(\"use {0}; describe {1}\".format(database, table))\n        if not describe or \"does not exist\" in describe:\n            return None\n        return [tuple([x.strip() for x in line.strip().split(\"\\t\")]) for line in describe.strip().split(\"\\n\")]\n\n    def partition_spec(self, partition):\n        \"\"\" Turns a dict into the a Hive partition specification string \"\"\"\n        return ','.join([\"{0}='{1}'\".format(k, v) for (k, v) in\n                         sorted(partition.items(), key=operator.itemgetter(0))])\n\n\nclass ApacheHiveCommandClient(HiveCommandClient):\n    \"\"\"\n    A subclass for the HiveCommandClient to (in some cases) ignore the return code from\n    the hive command so that we can just parse the output.\n    \"\"\"\n    def table_exists(self, table, database='default', partition={}):\n        if not partition:\n            # Hive 0.11 returns 17 as the exit status if the table does not exist.\n            # The actual message is: [Error 10001]: Table not found tablename\n            # stdout is empty and an error message is returned on stderr.\n            # This is why we can't check the return code on this command and\n            # assume if stdout is empty that the table doesn't exist.\n            stdout = run_hive_cmd('use {0}; describe {1}'.format(database, table), False)\n            if stdout:\n                return not \"Table not found\" in stdout\n            else:\n                # Hive returned a non-zero exit status and printed its output to stderr not stdout\n                return False\n        else:\n            stdout = run_hive_cmd(\"\"\"use %s; show partitions %s partition\n                                (%s)\"\"\" % (database, table, self.partition_spec(partition)), False)\n\n            if stdout:\n                return True\n            else:\n                return False\n\n    def table_schema(self, table, database='default'):\n        describe = run_hive_cmd(\"use {0}; describe {1}\".format(database, table), False)\n        if not describe or \"Table not found\" in describe:\n            return None\n        return [tuple([x.strip() for x in line.strip().split(\"\\t\")]) for line in describe.strip().split(\"\\n\")]\n\n\nclass MetastoreClient(HiveClient):\n    def table_location(self, table, database='default', partition={}):\n        with HiveThriftContext() as client:\n            if partition:\n                partition_str = self.partition_spec(partition)\n                thrift_table = client.get_partition_by_name(database, table, partition_str)\n            else:\n                thrift_table = client.get_table(database, table)\n            return thrift_table.sd.location\n\n    def table_exists(self, table, database='default', partition={}):\n        with HiveThriftContext() as client:\n            if not partition:\n                return table in client.get_all_tables(database)\n            else:\n                return partition in self._existing_partitions(table, database, client)\n\n    def _existing_partitions(self, table, database, client):\n        def _parse_partition_string(partition_string):\n            partition_def = {}\n            for part in partition_string.split(\"/\"):\n                name, value = part.split(\"=\")\n                partition_def[name] = value\n            return partition_def\n\n        # -1 is max_parts, the # of partition names to return (-1 = unlimited)\n        partition_strings = client.get_partition_names(database, table, -1)\n        return [_parse_partition_string(existing_partition) for existing_partition in partition_strings]\n\n    def table_schema(self, table, database='default'):\n        with HiveThriftContext() as client:\n            return [(field_schema.name, field_schema.type) for field_schema in client.get_schema(database, table)]\n\n    def partition_spec(self, partition):\n        return \"/\".join(\"%s=%s\" % (k, v) for (k, v) in sorted(partition.items(), key=operator.itemgetter(0)))\n\n\nclass HiveThriftContext(object):\n    \"\"\" Context manager for hive metastore client \"\"\"\n    def __enter__(self):\n        try:\n            from thrift import Thrift\n            from thrift.transport import TSocket\n            from thrift.transport import TTransport\n            from thrift.protocol import TBinaryProtocol\n            # Note that this will only work with a CDH release.\n            # This uses the thrift bindings generated by the ThriftHiveMetastore service in Beeswax.\n            # If using the Apache release of Hive this import will fail.\n            from hive_metastore import ThriftHiveMetastore\n            config = luigi.configuration.get_config()\n            host = config.get('hive', 'metastore_host')\n            port = config.getint('hive', 'metastore_port')\n            transport = TSocket.TSocket(host, port)\n            transport = TTransport.TBufferedTransport(transport)\n            protocol = TBinaryProtocol.TBinaryProtocol(transport)\n            transport.open()\n            self.transport = transport\n            return ThriftHiveMetastore.Client(protocol)\n        except ImportError, e:\n            raise Exception('Could not import Hive thrift library:' + str(e))\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        self.transport.close()\n\nif get_hive_syntax() == \"apache\":\n    default_client = ApacheHiveCommandClient()\nelse:\n    default_client = HiveCommandClient()\nclient = default_client\n\n\nclass HiveQueryTask(luigi.hadoop.BaseHadoopJobTask):\n    \"\"\" Task to run a hive query \"\"\"\n    # by default, we let hive figure these out.\n    n_reduce_tasks = None\n    bytes_per_reducer = None\n    reducers_max = None\n\n    @abc.abstractmethod\n    def query(self):\n        \"\"\" Text of query to run in hive \"\"\"\n        raise RuntimeError(\"Must implement query!\")\n\n    def hiverc(self):\n        \"\"\" Location of an rc file to run before the query \n            if hiverc-location key is specified in client.cfg, will default to the value there\n            otherwise returns None\n        \"\"\"\n        return luigi.configuration.get_config().get('hive', 'hiverc-location', default=None)\n\n    def hiveconfs(self):\n        \"\"\"\n        Returns an dict of key=value settings to be passed along\n        to the hive command line via --hiveconf. By default, sets\n        mapred.job.name to task_id and if not None, sets:\n        * mapred.reduce.tasks (n_reduce_tasks)\n        * mapred.fairscheduler.pool (pool) or mapred.job.queue.name (pool)\n        * hive.exec.reducers.bytes.per.reducer (bytes_per_reducer)\n        * hive.exec.reducers.max (reducers_max)\n        \"\"\"\n        jcs = {}\n        jcs['mapred.job.name'] = self.task_id\n        if self.n_reduce_tasks is not None:\n            jcs['mapred.reduce.tasks'] = self.n_reduce_tasks\n        if self.pool is not None:\n            # Supporting two schedulers: fair (default) and capacity using the same option\n            scheduler_type = luigi.configuration.get_config().get('hadoop', 'scheduler', 'fair')\n            if scheduler_type == 'fair':\n                jcs['mapred.fairscheduler.pool'] = self.pool\n            elif scheduler_type == 'capacity':\n                jcs['mapred.job.queue.name'] = self.pool\n        if self.bytes_per_reducer is not None:\n            jcs['hive.exec.reducers.bytes.per.reducer'] = self.bytes_per_reducer\n        if self.reducers_max is not None:\n            jcs['hive.exec.reducers.max'] = self.reducers_max\n        return jcs\n\n    def job_runner(self):\n        return HiveQueryRunner()\n\n\nclass HiveQueryRunner(luigi.hadoop.JobRunner):\n    \"\"\" Runs a HiveQueryTask by shelling out to hive \"\"\"\n\n    def prepare_outputs(self, job):\n        \"\"\" Called before job is started\n\n        If output is a `FileSystemTarget`, create parent directories so the hive command won't fail\n        \"\"\"\n        outputs = flatten(job.output())\n        for o in outputs:\n            if isinstance(o, FileSystemTarget):\n                parent_dir = os.path.dirname(o.path)\n                if parent_dir and not o.fs.exists(parent_dir):\n                    logger.info(\"Creating parent directory %r\", parent_dir)\n                    try:\n                        # there is a possible race condition\n                        # which needs to be handled here\n                        o.fs.mkdir(parent_dir)\n                    except FileAlreadyExists:\n                        pass\n\n    def run_job(self, job):\n        self.prepare_outputs(job)\n        with tempfile.NamedTemporaryFile() as f:\n            f.write(job.query())\n            f.flush()\n            arglist = [load_hive_cmd(), '-f', f.name]\n            if job.hiverc():\n                arglist += ['-i', job.hiverc()]\n            if job.hiveconfs():\n                for k, v in job.hiveconfs().iteritems():\n                    arglist += ['--hiveconf', '{0}={1}'.format(k, v)]\n\n            logger.info(arglist)\n            return luigi.hadoop.run_and_track_hadoop_job(arglist)\n\n\nclass HiveTableTarget(luigi.Target):\n    \"\"\" exists returns true if the table exists \"\"\"\n\n    def __init__(self, table, database='default', client=default_client):\n        self.database = database\n        self.table = table\n        self.hive_cmd = load_hive_cmd()\n        self.client = client\n\n    def exists(self):\n        logger.debug(\"Checking Hive table '%s.%s' exists\", self.database, self.table)\n        return self.client.table_exists(self.table, self.database)\n\n    @property\n    def path(self):\n        \"\"\"Returns the path to this table in HDFS\"\"\"\n        location = self.client.table_location(self.table, self.database)\n        if not location:\n            raise Exception(\"Couldn't find location for table: {0}\".format(str(self)))\n        return location\n\n    def open(self, mode):\n        return NotImplementedError(\"open() is not supported for HiveTableTarget\")\n\n\nclass HivePartitionTarget(luigi.Target):\n    \"\"\" exists returns true if the table's partition exists \"\"\"\n\n    def __init__(self, table, partition, database='default', fail_missing_table=True, client=default_client):\n        self.database = database\n        self.table = table\n        self.partition = partition\n        self.client = client\n\n        self.fail_missing_table = fail_missing_table\n\n    def exists(self):\n        try:\n            logger.debug(\"Checking Hive table '{d}.{t}' for partition {p}\".format(d=self.database, t=self.table, p=str(self.partition)))\n            return self.client.table_exists(self.table, self.database, self.partition)\n        except HiveCommandError, e:\n            if self.fail_missing_table:\n                raise\n            else:\n                if self.client.table_exists(self.table, self.database):\n                    # a real error occurred\n                    raise\n                else:\n                    # oh the table just doesn't exist\n                    return False\n\n    @property\n    def path(self):\n        \"\"\"Returns the path for this HiveTablePartitionTarget's data\"\"\"\n        location = self.client.table_location(self.table, self.database, self.partition)\n        if not location:\n            raise Exception(\"Couldn't find location for table: {0}\".format(str(self)))\n        return location\n\n    def open(self, mode):\n        return NotImplementedError(\"open() is not supported for HivePartitionTarget\")\n\n\nclass ExternalHiveTask(luigi.ExternalTask):\n    \"\"\" External task that depends on a Hive table/partition \"\"\"\n\n    database = luigi.Parameter(default='default')\n    table = luigi.Parameter()\n    # since this is an external task and will never be initialized from the CLI, partition can be any python object, in this case a dictionary\n    partition = luigi.Parameter(default=None, description='Python dictionary specifying the target partition e.g. {\"date\": \"2013-01-25\"}')\n\n    def output(self):\n        if self.partition is not None:\n            assert self.partition, \"partition required\"\n            return HivePartitionTarget(table=self.table,\n                                       partition=self.partition,\n                                       database=self.database)\n        else:\n            return HiveTableTarget(self.table, self.database)\n",
      "file_after": "# Licensed under the Apache License, Version 2.0 (the \"License\"); you may not\n# use this file except in compliance with the License. You may obtain a copy of\n# the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n# License for the specific language governing permissions and limitations under\n# the License.\n\nimport abc\nimport logging\nimport operator\nimport luigi\nimport luigi.hadoop\nfrom luigi.target import FileSystemTarget, FileAlreadyExists\nimport os\nimport subprocess\nimport tempfile\nfrom luigi.task import flatten\n\nlogger = logging.getLogger('luigi-interface')\n\n\nclass HiveCommandError(RuntimeError):\n    def __init__(self, message, out=None, err=None):\n        super(HiveCommandError, self).__init__(message, out, err)\n        self.message = message\n        self.out = out\n        self.err = err\n\n\ndef load_hive_cmd():\n    return luigi.configuration.get_config().get('hive', 'command', 'hive')\n\n\ndef get_hive_syntax():\n    return luigi.configuration.get_config().get('hive', 'release', 'cdh4')\n\n\ndef run_hive(args, check_return_code=True):\n    \"\"\"Runs the `hive` from the command line, passing in the given args, and\n       returning stdout.\n\n       With the apache release of Hive, so of the table existence checks\n       (which are done using DESCRIBE do not exit with a return code of 0\n       so we need an option to ignore the return code and just return stdout for parsing\n    \"\"\"\n    cmd = [load_hive_cmd()] + args\n    p = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n    stdout, stderr = p.communicate()\n    if check_return_code and p.returncode != 0:\n        raise HiveCommandError(\"Hive command: {0} failed with error code: {1}\".format(\" \".join(cmd), p.returncode),\n                               stdout, stderr)\n    return stdout\n\n\ndef run_hive_cmd(hivecmd, check_return_code=True):\n    \"\"\"Runs the given hive query and returns stdout\"\"\"\n    return run_hive(['-e', hivecmd], check_return_code)\n\n\ndef run_hive_script(script):\n    \"\"\"Runs the contents of the given script in hive and returns stdout\"\"\"\n    if not os.path.isfile(script):\n        raise RuntimeError(\"Hive script: {0} does not exist.\".format(script))\n    return run_hive(['-f', script])\n\n\nclass HiveClient(object):  # interface\n    __metaclass__ = abc.ABCMeta\n\n    @abc.abstractmethod\n    def table_location(self, table, database='default', partition={}):\n        \"\"\"\n        Returns location of db.table (or db.table.partition). partition is a dict of partition key to\n        value.\n        \"\"\"\n        pass\n\n    @abc.abstractmethod\n    def table_schema(self, table, database='default'):\n        \"\"\" Returns list of [(name, type)] for each column in database.table \"\"\"\n        pass\n\n    @abc.abstractmethod\n    def table_exists(self, table, database='default', partition={}):\n        \"\"\"\n        Returns true iff db.table (or db.table.partition) exists. partition is a dict of partition key to\n        value.\n        \"\"\"\n        pass\n\n    @abc.abstractmethod\n    def partition_spec(self, partition):\n        \"\"\" Turn a dict into a string partition specification \"\"\"\n        pass\n\n\nclass HiveCommandClient(HiveClient):\n    \"\"\" Uses `hive` invocations to find information \"\"\"\n    def table_location(self, table, database='default', partition={}):\n        cmd = \"use {0}; describe formatted {1}\".format(database, table)\n        if partition:\n            cmd += \" PARTITION ({0})\".format(self.partition_spec(partition))\n\n        stdout = run_hive_cmd(cmd)\n\n        for line in stdout.split(\"\\n\"):\n            if \"Location:\" in line:\n                return line.split(\"\\t\")[1]\n\n    def table_exists(self, table, database='default', partition={}):\n        if not partition:\n            stdout = run_hive_cmd('use {0}; show tables like \"{1}\";'.format(database, table))\n\n            return stdout and table in stdout\n        else:\n            stdout = run_hive_cmd(\"\"\"use %s; show partitions %s partition\n                                (%s)\"\"\" % (database, table, self.partition_spec(partition)))\n\n            if stdout:\n                return True\n            else:\n                return False\n\n    def table_schema(self, table, database='default'):\n        describe = run_hive_cmd(\"use {0}; describe {1}\".format(database, table))\n        if not describe or \"does not exist\" in describe:\n            return None\n        return [tuple([x.strip() for x in line.strip().split(\"\\t\")]) for line in describe.strip().split(\"\\n\")]\n\n    def partition_spec(self, partition):\n        \"\"\" Turns a dict into the a Hive partition specification string \"\"\"\n        return ','.join([\"{0}='{1}'\".format(k, v) for (k, v) in\n                         sorted(partition.items(), key=operator.itemgetter(0))])\n\n\nclass ApacheHiveCommandClient(HiveCommandClient):\n    \"\"\"\n    A subclass for the HiveCommandClient to (in some cases) ignore the return code from\n    the hive command so that we can just parse the output.\n    \"\"\"\n    def table_schema(self, table, database='default'):\n        describe = run_hive_cmd(\"use {0}; describe {1}\".format(database, table), False)\n        if not describe or \"Table not found\" in describe:\n            return None\n        return [tuple([x.strip() for x in line.strip().split(\"\\t\")]) for line in describe.strip().split(\"\\n\")]\n\n\nclass MetastoreClient(HiveClient):\n    def table_location(self, table, database='default', partition={}):\n        with HiveThriftContext() as client:\n            if partition:\n                partition_str = self.partition_spec(partition)\n                thrift_table = client.get_partition_by_name(database, table, partition_str)\n            else:\n                thrift_table = client.get_table(database, table)\n            return thrift_table.sd.location\n\n    def table_exists(self, table, database='default', partition={}):\n        with HiveThriftContext() as client:\n            if not partition:\n                return table in client.get_all_tables(database)\n            else:\n                return partition in self._existing_partitions(table, database, client)\n\n    def _existing_partitions(self, table, database, client):\n        def _parse_partition_string(partition_string):\n            partition_def = {}\n            for part in partition_string.split(\"/\"):\n                name, value = part.split(\"=\")\n                partition_def[name] = value\n            return partition_def\n\n        # -1 is max_parts, the # of partition names to return (-1 = unlimited)\n        partition_strings = client.get_partition_names(database, table, -1)\n        return [_parse_partition_string(existing_partition) for existing_partition in partition_strings]\n\n    def table_schema(self, table, database='default'):\n        with HiveThriftContext() as client:\n            return [(field_schema.name, field_schema.type) for field_schema in client.get_schema(database, table)]\n\n    def partition_spec(self, partition):\n        return \"/\".join(\"%s=%s\" % (k, v) for (k, v) in sorted(partition.items(), key=operator.itemgetter(0)))\n\n\nclass HiveThriftContext(object):\n    \"\"\" Context manager for hive metastore client \"\"\"\n    def __enter__(self):\n        try:\n            from thrift import Thrift\n            from thrift.transport import TSocket\n            from thrift.transport import TTransport\n            from thrift.protocol import TBinaryProtocol\n            # Note that this will only work with a CDH release.\n            # This uses the thrift bindings generated by the ThriftHiveMetastore service in Beeswax.\n            # If using the Apache release of Hive this import will fail.\n            from hive_metastore import ThriftHiveMetastore\n            config = luigi.configuration.get_config()\n            host = config.get('hive', 'metastore_host')\n            port = config.getint('hive', 'metastore_port')\n            transport = TSocket.TSocket(host, port)\n            transport = TTransport.TBufferedTransport(transport)\n            protocol = TBinaryProtocol.TBinaryProtocol(transport)\n            transport.open()\n            self.transport = transport\n            return ThriftHiveMetastore.Client(protocol)\n        except ImportError, e:\n            raise Exception('Could not import Hive thrift library:' + str(e))\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        self.transport.close()\n\nif get_hive_syntax() == \"apache\":\n    default_client = ApacheHiveCommandClient()\nelse:\n    default_client = HiveCommandClient()\nclient = default_client\n\n\nclass HiveQueryTask(luigi.hadoop.BaseHadoopJobTask):\n    \"\"\" Task to run a hive query \"\"\"\n    # by default, we let hive figure these out.\n    n_reduce_tasks = None\n    bytes_per_reducer = None\n    reducers_max = None\n\n    @abc.abstractmethod\n    def query(self):\n        \"\"\" Text of query to run in hive \"\"\"\n        raise RuntimeError(\"Must implement query!\")\n\n    def hiverc(self):\n        \"\"\" Location of an rc file to run before the query \n            if hiverc-location key is specified in client.cfg, will default to the value there\n            otherwise returns None\n        \"\"\"\n        return luigi.configuration.get_config().get('hive', 'hiverc-location', default=None)\n\n    def hiveconfs(self):\n        \"\"\"\n        Returns an dict of key=value settings to be passed along\n        to the hive command line via --hiveconf. By default, sets\n        mapred.job.name to task_id and if not None, sets:\n        * mapred.reduce.tasks (n_reduce_tasks)\n        * mapred.fairscheduler.pool (pool) or mapred.job.queue.name (pool)\n        * hive.exec.reducers.bytes.per.reducer (bytes_per_reducer)\n        * hive.exec.reducers.max (reducers_max)\n        \"\"\"\n        jcs = {}\n        jcs['mapred.job.name'] = self.task_id\n        if self.n_reduce_tasks is not None:\n            jcs['mapred.reduce.tasks'] = self.n_reduce_tasks\n        if self.pool is not None:\n            # Supporting two schedulers: fair (default) and capacity using the same option\n            scheduler_type = luigi.configuration.get_config().get('hadoop', 'scheduler', 'fair')\n            if scheduler_type == 'fair':\n                jcs['mapred.fairscheduler.pool'] = self.pool\n            elif scheduler_type == 'capacity':\n                jcs['mapred.job.queue.name'] = self.pool\n        if self.bytes_per_reducer is not None:\n            jcs['hive.exec.reducers.bytes.per.reducer'] = self.bytes_per_reducer\n        if self.reducers_max is not None:\n            jcs['hive.exec.reducers.max'] = self.reducers_max\n        return jcs\n\n    def job_runner(self):\n        return HiveQueryRunner()\n\n\nclass HiveQueryRunner(luigi.hadoop.JobRunner):\n    \"\"\" Runs a HiveQueryTask by shelling out to hive \"\"\"\n\n    def prepare_outputs(self, job):\n        \"\"\" Called before job is started\n\n        If output is a `FileSystemTarget`, create parent directories so the hive command won't fail\n        \"\"\"\n        outputs = flatten(job.output())\n        for o in outputs:\n            if isinstance(o, FileSystemTarget):\n                parent_dir = os.path.dirname(o.path)\n                if parent_dir and not o.fs.exists(parent_dir):\n                    logger.info(\"Creating parent directory %r\", parent_dir)\n                    try:\n                        # there is a possible race condition\n                        # which needs to be handled here\n                        o.fs.mkdir(parent_dir)\n                    except FileAlreadyExists:\n                        pass\n\n    def run_job(self, job):\n        self.prepare_outputs(job)\n        with tempfile.NamedTemporaryFile() as f:\n            f.write(job.query())\n            f.flush()\n            arglist = [load_hive_cmd(), '-f', f.name]\n            if job.hiverc():\n                arglist += ['-i', job.hiverc()]\n            if job.hiveconfs():\n                for k, v in job.hiveconfs().iteritems():\n                    arglist += ['--hiveconf', '{0}={1}'.format(k, v)]\n\n            logger.info(arglist)\n            return luigi.hadoop.run_and_track_hadoop_job(arglist)\n\n\nclass HiveTableTarget(luigi.Target):\n    \"\"\" exists returns true if the table exists \"\"\"\n\n    def __init__(self, table, database='default', client=default_client):\n        self.database = database\n        self.table = table\n        self.hive_cmd = load_hive_cmd()\n        self.client = client\n\n    def exists(self):\n        logger.debug(\"Checking Hive table '%s.%s' exists\", self.database, self.table)\n        return self.client.table_exists(self.table, self.database)\n\n    @property\n    def path(self):\n        \"\"\"Returns the path to this table in HDFS\"\"\"\n        location = self.client.table_location(self.table, self.database)\n        if not location:\n            raise Exception(\"Couldn't find location for table: {0}\".format(str(self)))\n        return location\n\n    def open(self, mode):\n        return NotImplementedError(\"open() is not supported for HiveTableTarget\")\n\n\nclass HivePartitionTarget(luigi.Target):\n    \"\"\" exists returns true if the table's partition exists \"\"\"\n\n    def __init__(self, table, partition, database='default', fail_missing_table=True, client=default_client):\n        self.database = database\n        self.table = table\n        self.partition = partition\n        self.client = client\n\n        self.fail_missing_table = fail_missing_table\n\n    def exists(self):\n        try:\n            logger.debug(\"Checking Hive table '{d}.{t}' for partition {p}\".format(d=self.database, t=self.table, p=str(self.partition)))\n            return self.client.table_exists(self.table, self.database, self.partition)\n        except HiveCommandError, e:\n            if self.fail_missing_table:\n                raise\n            else:\n                if self.client.table_exists(self.table, self.database):\n                    # a real error occurred\n                    raise\n                else:\n                    # oh the table just doesn't exist\n                    return False\n\n    @property\n    def path(self):\n        \"\"\"Returns the path for this HiveTablePartitionTarget's data\"\"\"\n        location = self.client.table_location(self.table, self.database, self.partition)\n        if not location:\n            raise Exception(\"Couldn't find location for table: {0}\".format(str(self)))\n        return location\n\n    def open(self, mode):\n        return NotImplementedError(\"open() is not supported for HivePartitionTarget\")\n\n\nclass ExternalHiveTask(luigi.ExternalTask):\n    \"\"\" External task that depends on a Hive table/partition \"\"\"\n\n    database = luigi.Parameter(default='default')\n    table = luigi.Parameter()\n    # since this is an external task and will never be initialized from the CLI, partition can be any python object, in this case a dictionary\n    partition = luigi.Parameter(default=None, description='Python dictionary specifying the target partition e.g. {\"date\": \"2013-01-25\"}')\n\n    def output(self):\n        if self.partition is not None:\n            assert self.partition, \"partition required\"\n            return HivePartitionTarget(table=self.table,\n                                       partition=self.partition,\n                                       database=self.database)\n        else:\n            return HiveTableTarget(self.table, self.database)\n",
      "file_patch": "@@ -114,9 +114,9 @@ class HiveCommandClient(HiveClient):\n \n     def table_exists(self, table, database='default', partition={}):\n         if not partition:\n-            stdout = run_hive_cmd('use {0}; describe {1}'.format(database, table))\n+            stdout = run_hive_cmd('use {0}; show tables like \"{1}\";'.format(database, table))\n \n-            return not \"does not exist\" in stdout\n+            return stdout and table in stdout\n         else:\n             stdout = run_hive_cmd(\"\"\"use %s; show partitions %s partition\n                                 (%s)\"\"\" % (database, table, self.partition_spec(partition)))\n@@ -143,28 +143,6 @@ class ApacheHiveCommandClient(HiveCommandClient):\n     A subclass for the HiveCommandClient to (in some cases) ignore the return code from\n     the hive command so that we can just parse the output.\n     \"\"\"\n-    def table_exists(self, table, database='default', partition={}):\n-        if not partition:\n-            # Hive 0.11 returns 17 as the exit status if the table does not exist.\n-            # The actual message is: [Error 10001]: Table not found tablename\n-            # stdout is empty and an error message is returned on stderr.\n-            # This is why we can't check the return code on this command and\n-            # assume if stdout is empty that the table doesn't exist.\n-            stdout = run_hive_cmd('use {0}; describe {1}'.format(database, table), False)\n-            if stdout:\n-                return not \"Table not found\" in stdout\n-            else:\n-                # Hive returned a non-zero exit status and printed its output to stderr not stdout\n-                return False\n-        else:\n-            stdout = run_hive_cmd(\"\"\"use %s; show partitions %s partition\n-                                (%s)\"\"\" % (database, table, self.partition_spec(partition)), False)\n-\n-            if stdout:\n-                return True\n-            else:\n-                return False\n-\n     def table_schema(self, table, database='default'):\n         describe = run_hive_cmd(\"use {0}; describe {1}\".format(database, table), False)\n         if not describe or \"Table not found\" in describe:\n",
      "files_name_in_blame_commit": [
        "hive.py",
        "hive_test.py"
      ]
    }
  },
  "commits_modify_file_before_fix": {
    "size": 50
  },
  "recursive_blame_commits": {}
}