{
  "id": "55",
  "blame_commit": {
    "commit": {
      "commit_id": "650c2c8cf9d711d35ab0ca7d1653ef53cbedaab3",
      "commit_message": "Add basic support for TF optimizers",
      "commit_author": "Francois Chollet",
      "commit_date": "2016-11-03 11:38:00",
      "commit_parent": "a9b6bef0624c67d6df1618ca63d8e8141b0df4d0"
    },
    "function": {
      "function_name": "get_updates",
      "function_code_before": "def get_updates(self, params, constraints, loss):\n    raise NotImplementedError",
      "function_code_after": "def get_updates(self, params, constraints, loss):\n    if constraints:\n        raise ValueError('TF optimizers do not support weights constraints. Either remove all weights constraints in your model, or use a Keras optimizer.')\n    grads = self.optimizer.compute_gradients(loss, params)\n    self.updates.append(K.update_add(self.iterations, 1))\n    opt_update = self.optimizer.apply_gradients(grads, global_step=self.iterations)\n    self.updates.append(opt_update)\n    return self.updates",
      "function_before_start_line": 64,
      "function_before_end_line": 65,
      "function_after_start_line": 573,
      "function_after_end_line": 584,
      "function_before_token_count": 13,
      "function_after_token_count": 79,
      "functions_name_modified_file": [
        "__init__",
        "set_weights",
        "from_config",
        "get_gradients",
        "weights",
        "optimizer_from_config",
        "get_weights",
        "get",
        "get_updates",
        "clip_norm",
        "get_config"
      ],
      "functions_name_all_files": [
        "__init__",
        "set_weights",
        "from_config",
        "get_gradients",
        "weights",
        "optimizer_from_config",
        "get_weights",
        "get",
        "get_updates",
        "clip_norm",
        "get_config"
      ],
      "functions_name_co_evolved_modified_file": [
        "__init__",
        "get_state",
        "from_config",
        "weights",
        "optimizer_from_config",
        "set_state",
        "get",
        "get_config"
      ],
      "functions_name_co_evolved_all_files": [
        "__init__",
        "get_state",
        "from_config",
        "weights",
        "optimizer_from_config",
        "set_state",
        "get",
        "get_config"
      ]
    },
    "file": {
      "file_name": "optimizers.py",
      "file_nloc": 494,
      "file_complexity": 94,
      "file_token_count": 4112,
      "file_before": "from __future__ import absolute_import\nfrom . import backend as K\nfrom .utils.generic_utils import get_from_module\nfrom six.moves import zip\n\n\ndef clip_norm(g, c, n):\n    if c > 0:\n        g = K.switch(n >= c, g * c / n, g)\n    return g\n\n\ndef optimizer_from_config(config, custom_objects={}):\n    all_classes = {\n        'sgd': SGD,\n        'rmsprop': RMSprop,\n        'adagrad': Adagrad,\n        'adadelta': Adadelta,\n        'adam': Adam,\n        'adamax': Adamax,\n        'nadam': Nadam,\n    }\n    class_name = config['class_name']\n    if class_name in custom_objects:\n        cls = custom_objects[class_name]\n    else:\n        if class_name.lower() not in all_classes:\n            raise ValueError('Optimizer class not found:', class_name)\n        cls = all_classes[class_name.lower()]\n    return cls.from_config(config['config'])\n\n\nclass Optimizer(object):\n    '''Abstract optimizer base class.\n\n    Note: this is the parent class of all optimizers, not an actual optimizer\n    that can be used for training models.\n\n    All Keras optimizers support the following keyword arguments:\n\n        clipnorm: float >= 0. Gradients will be clipped\n            when their L2 norm exceeds this value.\n        clipvalue: float >= 0. Gradients will be clipped\n            when their absolute value exceeds this value.\n    '''\n    def __init__(self, **kwargs):\n        allowed_kwargs = {'clipnorm', 'clipvalue'}\n        for k in kwargs:\n            if k not in allowed_kwargs:\n                raise Exception('Unexpected keyword argument '\n                                'passed to optimizer: ' + str(k))\n        self.__dict__.update(kwargs)\n        self.updates = []\n        self.weights = []\n\n    def get_state(self):\n        return [K.get_value(u[0]) for u in self.updates]\n\n    def set_state(self, value_list):\n        assert len(self.updates) == len(value_list)\n        for u, v in zip(self.updates, value_list):\n            K.set_value(u[0], v)\n\n    def get_updates(self, params, constraints, loss):\n        raise NotImplementedError\n\n    def get_gradients(self, loss, params):\n        grads = K.gradients(loss, params)\n        if hasattr(self, 'clipnorm') and self.clipnorm > 0:\n            norm = K.sqrt(sum([K.sum(K.square(g)) for g in grads]))\n            grads = [clip_norm(g, self.clipnorm, norm) for g in grads]\n        if hasattr(self, 'clipvalue') and self.clipvalue > 0:\n            grads = [K.clip(g, -self.clipvalue, self.clipvalue) for g in grads]\n        return grads\n\n    def set_weights(self, weights):\n        '''Sets the weights of the optimizer, from Numpy arrays.\n\n        Should only be called after computing the gradients\n        (otherwise the optimizer has no weights).\n\n        # Arguments\n            weights: a list of Numpy arrays. The number\n                of arrays and their shape must match\n                number of the dimensions of the weights\n                of the optimizer (i.e. it should match the\n                output of `get_weights`).\n        '''\n        params = self.weights\n        weight_value_tuples = []\n        param_values = K.batch_get_value(params)\n        for pv, p, w in zip(param_values, params, weights):\n            if pv.shape != w.shape:\n                raise Exception('Optimizer weight shape ' +\n                                str(pv.shape) +\n                                ' not compatible with '\n                                'provided weight shape ' + str(w.shape))\n            weight_value_tuples.append((p, w))\n        K.batch_set_value(weight_value_tuples)\n\n    def get_weights(self):\n        '''Returns the current weights of the optimizer,\n        as a list of numpy arrays.\n        '''\n        return K.batch_get_value(self.weights)\n\n    def get_config(self):\n        config = {}\n        if hasattr(self, 'clipnorm'):\n            config['clipnorm'] = self.clipnorm\n        if hasattr(self, 'clipvalue'):\n            config['clipvalue'] = self.clipvalue\n        return config\n\n    @classmethod\n    def from_config(cls, config):\n        return cls(**config)\n\n\nclass SGD(Optimizer):\n    '''Stochastic gradient descent, with support for momentum,\n    learning rate decay, and Nesterov momentum.\n\n    # Arguments\n        lr: float >= 0. Learning rate.\n        momentum: float >= 0. Parameter updates momentum.\n        decay: float >= 0. Learning rate decay over each update.\n        nesterov: boolean. Whether to apply Nesterov momentum.\n    '''\n    def __init__(self, lr=0.01, momentum=0., decay=0.,\n                 nesterov=False, **kwargs):\n        super(SGD, self).__init__(**kwargs)\n        self.__dict__.update(locals())\n        self.iterations = K.variable(0.)\n        self.lr = K.variable(lr)\n        self.momentum = K.variable(momentum)\n        self.decay = K.variable(decay)\n        self.inital_decay = decay\n\n    def get_updates(self, params, constraints, loss):\n        grads = self.get_gradients(loss, params)\n        self.updates = []\n\n        lr = self.lr\n        if self.inital_decay > 0:\n            lr *= (1. / (1. + self.decay * self.iterations))\n            self.updates .append(K.update_add(self.iterations, 1))\n\n        # momentum\n        shapes = [K.get_variable_shape(p) for p in params]\n        moments = [K.zeros(shape) for shape in shapes]\n        self.weights = [self.iterations] + moments\n        for p, g, m in zip(params, grads, moments):\n            v = self.momentum * m - lr * g  # velocity\n            self.updates.append(K.update(m, v))\n\n            if self.nesterov:\n                new_p = p + self.momentum * v - lr * g\n            else:\n                new_p = p + v\n\n            # apply constraints\n            if p in constraints:\n                c = constraints[p]\n                new_p = c(new_p)\n\n            self.updates.append(K.update(p, new_p))\n        return self.updates\n\n    def get_config(self):\n        config = {'lr': float(K.get_value(self.lr)),\n                  'momentum': float(K.get_value(self.momentum)),\n                  'decay': float(K.get_value(self.decay)),\n                  'nesterov': self.nesterov}\n        base_config = super(SGD, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\n\nclass RMSprop(Optimizer):\n    '''RMSProp optimizer.\n\n    It is recommended to leave the parameters of this optimizer\n    at their default values\n    (except the learning rate, which can be freely tuned).\n\n    This optimizer is usually a good choice for recurrent\n    neural networks.\n\n    # Arguments\n        lr: float >= 0. Learning rate.\n        rho: float >= 0.\n        epsilon: float >= 0. Fuzz factor.\n        decay: float >= 0. Learning rate decay over each update.\n    '''\n    def __init__(self, lr=0.001, rho=0.9, epsilon=1e-8, decay=0.,\n                 **kwargs):\n        super(RMSprop, self).__init__(**kwargs)\n        self.__dict__.update(locals())\n        self.lr = K.variable(lr)\n        self.rho = K.variable(rho)\n        self.decay = K.variable(decay)\n        self.inital_decay = decay\n        self.iterations = K.variable(0.)\n\n    def get_updates(self, params, constraints, loss):\n        grads = self.get_gradients(loss, params)\n        shapes = [K.get_variable_shape(p) for p in params]\n        accumulators = [K.zeros(shape) for shape in shapes]\n        self.weights = accumulators\n        self.updates = []\n\n        lr = self.lr\n        if self.inital_decay > 0:\n            lr *= (1. / (1. + self.decay * self.iterations))\n            self.updates.append(K.update_add(self.iterations, 1))\n\n        for p, g, a in zip(params, grads, accumulators):\n            # update accumulator\n            new_a = self.rho * a + (1. - self.rho) * K.square(g)\n            self.updates.append(K.update(a, new_a))\n            new_p = p - lr * g / (K.sqrt(new_a) + self.epsilon)\n\n            # apply constraints\n            if p in constraints:\n                c = constraints[p]\n                new_p = c(new_p)\n            self.updates.append(K.update(p, new_p))\n        return self.updates\n\n    def get_config(self):\n        config = {'lr': float(K.get_value(self.lr)),\n                  'rho': float(K.get_value(self.rho)),\n                  'decay': float(K.get_value(self.decay)),\n                  'epsilon': self.epsilon}\n        base_config = super(RMSprop, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\n\nclass Adagrad(Optimizer):\n    '''Adagrad optimizer.\n\n    It is recommended to leave the parameters of this optimizer\n    at their default values.\n\n    # Arguments\n        lr: float >= 0. Learning rate.\n        epsilon: float >= 0.\n\n    # References\n        - [Adaptive Subgradient Methods for Online Learning and Stochastic Optimization](http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf)\n    '''\n    def __init__(self, lr=0.01, epsilon=1e-8, decay=0., **kwargs):\n        super(Adagrad, self).__init__(**kwargs)\n        self.__dict__.update(locals())\n        self.lr = K.variable(lr)\n        self.decay = K.variable(decay)\n        self.inital_decay = decay\n        self.iterations = K.variable(0.)\n\n    def get_updates(self, params, constraints, loss):\n        grads = self.get_gradients(loss, params)\n        shapes = [K.get_variable_shape(p) for p in params]\n        accumulators = [K.zeros(shape) for shape in shapes]\n        self.weights = accumulators\n        self.updates = []\n\n        lr = self.lr\n        if self.inital_decay > 0:\n            lr *= (1. / (1. + self.decay * self.iterations))\n            self.updates.append(K.update_add(self.iterations, 1))\n\n        for p, g, a in zip(params, grads, accumulators):\n            new_a = a + K.square(g)  # update accumulator\n            self.updates.append(K.update(a, new_a))\n            new_p = p - lr * g / (K.sqrt(new_a) + self.epsilon)\n            # apply constraints\n            if p in constraints:\n                c = constraints[p]\n                new_p = c(new_p)\n            self.updates.append(K.update(p, new_p))\n        return self.updates\n\n    def get_config(self):\n        config = {'lr': float(K.get_value(self.lr)),\n                  'decay': float(K.get_value(self.decay)),\n                  'epsilon': self.epsilon}\n        base_config = super(Adagrad, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\n\nclass Adadelta(Optimizer):\n    '''Adadelta optimizer.\n\n    It is recommended to leave the parameters of this optimizer\n    at their default values.\n\n    # Arguments\n        lr: float >= 0. Learning rate.\n            It is recommended to leave it at the default value.\n        rho: float >= 0.\n        epsilon: float >= 0. Fuzz factor.\n\n    # References\n        - [Adadelta - an adaptive learning rate method](http://arxiv.org/abs/1212.5701)\n    '''\n    def __init__(self, lr=1.0, rho=0.95, epsilon=1e-8, decay=0.,\n                 **kwargs):\n        super(Adadelta, self).__init__(**kwargs)\n        self.__dict__.update(locals())\n        self.lr = K.variable(lr)\n        self.decay = K.variable(decay)\n        self.inital_decay = decay\n        self.iterations = K.variable(0.)\n\n    def get_updates(self, params, constraints, loss):\n        grads = self.get_gradients(loss, params)\n        shapes = [K.get_variable_shape(p) for p in params]\n        accumulators = [K.zeros(shape) for shape in shapes]\n        delta_accumulators = [K.zeros(shape) for shape in shapes]\n        self.weights = accumulators + delta_accumulators\n        self.updates = []\n\n        lr = self.lr\n        if self.inital_decay > 0:\n            lr *= (1. / (1. + self.decay * self.iterations))\n            self.updates.append(K.update_add(self.iterations, 1))\n\n        for p, g, a, d_a in zip(params, grads, accumulators, delta_accumulators):\n            # update accumulator\n            new_a = self.rho * a + (1. - self.rho) * K.square(g)\n            self.updates.append(K.update(a, new_a))\n\n            # use the new accumulator and the *old* delta_accumulator\n            update = g * K.sqrt(d_a + self.epsilon) / K.sqrt(new_a + self.epsilon)\n\n            new_p = p - lr * update\n            # apply constraints\n            if p in constraints:\n                c = constraints[p]\n                new_p = c(new_p)\n            self.updates.append(K.update(p, new_p))\n\n            # update delta_accumulator\n            new_d_a = self.rho * d_a + (1 - self.rho) * K.square(update)\n            self.updates.append(K.update(d_a, new_d_a))\n        return self.updates\n\n    def get_config(self):\n        config = {'lr': float(K.get_value(self.lr)),\n                  'rho': self.rho,\n                  'decay': float(K.get_value(self.decay)),\n                  'epsilon': self.epsilon}\n        base_config = super(Adadelta, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\n\nclass Adam(Optimizer):\n    '''Adam optimizer.\n\n    Default parameters follow those provided in the original paper.\n\n    # Arguments\n        lr: float >= 0. Learning rate.\n        beta_1/beta_2: floats, 0 < beta < 1. Generally close to 1.\n        epsilon: float >= 0. Fuzz factor.\n\n    # References\n        - [Adam - A Method for Stochastic Optimization](http://arxiv.org/abs/1412.6980v8)\n    '''\n    def __init__(self, lr=0.001, beta_1=0.9, beta_2=0.999,\n                 epsilon=1e-8, decay=0., **kwargs):\n        super(Adam, self).__init__(**kwargs)\n        self.__dict__.update(locals())\n        self.iterations = K.variable(0)\n        self.lr = K.variable(lr)\n        self.beta_1 = K.variable(beta_1)\n        self.beta_2 = K.variable(beta_2)\n        self.decay = K.variable(decay)\n        self.inital_decay = decay\n\n    def get_updates(self, params, constraints, loss):\n        grads = self.get_gradients(loss, params)\n        self.updates = [K.update_add(self.iterations, 1)]\n\n        lr = self.lr\n        if self.inital_decay > 0:\n            lr *= (1. / (1. + self.decay * self.iterations))\n\n        t = self.iterations + 1\n        lr_t = lr * K.sqrt(1. - K.pow(self.beta_2, t)) / (1. - K.pow(self.beta_1, t))\n\n        shapes = [K.get_variable_shape(p) for p in params]\n        ms = [K.zeros(shape) for shape in shapes]\n        vs = [K.zeros(shape) for shape in shapes]\n        self.weights = [self.iterations] + ms + vs\n\n        for p, g, m, v in zip(params, grads, ms, vs):\n            m_t = (self.beta_1 * m) + (1. - self.beta_1) * g\n            v_t = (self.beta_2 * v) + (1. - self.beta_2) * K.square(g)\n            p_t = p - lr_t * m_t / (K.sqrt(v_t) + self.epsilon)\n\n            self.updates.append(K.update(m, m_t))\n            self.updates.append(K.update(v, v_t))\n\n            new_p = p_t\n            # apply constraints\n            if p in constraints:\n                c = constraints[p]\n                new_p = c(new_p)\n            self.updates.append(K.update(p, new_p))\n        return self.updates\n\n    def get_config(self):\n        config = {'lr': float(K.get_value(self.lr)),\n                  'beta_1': float(K.get_value(self.beta_1)),\n                  'beta_2': float(K.get_value(self.beta_2)),\n                  'decay': float(K.get_value(self.decay)),\n                  'epsilon': self.epsilon}\n        base_config = super(Adam, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\n\nclass Adamax(Optimizer):\n    '''Adamax optimizer from Adam paper's Section 7. It is a variant\n     of Adam based on the infinity norm.\n\n    Default parameters follow those provided in the paper.\n\n    # Arguments\n        lr: float >= 0. Learning rate.\n        beta_1/beta_2: floats, 0 < beta < 1. Generally close to 1.\n        epsilon: float >= 0. Fuzz factor.\n\n    # References\n        - [Adam - A Method for Stochastic Optimization](http://arxiv.org/abs/1412.6980v8)\n    '''\n    def __init__(self, lr=0.002, beta_1=0.9, beta_2=0.999,\n                 epsilon=1e-8, decay=0., **kwargs):\n        super(Adamax, self).__init__(**kwargs)\n        self.__dict__.update(locals())\n        self.iterations = K.variable(0.)\n        self.lr = K.variable(lr)\n        self.beta_1 = K.variable(beta_1)\n        self.beta_2 = K.variable(beta_2)\n        self.decay = K.variable(decay)\n        self.inital_decay = decay\n\n    def get_updates(self, params, constraints, loss):\n        grads = self.get_gradients(loss, params)\n        self.updates = [K.update_add(self.iterations, 1)]\n\n        lr = self.lr\n        if self.inital_decay > 0:\n            lr *= (1. / (1. + self.decay * self.iterations))\n\n        t = self.iterations + 1\n        lr_t = lr / (1. - K.pow(self.beta_1, t))\n\n        shapes = [K.get_variable_shape(p) for p in params]\n        # zero init of 1st moment\n        ms = [K.zeros(shape) for shape in shapes]\n        # zero init of exponentially weighted infinity norm\n        us = [K.zeros(shape) for shape in shapes]\n        self.weights = [self.iterations] + ms + us\n\n        for p, g, m, u in zip(params, grads, ms, us):\n\n            m_t = (self.beta_1 * m) + (1. - self.beta_1) * g\n            u_t = K.maximum(self.beta_2 * u, K.abs(g))\n            p_t = p - lr_t * m_t / (u_t + self.epsilon)\n\n            self.updates.append(K.update(m, m_t))\n            self.updates.append(K.update(u, u_t))\n\n            new_p = p_t\n            # apply constraints\n            if p in constraints:\n                c = constraints[p]\n                new_p = c(new_p)\n            self.updates.append(K.update(p, new_p))\n        return self.updates\n\n    def get_config(self):\n        config = {'lr': float(K.get_value(self.lr)),\n                  'beta_1': float(K.get_value(self.beta_1)),\n                  'beta_2': float(K.get_value(self.beta_2)),\n                  'decay': float(K.get_value(self.decay)),\n                  'epsilon': self.epsilon}\n        base_config = super(Adamax, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\n\nclass Nadam(Optimizer):\n    '''\n    Nesterov Adam optimizer: Much like Adam is essentially RMSprop with momentum,\n    Nadam is Adam RMSprop with Nesterov momentum.\n\n    Default parameters follow those provided in the paper.\n    It is recommended to leave the parameters of this optimizer\n    at their default values.\n\n    # Arguments\n        lr: float >= 0. Learning rate.\n        beta_1/beta_2: floats, 0 < beta < 1. Generally close to 1.\n        epsilon: float >= 0. Fuzz factor.\n\n    # References\n        - [Nadam report](http://cs229.stanford.edu/proj2015/054_report.pdf)\n        - [On the importance of initialization and momentum in deep learning](http://www.cs.toronto.edu/~fritz/absps/momentum.pdf)\n    '''\n    def __init__(self, lr=0.002, beta_1=0.9, beta_2=0.999,\n                 epsilon=1e-8, schedule_decay=0.004, **kwargs):\n        super(Nadam, self).__init__(**kwargs)\n        self.__dict__.update(locals())\n        self.iterations = K.variable(0.)\n        self.m_schedule = K.variable(1.)\n        self.lr = K.variable(lr)\n        self.beta_1 = K.variable(beta_1)\n        self.beta_2 = K.variable(beta_2)\n        self.schedule_decay = schedule_decay\n\n    def get_updates(self, params, constraints, loss):\n        grads = self.get_gradients(loss, params)\n        self.updates = [K.update_add(self.iterations, 1)]\n\n        t = self.iterations + 1\n\n        # Due to the recommendations in [2], i.e. warming momentum schedule\n        momentum_cache_t = self.beta_1 * (1. - 0.5 * (K.pow(0.96, t * self.schedule_decay)))\n        momentum_cache_t_1 = self.beta_1 * (1. - 0.5 * (K.pow(0.96, (t + 1) * self.schedule_decay)))\n        m_schedule_new = self.m_schedule * momentum_cache_t\n        m_schedule_next = self.m_schedule * momentum_cache_t * momentum_cache_t_1\n        self.updates.append((self.m_schedule, m_schedule_new))\n\n        shapes = [K.get_variable_shape(p) for p in params]\n        ms = [K.zeros(shape) for shape in shapes]\n        vs = [K.zeros(shape) for shape in shapes]\n\n        self.weights = [self.iterations] + ms + vs\n\n        for p, g, m, v in zip(params, grads, ms, vs):\n            # the following equations given in [1]\n            g_prime = g / (1. - m_schedule_new)\n            m_t = self.beta_1 * m + (1. - self.beta_1) * g\n            m_t_prime = m_t / (1. - m_schedule_next)\n            v_t = self.beta_2 * v + (1. - self.beta_2) * K.square(g)\n            v_t_prime = v_t / (1. - K.pow(self.beta_2, t))\n            m_t_bar = (1. - momentum_cache_t) * g_prime + momentum_cache_t_1 * m_t_prime\n\n            self.updates.append(K.update(m, m_t))\n            self.updates.append(K.update(v, v_t))\n\n            p_t = p - self.lr * m_t_bar / (K.sqrt(v_t_prime) + self.epsilon)\n            new_p = p_t\n\n            # apply constraints\n            if p in constraints:\n                c = constraints[p]\n                new_p = c(new_p)\n            self.updates.append(K.update(p, new_p))\n        return self.updates\n\n    def get_config(self):\n        config = {'lr': float(K.get_value(self.lr)),\n                  'beta_1': float(K.get_value(self.beta_1)),\n                  'beta_2': float(K.get_value(self.beta_2)),\n                  'epsilon': self.epsilon,\n                  'schedule_decay': self.schedule_decay}\n        base_config = super(Nadam, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\n\n# aliases\nsgd = SGD\nrmsprop = RMSprop\nadagrad = Adagrad\nadadelta = Adadelta\nadam = Adam\nadamax = Adamax\nnadam = Nadam\n\n\ndef get(identifier, kwargs=None):\n    return get_from_module(identifier, globals(), 'optimizer',\n                           instantiate=True, kwargs=kwargs)\n",
      "file_after": "from __future__ import absolute_import\nfrom . import backend as K\nfrom .utils.generic_utils import get_from_module\nfrom six.moves import zip\n\n\ndef clip_norm(g, c, n):\n    if c > 0:\n        g = K.switch(n >= c, g * c / n, g)\n    return g\n\n\ndef optimizer_from_config(config, custom_objects={}):\n    all_classes = {\n        'sgd': SGD,\n        'rmsprop': RMSprop,\n        'adagrad': Adagrad,\n        'adadelta': Adadelta,\n        'adam': Adam,\n        'adamax': Adamax,\n        'nadam': Nadam,\n        'tfoptimizer': TFOptimizer,\n    }\n    class_name = config['class_name']\n    if class_name in custom_objects:\n        cls = custom_objects[class_name]\n    else:\n        if class_name.lower() not in all_classes:\n            raise ValueError('Optimizer class not found:', class_name)\n        cls = all_classes[class_name.lower()]\n    return cls.from_config(config['config'])\n\n\nclass Optimizer(object):\n    '''Abstract optimizer base class.\n\n    Note: this is the parent class of all optimizers, not an actual optimizer\n    that can be used for training models.\n\n    All Keras optimizers support the following keyword arguments:\n\n        clipnorm: float >= 0. Gradients will be clipped\n            when their L2 norm exceeds this value.\n        clipvalue: float >= 0. Gradients will be clipped\n            when their absolute value exceeds this value.\n    '''\n    def __init__(self, **kwargs):\n        allowed_kwargs = {'clipnorm', 'clipvalue'}\n        for k in kwargs:\n            if k not in allowed_kwargs:\n                raise Exception('Unexpected keyword argument '\n                                'passed to optimizer: ' + str(k))\n        self.__dict__.update(kwargs)\n        self.updates = []\n        self.weights = []\n\n    def get_updates(self, params, constraints, loss):\n        raise NotImplementedError\n\n    def get_gradients(self, loss, params):\n        grads = K.gradients(loss, params)\n        if hasattr(self, 'clipnorm') and self.clipnorm > 0:\n            norm = K.sqrt(sum([K.sum(K.square(g)) for g in grads]))\n            grads = [clip_norm(g, self.clipnorm, norm) for g in grads]\n        if hasattr(self, 'clipvalue') and self.clipvalue > 0:\n            grads = [K.clip(g, -self.clipvalue, self.clipvalue) for g in grads]\n        return grads\n\n    def set_weights(self, weights):\n        '''Sets the weights of the optimizer, from Numpy arrays.\n\n        Should only be called after computing the gradients\n        (otherwise the optimizer has no weights).\n\n        # Arguments\n            weights: a list of Numpy arrays. The number\n                of arrays and their shape must match\n                number of the dimensions of the weights\n                of the optimizer (i.e. it should match the\n                output of `get_weights`).\n        '''\n        params = self.weights\n        weight_value_tuples = []\n        param_values = K.batch_get_value(params)\n        for pv, p, w in zip(param_values, params, weights):\n            if pv.shape != w.shape:\n                raise Exception('Optimizer weight shape ' +\n                                str(pv.shape) +\n                                ' not compatible with '\n                                'provided weight shape ' + str(w.shape))\n            weight_value_tuples.append((p, w))\n        K.batch_set_value(weight_value_tuples)\n\n    def get_weights(self):\n        '''Returns the current weights of the optimizer,\n        as a list of numpy arrays.\n        '''\n        return K.batch_get_value(self.weights)\n\n    def get_config(self):\n        config = {}\n        if hasattr(self, 'clipnorm'):\n            config['clipnorm'] = self.clipnorm\n        if hasattr(self, 'clipvalue'):\n            config['clipvalue'] = self.clipvalue\n        return config\n\n    @classmethod\n    def from_config(cls, config):\n        return cls(**config)\n\n\nclass SGD(Optimizer):\n    '''Stochastic gradient descent, with support for momentum,\n    learning rate decay, and Nesterov momentum.\n\n    # Arguments\n        lr: float >= 0. Learning rate.\n        momentum: float >= 0. Parameter updates momentum.\n        decay: float >= 0. Learning rate decay over each update.\n        nesterov: boolean. Whether to apply Nesterov momentum.\n    '''\n    def __init__(self, lr=0.01, momentum=0., decay=0.,\n                 nesterov=False, **kwargs):\n        super(SGD, self).__init__(**kwargs)\n        self.__dict__.update(locals())\n        self.iterations = K.variable(0.)\n        self.lr = K.variable(lr)\n        self.momentum = K.variable(momentum)\n        self.decay = K.variable(decay)\n        self.inital_decay = decay\n\n    def get_updates(self, params, constraints, loss):\n        grads = self.get_gradients(loss, params)\n        self.updates = []\n\n        lr = self.lr\n        if self.inital_decay > 0:\n            lr *= (1. / (1. + self.decay * self.iterations))\n            self.updates .append(K.update_add(self.iterations, 1))\n\n        # momentum\n        shapes = [K.get_variable_shape(p) for p in params]\n        moments = [K.zeros(shape) for shape in shapes]\n        self.weights = [self.iterations] + moments\n        for p, g, m in zip(params, grads, moments):\n            v = self.momentum * m - lr * g  # velocity\n            self.updates.append(K.update(m, v))\n\n            if self.nesterov:\n                new_p = p + self.momentum * v - lr * g\n            else:\n                new_p = p + v\n\n            # apply constraints\n            if p in constraints:\n                c = constraints[p]\n                new_p = c(new_p)\n\n            self.updates.append(K.update(p, new_p))\n        return self.updates\n\n    def get_config(self):\n        config = {'lr': float(K.get_value(self.lr)),\n                  'momentum': float(K.get_value(self.momentum)),\n                  'decay': float(K.get_value(self.decay)),\n                  'nesterov': self.nesterov}\n        base_config = super(SGD, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\n\nclass RMSprop(Optimizer):\n    '''RMSProp optimizer.\n\n    It is recommended to leave the parameters of this optimizer\n    at their default values\n    (except the learning rate, which can be freely tuned).\n\n    This optimizer is usually a good choice for recurrent\n    neural networks.\n\n    # Arguments\n        lr: float >= 0. Learning rate.\n        rho: float >= 0.\n        epsilon: float >= 0. Fuzz factor.\n        decay: float >= 0. Learning rate decay over each update.\n    '''\n    def __init__(self, lr=0.001, rho=0.9, epsilon=1e-8, decay=0.,\n                 **kwargs):\n        super(RMSprop, self).__init__(**kwargs)\n        self.__dict__.update(locals())\n        self.lr = K.variable(lr)\n        self.rho = K.variable(rho)\n        self.decay = K.variable(decay)\n        self.inital_decay = decay\n        self.iterations = K.variable(0.)\n\n    def get_updates(self, params, constraints, loss):\n        grads = self.get_gradients(loss, params)\n        shapes = [K.get_variable_shape(p) for p in params]\n        accumulators = [K.zeros(shape) for shape in shapes]\n        self.weights = accumulators\n        self.updates = []\n\n        lr = self.lr\n        if self.inital_decay > 0:\n            lr *= (1. / (1. + self.decay * self.iterations))\n            self.updates.append(K.update_add(self.iterations, 1))\n\n        for p, g, a in zip(params, grads, accumulators):\n            # update accumulator\n            new_a = self.rho * a + (1. - self.rho) * K.square(g)\n            self.updates.append(K.update(a, new_a))\n            new_p = p - lr * g / (K.sqrt(new_a) + self.epsilon)\n\n            # apply constraints\n            if p in constraints:\n                c = constraints[p]\n                new_p = c(new_p)\n            self.updates.append(K.update(p, new_p))\n        return self.updates\n\n    def get_config(self):\n        config = {'lr': float(K.get_value(self.lr)),\n                  'rho': float(K.get_value(self.rho)),\n                  'decay': float(K.get_value(self.decay)),\n                  'epsilon': self.epsilon}\n        base_config = super(RMSprop, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\n\nclass Adagrad(Optimizer):\n    '''Adagrad optimizer.\n\n    It is recommended to leave the parameters of this optimizer\n    at their default values.\n\n    # Arguments\n        lr: float >= 0. Learning rate.\n        epsilon: float >= 0.\n\n    # References\n        - [Adaptive Subgradient Methods for Online Learning and Stochastic Optimization](http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf)\n    '''\n    def __init__(self, lr=0.01, epsilon=1e-8, decay=0., **kwargs):\n        super(Adagrad, self).__init__(**kwargs)\n        self.__dict__.update(locals())\n        self.lr = K.variable(lr)\n        self.decay = K.variable(decay)\n        self.inital_decay = decay\n        self.iterations = K.variable(0.)\n\n    def get_updates(self, params, constraints, loss):\n        grads = self.get_gradients(loss, params)\n        shapes = [K.get_variable_shape(p) for p in params]\n        accumulators = [K.zeros(shape) for shape in shapes]\n        self.weights = accumulators\n        self.updates = []\n\n        lr = self.lr\n        if self.inital_decay > 0:\n            lr *= (1. / (1. + self.decay * self.iterations))\n            self.updates.append(K.update_add(self.iterations, 1))\n\n        for p, g, a in zip(params, grads, accumulators):\n            new_a = a + K.square(g)  # update accumulator\n            self.updates.append(K.update(a, new_a))\n            new_p = p - lr * g / (K.sqrt(new_a) + self.epsilon)\n            # apply constraints\n            if p in constraints:\n                c = constraints[p]\n                new_p = c(new_p)\n            self.updates.append(K.update(p, new_p))\n        return self.updates\n\n    def get_config(self):\n        config = {'lr': float(K.get_value(self.lr)),\n                  'decay': float(K.get_value(self.decay)),\n                  'epsilon': self.epsilon}\n        base_config = super(Adagrad, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\n\nclass Adadelta(Optimizer):\n    '''Adadelta optimizer.\n\n    It is recommended to leave the parameters of this optimizer\n    at their default values.\n\n    # Arguments\n        lr: float >= 0. Learning rate.\n            It is recommended to leave it at the default value.\n        rho: float >= 0.\n        epsilon: float >= 0. Fuzz factor.\n\n    # References\n        - [Adadelta - an adaptive learning rate method](http://arxiv.org/abs/1212.5701)\n    '''\n    def __init__(self, lr=1.0, rho=0.95, epsilon=1e-8, decay=0.,\n                 **kwargs):\n        super(Adadelta, self).__init__(**kwargs)\n        self.__dict__.update(locals())\n        self.lr = K.variable(lr)\n        self.decay = K.variable(decay)\n        self.inital_decay = decay\n        self.iterations = K.variable(0.)\n\n    def get_updates(self, params, constraints, loss):\n        grads = self.get_gradients(loss, params)\n        shapes = [K.get_variable_shape(p) for p in params]\n        accumulators = [K.zeros(shape) for shape in shapes]\n        delta_accumulators = [K.zeros(shape) for shape in shapes]\n        self.weights = accumulators + delta_accumulators\n        self.updates = []\n\n        lr = self.lr\n        if self.inital_decay > 0:\n            lr *= (1. / (1. + self.decay * self.iterations))\n            self.updates.append(K.update_add(self.iterations, 1))\n\n        for p, g, a, d_a in zip(params, grads, accumulators, delta_accumulators):\n            # update accumulator\n            new_a = self.rho * a + (1. - self.rho) * K.square(g)\n            self.updates.append(K.update(a, new_a))\n\n            # use the new accumulator and the *old* delta_accumulator\n            update = g * K.sqrt(d_a + self.epsilon) / K.sqrt(new_a + self.epsilon)\n\n            new_p = p - lr * update\n            # apply constraints\n            if p in constraints:\n                c = constraints[p]\n                new_p = c(new_p)\n            self.updates.append(K.update(p, new_p))\n\n            # update delta_accumulator\n            new_d_a = self.rho * d_a + (1 - self.rho) * K.square(update)\n            self.updates.append(K.update(d_a, new_d_a))\n        return self.updates\n\n    def get_config(self):\n        config = {'lr': float(K.get_value(self.lr)),\n                  'rho': self.rho,\n                  'decay': float(K.get_value(self.decay)),\n                  'epsilon': self.epsilon}\n        base_config = super(Adadelta, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\n\nclass Adam(Optimizer):\n    '''Adam optimizer.\n\n    Default parameters follow those provided in the original paper.\n\n    # Arguments\n        lr: float >= 0. Learning rate.\n        beta_1/beta_2: floats, 0 < beta < 1. Generally close to 1.\n        epsilon: float >= 0. Fuzz factor.\n\n    # References\n        - [Adam - A Method for Stochastic Optimization](http://arxiv.org/abs/1412.6980v8)\n    '''\n    def __init__(self, lr=0.001, beta_1=0.9, beta_2=0.999,\n                 epsilon=1e-8, decay=0., **kwargs):\n        super(Adam, self).__init__(**kwargs)\n        self.__dict__.update(locals())\n        self.iterations = K.variable(0)\n        self.lr = K.variable(lr)\n        self.beta_1 = K.variable(beta_1)\n        self.beta_2 = K.variable(beta_2)\n        self.decay = K.variable(decay)\n        self.inital_decay = decay\n\n    def get_updates(self, params, constraints, loss):\n        grads = self.get_gradients(loss, params)\n        self.updates = [K.update_add(self.iterations, 1)]\n\n        lr = self.lr\n        if self.inital_decay > 0:\n            lr *= (1. / (1. + self.decay * self.iterations))\n\n        t = self.iterations + 1\n        lr_t = lr * K.sqrt(1. - K.pow(self.beta_2, t)) / (1. - K.pow(self.beta_1, t))\n\n        shapes = [K.get_variable_shape(p) for p in params]\n        ms = [K.zeros(shape) for shape in shapes]\n        vs = [K.zeros(shape) for shape in shapes]\n        self.weights = [self.iterations] + ms + vs\n\n        for p, g, m, v in zip(params, grads, ms, vs):\n            m_t = (self.beta_1 * m) + (1. - self.beta_1) * g\n            v_t = (self.beta_2 * v) + (1. - self.beta_2) * K.square(g)\n            p_t = p - lr_t * m_t / (K.sqrt(v_t) + self.epsilon)\n\n            self.updates.append(K.update(m, m_t))\n            self.updates.append(K.update(v, v_t))\n\n            new_p = p_t\n            # apply constraints\n            if p in constraints:\n                c = constraints[p]\n                new_p = c(new_p)\n            self.updates.append(K.update(p, new_p))\n        return self.updates\n\n    def get_config(self):\n        config = {'lr': float(K.get_value(self.lr)),\n                  'beta_1': float(K.get_value(self.beta_1)),\n                  'beta_2': float(K.get_value(self.beta_2)),\n                  'decay': float(K.get_value(self.decay)),\n                  'epsilon': self.epsilon}\n        base_config = super(Adam, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\n\nclass Adamax(Optimizer):\n    '''Adamax optimizer from Adam paper's Section 7. It is a variant\n     of Adam based on the infinity norm.\n\n    Default parameters follow those provided in the paper.\n\n    # Arguments\n        lr: float >= 0. Learning rate.\n        beta_1/beta_2: floats, 0 < beta < 1. Generally close to 1.\n        epsilon: float >= 0. Fuzz factor.\n\n    # References\n        - [Adam - A Method for Stochastic Optimization](http://arxiv.org/abs/1412.6980v8)\n    '''\n    def __init__(self, lr=0.002, beta_1=0.9, beta_2=0.999,\n                 epsilon=1e-8, decay=0., **kwargs):\n        super(Adamax, self).__init__(**kwargs)\n        self.__dict__.update(locals())\n        self.iterations = K.variable(0.)\n        self.lr = K.variable(lr)\n        self.beta_1 = K.variable(beta_1)\n        self.beta_2 = K.variable(beta_2)\n        self.decay = K.variable(decay)\n        self.inital_decay = decay\n\n    def get_updates(self, params, constraints, loss):\n        grads = self.get_gradients(loss, params)\n        self.updates = [K.update_add(self.iterations, 1)]\n\n        lr = self.lr\n        if self.inital_decay > 0:\n            lr *= (1. / (1. + self.decay * self.iterations))\n\n        t = self.iterations + 1\n        lr_t = lr / (1. - K.pow(self.beta_1, t))\n\n        shapes = [K.get_variable_shape(p) for p in params]\n        # zero init of 1st moment\n        ms = [K.zeros(shape) for shape in shapes]\n        # zero init of exponentially weighted infinity norm\n        us = [K.zeros(shape) for shape in shapes]\n        self.weights = [self.iterations] + ms + us\n\n        for p, g, m, u in zip(params, grads, ms, us):\n\n            m_t = (self.beta_1 * m) + (1. - self.beta_1) * g\n            u_t = K.maximum(self.beta_2 * u, K.abs(g))\n            p_t = p - lr_t * m_t / (u_t + self.epsilon)\n\n            self.updates.append(K.update(m, m_t))\n            self.updates.append(K.update(u, u_t))\n\n            new_p = p_t\n            # apply constraints\n            if p in constraints:\n                c = constraints[p]\n                new_p = c(new_p)\n            self.updates.append(K.update(p, new_p))\n        return self.updates\n\n    def get_config(self):\n        config = {'lr': float(K.get_value(self.lr)),\n                  'beta_1': float(K.get_value(self.beta_1)),\n                  'beta_2': float(K.get_value(self.beta_2)),\n                  'decay': float(K.get_value(self.decay)),\n                  'epsilon': self.epsilon}\n        base_config = super(Adamax, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\n\nclass Nadam(Optimizer):\n    '''\n    Nesterov Adam optimizer: Much like Adam is essentially RMSprop with momentum,\n    Nadam is Adam RMSprop with Nesterov momentum.\n\n    Default parameters follow those provided in the paper.\n    It is recommended to leave the parameters of this optimizer\n    at their default values.\n\n    # Arguments\n        lr: float >= 0. Learning rate.\n        beta_1/beta_2: floats, 0 < beta < 1. Generally close to 1.\n        epsilon: float >= 0. Fuzz factor.\n\n    # References\n        - [Nadam report](http://cs229.stanford.edu/proj2015/054_report.pdf)\n        - [On the importance of initialization and momentum in deep learning](http://www.cs.toronto.edu/~fritz/absps/momentum.pdf)\n    '''\n    def __init__(self, lr=0.002, beta_1=0.9, beta_2=0.999,\n                 epsilon=1e-8, schedule_decay=0.004, **kwargs):\n        super(Nadam, self).__init__(**kwargs)\n        self.__dict__.update(locals())\n        self.iterations = K.variable(0.)\n        self.m_schedule = K.variable(1.)\n        self.lr = K.variable(lr)\n        self.beta_1 = K.variable(beta_1)\n        self.beta_2 = K.variable(beta_2)\n        self.schedule_decay = schedule_decay\n\n    def get_updates(self, params, constraints, loss):\n        grads = self.get_gradients(loss, params)\n        self.updates = [K.update_add(self.iterations, 1)]\n\n        t = self.iterations + 1\n\n        # Due to the recommendations in [2], i.e. warming momentum schedule\n        momentum_cache_t = self.beta_1 * (1. - 0.5 * (K.pow(0.96, t * self.schedule_decay)))\n        momentum_cache_t_1 = self.beta_1 * (1. - 0.5 * (K.pow(0.96, (t + 1) * self.schedule_decay)))\n        m_schedule_new = self.m_schedule * momentum_cache_t\n        m_schedule_next = self.m_schedule * momentum_cache_t * momentum_cache_t_1\n        self.updates.append((self.m_schedule, m_schedule_new))\n\n        shapes = [K.get_variable_shape(p) for p in params]\n        ms = [K.zeros(shape) for shape in shapes]\n        vs = [K.zeros(shape) for shape in shapes]\n\n        self.weights = [self.iterations] + ms + vs\n\n        for p, g, m, v in zip(params, grads, ms, vs):\n            # the following equations given in [1]\n            g_prime = g / (1. - m_schedule_new)\n            m_t = self.beta_1 * m + (1. - self.beta_1) * g\n            m_t_prime = m_t / (1. - m_schedule_next)\n            v_t = self.beta_2 * v + (1. - self.beta_2) * K.square(g)\n            v_t_prime = v_t / (1. - K.pow(self.beta_2, t))\n            m_t_bar = (1. - momentum_cache_t) * g_prime + momentum_cache_t_1 * m_t_prime\n\n            self.updates.append(K.update(m, m_t))\n            self.updates.append(K.update(v, v_t))\n\n            p_t = p - self.lr * m_t_bar / (K.sqrt(v_t_prime) + self.epsilon)\n            new_p = p_t\n\n            # apply constraints\n            if p in constraints:\n                c = constraints[p]\n                new_p = c(new_p)\n            self.updates.append(K.update(p, new_p))\n        return self.updates\n\n    def get_config(self):\n        config = {'lr': float(K.get_value(self.lr)),\n                  'beta_1': float(K.get_value(self.beta_1)),\n                  'beta_2': float(K.get_value(self.beta_2)),\n                  'epsilon': self.epsilon,\n                  'schedule_decay': self.schedule_decay}\n        base_config = super(Nadam, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\n\nclass TFOptimizer(Optimizer):\n\n    def __init__(self, optimizer):\n        self.optimizer = optimizer\n        self.iterations = K.variable(0.)\n        self.updates = []\n\n    def get_updates(self, params, constraints, loss):\n        if constraints:\n            raise ValueError('TF optimizers do not support '\n                             'weights constraints. Either remove '\n                             'all weights constraints in your model, '\n                             'or use a Keras optimizer.')\n        grads = self.optimizer.compute_gradients(loss, params)\n        self.updates.append(K.update_add(self.iterations, 1))\n        opt_update = self.optimizer.apply_gradients(\n            grads, global_step=self.iterations)\n        self.updates.append(opt_update)\n        return self.updates\n\n    @property\n    def weights(self):\n        raise NotImplementedError\n\n    def get_config(self):\n        raise NotImplementedError\n\n    def from_config(self, config):\n        raise NotImplementedError\n\n\n# aliases\nsgd = SGD\nrmsprop = RMSprop\nadagrad = Adagrad\nadadelta = Adadelta\nadam = Adam\nadamax = Adamax\nnadam = Nadam\n\n\ndef get(identifier, kwargs=None):\n    if K.backend() == 'tensorflow':\n        # Wrap TF optimizer instances\n        import tensorflow as tf\n        if isinstance(identifier, tf.train.Optimizer):\n            return TFOptimizer(identifier)\n    # Instantiate a Keras optimizer\n    return get_from_module(identifier, globals(), 'optimizer',\n                           instantiate=True, kwargs=kwargs)\n",
      "file_patch": "@@ -19,6 +19,7 @@ def optimizer_from_config(config, custom_objects={}):\n         'adam': Adam,\n         'adamax': Adamax,\n         'nadam': Nadam,\n+        'tfoptimizer': TFOptimizer,\n     }\n     class_name = config['class_name']\n     if class_name in custom_objects:\n@@ -53,14 +54,6 @@ class Optimizer(object):\n         self.updates = []\n         self.weights = []\n \n-    def get_state(self):\n-        return [K.get_value(u[0]) for u in self.updates]\n-\n-    def set_state(self, value_list):\n-        assert len(self.updates) == len(value_list)\n-        for u, v in zip(self.updates, value_list):\n-            K.set_value(u[0], v)\n-\n     def get_updates(self, params, constraints, loss):\n         raise NotImplementedError\n \n@@ -570,6 +563,37 @@ class Nadam(Optimizer):\n         return dict(list(base_config.items()) + list(config.items()))\n \n \n+class TFOptimizer(Optimizer):\n+\n+    def __init__(self, optimizer):\n+        self.optimizer = optimizer\n+        self.iterations = K.variable(0.)\n+        self.updates = []\n+\n+    def get_updates(self, params, constraints, loss):\n+        if constraints:\n+            raise ValueError('TF optimizers do not support '\n+                             'weights constraints. Either remove '\n+                             'all weights constraints in your model, '\n+                             'or use a Keras optimizer.')\n+        grads = self.optimizer.compute_gradients(loss, params)\n+        self.updates.append(K.update_add(self.iterations, 1))\n+        opt_update = self.optimizer.apply_gradients(\n+            grads, global_step=self.iterations)\n+        self.updates.append(opt_update)\n+        return self.updates\n+\n+    @property\n+    def weights(self):\n+        raise NotImplementedError\n+\n+    def get_config(self):\n+        raise NotImplementedError\n+\n+    def from_config(self, config):\n+        raise NotImplementedError\n+\n+\n # aliases\n sgd = SGD\n rmsprop = RMSprop\n@@ -581,5 +605,11 @@ nadam = Nadam\n \n \n def get(identifier, kwargs=None):\n+    if K.backend() == 'tensorflow':\n+        # Wrap TF optimizer instances\n+        import tensorflow as tf\n+        if isinstance(identifier, tf.train.Optimizer):\n+            return TFOptimizer(identifier)\n+    # Instantiate a Keras optimizer\n     return get_from_module(identifier, globals(), 'optimizer',\n                            instantiate=True, kwargs=kwargs)\n",
      "files_name_in_blame_commit": [
        "optimizers.py"
      ]
    }
  },
  "commits_modify_file_before_fix": {
    "size": 114
  },
  "recursive_blame_commits": {
    "recursive_blame_function_lines": {
      "64": {
        "commit_id": "37fe48b744b00f9e1d4eb7da481cd6a322743c9b",
        "line_code": "    def get_updates(self, params, constraints, loss):",
        "commit_date": "2015-07-15 23:24:42",
        "valid": 1
      },
      "65": {
        "commit_id": "37a1db225420851cc668600c49697d9a2057f098",
        "line_code": "        raise NotImplementedError",
        "commit_date": "2015-03-27 17:59:42",
        "valid": 1
      }
    },
    "commits": {
      "37fe48b744b00f9e1d4eb7da481cd6a322743c9b": {
        "commit": {
          "commit_id": "37fe48b744b00f9e1d4eb7da481cd6a322743c9b",
          "commit_message": "Match get_updates signature",
          "commit_author": "Ken Terao",
          "commit_date": "2015-07-15 23:24:42",
          "commit_parent": "fd1d5908c6b050a64f0eee3571f73a1983bd26d4"
        },
        "function": {
          "function_name": "get_updates",
          "function_code_before": "def get_updates(self, params, regularizers, constraints, loss):\n    raise NotImplementedError",
          "function_code_after": "def get_updates(self, params, constraints, loss):\n    raise NotImplementedError",
          "function_before_start_line": 19,
          "function_before_end_line": 20,
          "function_after_start_line": 19,
          "function_after_end_line": 20,
          "function_before_token_count": 15,
          "function_after_token_count": 0,
          "functions_name_modified_file": [
            "__init__",
            "get_gradients",
            "kl_divergence",
            "get",
            "get_updates",
            "clip_norm"
          ],
          "functions_name_all_files": [
            "__init__",
            "get_gradients",
            "kl_divergence",
            "get",
            "get_updates",
            "clip_norm"
          ],
          "functions_name_co_evolved_modified_file": [],
          "functions_name_co_evolved_all_files": []
        },
        "file": {
          "file_name": "optimizers.py",
          "file_nloc": 127,
          "file_complexity": 30,
          "file_token_count": 1346,
          "file_before": "from __future__ import absolute_import\nimport theano\nimport theano.tensor as T\nimport numpy as np\n\nfrom .utils.theano_utils import shared_zeros, shared_scalar\nfrom six.moves import zip\n\ndef clip_norm(g, c, n):\n    if c > 0:\n        g = T.switch(T.ge(n, c), g*c/n, g)\n    return g\n\ndef kl_divergence(p, p_hat):\n    return p_hat - p + p*T.log(p/p_hat)\n\nclass Optimizer(object):\n    \n    def get_updates(self, params, regularizers, constraints,  loss):\n        raise NotImplementedError\n\n    def get_gradients(self, loss, params):\n\n        grads = T.grad(loss, params)\n\n        if hasattr(self, 'clipnorm') and self.clipnorm > 0:\n            norm = T.sqrt(sum([T.sum(g**2) for g in grads]))\n            grads = [clip_norm(g, self.clipnorm, norm) for g in grads]\n\n        return grads\n\n\nclass SGD(Optimizer):\n\n    def __init__(self, lr=0.01, momentum=0., decay=0., nesterov=False, *args, **kwargs):\n        self.__dict__.update(kwargs)\n        self.__dict__.update(locals())\n        self.iterations = shared_scalar(0)\n\n    def get_updates(self, params, constraints, loss):\n        grads = self.get_gradients(loss, params)\n        lr = self.lr * (1.0 / (1.0 + self.decay * self.iterations))\n        updates = [(self.iterations, self.iterations+1.)]\n\n        for p, g, c in zip(params, grads, constraints):\n            m = shared_zeros(p.get_value().shape) # momentum\n            v = self.momentum * m - lr * g # velocity\n            updates.append((m, v)) \n\n            if self.nesterov:\n                new_p = p + self.momentum * v - lr * g\n            else:\n                new_p = p + v\n\n            updates.append((p, c(new_p))) # apply constraints\n        return updates\n\n\nclass RMSprop(Optimizer):\n\n    def __init__(self, lr=0.001, rho=0.9, epsilon=1e-6, *args, **kwargs):\n        self.__dict__.update(kwargs)\n        self.__dict__.update(locals())\n\n    def get_updates(self, params, constraints, loss):\n        grads = self.get_gradients(loss, params)\n        accumulators = [shared_zeros(p.get_value().shape) for p in params]\n        updates = []\n\n        for p, g, a, c in zip(params, grads, accumulators, constraints):\n            new_a = self.rho * a + (1 - self.rho) * g ** 2 # update accumulator\n            updates.append((a, new_a))\n\n            new_p = p - self.lr * g / T.sqrt(new_a + self.epsilon)\n            updates.append((p, c(new_p))) # apply constraints\n            \n        return updates\n\n\nclass Adagrad(Optimizer):\n\n    def __init__(self, lr=0.01, epsilon=1e-6, *args, **kwargs):\n        self.__dict__.update(kwargs)\n        self.__dict__.update(locals())\n\n    def get_updates(self, params, constraints, loss):\n        grads = self.get_gradients(loss, params)\n        accumulators = [shared_zeros(p.get_value().shape) for p in params]\n        updates = []\n\n        for p, g, a, c in zip(params, grads, accumulators, constraints):\n            new_a = a + g ** 2 # update accumulator\n            updates.append((a, new_a))\n\n            new_p = p - self.lr * g / T.sqrt(new_a + self.epsilon)\n            updates.append((p, c(new_p))) # apply constraints\n        return updates\n\n\nclass Adadelta(Optimizer):\n    '''\n        Reference: http://arxiv.org/abs/1212.5701\n    '''\n    def __init__(self, lr=1.0, rho=0.95, epsilon=1e-6, *args, **kwargs):\n        self.__dict__.update(kwargs)\n        self.__dict__.update(locals())\n\n    def get_updates(self, params, constraints, loss):\n        grads = self.get_gradients(loss, params)\n        accumulators = [shared_zeros(p.get_value().shape) for p in params]\n        delta_accumulators = [shared_zeros(p.get_value().shape) for p in params]\n        updates = []\n\n        for p, g, a, d_a, c in zip(params, grads, accumulators, delta_accumulators, constraints):\n            new_a = self.rho * a + (1 - self.rho) * g ** 2 # update accumulator\n            updates.append((a, new_a))\n\n            # use the new accumulator and the *old* delta_accumulator\n            update = g * T.sqrt(d_a + self.epsilon) / T.sqrt(new_a + self.epsilon)\n\n            new_p = p - self.lr * update\n            updates.append((p, c(new_p))) # apply constraints\n\n            # update delta_accumulator\n            new_d_a = self.rho * d_a + (1 - self.rho) * update ** 2\n            updates.append((d_a, new_d_a))\n        return updates\n\n\nclass Adam(Optimizer):\n    '''\n        Reference: http://arxiv.org/abs/1412.6980\n\n        Default parameters follow those provided in the original paper\n\n        lambda is renamed kappa.\n    '''\n    def __init__(self, lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-8, kappa=1-1e-8, *args, **kwargs):\n        self.__dict__.update(kwargs)\n        self.__dict__.update(locals())\n        self.iterations = shared_scalar(0)\n\n    def get_updates(self, params, constraints, loss):\n        grads = self.get_gradients(loss, params)\n        updates = [(self.iterations, self.iterations+1.)]\n\n        i = self.iterations\n        beta_1_t = self.beta_1 * (self.kappa**i)\n\n        # the update below seems missing from the paper, but is obviously required\n        beta_2_t = self.beta_2 * (self.kappa**i) \n\n        for p, g, c in zip(params, grads, constraints):\n            m = theano.shared(p.get_value() * 0.) # zero init of moment\n            v = theano.shared(p.get_value() * 0.) # zero init of velocity\n\n            m_t = (beta_1_t * m) + (1 - beta_1_t) * g\n            v_t = (beta_2_t * v) + (1 - beta_2_t) * (g**2)\n\n            m_b_t = m_t / (1 - beta_1_t)\n            v_b_t = v_t / (1 - beta_2_t)\n\n            p_t = p - self.lr * m_b_t / (T.sqrt(v_b_t) + self.epsilon)\n            \n            updates.append((m, m_t))\n            updates.append((v, v_t))\n            updates.append((p, c(p_t))) # apply constraints\n        return updates\n\n# aliases\nsgd = SGD\nrmsprop = RMSprop\nadagrad = Adagrad\nadadelta = Adadelta\nadam = Adam\n\nfrom .utils.generic_utils import get_from_module\ndef get(identifier):\n    return get_from_module(identifier, globals(), 'optimizer', instantiate=True)\n",
          "file_after": "from __future__ import absolute_import\nimport theano\nimport theano.tensor as T\nimport numpy as np\n\nfrom .utils.theano_utils import shared_zeros, shared_scalar\nfrom six.moves import zip\n\ndef clip_norm(g, c, n):\n    if c > 0:\n        g = T.switch(T.ge(n, c), g*c/n, g)\n    return g\n\ndef kl_divergence(p, p_hat):\n    return p_hat - p + p*T.log(p/p_hat)\n\nclass Optimizer(object):\n    \n    def get_updates(self, params, constraints, loss):\n        raise NotImplementedError\n\n    def get_gradients(self, loss, params):\n\n        grads = T.grad(loss, params)\n\n        if hasattr(self, 'clipnorm') and self.clipnorm > 0:\n            norm = T.sqrt(sum([T.sum(g**2) for g in grads]))\n            grads = [clip_norm(g, self.clipnorm, norm) for g in grads]\n\n        return grads\n\n\nclass SGD(Optimizer):\n\n    def __init__(self, lr=0.01, momentum=0., decay=0., nesterov=False, *args, **kwargs):\n        self.__dict__.update(kwargs)\n        self.__dict__.update(locals())\n        self.iterations = shared_scalar(0)\n\n    def get_updates(self, params, constraints, loss):\n        grads = self.get_gradients(loss, params)\n        lr = self.lr * (1.0 / (1.0 + self.decay * self.iterations))\n        updates = [(self.iterations, self.iterations+1.)]\n\n        for p, g, c in zip(params, grads, constraints):\n            m = shared_zeros(p.get_value().shape) # momentum\n            v = self.momentum * m - lr * g # velocity\n            updates.append((m, v)) \n\n            if self.nesterov:\n                new_p = p + self.momentum * v - lr * g\n            else:\n                new_p = p + v\n\n            updates.append((p, c(new_p))) # apply constraints\n        return updates\n\n\nclass RMSprop(Optimizer):\n\n    def __init__(self, lr=0.001, rho=0.9, epsilon=1e-6, *args, **kwargs):\n        self.__dict__.update(kwargs)\n        self.__dict__.update(locals())\n\n    def get_updates(self, params, constraints, loss):\n        grads = self.get_gradients(loss, params)\n        accumulators = [shared_zeros(p.get_value().shape) for p in params]\n        updates = []\n\n        for p, g, a, c in zip(params, grads, accumulators, constraints):\n            new_a = self.rho * a + (1 - self.rho) * g ** 2 # update accumulator\n            updates.append((a, new_a))\n\n            new_p = p - self.lr * g / T.sqrt(new_a + self.epsilon)\n            updates.append((p, c(new_p))) # apply constraints\n            \n        return updates\n\n\nclass Adagrad(Optimizer):\n\n    def __init__(self, lr=0.01, epsilon=1e-6, *args, **kwargs):\n        self.__dict__.update(kwargs)\n        self.__dict__.update(locals())\n\n    def get_updates(self, params, constraints, loss):\n        grads = self.get_gradients(loss, params)\n        accumulators = [shared_zeros(p.get_value().shape) for p in params]\n        updates = []\n\n        for p, g, a, c in zip(params, grads, accumulators, constraints):\n            new_a = a + g ** 2 # update accumulator\n            updates.append((a, new_a))\n\n            new_p = p - self.lr * g / T.sqrt(new_a + self.epsilon)\n            updates.append((p, c(new_p))) # apply constraints\n        return updates\n\n\nclass Adadelta(Optimizer):\n    '''\n        Reference: http://arxiv.org/abs/1212.5701\n    '''\n    def __init__(self, lr=1.0, rho=0.95, epsilon=1e-6, *args, **kwargs):\n        self.__dict__.update(kwargs)\n        self.__dict__.update(locals())\n\n    def get_updates(self, params, constraints, loss):\n        grads = self.get_gradients(loss, params)\n        accumulators = [shared_zeros(p.get_value().shape) for p in params]\n        delta_accumulators = [shared_zeros(p.get_value().shape) for p in params]\n        updates = []\n\n        for p, g, a, d_a, c in zip(params, grads, accumulators, delta_accumulators, constraints):\n            new_a = self.rho * a + (1 - self.rho) * g ** 2 # update accumulator\n            updates.append((a, new_a))\n\n            # use the new accumulator and the *old* delta_accumulator\n            update = g * T.sqrt(d_a + self.epsilon) / T.sqrt(new_a + self.epsilon)\n\n            new_p = p - self.lr * update\n            updates.append((p, c(new_p))) # apply constraints\n\n            # update delta_accumulator\n            new_d_a = self.rho * d_a + (1 - self.rho) * update ** 2\n            updates.append((d_a, new_d_a))\n        return updates\n\n\nclass Adam(Optimizer):\n    '''\n        Reference: http://arxiv.org/abs/1412.6980\n\n        Default parameters follow those provided in the original paper\n\n        lambda is renamed kappa.\n    '''\n    def __init__(self, lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-8, kappa=1-1e-8, *args, **kwargs):\n        self.__dict__.update(kwargs)\n        self.__dict__.update(locals())\n        self.iterations = shared_scalar(0)\n\n    def get_updates(self, params, constraints, loss):\n        grads = self.get_gradients(loss, params)\n        updates = [(self.iterations, self.iterations+1.)]\n\n        i = self.iterations\n        beta_1_t = self.beta_1 * (self.kappa**i)\n\n        # the update below seems missing from the paper, but is obviously required\n        beta_2_t = self.beta_2 * (self.kappa**i) \n\n        for p, g, c in zip(params, grads, constraints):\n            m = theano.shared(p.get_value() * 0.) # zero init of moment\n            v = theano.shared(p.get_value() * 0.) # zero init of velocity\n\n            m_t = (beta_1_t * m) + (1 - beta_1_t) * g\n            v_t = (beta_2_t * v) + (1 - beta_2_t) * (g**2)\n\n            m_b_t = m_t / (1 - beta_1_t)\n            v_b_t = v_t / (1 - beta_2_t)\n\n            p_t = p - self.lr * m_b_t / (T.sqrt(v_b_t) + self.epsilon)\n            \n            updates.append((m, m_t))\n            updates.append((v, v_t))\n            updates.append((p, c(p_t))) # apply constraints\n        return updates\n\n# aliases\nsgd = SGD\nrmsprop = RMSprop\nadagrad = Adagrad\nadadelta = Adadelta\nadam = Adam\n\nfrom .utils.generic_utils import get_from_module\ndef get(identifier):\n    return get_from_module(identifier, globals(), 'optimizer', instantiate=True)\n",
          "file_patch": "@@ -16,7 +16,7 @@ def kl_divergence(p, p_hat):\n \n class Optimizer(object):\n     \n-    def get_updates(self, params, regularizers, constraints,  loss):\n+    def get_updates(self, params, constraints, loss):\n         raise NotImplementedError\n \n     def get_gradients(self, loss, params):\n",
          "files_name_in_blame_commit": [
            "optimizers.py"
          ]
        }
      },
      "37a1db225420851cc668600c49697d9a2057f098": {
        "commit": {
          "commit_id": "37a1db225420851cc668600c49697d9a2057f098",
          "commit_message": "Add initial public version of Keras",
          "commit_author": "Francois",
          "commit_date": "2015-03-27 17:59:42",
          "commit_parent": "1f0bd8cac393d7da2e5845b5ffce5f4a8568006f"
        },
        "function": {
          "function_name": "get_updates",
          "function_code_before": "",
          "function_code_after": "def get_updates(self, params, grads):\n    raise NotImplementedError",
          "function_before_start_line": "",
          "function_before_end_line": "",
          "function_after_start_line": 13,
          "function_after_end_line": 14,
          "function_before_token_count": 0,
          "function_after_token_count": 11,
          "functions_name_modified_file": [
            "__init__",
            "get_gradients",
            "get",
            "get_updates",
            "clip_norm"
          ],
          "functions_name_all_files": [
            "flow",
            "output",
            "get_gradients",
            "standardize",
            "texts_to_sequences",
            "sharedX",
            "tanh",
            "evaluate",
            "random_rotation",
            "lecun_uniform",
            "random_barrel_transform",
            "base_filter",
            "get_weights",
            "text_to_word_sequence",
            "floatX",
            "vertical_flip",
            "orthogonal",
            "hard_sigmoid",
            "__init__",
            "categorical_crossentropy",
            "load_img",
            "load_data",
            "fit_on_sequences",
            "random_shift",
            "compile",
            "random_channel_shift",
            "list_pictures",
            "connect",
            "get_updates",
            "fit",
            "random_zoom",
            "_step",
            "shared_ones",
            "mean_absolute_error",
            "fit_on_texts",
            "random_transform",
            "softplus",
            "test",
            "uniform",
            "load_array",
            "sequences_to_matrix",
            "sigmoid",
            "shared_zeros",
            "array_to_img",
            "probas_to_classes",
            "predict_proba",
            "make_tuple",
            "relu",
            "get_input",
            "hinge",
            "get_from_module",
            "set_weights",
            "squared_hinge",
            "linear",
            "binary_crossentropy",
            "texts_to_matrix",
            "shared_scalar",
            "softmax",
            "random_shear",
            "accuracy",
            "save_array",
            "multiclass_logloss",
            "train",
            "one_hot",
            "to_categorical",
            "make_reuters_dataset",
            "standardize_y",
            "binary_logloss",
            "horizontal_flip",
            "normal",
            "get_file",
            "categorical_probas_to_classes",
            "add",
            "clip_norm",
            "get",
            "predict_classes",
            "pad_sequences",
            "img_to_array",
            "mean_squared_error",
            "update",
            "alloc_zeros_matrix"
          ],
          "functions_name_co_evolved_modified_file": [
            "__init__",
            "get",
            "clip_norm",
            "get_gradients"
          ],
          "functions_name_co_evolved_all_files": [
            "flow",
            "output",
            "get_gradients",
            "standardize",
            "texts_to_sequences",
            "sharedX",
            "tanh",
            "evaluate",
            "random_rotation",
            "lecun_uniform",
            "random_barrel_transform",
            "base_filter",
            "get_weights",
            "text_to_word_sequence",
            "floatX",
            "vertical_flip",
            "orthogonal",
            "hard_sigmoid",
            "__init__",
            "categorical_crossentropy",
            "load_img",
            "load_data",
            "fit_on_sequences",
            "random_shift",
            "compile",
            "random_channel_shift",
            "list_pictures",
            "connect",
            "fit",
            "random_zoom",
            "_step",
            "shared_ones",
            "mean_absolute_error",
            "fit_on_texts",
            "random_transform",
            "softplus",
            "test",
            "uniform",
            "load_array",
            "sequences_to_matrix",
            "sigmoid",
            "shared_zeros",
            "array_to_img",
            "probas_to_classes",
            "predict_proba",
            "make_tuple",
            "relu",
            "get_input",
            "hinge",
            "get_from_module",
            "set_weights",
            "squared_hinge",
            "linear",
            "binary_crossentropy",
            "texts_to_matrix",
            "shared_scalar",
            "softmax",
            "random_shear",
            "accuracy",
            "save_array",
            "multiclass_logloss",
            "train",
            "one_hot",
            "to_categorical",
            "make_reuters_dataset",
            "standardize_y",
            "binary_logloss",
            "horizontal_flip",
            "normal",
            "get_file",
            "categorical_probas_to_classes",
            "add",
            "clip_norm",
            "get",
            "predict_classes",
            "pad_sequences",
            "img_to_array",
            "mean_squared_error",
            "update",
            "alloc_zeros_matrix"
          ]
        },
        "file": {
          "file_name": "optimizers.py",
          "file_nloc": 96,
          "file_complexity": 33,
          "file_token_count": 1056,
          "file_before": null,
          "file_after": "import theano\nimport theano.tensor as T\nimport numpy as np\n\nfrom utils.theano_utils import shared_zeros, shared_scalar\n\ndef clip_norm(g, c, n):\n    if c > 0:\n        g = T.switch(T.ge(n, c), g*c/n, g)\n    return g\n\nclass Optimizer(object):\n    def get_updates(self, params, grads):\n        raise NotImplementedError\n\n    def get_gradients(self, cost, params):\n        grads = T.grad(cost, params)\n\n        if hasattr(self, 'clipnorm') and self.clipnorm > 0:\n            norm = T.sqrt(sum([T.sum(g**2) for g in grads]))\n            grads = [clip_norm(g, c, norm) for g in grads]\n\n        new_grads = []\n        for p, g in zip(params, grads):\n            if hasattr(self, 'l1') and self.l1 > 0:\n                g += T.sgn(p) * self.l1\n\n            if hasattr(self, 'l2') and self.l2 > 0:\n                g += p * self.l2\n\n            if hasattr(self, 'maxnorm') and self.maxnorm > 0:\n                norms = T.sqrt(T.sum(T.sqr(p), axis=0))\n                desired = T.clip(norms, 0, self.maxnorm)\n                p = p * (desired / (1e-7 + norms))\n\n            new_grads.append(g)\n        return new_grads\n\n\nclass SGD(Optimizer):\n\n    def __init__(self, lr=0.01, momentum=0., decay=0., nesterov=False, *args, **kwargs):\n        self.__dict__.update(locals())\n        self.iterations = shared_scalar(0)\n\n    def get_updates(self, params, cost):\n        grads = self.get_gradients(cost, params)\n        lr = self.lr - self.decay * self.iterations\n        updates = [(self.iterations, self.iterations+1.)]\n\n        for p, g in zip(params, grads):\n            m = shared_zeros(p.get_value().shape) # momentum\n            v = self.momentum * m - lr * g # velocity\n            updates.append((m, v)) \n\n            if self.nesterov:\n                new_p = p + self.momentum * v - lr * g\n            else:\n                new_p = p + v\n            updates.append((p, new_p))\n        return updates\n\n\nclass RMSprop(Optimizer):\n\n    def __init__(self, lr=0.001, rho=0.9, epsilon=1e-6, *args, **kwargs):\n        self.__dict__.update(locals())\n\n    def get_updates(self, params, cost):\n        grads = self.get_gradients(cost, params)\n        accumulators = [shared_zeros(p.get_value().shape) for p in params]\n        updates = []\n\n        for p, g, a in zip(params, grads, accumulators):\n            new_a = self.rho * a + (1 - self.rho) * g ** 2 # update accumulator\n            updates.append((a, new_a))\n\n            new_p = p - self.lr * g / T.sqrt(new_a + self.epsilon)\n            updates.append((p, new_p))\n        return updates\n\n\nclass Adagrad(Optimizer):\n\n    def __init__(self, lr=0.01, epsilon=1e-6, *args, **kwargs):\n        self.__dict__.update(locals())\n\n    def get_updates(self, params, cost):\n        grads = self.get_gradients(cost, params)\n        accumulators = [shared_zeros(p.get_value().shape) for p in params]\n        updates = []\n\n        for p, g, a in zip(params, grads, accumulators):\n            new_a = a + g ** 2 # update accumulator\n            updates.append((a, new_a))\n\n            new_p = p - self.lr * g / T.sqrt(new_a + self.epsilon)\n            updates.append((p, new_p))\n        return updates\n\n\nclass Adadelta(Optimizer):\n    \n    def __init__(self, lr=1.0, rho=0.95, epsilon=1e-6, *args, **kwargs):\n        self.__dict__.update(locals())\n\n    def get_updates(self, params, cost):\n        grads = self.get_gradients(cost, params)\n        accumulators = [shared_zeros(p.get_value().shape) for p in params]\n        delta_accumulators = [shared_zeros(p.get_value().shape) for p in params]\n        updates = []\n\n        for p, g, a, d_a in zip(params, grads, accumulators, delta_accumulators):\n            new_a = self.rho * a + (1 - self.rho) * g ** 2 # update accumulator\n            updates.append((a, new_a))\n\n            # use the new accumulator and the *old* delta_accumulator\n            update = g * T.sqrt(d_a + self.epsilon) / T.sqrt(new_a + self.epsilon)\n\n            new_p = p - self.lr * update\n            updates.append((p, new_p))\n\n            # update delta_accumulator\n            new_d_a = self.rho * d_a + (1 - self.rho) * update ** 2\n            updates.append((d_a, new_d_a))\n        return updates\n\n# aliases\nsgd = SGD\nrmsprop = RMSprop\nadagrad = Adagrad\nadadelta = Adadelta\n\nfrom utils.generic_utils import get_from_module\ndef get(identifier):\n    return get_from_module(identifier, globals(), 'optimizer', instantiate=True)\n",
          "file_patch": "@@ -0,0 +1,136 @@\n+import theano\n+import theano.tensor as T\n+import numpy as np\n+\n+from utils.theano_utils import shared_zeros, shared_scalar\n+\n+def clip_norm(g, c, n):\n+    if c > 0:\n+        g = T.switch(T.ge(n, c), g*c/n, g)\n+    return g\n+\n+class Optimizer(object):\n+    def get_updates(self, params, grads):\n+        raise NotImplementedError\n+\n+    def get_gradients(self, cost, params):\n+        grads = T.grad(cost, params)\n+\n+        if hasattr(self, 'clipnorm') and self.clipnorm > 0:\n+            norm = T.sqrt(sum([T.sum(g**2) for g in grads]))\n+            grads = [clip_norm(g, c, norm) for g in grads]\n+\n+        new_grads = []\n+        for p, g in zip(params, grads):\n+            if hasattr(self, 'l1') and self.l1 > 0:\n+                g += T.sgn(p) * self.l1\n+\n+            if hasattr(self, 'l2') and self.l2 > 0:\n+                g += p * self.l2\n+\n+            if hasattr(self, 'maxnorm') and self.maxnorm > 0:\n+                norms = T.sqrt(T.sum(T.sqr(p), axis=0))\n+                desired = T.clip(norms, 0, self.maxnorm)\n+                p = p * (desired / (1e-7 + norms))\n+\n+            new_grads.append(g)\n+        return new_grads\n+\n+\n+class SGD(Optimizer):\n+\n+    def __init__(self, lr=0.01, momentum=0., decay=0., nesterov=False, *args, **kwargs):\n+        self.__dict__.update(locals())\n+        self.iterations = shared_scalar(0)\n+\n+    def get_updates(self, params, cost):\n+        grads = self.get_gradients(cost, params)\n+        lr = self.lr - self.decay * self.iterations\n+        updates = [(self.iterations, self.iterations+1.)]\n+\n+        for p, g in zip(params, grads):\n+            m = shared_zeros(p.get_value().shape) # momentum\n+            v = self.momentum * m - lr * g # velocity\n+            updates.append((m, v)) \n+\n+            if self.nesterov:\n+                new_p = p + self.momentum * v - lr * g\n+            else:\n+                new_p = p + v\n+            updates.append((p, new_p))\n+        return updates\n+\n+\n+class RMSprop(Optimizer):\n+\n+    def __init__(self, lr=0.001, rho=0.9, epsilon=1e-6, *args, **kwargs):\n+        self.__dict__.update(locals())\n+\n+    def get_updates(self, params, cost):\n+        grads = self.get_gradients(cost, params)\n+        accumulators = [shared_zeros(p.get_value().shape) for p in params]\n+        updates = []\n+\n+        for p, g, a in zip(params, grads, accumulators):\n+            new_a = self.rho * a + (1 - self.rho) * g ** 2 # update accumulator\n+            updates.append((a, new_a))\n+\n+            new_p = p - self.lr * g / T.sqrt(new_a + self.epsilon)\n+            updates.append((p, new_p))\n+        return updates\n+\n+\n+class Adagrad(Optimizer):\n+\n+    def __init__(self, lr=0.01, epsilon=1e-6, *args, **kwargs):\n+        self.__dict__.update(locals())\n+\n+    def get_updates(self, params, cost):\n+        grads = self.get_gradients(cost, params)\n+        accumulators = [shared_zeros(p.get_value().shape) for p in params]\n+        updates = []\n+\n+        for p, g, a in zip(params, grads, accumulators):\n+            new_a = a + g ** 2 # update accumulator\n+            updates.append((a, new_a))\n+\n+            new_p = p - self.lr * g / T.sqrt(new_a + self.epsilon)\n+            updates.append((p, new_p))\n+        return updates\n+\n+\n+class Adadelta(Optimizer):\n+    \n+    def __init__(self, lr=1.0, rho=0.95, epsilon=1e-6, *args, **kwargs):\n+        self.__dict__.update(locals())\n+\n+    def get_updates(self, params, cost):\n+        grads = self.get_gradients(cost, params)\n+        accumulators = [shared_zeros(p.get_value().shape) for p in params]\n+        delta_accumulators = [shared_zeros(p.get_value().shape) for p in params]\n+        updates = []\n+\n+        for p, g, a, d_a in zip(params, grads, accumulators, delta_accumulators):\n+            new_a = self.rho * a + (1 - self.rho) * g ** 2 # update accumulator\n+            updates.append((a, new_a))\n+\n+            # use the new accumulator and the *old* delta_accumulator\n+            update = g * T.sqrt(d_a + self.epsilon) / T.sqrt(new_a + self.epsilon)\n+\n+            new_p = p - self.lr * update\n+            updates.append((p, new_p))\n+\n+            # update delta_accumulator\n+            new_d_a = self.rho * d_a + (1 - self.rho) * update ** 2\n+            updates.append((d_a, new_d_a))\n+        return updates\n+\n+# aliases\n+sgd = SGD\n+rmsprop = RMSprop\n+adagrad = Adagrad\n+adadelta = Adadelta\n+\n+from utils.generic_utils import get_from_module\n+def get(identifier):\n+    return get_from_module(identifier, globals(), 'optimizer', instantiate=True)\n",
          "files_name_in_blame_commit": [
            "activations.py",
            "generic_utils.py",
            "text.py",
            "__init__.py",
            "recurrent.py",
            "core.py",
            "sequence.py",
            "models.py",
            "np_utils.py",
            "normalization.py",
            "cifar10.py",
            "cifar10_cnn.py",
            "initializations.py",
            "imdb_lstm.py",
            "objectives.py",
            "optimizers.py",
            "imdb.py",
            "data_utils.py",
            "image.py",
            "convolutional.py",
            "reuters_mlp.py",
            "advanced_activations.py",
            "theano_utils.py",
            "reuters.py"
          ]
        }
      }
    }
  }
}