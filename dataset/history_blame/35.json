{
  "id": "35",
  "blame_commit": {
    "commit": {
      "commit_id": "7b0349f0545011a6cac2422b8d8d0f409ffd1e15",
      "commit_message": "ENH: API change / refactoring in Series.__getitem__ and __setitem__ to implement #86, related tightening of integer index handling from #592",
      "commit_author": "Wes McKinney",
      "commit_date": "2012-01-13 12:53:56",
      "commit_parent": "f735a67561c2a316f6538d9fd344e861771d4916"
    },
    "function": {
      "function_name": "_get_with",
      "function_code_before": "",
      "function_code_after": "",
      "function_before_start_line": "",
      "function_before_end_line": "",
      "function_after_start_line": "",
      "function_after_end_line": "",
      "function_before_token_count": 0,
      "function_after_token_count": 0,
      "functions_name_modified_file": [
        "clip_lower",
        "map",
        "cumprod",
        "swaplevel",
        "_get_rename_function",
        "quantile",
        "order",
        "to_dict",
        "__new__",
        "tail",
        "rank",
        "to_csv",
        "__getitem__",
        "__init__",
        "weekday",
        "keys",
        "describe",
        "isin",
        "clip_upper",
        "__setstate__",
        "to_string",
        "_binop",
        "asfreq",
        "diff",
        "plot",
        "_sanitize_array",
        "shift",
        "reindex_like",
        "_get_repr",
        "argsort",
        "__contains__",
        "_check_bool_indexer",
        "nunique",
        "median",
        "std",
        "autocorr",
        "iteritems",
        "take",
        "from_csv",
        "_set_labels",
        "__reduce__",
        "__str__",
        "hist",
        "reorder_levels",
        "count",
        "prod",
        "_agg_by_level",
        "align",
        "mad",
        "asof",
        "cov",
        "append",
        "interpolate",
        "var",
        "skew",
        "idxmin",
        "combine_first",
        "unstack",
        "cumsum",
        "value_counts",
        "_flex_method",
        "combine",
        "apply",
        "__hash__",
        "sort_index",
        "_get_values",
        "remove_na",
        "idxmax",
        "_arith_method",
        "__repr__",
        "first_valid_index",
        "fillna",
        "__array_finalize__",
        "sortlevel",
        "mean",
        "copy",
        "clip",
        "__getslice__",
        "__setitem__",
        "to_sparse",
        "max",
        "sum",
        "corr",
        "ix",
        "reindex",
        "set_value",
        "__setslice__",
        "_tidy_repr",
        "__iter__",
        "rename",
        "_unbox",
        "_set_with",
        "_set_values",
        "round",
        "last_valid_index",
        "get_value",
        "get",
        "_constructor",
        "_get_with",
        "_reindex_indexer",
        "values",
        "_maybe_match_name",
        "min",
        "head",
        "dropna",
        "sort"
      ],
      "functions_name_all_files": [
        "test_ix_setitem_boolean",
        "map",
        "test_ix_setitem_corner",
        "_group_shape",
        "test_getitem",
        "test_combine_first_mixed_bug",
        "test_join_many_mixed",
        "_groupby_indices",
        "test_operators_frame",
        "test_argsort_preserve_name",
        "_get_axis",
        "test_join_with_len0",
        "select",
        "get_group_index",
        "test_getitem_fancy_1d",
        "test_get_agg_axis",
        "order",
        "test_arith_flex_frame",
        "_python_apply_general",
        "test_operators",
        "test_join",
        "test_getitem_fancy",
        "test_ix_getitem_not_monotonic",
        "test_rolling_count",
        "to_csv",
        "test_unstack_to_series",
        "test_groupby_multilevel_with_transform",
        "test_fromDict",
        "_getitem_tuple",
        "setUp",
        "test_getitem_boolean_object",
        "describe",
        "test_idxmin",
        "clip_upper",
        "test_rename_objects",
        "ngroups",
        "test_xs",
        "test_getitem_regression",
        "test_constructor_empty_list",
        "test_merge_right_vs_left",
        "test_getitem_iterator",
        "diff",
        "test_convert_objects",
        "test_multilevel_name_print",
        "test_contains",
        "test_getitem_pop_assign_name",
        "test_constructor",
        "test_constructor_ndarray",
        "_sanitize_array",
        "test_constructor_Series_copy_bug",
        "test_getitem_get",
        "size",
        "test_pop",
        "test_constructor_dict_block",
        "test_cumsum",
        "test_name_printing",
        "test_align_nocopy",
        "argsort",
        "__contains__",
        "test_truncate_copy",
        "test_to_csv_float32_nanrep",
        "nunique",
        "_check_stat_op",
        "test_all_any",
        "test_pad_nan",
        "drop",
        "test_prod_numpy16_bug",
        "test_value_counts_nunique",
        "test_constructor_orient",
        "from_csv",
        "test_rolling_apply",
        "test_rolling_skew",
        "hist",
        "test_rolling_sum",
        "test_to_csv_from_csv",
        "test_reindex_objects",
        "test_getitem_ambiguous_keyerror",
        "_getitem_lowerdim",
        "generate_groups",
        "test_flex_binary_frame",
        "test_getitem_slice_not_sorted",
        "test_to_string_mixed",
        "_agg_by_level",
        "test_getitem_box_float64",
        "_check_structures",
        "test_interpolate",
        "test_getitem_fancy_slice_integers_step",
        "_iterate_slices",
        "test_stack_level_name",
        "test_apply_with_args_kwds",
        "test_nonzero",
        "test_binops_level",
        "test_merge_index_singlekey_inner",
        "test_right_outer_join",
        "value_counts",
        "test_sortlevel_by_name",
        "combine",
        "indices",
        "test_left_outer_join",
        "test_from_records_bad_index_column",
        "test_to_string_no_header",
        "test_operators_empty_int_corner",
        "test_constructor_empty",
        "test_binop_maybe_preserve_name",
        "test_arith_flex_series",
        "test_sum_bool",
        "test_fancy_index_int_labels_exceptions",
        "test_comparisons",
        "test_filter",
        "test_constructor_dict_dont_upcast",
        "test_to_html",
        "test_swaplevel_panel",
        "test_partial_ix_missing",
        "test_round",
        "test_applymap",
        "test_drop_duplicates",
        "test_fillna_bug",
        "test_pivot_duplicates",
        "test_ix_preserve_names",
        "test_to_string_right_justify_cols",
        "test_getitem_out_of_bounds",
        "test_constructor_Series_named",
        "test_copy_name",
        "to_sparse",
        "test_rolling_std",
        "test_sum_inf",
        "test_reindex_level",
        "test_ix_getitem_setitem_integer_slice_keyerrors",
        "test_append_records",
        "test_from_records_sequencelike",
        "test_to_string_float_formatting",
        "test_std",
        "test_cumprod",
        "test_reindex_bool_pad",
        "test_get_axis_etc",
        "test_sum_object",
        "test_sum",
        "_tidy_repr",
        "test_stack_unstack_multiple",
        "_expand_axes",
        "test_rolling_quantile",
        "test_corr",
        "test_panel_join_many",
        "_aggregate_series_pure_python",
        "test_dropna_corner",
        "test_partial_set",
        "test_more_asMatrix",
        "_reindex_axis",
        "test_series_setitem",
        "_constructor",
        "test_mad",
        "ndim",
        "test_filter_corner",
        "test_min",
        "add_suffix",
        "_maybe_match_name",
        "test_skew",
        "_check_ew",
        "test_join_multiindex",
        "test_fancy_2d",
        "test_count_level_series",
        "clip_lower",
        "test_ewmvol",
        "test_join_unconsolidated",
        "test_getitem_toplevel",
        "cumprod",
        "test_constructor_tuples",
        "load",
        "test_fromValue",
        "test_getitem_boolean_missing",
        "test_getitem_setitem_boolean_misaligned",
        "_is_integer_dtype",
        "test_to_csv",
        "ids",
        "test_apply_raw",
        "test_from_csv",
        "test_merge_common",
        "_is_integer_slice",
        "test_merge_overlap",
        "__getattr__",
        "get_test_data",
        "test_set_value_resize",
        "test_scalarop_preserve_name",
        "test_iter",
        "test_constructor_series_copy",
        "test_var",
        "tail",
        "test_getitem_simple",
        "rank",
        "test_to_csv_withcommas",
        "test_unstack",
        "test_constructor_mixed_dict_and_Series",
        "__getitem__",
        "test_boolean_indexing",
        "keys",
        "_multi_iter",
        "test_fancy_getitem_int_labels",
        "test_combineFunc",
        "test_pivot_empty",
        "test_get_set_value_no_partial_indexing",
        "test_operators_corner",
        "_make_wrapper",
        "save",
        "to_string",
        "test_ix_getitem",
        "_get_label",
        "_check_moment_func",
        "test_getitem_int64",
        "test_constructor_rec",
        "test_setitem_always_copy",
        "_setitem_with_indexer",
        "test_join_index_more",
        "test_concat_series",
        "test_cumsum_corner",
        "test_rolling_cov",
        "_group_index",
        "test_basic_setitem_with_labels",
        "test_append_series",
        "_check_ew_ndarray",
        "test_sort_index",
        "test_stats_mixed_type",
        "_get_axis_number",
        "test_combineMult",
        "test_setindex",
        "test_fancy_slice_partial",
        "test_mean_corner",
        "test_append_different_columns",
        "_wrap_frames",
        "test_setitem_corner",
        "test_join_empty_bug",
        "test_mpl_compat_hack",
        "test_copy",
        "_is_label_like",
        "test_setitem_fancy_mixed_2d",
        "__str__",
        "test_count",
        "test_sum_bools",
        "test_getitem_setitem_slice_integers",
        "test_cython_right_outer_join",
        "prod",
        "test_constructor_list_of_lists",
        "test_get_loc_single_level",
        "test_unstack_preserve_types",
        "test_stack_unstack",
        "test_constructor_corner",
        "test_xs_corner",
        "test_operators_date",
        "test_mean",
        "test_constructor_cast",
        "test_getitem_int",
        "test_append_preserve_name",
        "test_constructor_scalar",
        "skew",
        "test_xs_partial",
        "aggregate",
        "idxmin",
        "test_align",
        "test_ewmcov",
        "test_concat_keys_specific_levels",
        "_clear_item_cache",
        "test_deepcopy",
        "_compress_group_index",
        "test_alignment",
        "test_unstack_bug",
        "__hash__",
        "test_concat_keys_and_levels",
        "idxmax",
        "get_group",
        "_get_group_levels",
        "test_as_matrix_consolidate",
        "test_join_inner_multiindex",
        "__repr__",
        "test_multilevel_consolidate",
        "test_rolling_median",
        "fillna",
        "test_rolling_corr_pairwise",
        "test_map",
        "__array_finalize__",
        "sortlevel",
        "clip",
        "test_join_index_mixed",
        "_slice",
        "test_append_many",
        "__getslice__",
        "test_select",
        "__setitem__",
        "_check_ndarray",
        "test_operators_combine",
        "test_sort",
        "test_cython_full_outer_join",
        "test_from_records_to_records",
        "test_to_csv_multiindex",
        "_is_list_like",
        "test_setitem_cast",
        "_convert_to_indexer",
        "test_concat_dict",
        "add_prefix",
        "test_count_level",
        "test_transpose",
        "test_stack",
        "set_value",
        "__setslice__",
        "test_constructor_Series_differently_indexed",
        "test_boolean_set_uncons",
        "test_map_int",
        "__iter__",
        "test_from_records_with_index_data",
        "test_apply_mixed_dtype_corner",
        "test_neg",
        "test_handle_overlap",
        "test_groupby_transform",
        "test_reindex_corner",
        "_make_labels",
        "test_pivot",
        "test_join_on_inner",
        "test_delevel_infer_dtype",
        "last_valid_index",
        "test_slice_can_reorder_not_uniquely_indexed",
        "test_keys",
        "get",
        "_get_with",
        "test_constructor_bool",
        "test_reindex_pad",
        "test_append",
        "_get_item_cache",
        "test_getitem_boolean_list",
        "_get_groupings",
        "values",
        "_assert_same_contents",
        "test_info",
        "test_weekday",
        "dropna",
        "test_merge_misspecified",
        "test_fillna_col_reordering",
        "test_as_matrix_numeric_cols",
        "test_diff",
        "groups",
        "test_ewma_span_com_args",
        "test_rename_nocopy",
        "_get_slice_axis",
        "test_ix_getitem_iterator",
        "test_to_string_no_index",
        "swaplevel",
        "test_sort_index_multicolumn",
        "test_iteritems",
        "test_setitem_ambig",
        "_get_rename_function",
        "test_join_index_series",
        "test_ewmcorr",
        "_set_item",
        "test_apply_reduce_Series",
        "test_count_objects",
        "test_ewma",
        "test_get_axis",
        "test_reset_index",
        "test_combineSeries",
        "test_median_corner",
        "test_cov",
        "test_frame_series_agg_multiple_levels",
        "_getitem_iterable",
        "_convert_tuple",
        "test_constructor_dict",
        "__new__",
        "test_take",
        "test_setitem_boolean",
        "test_add_index",
        "test_operators_reverse_object",
        "test_apply_convert_objects",
        "test_append_missing_column_proper_upcast",
        "test_getitem_fancy_scalar",
        "test_constructor_ragged",
        "__init__",
        "test_valid",
        "_is_indexed_like",
        "test_setitem_list",
        "_is_series",
        "test_setitem_boolean_missing",
        "test_fill_corner",
        "_aggregate_series_fast",
        "test_constructor_from_items",
        "test_apply_yield_list",
        "test_eng_float_formatter",
        "test_getitem_partial",
        "_binop",
        "test_getitem_slice_bug",
        "test_ewmvar",
        "test_lookup",
        "asfreq",
        "test_compress_group_combinations",
        "test_repr",
        "_is_integer_index",
        "truncate",
        "test_basic_getitem_with_labels",
        "plot",
        "reindex_like",
        "test_getitem_setitem_integer_slice_keyerrors",
        "test_asfreq_DateRange",
        "test_quantile",
        "test_series_getitem_not_sorted",
        "_get_repr",
        "test_ix_setitem",
        "test_constructor_tuple_of_tuples",
        "test_setitem_clear_caches",
        "_ensure_platform_int",
        "test_reindex_preserve_levels",
        "test_sort_index_name",
        "test_constructor_mixed",
        "_check_bool_indexer",
        "test_iterkv_names",
        "test_xs_view",
        "test_apply_axis1",
        "test_to_dict",
        "_restrict_to_columns",
        "std",
        "test_full_outer_join",
        "take",
        "_get_axis_name",
        "test_max",
        "test_frame_getitem_not_sorted",
        "axes",
        "test_rename",
        "test_merge_nocopy",
        "_wrap_applied_output",
        "_python_agg_general",
        "test_as_matrix",
        "test_cython_inner_join",
        "test_assign_columns",
        "agg",
        "test_get_numeric_data",
        "__len__",
        "_generator_factory",
        "test_constructor_ndarray_copy",
        "test_apply_modify_traceback",
        "test_ne",
        "transform",
        "align",
        "consolidate",
        "test_constructor_frame_copy",
        "_box_item_values",
        "test_constructor_dict_cast",
        "test_setslice",
        "astype",
        "test_logical_operators",
        "cython_aggregate",
        "_maybe_convert_ix",
        "test_sortlevel_mixed",
        "test_merge_on_multikey",
        "append",
        "interpolate",
        "var",
        "test_getitem_fancy_2d",
        "test_shift",
        "_need_slice",
        "_flex_method",
        "apply",
        "test_setitem_fancy_boolean",
        "test_corrwith",
        "sort_index",
        "_convert_grouper",
        "test_shift_int",
        "test_combine_first",
        "remove_na",
        "test_panel_join",
        "test_rolling_corr",
        "test_to_sparse_pass_name",
        "primary",
        "test_sum_corner",
        "test_slice",
        "test_repr_to_string",
        "test_transpose_get_view",
        "test_dropIncompleteRows",
        "test_setitem_change_dtype",
        "test_constructor_maskedarray",
        "_tuplify",
        "test_series_put_names",
        "test_stat_operators_attempt_obj_array",
        "test_preserveRefs",
        "test_fancy_setitem_int_labels",
        "test_describe",
        "test_reindex_backfill",
        "test_constructor_list_of_dicts",
        "test_join_on_series",
        "test_rolling_var",
        "test_truncate",
        "max",
        "corr",
        "test_dot",
        "ix",
        "test_idxmax",
        "test_fancy_getitem_slice_mixed",
        "test_to_csv_bug",
        "test_reindex_boolean",
        "_check_join",
        "test_repr_embedded_ndarray",
        "test_frame_getitem_view",
        "test_to_html_with_no_bold",
        "test_apply_ignore_failures",
        "rename",
        "test_clip",
        "test_sortlevel",
        "_set_axis",
        "_set_with",
        "_set_values",
        "test_to_string_unicode",
        "test_prod",
        "test_apply_broadcast",
        "test_get_set_boolean_different_order",
        "decons_group_index",
        "test_setitem_fancy_exceptions",
        "test_stale_cached_series_bug_473",
        "test_dropEmptyRows",
        "test_isin",
        "test_getitem_boolean",
        "get_value",
        "_is_mixed_type",
        "test_reindex",
        "_aggregate_multiple_funcs",
        "test_apply_differently_indexed",
        "test_rolling_max",
        "_check_accum_op",
        "test_merge_index_singlekey_right_vs_left",
        "test_insert_index",
        "test_join_float64_float32",
        "_wrap_aggregated_output",
        "test_head_tail",
        "test_combine_first_mixed",
        "_check_ew_structures",
        "test_repr_tuples",
        "test_values",
        "test_join_overlap",
        "test_to_string_format_na",
        "test_concat_with_group_keys",
        "test_count_level_corner",
        "test_frame_group_ops",
        "test_repr_corner",
        "test_join_on",
        "test_dtypes",
        "test_asfreq",
        "test_handle_empty_objects",
        "test_len",
        "test_combineFrame",
        "test_swaplevel",
        "test_reindex_axis",
        "test_multilevel_preserve_name",
        "quantile",
        "test_consolidate",
        "_check_binary_ew",
        "test_apply_empty_infer_type",
        "group_index",
        "test_cast_internals",
        "test_setitem_fancy_2d",
        "to_dict",
        "test_constructor_more",
        "_assert_all_na",
        "_maybe_droplevels",
        "test_astype",
        "test_get",
        "test_get_X_columns",
        "_ensure_int64",
        "_aggregate_item_by_item",
        "test_rolling_min",
        "pop",
        "test_setitem_fancy_scalar",
        "test_describe_objects",
        "test_is_lexsorted",
        "weekday",
        "_consolidate_inplace",
        "isin",
        "test_rank",
        "test_setitem_single_column_mixed",
        "_aggregate_series",
        "_get_axes",
        "counts",
        "__setstate__",
        "test_setitem_fancy_1d",
        "test_get_value",
        "test_combine_generic",
        "test_stack_unstack_preserve_names",
        "test_modify_values",
        "_is_label_slice",
        "_join_by_hand",
        "test_array_interface",
        "test_concat_multiindex_with_keys",
        "shift",
        "test_dropna",
        "rename_axis",
        "test_constructor_manager_resize",
        "test_combine_first_name",
        "test_setitem_ambiguous_keyerror",
        "__delitem__",
        "test_isnull",
        "test_rolling_kurt",
        "test_convert_objects_no_conversion",
        "median",
        "autocorr",
        "iteritems",
        "test_constructor_scalar_inference",
        "test_series_group_min_max",
        "test_getitem_fancy_ints",
        "test_cython_left_outer_join",
        "_set_labels",
        "test_delitem_corner",
        "__reduce__",
        "reorder_levels",
        "test_getitem_list",
        "count",
        "test_groupby_multilevel",
        "test_frame_setitem_ix",
        "test_inner_join",
        "_obj_with_exclusions",
        "test_constructor_default_index",
        "test_product",
        "test_insert",
        "test_describe_no_numeric",
        "labels",
        "test_panel_join_overlap",
        "_get_multi_index",
        "groupby",
        "name",
        "mad",
        "asof",
        "test_setitem_tuple",
        "test_getitem_setitem_fancy_exceptions",
        "test_panel_concat_other_axes",
        "test_iterrows",
        "test_pickle_preserve_name",
        "test_decons",
        "cov",
        "test_dropna_preserve_name",
        "test_set_columns",
        "combine_first",
        "unstack",
        "_cython_agg_general",
        "test_reindex_like",
        "cumsum",
        "test_getitem_setitem_boolean_corner",
        "_aggregate_generic",
        "_get_values",
        "test_align_sameindex",
        "test_constructor_list_of_tuples",
        "test_setitem_boolean_column",
        "test_unstack_level_name",
        "test_reorder_levels",
        "test_apply",
        "test_getattr",
        "test_setitem",
        "_arith_method",
        "test_set_value",
        "first_valid_index",
        "test_fillna",
        "mean",
        "copy",
        "test_is_mixed_type",
        "test_order",
        "test_reindex_columns",
        "test_reindex_int",
        "test_sort_index_preserve_levels",
        "test_frame_column_inplace_sort_exception",
        "test_constructor_DataFrame",
        "_wrap_array",
        "sum",
        "test_array_finalize",
        "test_corrwith_with_objects",
        "_aggregate_named",
        "test_stack_mixed_dtype",
        "test_rolling_mean",
        "reindex",
        "test_set_index",
        "test_join_many",
        "test_combineAdd",
        "test_median",
        "test_asof",
        "test_series_getitem",
        "test_join_on_singlekey_list",
        "test_concat_single_with_key",
        "test_single_element_ix_dont_upcast",
        "_unbox",
        "test_getitem_preserve_name",
        "test_argsort",
        "test_notnull",
        "test_to_string_unicode_columns",
        "round",
        "test_constructor_mix_series_nonseries",
        "test_pickle",
        "test_setitem_list_not_dataframe",
        "_reindex_indexer",
        "test_first_last_valid",
        "test_autocorr",
        "test_series_slice_partial",
        "_getitem_axis",
        "test_corr_rank",
        "_pickle_roundtrip",
        "test_reindex_bool",
        "test_to_string",
        "test_series_frame_radd_bug",
        "test_join_on_pass_vector",
        "test_delitem",
        "min",
        "_get_integer",
        "test_getitem_fancy_boolean",
        "test_boolean_set_mixed_type",
        "test_join_index",
        "test_merge_different_column_key_names",
        "head",
        "test_crossed_dtypes_weird_corner",
        "test_add_prefix_suffix",
        "sort"
      ],
      "functions_name_co_evolved_modified_file": [
        "_set_values",
        "_index_with",
        "__setslice__",
        "_get_values",
        "_set_labels",
        "__getslice__",
        "__setitem__",
        "_check_bool_indexer",
        "__getitem__",
        "_set_with"
      ],
      "functions_name_co_evolved_all_files": [
        "_setitem_with_indexer",
        "test_getitem_setitem_fancy_exceptions",
        "truncate",
        "_convert_to_indexer",
        "test_basic_getitem_with_labels",
        "test_getitem_setitem_boolean_misaligned",
        "test_basic_setitem_with_labels",
        "__setslice__",
        "_check_binary_ew",
        "test_series_getitem",
        "_check_bool_indexer",
        "_get_values",
        "_set_with",
        "_getitem_iterable",
        "_set_values",
        "_index_with",
        "test_append_different_columns",
        "test_series_setitem",
        "_set_labels",
        "__getitem__",
        "_getitem_tuple",
        "_getitem_axis",
        "test_getitem_setitem_slice_integers",
        "generate_groups",
        "_slice",
        "_get_integer",
        "__getslice__",
        "__setitem__",
        "_get_label",
        "_get"
      ]
    },
    "file": {
      "file_name": "series.py",
      "file_nloc": 1056,
      "file_complexity": 305,
      "file_token_count": 8438,
      "file_before": "\"\"\"\nData structure for 1-dimensional cross-sectional and time series data\n\"\"\"\n\n# pylint: disable=E1101,E1103\n# pylint: disable=W0703,W0622,W0613,W0201\n\nfrom itertools import izip\nimport csv\nimport operator\n\nfrom numpy import nan, ndarray\nimport numpy as np\nimport numpy.ma as ma\n\nfrom pandas.core.common import (isnull, notnull, _is_bool_indexer,\n                                _default_index, _maybe_upcast,\n                                _asarray_tuplesafe,\n                                AmbiguousIndexError)\nfrom pandas.core.daterange import DateRange\nfrom pandas.core.index import Index, MultiIndex, _ensure_index\nfrom pandas.core.indexing import _SeriesIndexer, _maybe_droplevels\nfrom pandas.util import py3compat\nfrom pandas.util.terminal import get_terminal_size\nimport pandas.core.common as com\nimport pandas.core.datetools as datetools\nimport pandas.core.generic as generic\nimport pandas.core.nanops as nanops\nimport pandas._tseries as lib\nimport pandas._engines as _gin\n\nfrom pandas.util.decorators import Appender, Substitution\n\n__all__ = ['Series', 'TimeSeries']\n\n#-------------------------------------------------------------------------------\n# Wrapper function for Series arithmetic methods\n\ndef _arith_method(op, name):\n    \"\"\"\n    Wrapper function for Series arithmetic operations, to avoid\n    code duplication.\n    \"\"\"\n    def wrapper(self, other):\n        from pandas.core.frame import DataFrame\n\n        if isinstance(other, Series):\n            if self.index.equals(other.index):\n                name = _maybe_match_name(self, other)\n                return Series(op(self.values, other.values), index=self.index,\n                              name=name)\n\n            this_reindexed, other_reindexed = self.align(other, join='outer',\n                                                         copy=False)\n            arr = op(this_reindexed.values, other_reindexed.values)\n\n            name = _maybe_match_name(self, other)\n            return Series(arr, index=this_reindexed.index, name=name)\n        elif isinstance(other, DataFrame):\n            return NotImplemented\n        else:\n            # scalars\n            return Series(op(self.values, other), index=self.index,\n                          name=self.name)\n    return wrapper\n\ndef _maybe_match_name(a, b):\n    name = None\n    if a.name == b.name:\n        name = a.name\n    return name\n\ndef _flex_method(op, name):\n    doc = \"\"\"\n    Binary operator %s with support to substitute a fill_value for missing data\n    in one of the inputs\n\n    Parameters\n    ----------\n    other: Series or scalar value\n    fill_value : None or float value, default None (NaN)\n        Fill missing (NaN) values with this value. If both Series are\n        missing, the result will be missing\n    level : int or name\n        Broadcast across a level, matching Index values on the\n        passed MultiIndex level\n\n    Returns\n    -------\n    result : Series\n    \"\"\" % name\n\n    @Appender(doc)\n    def f(self, other, level=None, fill_value=None):\n        return self._binop(other, op, level=level, fill_value=fill_value)\n\n    f.__name__ = name\n    return f\n\ndef _unbox(func):\n    @Appender(func.__doc__)\n    def f(self, *args, **kwargs):\n        result = func(self, *args, **kwargs)\n        if isinstance(result, np.ndarray) and result.ndim == 0:\n            return result.item()\n        else:  # pragma: no cover\n            return result\n    f.__name__ = func.__name__\n    return f\n\n_stat_doc = \"\"\"\nReturn %(name)s  of values\n%(na_action)s\n\nParameters\n----------\nskipna : boolean, default True\n    Exclude NA/null values\nlevel : int, default None\n    If the axis is a MultiIndex (hierarchical), count along a\n    particular level, collapsing into a smaller Series\n%(extras)s\nReturns\n-------\n%(shortname)s : float (or Series if level specified)\n\"\"\"\n_doc_exclude_na = \"NA/null values are excluded\"\n_doc_ndarray_interface = (\"Extra parameters are to preserve ndarray\"\n                          \"interface.\\n\")\n\n#-------------------------------------------------------------------------------\n# Series class\n\nclass Series(np.ndarray, generic.PandasObject):\n    _AXIS_NUMBERS = {\n        'index' : 0\n    }\n\n    _AXIS_NAMES = dict((v, k) for k, v in _AXIS_NUMBERS.iteritems())\n\n    __slots__ = ['_index', 'name']\n\n    def __new__(cls, data=None, index=None, dtype=None, name=None,\n                copy=False):\n        if data is None:\n            data = {}\n\n        if isinstance(data, Series):\n            if index is None:\n                index = data.index\n        elif isinstance(data, dict):\n            if index is None:\n                index = Index(sorted(data))\n            else:\n                index = _ensure_index(index)\n            try:\n                data = lib.fast_multiget(data, index, default=np.nan)\n            except TypeError:\n                data = [data.get(i, nan) for i in index]\n\n        subarr = _sanitize_array(data, index, dtype, copy,\n                                 raise_cast_failure=True)\n\n        if not isinstance(subarr, np.ndarray):\n            return subarr\n\n        if index is None:\n            index = _default_index(len(subarr))\n        else:\n            index = _ensure_index(index)\n\n        # Change the class of the array to be the subclass type.\n        if index.is_all_dates:\n            subarr = subarr.view(TimeSeries)\n        else:\n            subarr = subarr.view(Series)\n        subarr.index = index\n        subarr.name = name\n\n        return subarr\n\n    def __init__(self, data=None, index=None, dtype=None, name=None,\n                 copy=False):\n        \"\"\"One-dimensional ndarray with axis labels (including time\nseries). Labels must be unique and can any hashable type. The object supports\nboth integer- and label-based indexing and provides a host of methods for\nperforming operations involving the index. Statistical methods from ndarray have\nbeen overridden to automatically exclude missing data (currently represented as\nNaN)\n\nOperations between Series (+, -, /, *, **) align values based on their\nassociated index values-- they need not be the same length. The result\nindex will be the sorted union of the two indexes.\n\nParameters\n----------\ndata : array-like, dict, or scalar value\n    Contains data stored in Series\nindex : array-like or Index (1d)\n\n    Values must be unique and hashable, same length as data. Index object\n    (or other iterable of same length as data) Will default to\n    np.arange(len(data)) if not provided. If both a dict and index sequence\n    are used, the index will override the keys found in the dict.\n\ndtype : numpy.dtype or None\n    If None, dtype will be inferred copy : boolean, default False Copy\n    input data\ncopy : boolean, default False\n        \"\"\"\n        pass\n\n    @property\n    def _constructor(self):\n        return Series\n\n    def __hash__(self):\n        raise TypeError('unhashable type')\n\n    _index = None\n    index = lib.SeriesIndex()\n\n    # def _get_index(self):\n    #     return self._index\n\n    # def _set_index(self, index):\n    #     if not isinstance(index, _INDEX_TYPES):\n    #         raise TypeError(\"Expected index to be in %s; was %s.\"\n    #                         % (_INDEX_TYPES, type(index)))\n\n    #     if len(self) != len(index):\n    #         raise AssertionError('Lengths of index and values did not match!')\n\n    #     self._index = _ensure_index(index)\n\n    # index = property(fget=_get_index, fset=_set_index)\n\n    def __array_finalize__(self, obj):\n        \"\"\"\n        Gets called after any ufunc or other array operations, necessary\n        to pass on the index.\n        \"\"\"\n        self._index = getattr(obj, '_index', None)\n        self.name = getattr(obj, 'name', None)\n\n    def __contains__(self, key):\n        return key in self.index\n\n    def __reduce__(self):\n        \"\"\"Necessary for making this object picklable\"\"\"\n        object_state = list(ndarray.__reduce__(self))\n        subclass_state = (self.index, self.name)\n        object_state[2] = (object_state[2], subclass_state)\n        return tuple(object_state)\n\n    def __setstate__(self, state):\n        \"\"\"Necessary for making this object picklable\"\"\"\n        nd_state, own_state = state\n        ndarray.__setstate__(self, nd_state)\n\n        # backwards compat\n        index, name = own_state[0], None\n        if len(own_state) > 1:\n            name = own_state[1]\n\n        self.index = index\n        self.name = name\n\n    _ix = None\n\n    @property\n    def ix(self):\n        if self._ix is None:\n            self._ix = _SeriesIndexer(self)\n\n        return self._ix\n\n    def __getitem__(self, key):\n        index = self.index\n\n        # Label-based\n        try:\n            return index._engine.get_value(self, key)\n        except KeyError, e1:\n            if isinstance(index, MultiIndex):\n                values = self.values\n                try:\n                    loc = index.get_loc(key)\n                    # TODO: what if a level contains tuples??\n                    new_index = index[loc]\n                    new_index = _maybe_droplevels(new_index, key)\n                    return Series(values[loc], index=new_index,\n                                  name=self.name)\n                except KeyError:\n                    pass\n\n            if index.inferred_type == 'integer':\n                raise\n\n            try:\n                return _gin.get_value_at(self, key)\n            except IndexError:\n                raise\n            except Exception, _:\n                pass\n            raise e1\n        except TypeError:\n            pass\n\n        # boolean\n\n        # special handling of boolean data with NAs stored in object\n        # arrays. Since we can't represent NA with dtype=bool\n        if _is_bool_indexer(key):\n            key = self._check_bool_indexer(key)\n            key = np.asarray(key, dtype=bool)\n\n        return self._index_with(key)\n\n    def _index_with(self, key):\n        # other: fancy integer or otherwise\n        # [slice(0, 5, None)] will break if you convert to ndarray,\n        # e.g. as requested by np.median\n\n        try:\n            return Series(self.values[key], index=self.index[key],\n                          name=self.name)\n        except Exception:\n            return self.values[key]\n\n    def get(self, label, default=None):\n        \"\"\"\n        Returns value occupying requested label, default to specified\n        missing value if not present. Analogous to dict.get\n\n        Parameters\n        ----------\n        label : object\n            Label value looking for\n        default : object, optional\n            Value to return if label not in index\n\n        Returns\n        -------\n        y : scalar\n        \"\"\"\n        try:\n            return self.get_value(label)\n        except KeyError:\n            return default\n\n    def get_value(self, label):\n        \"\"\"\n        Quickly retrieve single value at passed index label\n\n        Parameters\n        ----------\n        index : label\n\n        Returns\n        -------\n        value : scalar value\n        \"\"\"\n        return self.index._engine.get_value(self, label)\n\n    def set_value(self, label, value):\n        \"\"\"\n        Quickly set single value at passed label. If label is not contained, a\n        new object is created with the label placed at the end of the result\n        index\n\n        Parameters\n        ----------\n        label : object\n            Partial indexing with MultiIndex not allowed\n        value : object\n            Scalar value\n\n        Returns\n        -------\n        series : Series\n            If label is contained, will be reference to calling Series,\n            otherwise a new object\n        \"\"\"\n        try:\n            self.index._engine.set_value(self, label, value)\n            return self\n        except KeyError:\n            new_index = np.concatenate([self.index.values, [label]])\n            new_values = np.concatenate([self.values, [value]])\n            return Series(new_values, index=new_index, name=self.name)\n\n    # help out SparseSeries\n    _get_val_at = ndarray.__getitem__\n\n    def __getslice__(self, i, j):\n        if i < 0:\n            i = 0\n        if j < 0:\n            j = 0\n        slobj = slice(i, j)\n        return self.__getitem__(slobj)\n\n    def __setitem__(self, key, value):\n        values = self.values\n        try:\n            values[self.index.get_loc(key)] = value\n            return\n        except KeyError:\n            if (com.is_integer(key)\n                and not self.index.inferred_type == 'integer'):\n\n                values[key] = value\n                return\n\n            raise KeyError('%s not in this series!' % str(key))\n        except TypeError:\n            # Could not hash item\n            pass\n\n        key = self._check_bool_indexer(key)\n\n        # special handling of boolean data with NAs stored in object\n        # arrays. Sort of an elaborate hack since we can't represent boolean\n        # NA. Hmm\n        if isinstance(key, np.ndarray) and key.dtype == np.object_:\n            if set([True, False]).issubset(set(key)):\n                key = np.asarray(key, dtype=bool)\n                values[key] = value\n                return\n\n        values[key] = value\n\n    def _check_bool_indexer(self, key):\n        # boolean indexing, need to check that the data are aligned, otherwise\n        # disallowed\n        result = key\n        if isinstance(key, Series) and key.dtype == np.bool_:\n            if not key.index.equals(self.index):\n                result = key.reindex(self.index)\n\n        if isinstance(result, np.ndarray) and result.dtype == np.object_:\n            mask = isnull(result)\n            if mask.any():\n                raise ValueError('cannot index with vector containing '\n                                 'NA / NaN values')\n\n        return result\n\n    def __setslice__(self, i, j, value):\n        \"\"\"Set slice equal to given value(s)\"\"\"\n        ndarray.__setslice__(self, i, j, value)\n\n    def __repr__(self):\n        \"\"\"Clean string representation of a Series\"\"\"\n        width, height = get_terminal_size()\n        max_rows = (height if com.print_config.max_rows == 0\n                    else com.print_config.max_rows)\n        if len(self.index) > max_rows:\n            result = self._tidy_repr(min(30, max_rows - 4))\n        elif len(self.index) > 0:\n            result = self._get_repr(print_header=True,\n                                    length=len(self) > 50,\n                                    name=True)\n        else:\n            result = '%s' % ndarray.__repr__(self)\n\n        return result\n\n    def _tidy_repr(self, max_vals=20):\n        num = max_vals // 2\n        head = self[:num]._get_repr(print_header=True, length=False,\n                                    name=False)\n        tail = self[-(max_vals - num):]._get_repr(print_header=False,\n                                                  length=False,\n                                                  name=False)\n        result = head + '\\n...\\n' + tail\n        namestr = \"Name: %s, \" % str(self.name) if self.name else \"\"\n        result = '%s\\n%sLength: %d' % (result, namestr, len(self))\n        return result\n\n    def to_string(self, buf=None, na_rep='NaN', float_format=None,\n                  nanRep=None, length=False, name=False):\n        if nanRep is not None:  # pragma: no cover\n            import warnings\n            warnings.warn(\"nanRep is deprecated, use na_rep\",\n                          FutureWarning)\n            na_rep = nanRep\n\n        the_repr = self._get_repr(float_format=float_format, na_rep=na_rep,\n                                  length=length, name=name)\n        if buf is None:\n            return the_repr\n        else:\n            print >> buf, the_repr\n\n    def _get_repr(self, name=False, print_header=False, length=True,\n                  na_rep='NaN', float_format=None):\n        if len(self) == 0:\n            return ''\n\n        vals = self.values\n        index = self.index\n\n        is_multi = isinstance(index, MultiIndex)\n        if is_multi:\n            have_header = any(name for name in index.names)\n            string_index = index.format(names=True)\n            header, string_index = string_index[0], string_index[1:]\n        else:\n            have_header = index.name is not None\n            header = str(index.name)\n            string_index = index.format()\n\n        maxlen = max(len(x) for x in string_index)\n        padSpace = min(maxlen, 60)\n\n        if float_format is None:\n            float_format = com.print_config.float_format\n            if float_format is None:\n                float_format = com._float_format_default\n\n        def _format(k, v, extra=0):\n            # GH #490\n            if not isinstance(v, np.ndarray) and isnull(v):\n                v = na_rep\n            if com.is_float(v):\n                v = float_format(v)\n            strv = ' ' * extra + str(v).replace('\\n', ' ')\n            return '%s    %s' % (str(k).ljust(padSpace), strv)\n\n        # floating point handling\n        if self.dtype == 'O':\n            is_float = (self.map(com.is_float) & self.notnull()).values\n            leading_space = is_float.any()\n\n            res = []\n            for i, (k, v) in enumerate(izip(string_index, vals)):\n                if not is_float[i] and leading_space:\n                    res.append(_format(k, v, extra=1))\n                else:\n                    res.append(_format(k, v))\n        else:\n            res = [_format(idx, v) for idx, v in izip(string_index, vals)]\n\n        if print_header and have_header:\n            res.insert(0, header)\n\n        footer = ''\n        if name:\n            footer += \"Name: %s\" % str(self.name) if self.name else ''\n\n        if length:\n            if footer:\n                footer += ', '\n            footer += 'Length: %d' % len(self)\n\n        if footer:\n            res.append(footer)\n\n        return '\\n'.join(res)\n\n    def __str__(self):\n        return repr(self)\n\n    def __iter__(self):\n        return iter(self.values)\n\n    def iteritems(self):\n        \"\"\"\n        Lazily iterate over (index, value) tuples\n        \"\"\"\n        return izip(iter(self.index), iter(self))\n\n    iterkv = iteritems\n    if py3compat.PY3:  # pragma: no cover\n        items = iteritems\n\n    #----------------------------------------------------------------------\n    #   Arithmetic operators\n\n    __add__ = _arith_method(operator.add, '__add__')\n    __sub__ = _arith_method(operator.sub, '__sub__')\n    __mul__ = _arith_method(operator.mul, '__mul__')\n    __truediv__ = _arith_method(operator.truediv, '__truediv__')\n    __floordiv__ = _arith_method(operator.floordiv, '__floordiv__')\n    __pow__ = _arith_method(operator.pow, '__pow__')\n\n    __radd__ = _arith_method(lambda x, y: y + x, '__add__')\n    __rmul__ = _arith_method(operator.mul, '__mul__')\n    __rsub__ = _arith_method(lambda x, y: y - x, '__sub__')\n    __rtruediv__ = _arith_method(lambda x, y: y / x, '__truediv__')\n    __rfloordiv__ = _arith_method(lambda x, y: y // x, '__floordiv__')\n    __rpow__ = _arith_method(lambda x, y: y ** x, '__pow__')\n\n    # Inplace operators\n    __iadd__ = __add__\n    __isub__ = __sub__\n    __imul__ = __mul__\n    __itruediv__ = __truediv__\n    __ifloordiv__ = __floordiv__\n    __ipow__ = __pow__\n\n    # Python 2 division operators\n    if not py3compat.PY3:\n        __div__ = _arith_method(operator.div, '__div__')\n        __rdiv__ = _arith_method(lambda x, y: y / x, '__div__')\n        __idiv__ = __div__\n\n    #----------------------------------------------------------------------\n    # unbox reductions\n\n    all = _unbox(np.ndarray.all)\n    any = _unbox(np.ndarray.any)\n\n    #----------------------------------------------------------------------\n    # Misc public methods\n\n    def keys(self):\n        \"Alias for index\"\n        return self.index\n\n    # alas, I wish this worked\n    # values = lib.ValuesProperty()\n\n    @property\n    def values(self):\n        \"\"\"\n        Return Series as ndarray\n\n        Returns\n        -------\n        arr : numpy.ndarray\n        \"\"\"\n        return self.view(ndarray)\n\n    def copy(self):\n        \"\"\"\n        Return new Series with copy of underlying values\n\n        Returns\n        -------\n        cp : Series\n        \"\"\"\n        return Series(self.values.copy(), index=self.index, name=self.name)\n\n    def to_dict(self):\n        \"\"\"\n        Convert Series to {label -> value} dict\n\n        Returns\n        -------\n        value_dict : dict\n        \"\"\"\n        return dict(self.iteritems())\n\n    def to_sparse(self, kind='block', fill_value=None):\n        \"\"\"\n        Convert Series to SparseSeries\n\n        Parameters\n        ----------\n        kind : {'block', 'integer'}\n        fill_value : float, defaults to NaN (missing)\n\n        Returns\n        -------\n        sp : SparseSeries\n        \"\"\"\n        from pandas.core.sparse import SparseSeries\n        return SparseSeries(self, kind=kind, fill_value=fill_value,\n                            name=self.name)\n\n    def head(self, n=5):\n        \"\"\"Returns first n rows of Series\n        \"\"\"\n        return self[:n]\n\n    def tail(self, n=5):\n        \"\"\"Returns last n rows of Series\n        \"\"\"\n        return self[-n:]\n\n    #----------------------------------------------------------------------\n    # Statistics, overridden ndarray methods\n\n    # TODO: integrate bottleneck\n\n    def count(self, level=None):\n        \"\"\"\n        Return number of non-NA/null observations in the Series\n\n        Parameters\n        ----------\n        level : int, default None\n            If the axis is a MultiIndex (hierarchical), count along a\n            particular level, collapsing into a smaller Series\n\n        Returns\n        -------\n        nobs : int or Series (if level specified)\n        \"\"\"\n        if level is not None:\n            mask = notnull(self.values)\n            level_index = self.index.levels[level]\n\n            if len(self) == 0:\n                return Series(0, index=level_index)\n\n            # call cython function\n            max_bin = len(level_index)\n            counts = lib.count_level_1d(mask.view(np.uint8),\n                                        self.index.labels[level], max_bin)\n            return Series(counts, index=level_index)\n\n        return notnull(self.values).sum()\n\n    def value_counts(self):\n        \"\"\"\n        Returns Series containing counts of unique values. The resulting Series\n        will be in descending order so that the first element is the most\n        frequently-occurring element. Excludes NA values\n\n        Returns\n        -------\n        counts : Series\n        \"\"\"\n        from collections import defaultdict\n        counter = defaultdict(lambda: 0)\n        for value in self.dropna().values:\n            counter[value] += 1\n        return Series(counter).order(ascending=False)\n\n    def nunique(self):\n        \"\"\"\n        Return count of unique elements in the Series\n\n        Returns\n        -------\n        nunique : int\n        \"\"\"\n        return len(self.value_counts())\n\n    @Substitution(name='sum', shortname='sum', na_action=_doc_exclude_na,\n                  extras=_doc_ndarray_interface)\n    @Appender(_stat_doc)\n    def sum(self, axis=0, dtype=None, out=None, skipna=True, level=None):\n        if level is not None:\n            return self._agg_by_level('sum', level=level, skipna=skipna)\n        return nanops.nansum(self.values, skipna=skipna)\n\n    @Substitution(name='mean', shortname='mean', na_action=_doc_exclude_na,\n                  extras=_doc_ndarray_interface)\n    @Appender(_stat_doc)\n    def mean(self, axis=0, dtype=None, out=None, skipna=True, level=None):\n        if level is not None:\n            return self._agg_by_level('mean', level=level, skipna=skipna)\n        return nanops.nanmean(self.values, skipna=skipna)\n\n    @Substitution(name='mean absolute deviation', shortname='mad',\n                  na_action=_doc_exclude_na, extras='')\n    @Appender(_stat_doc)\n    def mad(self, skipna=True, level=None):\n        if level is not None:\n            return self._agg_by_level('mad', level=level, skipna=skipna)\n\n        demeaned = self - self.mean(skipna=skipna)\n        return np.abs(demeaned).mean(skipna=skipna)\n\n    @Substitution(name='median', shortname='median',\n                  na_action=_doc_exclude_na, extras='')\n    @Appender(_stat_doc)\n    def median(self, skipna=True, level=None):\n        if level is not None:\n            return self._agg_by_level('median', level=level, skipna=skipna)\n        return nanops.nanmedian(self.values, skipna=skipna)\n\n    @Substitution(name='product', shortname='product',\n                  na_action=_doc_exclude_na, extras='')\n    @Appender(_stat_doc)\n    def prod(self, axis=None, dtype=None, out=None, skipna=True, level=None):\n        if level is not None:\n            return self._agg_by_level('prod', level=level, skipna=skipna)\n        return nanops.nanprod(self.values, skipna=skipna)\n\n    @Substitution(name='minimum', shortname='min',\n                  na_action=_doc_exclude_na, extras='')\n    @Appender(_stat_doc)\n    def min(self, axis=None, out=None, skipna=True, level=None):\n        if level is not None:\n            return self._agg_by_level('min', level=level, skipna=skipna)\n        return nanops.nanmin(self.values, skipna=skipna)\n\n    @Substitution(name='maximum', shortname='max',\n                  na_action=_doc_exclude_na, extras='')\n    @Appender(_stat_doc)\n    def max(self, axis=None, out=None, skipna=True, level=None):\n        if level is not None:\n            return self._agg_by_level('max', level=level, skipna=skipna)\n        return nanops.nanmax(self.values, skipna=skipna)\n\n    @Substitution(name='unbiased standard deviation', shortname='stdev',\n                  na_action=_doc_exclude_na, extras='')\n    @Appender(_stat_doc)\n    def std(self, axis=None, dtype=None, out=None, ddof=1, skipna=True,\n            level=None):\n        if level is not None:\n            return self._agg_by_level('std', level=level, skipna=skipna)\n        return np.sqrt(nanops.nanvar(self.values, skipna=skipna))\n\n    @Substitution(name='unbiased variance', shortname='var',\n                  na_action=_doc_exclude_na, extras='')\n    @Appender(_stat_doc)\n    def var(self, axis=None, dtype=None, out=None, ddof=1, skipna=True,\n            level=None):\n        if level is not None:\n            return self._agg_by_level('var', level=level, skipna=skipna)\n        return nanops.nanvar(self.values, skipna=skipna)\n\n    @Substitution(name='unbiased skewness', shortname='skew',\n                  na_action=_doc_exclude_na, extras='')\n    @Appender(_stat_doc)\n    def skew(self, skipna=True, level=None):\n        if level is not None:\n            return self._agg_by_level('skew', level=level, skipna=skipna)\n\n        return nanops.nanskew(self.values, skipna=skipna)\n\n    def _agg_by_level(self, name, level=0, skipna=True):\n        grouped = self.groupby(level=level)\n        if hasattr(grouped, name) and skipna:\n            return getattr(grouped, name)()\n        method = getattr(type(self), name)\n        applyf = lambda x: method(x, skipna=skipna)\n        return grouped.aggregate(applyf)\n\n    def idxmin(self, axis=None, out=None, skipna=True):\n        \"\"\"\n        Index of first occurence of minimum of values.\n\n        Parameters\n        ----------\n        skipna : boolean, default True\n            Exclude NA/null values\n\n        Returns\n        -------\n        idxmin : Index of mimimum of values\n        \"\"\"\n        i = nanops.nanargmin(self.values, skipna=skipna)\n        if i == -1:\n            return np.nan\n        return self.index[i]\n\n    def idxmax(self, axis=None, out=None, skipna=True):\n        \"\"\"\n        Index of first occurence of maximum of values.\n\n        Parameters\n        ----------\n        skipna : boolean, default True\n            Exclude NA/null values\n\n        Returns\n        -------\n        idxmax : Index of mimimum of values\n        \"\"\"\n        i = nanops.nanargmax(self.values, skipna=skipna)\n        if i == -1:\n            return np.nan\n        return self.index[i]\n\n    def cumsum(self, axis=0, dtype=None, out=None, skipna=True):\n        \"\"\"\n        Cumulative sum of values. Preserves locations of NaN values\n\n        Extra parameters are to preserve ndarray interface.\n\n        Parameters\n        ----------\n        skipna : boolean, default True\n            Exclude NA/null values\n\n        Returns\n        -------\n        cumsum : Series\n        \"\"\"\n        arr = self.values.copy()\n\n        do_mask = skipna and not issubclass(self.dtype.type, np.integer)\n        if do_mask:\n            mask = isnull(arr)\n            np.putmask(arr, mask, 0.)\n\n        result = arr.cumsum()\n\n        if do_mask:\n            np.putmask(result, mask, np.nan)\n\n        return Series(result, index=self.index)\n\n    def cumprod(self, axis=0, dtype=None, out=None, skipna=True):\n        \"\"\"\n        Cumulative product of values. Preserves locations of NaN values\n\n        Extra parameters are to preserve ndarray interface.\n\n        Parameters\n        ----------\n        skipna : boolean, default True\n            Exclude NA/null values\n\n        Returns\n        -------\n        cumprod : Series\n        \"\"\"\n        arr = self.values.copy()\n\n        do_mask = skipna and not issubclass(self.dtype.type, np.integer)\n        if do_mask:\n            mask = isnull(arr)\n            np.putmask(arr, mask, 1.)\n\n        result = arr.cumprod()\n\n        if do_mask:\n            np.putmask(result, mask, np.nan)\n\n        return Series(result, index=self.index)\n\n    @Appender(np.ndarray.round.__doc__)\n    def round(self, decimals=0, out=None):\n        \"\"\"\n\n        \"\"\"\n        result = self.values.round(decimals, out=out)\n        if out is None:\n            result = Series(result, index=self.index, name=self.name)\n\n        return result\n\n    def quantile(self, q=0.5):\n        \"\"\"\n        Return value at the given quantile, a la scoreatpercentile in\n        scipy.stats\n\n        Parameters\n        ----------\n        q : quantile\n            0 <= q <= 1\n\n        Returns\n        -------\n        quantile : float\n        \"\"\"\n        from scipy.stats import scoreatpercentile\n        valid_values = self.dropna().values\n        if len(valid_values) == 0:\n            return np.nan\n        return scoreatpercentile(valid_values, q * 100)\n\n    def describe(self):\n        \"\"\"\n        Generate various summary statistics of Series, excluding NaN\n        values. These include: count, mean, std, min, max, and 10%/50%/90%\n        quantiles\n\n        Returns\n        -------\n        desc : Series\n        \"\"\"\n        try:\n            from collections import Counter\n        except ImportError:  # pragma: no cover\n            # For Python < 2.7, we include a local copy of this:\n            from pandas.util.counter import Counter\n\n        if self.dtype == object:\n            names = ['count', 'unique', 'top', 'freq']\n\n            objcounts = Counter(self.dropna().values)\n            top, freq = objcounts.most_common(1)[0]\n            data = [self.count(), len(objcounts), top, freq]\n\n        else:\n            names = ['count', 'mean', 'std', 'min',\n                     '25%', '50%', '75%', 'max']\n\n            data = [self.count(), self.mean(), self.std(), self.min(),\n                    self.quantile(.25), self.median(), self.quantile(.75),\n                    self.max()]\n\n        return Series(data, index=names)\n\n    def corr(self, other, method='pearson'):\n        \"\"\"\n        Compute correlation two Series, excluding missing values\n\n        Parameters\n        ----------\n        other : Series\n        method : {'pearson', 'kendall', 'spearman'}\n            pearson : standard correlation coefficient\n            kendall : Kendall Tau correlation coefficient\n            spearman : Spearman rank correlation\n\n        Returns\n        -------\n        correlation : float\n        \"\"\"\n        this, other = self.align(other, join='inner', copy=False)\n        return nanops.nancorr(this.values, other.values, method=method)\n\n    def cov(self, other):\n        \"\"\"\n        Compute covariance with Series, excluding missing values\n\n        Parameters\n        ----------\n        other : Series\n\n        Returns\n        -------\n        covariance : float\n        \"\"\"\n        this, other = self.align(other, join='inner')\n        if len(this) == 0:\n            return np.nan\n        return nanops.nancov(this.values, other.values)\n\n    def diff(self, periods=1):\n        \"\"\"\n        1st discrete difference of object\n\n        Parameters\n        ----------\n        periods : int, default 1\n            Periods to shift for forming difference\n\n        Returns\n        -------\n        diffed : Series\n        \"\"\"\n        return (self - self.shift(periods))\n\n    def autocorr(self):\n        \"\"\"\n        Lag-1 autocorrelation\n\n        Returns\n        -------\n        autocorr : float\n        \"\"\"\n        return self.corr(self.shift(1))\n\n    def clip(self, lower=None, upper=None, out=None):\n        \"\"\"\n        Trim values at input threshold(s)\n\n        Parameters\n        ----------\n        lower : float, default None\n        upper : float, default None\n\n        Returns\n        -------\n        clipped : Series\n        \"\"\"\n        if out is not None:  # pragma: no cover\n            raise Exception('out argument is not supported yet')\n\n        result = self\n        if lower is not None:\n            result = result.clip_lower(lower)\n        if upper is not None:\n            result = result.clip_upper(upper)\n\n        return result\n\n    def clip_upper(self, threshold):\n        \"\"\"\n        Return copy of series with values above given value truncated\n\n        See also\n        --------\n        clip\n\n        Returns\n        -------\n        clipped : Series\n        \"\"\"\n        return np.where(self > threshold, threshold, self)\n\n    def clip_lower(self, threshold):\n        \"\"\"\n        Return copy of series with values below given value truncated\n\n        See also\n        --------\n        clip\n\n        Returns\n        -------\n        clipped : Series\n        \"\"\"\n        return np.where(self < threshold, threshold, self)\n\n#-------------------------------------------------------------------------------\n# Combination\n\n    def append(self, to_append):\n        \"\"\"\n        Concatenate two or more Series. The indexes must not overlap\n\n        Parameters\n        ----------\n        to_append : Series or list/tuple of Series\n\n        Returns\n        -------\n        appended : Series\n        \"\"\"\n        from pandas.tools.merge import concat\n        if isinstance(to_append, (list, tuple)):\n            to_concat = [self] + to_append\n        else:\n            to_concat = [self, to_append]\n        return concat(to_concat, ignore_index=False, verify_integrity=True)\n\n    def _binop(self, other, func, level=None, fill_value=None):\n        \"\"\"\n        Perform generic binary operation with optional fill value\n\n        Parameters\n        ----------\n        other : Series\n        func : binary operator\n        fill_value : float or object\n            Value to substitute for NA/null values. If both Series are NA in a\n            location, the result will be NA regardless of the passed fill value\n        level : int or name\n            Broadcast across a level, matching Index values on the\n            passed MultiIndex level\n\n        Returns\n        -------\n        combined : Series\n        \"\"\"\n        assert(isinstance(other, Series))\n\n        new_index = self.index\n        this = self\n\n        if not self.index.equals(other.index):\n            this, other = self.align(other, level=level, join='outer')\n            new_index = this.index\n\n        this_vals = this.values\n        other_vals = other.values\n\n        if fill_value is not None:\n            this_mask = isnull(this_vals)\n            other_mask = isnull(other_vals)\n            this_vals = this_vals.copy()\n            other_vals = other_vals.copy()\n\n            # one but not both\n            mask = this_mask ^ other_mask\n            this_vals[this_mask & mask] = fill_value\n            other_vals[other_mask & mask] = fill_value\n\n        result = func(this_vals, other_vals)\n        name = _maybe_match_name(self, other)\n        return Series(result, index=new_index, name=name)\n\n    add = _flex_method(operator.add, 'add')\n    sub = _flex_method(operator.sub, 'subtract')\n    mul = _flex_method(operator.mul, 'multiply')\n    try:\n        div = _flex_method(operator.div, 'divide')\n    except AttributeError:  # pragma: no cover\n        # Python 3\n        div = _flex_method(operator.truediv, 'divide')\n\n    def combine(self, other, func, fill_value=nan):\n        \"\"\"\n        Perform elementwise binary operation on two Series using given function\n        with optional fill value when an index is missing from one Series or\n        the other\n\n        Parameters\n        ----------\n        other : Series or scalar value\n        func : function\n        fill_value : scalar value\n\n        Returns\n        -------\n        result : Series\n        \"\"\"\n        if isinstance(other, Series):\n            new_index = self.index + other.index\n            new_name = _maybe_match_name(self, other)\n            new_values = np.empty(len(new_index), dtype=self.dtype)\n            for i, idx in enumerate(new_index):\n                lv = self.get(idx, fill_value)\n                rv = other.get(idx, fill_value)\n                new_values[i] = func(lv, rv)\n        else:\n            new_index = self.index\n            new_values = func(self.values, other)\n            new_name = self.name\n        return Series(new_values, index=new_index, name=new_name)\n\n    def combine_first(self, other):\n        \"\"\"\n        Combine Series values, choosing the calling Series's values\n        first. Result index will be the union of the two indexes\n\n        Parameters\n        ----------\n        other : Series\n\n        Returns\n        -------\n        y : Series\n        \"\"\"\n        new_index = self.index + other.index\n        this = self.reindex(new_index, copy=False)\n        other = other.reindex(new_index, copy=False)\n        name = _maybe_match_name(self, other)\n        return Series(np.where(isnull(this), other, this), index=new_index,\n                      name=name)\n\n    #----------------------------------------------------------------------\n    # Reindexing, sorting\n\n    def sort(self, axis=0, kind='quicksort', order=None):\n        \"\"\"\n        Sort values and index labels in place, for compatibility with\n        ndarray. No return value\n        \"\"\"\n        sortedSeries = self.order(na_last=True)\n\n        true_base = self\n        while true_base.base is not None:\n            true_base = true_base.base\n\n        if (true_base is not None and\n            (true_base.ndim != 1 or true_base.shape != self.shape)):\n            raise Exception('This Series is a view of some other array, to '\n                            'sort in-place you must create a copy')\n\n        self[:] = sortedSeries\n        self.index = sortedSeries.index\n\n    def sort_index(self, ascending=True):\n        \"\"\"\n        Sort object by labels (along an axis)\n\n        Parameters\n        ----------\n        ascending : boolean, default True\n            Sort ascending vs. descending\n\n        Returns\n        -------\n        sorted_obj : Series\n        \"\"\"\n        labels = self.index\n        sort_index = labels.argsort()\n        if not ascending:\n            sort_index = sort_index[::-1]\n        new_labels = labels.take(sort_index)\n        new_values = self.values.take(sort_index)\n        return Series(new_values, new_labels, name=self.name)\n\n    def argsort(self, axis=0, kind='quicksort', order=None):\n        \"\"\"\n        Overrides ndarray.argsort. Argsorts the value, omitting NA/null values,\n        and places the result in the same locations as the non-NA values\n\n        Returns\n        -------\n        argsorted : Series\n        \"\"\"\n        values = self.values\n        mask = isnull(values)\n\n        if mask.any():\n            result = values.copy()\n            notmask = -mask\n            result[notmask] = np.argsort(values[notmask])\n            return Series(result, index=self.index, name=self.name)\n        else:\n            return Series(np.argsort(values), index=self.index, name=self.name)\n\n    def rank(self):\n        \"\"\"\n        Compute data ranks (1 through n). Equal values are assigned a rank that\n        is the average of the ranks of those values\n\n        Returns\n        -------\n        ranks : Series\n        \"\"\"\n        try:\n            ranks = lib.rank_1d_float64(self.values)\n        except Exception:\n            ranks = lib.rank_1d_generic(self.values)\n        return Series(ranks, index=self.index, name=self.name)\n\n    def order(self, na_last=True, ascending=True):\n        \"\"\"\n        Sorts Series object, by value, maintaining index-value link\n\n        Parameters\n        ----------\n        na_last : boolean (optional, default=True)\n            Put NaN's at beginning or end\n        ascending : boolean, default True\n            Sort ascending. Passing False sorts descending\n\n        Returns\n        -------\n        y : Series\n        \"\"\"\n        def _try_mergesort(arr):\n            # easier to ask forgiveness than permission\n            try:\n                return arr.argsort(kind='mergesort')\n            except TypeError:\n                # stable sort not available for object dtype\n                return arr.argsort()\n\n        arr = self.values\n        sortedIdx = np.empty(len(self), dtype=np.int32)\n\n        bad = isnull(arr)\n\n        good = -bad\n        idx = np.arange(len(self))\n\n        argsorted = _try_mergesort(arr[good])\n\n        if not ascending:\n            argsorted = argsorted[::-1]\n\n        if na_last:\n            n = good.sum()\n            sortedIdx[:n] = idx[good][argsorted]\n            sortedIdx[n:] = idx[bad]\n        else:\n            n = bad.sum()\n            sortedIdx[n:] = idx[good][argsorted]\n            sortedIdx[:n] = idx[bad]\n\n        return Series(arr[sortedIdx], index=self.index[sortedIdx],\n                      name=self.name)\n\n    def sortlevel(self, level=0, ascending=True):\n        \"\"\"\n        Sort Series with MultiIndex by chosen level. Data will be\n        lexicographically sorted by the chosen level followed by the other\n        levels (in order)\n\n        Parameters\n        ----------\n        level : int\n        ascending : bool, default True\n\n        Returns\n        -------\n        sorted : Series\n        \"\"\"\n        if not isinstance(self.index, MultiIndex):\n            raise Exception('can only sort by level with a hierarchical index')\n\n        new_index, indexer = self.index.sortlevel(level, ascending=ascending)\n        new_values = self.values.take(indexer)\n        return Series(new_values, index=new_index, name=self.name)\n\n    def swaplevel(self, i, j, copy=True):\n        \"\"\"\n        Swap levels i and j in a MultiIndex\n\n        Returns\n        -------\n        swapped : Series\n        \"\"\"\n        new_index = self.index.swaplevel(i, j)\n        return Series(self.values, index=new_index, copy=copy, name=self.name)\n\n    def reorder_levels(self, order):\n        \"\"\"\n        Rearrange index levels using input order. May not drop or duplicate\n        levels\n\n        Parameters\n        ----------\n        order: list of int representing new level order.\n               (reference level by number not by key)\n        axis: where to reorder levels\n\n        Returns\n        -------\n        type of caller (new object)\n        \"\"\"\n        if not isinstance(self.index, MultiIndex):  # pragma: no cover\n            raise Exception('Can only reorder levels on a hierarchical axis.')\n\n        result = self.copy()\n        result.index = result.index.reorder_levels(order)\n        return result\n\n    def unstack(self, level=-1):\n        \"\"\"\n        Unstack, a.k.a. pivot, Series with MultiIndex to produce DataFrame\n\n        Parameters\n        ----------\n        level : int, string, or list of these, default last level\n            Level(s) to unstack, can pass level name\n\n        Examples\n        --------\n        >>> s\n        one  a   1.\n        one  b   2.\n        two  a   3.\n        two  b   4.\n\n        >>> s.unstack(level=-1)\n             a   b\n        one  1.  2.\n        two  3.  4.\n\n        >>> s.unstack(level=0)\n           one  two\n        a  1.   2.\n        b  3.   4.\n\n        Returns\n        -------\n        unstacked : DataFrame\n        \"\"\"\n        from pandas.core.reshape import unstack\n        if isinstance(level, (tuple, list)):\n            result = self\n            for lev in level:\n                result = unstack(result, lev)\n            return result\n        else:\n            return unstack(self, level)\n\n    #----------------------------------------------------------------------\n    # function application\n\n    def map(self, arg):\n        \"\"\"\n        Map values of Series using input correspondence (which can be\n        a dict, Series, or function)\n\n        Parameters\n        ----------\n        arg : function, dict, or Series\n\n        Examples\n        --------\n        >>> x\n        one   1\n        two   2\n        three 3\n\n        >>> y\n        1  foo\n        2  bar\n        3  baz\n\n        >>> x.map(y)\n        one   foo\n        two   bar\n        three baz\n\n        Returns\n        -------\n        y : Series\n            same index as caller\n        \"\"\"\n        if isinstance(arg, (dict, Series)):\n            if isinstance(arg, dict):\n                arg = Series(arg)\n\n            indexer = lib.merge_indexer_object(self.values.astype(object),\n                                               arg.index.indexMap)\n\n            new_values = com.take_1d(np.asarray(arg), indexer)\n            return Series(new_values, index=self.index, name=self.name)\n        else:\n            mapped = lib.map_infer(self.values, arg)\n            return Series(mapped, index=self.index, name=self.name)\n\n    def apply(self, func):\n        \"\"\"\n        Invoke function on values of Series. Can be ufunc or Python function\n        expecting only single values\n\n        Parameters\n        ----------\n        func : function\n\n        Returns\n        -------\n        y : Series\n        \"\"\"\n        try:\n            result = func(self)\n            if not isinstance(result, Series):\n                result = Series(result, index=self.index, name=self.name)\n            return result\n        except Exception:\n            mapped = lib.map_infer(self.values, func)\n            return Series(mapped, index=self.index, name=self.name)\n\n    def align(self, other, join='outer', level=None, copy=True):\n        \"\"\"\n        Align two Series object with the specified join method\n\n        Parameters\n        ----------\n        other : Series\n        join : {'outer', 'inner', 'left', 'right'}, default 'outer'\n        level : int or name\n            Broadcast across a level, matching Index values on the\n            passed MultiIndex level\n        copy : boolean, default True\n            Always return new objects. If copy=False and no reindexing is\n            required, the same object will be returned (for better performance)\n\n        Returns\n        -------\n        (left, right) : (Series, Series)\n            Aligned Series\n        \"\"\"\n        join_index, lidx, ridx = self.index.join(other.index, how=join,\n                                                 level=level,\n                                                 return_indexers=True)\n\n        left = self._reindex_indexer(join_index, lidx, copy)\n        right = other._reindex_indexer(join_index, ridx, copy)\n        return left, right\n\n    def _reindex_indexer(self, new_index, indexer, copy):\n        if indexer is not None:\n            new_values = com.take_1d(self.values, indexer)\n        else:\n            if copy:\n                return self.copy()\n            else:\n                return self\n\n        # be subclass-friendly\n        return self._constructor(new_values, new_index, name=self.name)\n\n    def reindex(self, index=None, method=None, level=None, copy=True):\n        \"\"\"Conform Series to new index with optional filling logic, placing\n        NA/NaN in locations having no value in the previous index. A new object\n        is produced unless the new index is equivalent to the current one and\n        copy=False\n\n        Parameters\n        ----------\n        index : array-like or Index\n            New labels / index to conform to. Preferably an Index object to\n            avoid duplicating data\n        method : {'backfill', 'bfill', 'pad', 'ffill', None}\n            Method to use for filling holes in reindexed Series\n            pad / ffill: propagate LAST valid observation forward to next valid\n            backfill / bfill: use NEXT valid observation to fill gap\n        copy : boolean, default True\n            Return a new object, even if the passed indexes are the same\n        level : int or name\n            Broadcast across a level, matching Index values on the\n            passed MultiIndex level\n\n        Returns\n        -------\n        reindexed : Series\n        \"\"\"\n        index = _ensure_index(index)\n        if self.index.equals(index):\n            if copy:\n                return self.copy()\n            else:\n                return self\n\n        if len(self.index) == 0:\n            return Series(nan, index=index, name=self.name)\n\n        new_index, fill_vec = self.index.reindex(index, method=method,\n                                                 level=level)\n        new_values = com.take_1d(self.values, fill_vec)\n        return Series(new_values, index=new_index, name=self.name)\n\n    def reindex_like(self, other, method=None):\n        \"\"\"\n        Reindex Series to match index of another Series, optionally with\n        filling logic\n\n        Parameters\n        ----------\n        other : Series\n        method : string or None\n            See Series.reindex docstring\n\n        Notes\n        -----\n        Like calling s.reindex(other.index, method=...)\n\n        Returns\n        -------\n        reindexed : Series\n        \"\"\"\n        return self.reindex(other.index, method=method)\n\n    def take(self, indices, axis=0):\n        \"\"\"\n        Analogous to ndarray.take, return Series corresponding to requested\n        indices\n\n        Parameters\n        ----------\n        indices : list / array of ints\n\n        Returns\n        -------\n        taken : Series\n        \"\"\"\n        new_index = self.index.take(indices)\n        new_values = self.values.take(indices)\n        return Series(new_values, index=new_index, name=self.name)\n\n    truncate = generic.truncate\n\n    def fillna(self, value=None, method='pad'):\n        \"\"\"\n        Fill NA/NaN values using the specified method\n\n        Parameters\n        ----------\n        value : any kind (should be same type as array)\n            Value to use to fill holes (e.g. 0)\n        method : {'backfill', 'bfill', 'pad', 'ffill', None}, default 'pad'\n            Method to use for filling holes in reindexed Series\n            pad / ffill: propagate last valid observation forward to next valid\n            backfill / bfill: use NEXT valid observation to fill gap\n\n        See also\n        --------\n        reindex, asfreq\n\n        Returns\n        -------\n        filled : Series\n        \"\"\"\n        if value is not None:\n            newSeries = self.copy()\n            newSeries[isnull(newSeries)] = value\n            return newSeries\n        else:\n            if method is None:  # pragma: no cover\n                raise ValueError('must specify a fill method')\n\n            method = method.lower()\n\n            if method == 'ffill':\n                method = 'pad'\n            if method == 'bfill':\n                method = 'backfill'\n\n            mask = isnull(self.values)\n\n            # sadness. for Python 2.5 compatibility\n            mask = mask.astype(np.uint8)\n\n            if method == 'pad':\n                indexer = lib.get_pad_indexer(mask)\n            elif method == 'backfill':\n                indexer = lib.get_backfill_indexer(mask)\n\n            new_values = self.values.take(indexer)\n            return Series(new_values, index=self.index, name=self.name)\n\n    def isin(self, values):\n        \"\"\"\n        Return boolean vector showing whether each element in the Series is\n        exactly contained in the passed sequence of values\n\n        Parameters\n        ----------\n        values : sequence\n\n        Returns\n        -------\n        isin : Series (boolean dtype)\n        \"\"\"\n        value_set = set(values)\n        result = lib.ismember(self, value_set)\n        # return self.map(value_set.__contains__)\n        return Series(result, self.index, name=self.name)\n\n#-------------------------------------------------------------------------------\n# Miscellaneous\n\n    def plot(self, label=None, kind='line', use_index=True, rot=30, ax=None,\n             style='-', grid=True, logy=False, **kwds):\n        \"\"\"\n        Plot the input series with the index on the x-axis using matplotlib\n\n        Parameters\n        ----------\n        label : label argument to provide to plot\n        kind : {'line', 'bar'}\n        rot : int, default 30\n            Rotation for tick labels\n        use_index : boolean, default True\n            Plot index as axis tick labels\n        ax : matplotlib axis object\n            If not passed, uses gca()\n        style : string, default '-'\n            matplotlib line style to use\n        kwds : keywords\n            To be passed to the actual plotting function\n\n        Notes\n        -----\n        See matplotlib documentation online for more on this subject\n        Intended to be used in ipython --pylab mode\n        \"\"\"\n        import matplotlib.pyplot as plt\n\n        if label is not None:\n            kwds = kwds.copy()\n            kwds['label'] = label\n\n        N = len(self)\n\n        if ax is None:\n            ax = plt.gca()\n\n        if kind == 'line':\n            if use_index:\n                x = np.asarray(self.index)\n            else:\n                x = range(len(self))\n\n            if logy:\n                ax.semilogy(x, self.values.astype(float), style, **kwds)\n            else:\n                ax.plot(x, self.values.astype(float), style, **kwds)\n        elif kind == 'bar':\n            xinds = np.arange(N) + 0.25\n            ax.bar(xinds, self.values.astype(float), 0.5,\n                   bottom=np.zeros(N), linewidth=1, **kwds)\n\n            if N < 10:\n                fontsize = 12\n            else:\n                fontsize = 10\n\n            ax.set_xticks(xinds + 0.25)\n            ax.set_xticklabels(self.index, rotation=rot, fontsize=fontsize)\n\n        ax.grid(grid)\n\n        # try to make things prettier\n        try:\n            fig = plt.gcf()\n            fig.autofmt_xdate()\n        except Exception:  # pragma: no cover\n            pass\n\n        plt.draw_if_interactive()\n\n        return ax\n\n    def hist(self, ax=None, grid=True, **kwds):\n        \"\"\"\n        Draw histogram of the input series using matplotlib\n\n        Parameters\n        ----------\n        ax : matplotlib axis object\n            If not passed, uses gca()\n        kwds : keywords\n            To be passed to the actual plotting function\n\n        Notes\n        -----\n        See matplotlib documentation online for more on this\n\n        \"\"\"\n        import matplotlib.pyplot as plt\n\n        if ax is None:\n            ax = plt.gca()\n\n        values = self.dropna().values\n\n        ax.hist(values, **kwds)\n        ax.grid(grid)\n\n        return ax\n\n    @classmethod\n    def from_csv(cls, path, sep=',', parse_dates=True):\n        \"\"\"\n        Read delimited file into Series\n\n        Parameters\n        ----------\n        path : string\n        sep : string, default ','\n            Field delimiter\n        parse_dates : boolean, default True\n            Parse dates. Different default from read_table\n\n        Returns\n        -------\n        y : Series\n        \"\"\"\n        from pandas.core.frame import DataFrame\n        df = DataFrame.from_csv(path, header=None, sep=sep, parse_dates=parse_dates)\n        return df[df.columns[0]]\n\n    def to_csv(self, path):\n        \"\"\"\n        Write the Series to a CSV file\n\n        Parameters\n        ----------\n        path : string or None\n            Output filepath. If None, write to stdout\n        \"\"\"\n        f = open(path, 'w')\n        csvout = csv.writer(f, lineterminator='\\n')\n        csvout.writerows(self.iteritems())\n        f.close()\n\n    def dropna(self):\n        \"\"\"\n        Return Series without null values\n\n        Returns\n        -------\n        valid : Series\n        \"\"\"\n        return remove_na(self)\n\n    valid = dropna\n\n    isnull = isnull\n    notnull = notnull\n\n    def first_valid_index(self):\n        \"\"\"\n        Return label for first non-NA/null value\n        \"\"\"\n        if len(self) == 0:\n            return None\n\n        mask = isnull(self.values)\n        i = mask.argmin()\n        if mask[i]:\n            return None\n        else:\n            return self.index[i]\n\n    def last_valid_index(self):\n        \"\"\"\n        Return label for last non-NA/null value\n        \"\"\"\n        if len(self) == 0:\n            return None\n\n        mask = isnull(self.values[::-1])\n        i = mask.argmin()\n        if mask[i]:\n            return None\n        else:\n            return self.index[len(self) - i - 1]\n\n    #----------------------------------------------------------------------\n    # Time series-oriented methods\n\n    def shift(self, periods, offset=None, **kwds):\n        \"\"\"\n        Shift the index of the Series by desired number of periods with an\n        optional time offset\n\n        Parameters\n        ----------\n        periods : int\n            Number of periods to move, can be positive or negative\n        offset : DateOffset, timedelta, or time rule string, optional\n            Increment to use from datetools module or time rule (e.g. 'EOM')\n\n        Returns\n        -------\n        shifted : Series\n        \"\"\"\n        if periods == 0:\n            return self.copy()\n\n        offset = kwds.get('timeRule', offset)\n        if isinstance(offset, basestring):\n            offset = datetools.getOffset(offset)\n\n        if offset is None:\n            new_values = np.empty(len(self), dtype=self.dtype)\n            new_values = _maybe_upcast(new_values)\n\n            if periods > 0:\n                new_values[periods:] = self.values[:-periods]\n                new_values[:periods] = nan\n            elif periods < 0:\n                new_values[:periods] = self.values[-periods:]\n                new_values[periods:] = nan\n\n            return Series(new_values, index=self.index, name=self.name)\n        else:\n            return Series(self, index=self.index.shift(periods, offset),\n                          name=self.name)\n\n    def asof(self, date):\n        \"\"\"\n        Return last good (non-NaN) value in TimeSeries if value is NaN for\n        requested date.\n\n        If there is no good value, NaN is returned.\n\n        Parameters\n        ----------\n        date : datetime or similar value\n\n        Notes\n        -----\n        Dates are assumed to be sorted\n\n        Returns\n        -------\n        value or NaN\n        \"\"\"\n        if isinstance(date, basestring):\n            date = datetools.to_datetime(date)\n\n        v = self.get(date)\n\n        if isnull(v):\n            candidates = self.index[notnull(self)]\n            index = candidates.searchsorted(date)\n\n            if index > 0:\n                asOfDate = candidates[index - 1]\n            else:\n                return nan\n\n            return self.get(asOfDate)\n        else:\n            return v\n\n    def asfreq(self, freq, method=None):\n        \"\"\"\n        Convert this TimeSeries to the provided frequency using DateOffset\n        object or time rule. Optionally provide fill method to pad/backfill\n        missing values.\n\n        Parameters\n        ----------\n        offset : DateOffset object, or string in {'WEEKDAY', 'EOM'}\n            DateOffset object or subclass (e.g. monthEnd)\n        method : {'backfill', 'pad', None}\n            Method to use for filling holes in new index\n\n        Returns\n        -------\n        converted : TimeSeries\n        \"\"\"\n        if isinstance(freq, datetools.DateOffset):\n            dateRange = DateRange(self.index[0], self.index[-1], offset=freq)\n        else:\n            dateRange = DateRange(self.index[0], self.index[-1], time_rule=freq)\n\n        return self.reindex(dateRange, method=method)\n\n    def interpolate(self, method='linear'):\n        \"\"\"\n        Interpolate missing values (after the first valid value)\n\n        Parameters\n        ----------\n        method : {'linear', 'time'}\n            Interpolation method.\n            Time interpolation works on daily and higher resolution\n            data to interpolate given length of interval\n\n        Returns\n        -------\n        interpolated : Series\n        \"\"\"\n        if method == 'time':\n            if not isinstance(self, TimeSeries):\n                raise Exception('time-weighted interpolation only works'\n                                'on TimeSeries')\n            inds = np.array([d.toordinal() for d in self.index])\n        else:\n            inds = np.arange(len(self))\n\n        values = self.values\n\n        invalid = isnull(values)\n        valid = -invalid\n\n        firstIndex = valid.argmax()\n        valid = valid[firstIndex:]\n        invalid = invalid[firstIndex:]\n        inds = inds[firstIndex:]\n\n        result = values.copy()\n        result[firstIndex:][invalid] = np.interp(inds[invalid], inds[valid],\n                                                 values[firstIndex:][valid])\n\n        return Series(result, index=self.index, name=self.name)\n\n    def rename(self, mapper):\n        \"\"\"\n        Alter Series index using dict or function\n\n        Parameters\n        ----------\n        mapper : dict-like or function\n            Transformation to apply to each index\n\n        Notes\n        -----\n        Function / dict values must be unique (1-to-1)\n\n        Examples\n        --------\n        >>> x\n        foo 1\n        bar 2\n        baz 3\n\n        >>> x.rename(str.upper)\n        FOO 1\n        BAR 2\n        BAZ 3\n\n        >>> x.rename({'foo' : 'a', 'bar' : 'b', 'baz' : 'c'})\n        a 1\n        b 2\n        c 3\n\n        Returns\n        -------\n        renamed : Series (new object)\n        \"\"\"\n        mapper_f = _get_rename_function(mapper)\n        result = self.copy()\n        result.index = [mapper_f(x) for x in self.index]\n\n        return result\n\n    @property\n    def weekday(self):\n        return Series([d.weekday() for d in self.index], index=self.index)\n\n\nclass TimeSeries(Series):\n    pass\n\n_INDEX_TYPES = ndarray, Index, list, tuple\n\n#-------------------------------------------------------------------------------\n# Supplementary functions\n\ndef remove_na(arr):\n    \"\"\"\n    Return array containing only true/non-NaN values, possibly empty.\n    \"\"\"\n    return arr[notnull(arr)]\n\n\ndef _sanitize_array(data, index, dtype=None, copy=False,\n                    raise_cast_failure=False):\n    if isinstance(data, ma.MaskedArray):\n        mask = ma.getmaskarray(data)\n        data = ma.copy(data)\n        data[mask] = np.nan\n\n    try:\n        subarr = np.array(data, dtype=dtype, copy=copy)\n    except (ValueError, TypeError):\n        if dtype and raise_cast_failure:\n            raise\n        else:  # pragma: no cover\n            subarr = np.array(data, dtype=object)\n\n    if subarr.ndim == 0:\n        if isinstance(data, list):  # pragma: no cover\n            subarr = np.array(data, dtype=object)\n        elif index is not None:\n            value = data\n\n            # If we create an empty array using a string to infer\n            # the dtype, NumPy will only allocate one character per entry\n            # so this is kind of bad. Alternately we could use np.repeat\n            # instead of np.empty (but then you still don't want things\n            # coming out as np.str_!\n            if isinstance(value, basestring) and dtype is None:\n                dtype = np.object_\n\n            if dtype is None:\n                subarr = np.empty(len(index), dtype=type(value))\n            else:\n                subarr = np.empty(len(index), dtype=dtype)\n            subarr.fill(value)\n        else:\n            return subarr.item()\n    elif subarr.ndim > 1:\n        if isinstance(data, np.ndarray):\n            raise Exception('Data must be 1-dimensional')\n        else:\n            subarr = _asarray_tuplesafe(data, dtype=dtype)\n\n    # This is to prevent mixed-type Series getting all casted to\n    # NumPy string type, e.g. NaN --> '-1#IND'.\n    if issubclass(subarr.dtype.type, basestring):\n        subarr = np.array(data, dtype=object, copy=copy)\n\n    return subarr\n\ndef _get_rename_function(mapper):\n    if isinstance(mapper, (dict, Series)):\n        def f(x):\n            if x in mapper:\n                return mapper[x]\n            else:\n                return x\n    else:\n        f = mapper\n\n    return f\n",
      "file_after": "\"\"\"\nData structure for 1-dimensional cross-sectional and time series data\n\"\"\"\n\n# pylint: disable=E1101,E1103\n# pylint: disable=W0703,W0622,W0613,W0201\n\nfrom itertools import izip\nimport csv\nimport operator\n\nfrom numpy import nan, ndarray\nimport numpy as np\nimport numpy.ma as ma\n\nfrom pandas.core.common import (isnull, notnull, _is_bool_indexer,\n                                _default_index, _maybe_upcast,\n                                _asarray_tuplesafe,\n                                AmbiguousIndexError)\nfrom pandas.core.daterange import DateRange\nfrom pandas.core.index import Index, MultiIndex, _ensure_index\nfrom pandas.core.indexing import _SeriesIndexer, _maybe_droplevels\nfrom pandas.util import py3compat\nfrom pandas.util.terminal import get_terminal_size\nimport pandas.core.common as com\nimport pandas.core.datetools as datetools\nimport pandas.core.generic as generic\nimport pandas.core.nanops as nanops\nimport pandas._tseries as lib\nimport pandas._engines as _gin\n\nfrom pandas.util.decorators import Appender, Substitution\n\n__all__ = ['Series', 'TimeSeries']\n\n#-------------------------------------------------------------------------------\n# Wrapper function for Series arithmetic methods\n\ndef _arith_method(op, name):\n    \"\"\"\n    Wrapper function for Series arithmetic operations, to avoid\n    code duplication.\n    \"\"\"\n    def wrapper(self, other):\n        from pandas.core.frame import DataFrame\n\n        if isinstance(other, Series):\n            if self.index.equals(other.index):\n                name = _maybe_match_name(self, other)\n                return Series(op(self.values, other.values), index=self.index,\n                              name=name)\n\n            this_reindexed, other_reindexed = self.align(other, join='outer',\n                                                         copy=False)\n            arr = op(this_reindexed.values, other_reindexed.values)\n\n            name = _maybe_match_name(self, other)\n            return Series(arr, index=this_reindexed.index, name=name)\n        elif isinstance(other, DataFrame):\n            return NotImplemented\n        else:\n            # scalars\n            return Series(op(self.values, other), index=self.index,\n                          name=self.name)\n    return wrapper\n\ndef _maybe_match_name(a, b):\n    name = None\n    if a.name == b.name:\n        name = a.name\n    return name\n\ndef _flex_method(op, name):\n    doc = \"\"\"\n    Binary operator %s with support to substitute a fill_value for missing data\n    in one of the inputs\n\n    Parameters\n    ----------\n    other: Series or scalar value\n    fill_value : None or float value, default None (NaN)\n        Fill missing (NaN) values with this value. If both Series are\n        missing, the result will be missing\n    level : int or name\n        Broadcast across a level, matching Index values on the\n        passed MultiIndex level\n\n    Returns\n    -------\n    result : Series\n    \"\"\" % name\n\n    @Appender(doc)\n    def f(self, other, level=None, fill_value=None):\n        return self._binop(other, op, level=level, fill_value=fill_value)\n\n    f.__name__ = name\n    return f\n\ndef _unbox(func):\n    @Appender(func.__doc__)\n    def f(self, *args, **kwargs):\n        result = func(self, *args, **kwargs)\n        if isinstance(result, np.ndarray) and result.ndim == 0:\n            return result.item()\n        else:  # pragma: no cover\n            return result\n    f.__name__ = func.__name__\n    return f\n\n_stat_doc = \"\"\"\nReturn %(name)s  of values\n%(na_action)s\n\nParameters\n----------\nskipna : boolean, default True\n    Exclude NA/null values\nlevel : int, default None\n    If the axis is a MultiIndex (hierarchical), count along a\n    particular level, collapsing into a smaller Series\n%(extras)s\nReturns\n-------\n%(shortname)s : float (or Series if level specified)\n\"\"\"\n_doc_exclude_na = \"NA/null values are excluded\"\n_doc_ndarray_interface = (\"Extra parameters are to preserve ndarray\"\n                          \"interface.\\n\")\n\n#-------------------------------------------------------------------------------\n# Series class\n\nclass Series(np.ndarray, generic.PandasObject):\n    _AXIS_NUMBERS = {\n        'index' : 0\n    }\n\n    _AXIS_NAMES = dict((v, k) for k, v in _AXIS_NUMBERS.iteritems())\n\n    __slots__ = ['_index', 'name']\n\n    def __new__(cls, data=None, index=None, dtype=None, name=None,\n                copy=False):\n        if data is None:\n            data = {}\n\n        if isinstance(data, Series):\n            if index is None:\n                index = data.index\n        elif isinstance(data, dict):\n            if index is None:\n                index = Index(sorted(data))\n            else:\n                index = _ensure_index(index)\n            try:\n                data = lib.fast_multiget(data, index, default=np.nan)\n            except TypeError:\n                data = [data.get(i, nan) for i in index]\n\n        subarr = _sanitize_array(data, index, dtype, copy,\n                                 raise_cast_failure=True)\n\n        if not isinstance(subarr, np.ndarray):\n            return subarr\n\n        if index is None:\n            index = _default_index(len(subarr))\n        else:\n            index = _ensure_index(index)\n\n        # Change the class of the array to be the subclass type.\n        if index.is_all_dates:\n            subarr = subarr.view(TimeSeries)\n        else:\n            subarr = subarr.view(Series)\n        subarr.index = index\n        subarr.name = name\n\n        return subarr\n\n    def __init__(self, data=None, index=None, dtype=None, name=None,\n                 copy=False):\n        \"\"\"One-dimensional ndarray with axis labels (including time\nseries). Labels must be unique and can any hashable type. The object supports\nboth integer- and label-based indexing and provides a host of methods for\nperforming operations involving the index. Statistical methods from ndarray have\nbeen overridden to automatically exclude missing data (currently represented as\nNaN)\n\nOperations between Series (+, -, /, *, **) align values based on their\nassociated index values-- they need not be the same length. The result\nindex will be the sorted union of the two indexes.\n\nParameters\n----------\ndata : array-like, dict, or scalar value\n    Contains data stored in Series\nindex : array-like or Index (1d)\n\n    Values must be unique and hashable, same length as data. Index object\n    (or other iterable of same length as data) Will default to\n    np.arange(len(data)) if not provided. If both a dict and index sequence\n    are used, the index will override the keys found in the dict.\n\ndtype : numpy.dtype or None\n    If None, dtype will be inferred copy : boolean, default False Copy\n    input data\ncopy : boolean, default False\n        \"\"\"\n        pass\n\n    @property\n    def _constructor(self):\n        return Series\n\n    def __hash__(self):\n        raise TypeError('unhashable type')\n\n    _index = None\n    index = lib.SeriesIndex()\n\n    def __array_finalize__(self, obj):\n        \"\"\"\n        Gets called after any ufunc or other array operations, necessary\n        to pass on the index.\n        \"\"\"\n        self._index = getattr(obj, '_index', None)\n        self.name = getattr(obj, 'name', None)\n\n    def __contains__(self, key):\n        return key in self.index\n\n    def __reduce__(self):\n        \"\"\"Necessary for making this object picklable\"\"\"\n        object_state = list(ndarray.__reduce__(self))\n        subclass_state = (self.index, self.name)\n        object_state[2] = (object_state[2], subclass_state)\n        return tuple(object_state)\n\n    def __setstate__(self, state):\n        \"\"\"Necessary for making this object picklable\"\"\"\n        nd_state, own_state = state\n        ndarray.__setstate__(self, nd_state)\n\n        # backwards compat\n        index, name = own_state[0], None\n        if len(own_state) > 1:\n            name = own_state[1]\n\n        self.index = index\n        self.name = name\n\n    _ix = None\n\n    @property\n    def ix(self):\n        if self._ix is None:\n            self._ix = _SeriesIndexer(self)\n\n        return self._ix\n\n    def __getitem__(self, key):\n        index = self.index\n\n        # Label-based\n        try:\n            return index._engine.get_value(self, key)\n        except KeyError, e1:\n            if isinstance(index, MultiIndex):\n                values = self.values\n                try:\n                    loc = index.get_loc(key)\n                    # TODO: what if a level contains tuples??\n                    new_index = index[loc]\n                    new_index = _maybe_droplevels(new_index, key)\n                    return Series(values[loc], index=new_index,\n                                  name=self.name)\n                except KeyError:\n                    pass\n\n            if index.inferred_type == 'integer':\n                raise\n\n            try:\n                return _gin.get_value_at(self, key)\n            except IndexError:\n                raise\n            except Exception, _:\n                pass\n            raise e1\n        except TypeError:\n            pass\n\n        # boolean\n\n        # special handling of boolean data with NAs stored in object\n        # arrays. Since we can't represent NA with dtype=bool\n        if _is_bool_indexer(key):\n            key = self._check_bool_indexer(key)\n            key = np.asarray(key, dtype=bool)\n\n        return self._get_with(key)\n\n    def _get_with(self, key):\n        # other: fancy integer or otherwise\n        if isinstance(key, slice):\n            indexer = self.ix._convert_to_indexer(key, axis=0)\n            return self._get_values(indexer)\n        else:\n            # mpl hackaround\n            if isinstance(key, tuple):\n                try:\n                    return self._get_values(key)\n                except Exception:\n                    pass\n\n            if not isinstance(key, (list, np.ndarray)):\n                key = list(key)\n\n            key_type = lib.infer_dtype(key)\n\n            if key_type == 'integer':\n                if self.index.inferred_type == 'integer':\n                    return self.reindex(key)\n                else:\n                    return self._get_values(key)\n            elif key_type == 'boolean':\n                return self._get_values(key)\n            else:\n                try:\n                    return self.reindex(key)\n                except Exception:\n                    # [slice(0, 5, None)] will break if you convert to ndarray,\n                    # e.g. as requested by np.median\n                    # hack\n                    if isinstance(key[0], slice):\n                        return self._get_values(key)\n                    raise\n\n    def __setitem__(self, key, value):\n        values = self.values\n        try:\n            values[self.index.get_loc(key)] = value\n            return\n        except KeyError:\n            if (com.is_integer(key)\n                and not self.index.inferred_type == 'integer'):\n\n                values[key] = value\n                return\n\n            raise KeyError('%s not in this series!' % str(key))\n        except TypeError:\n            # Could not hash item\n            pass\n\n        if _is_bool_indexer(key):\n            key = self._check_bool_indexer(key)\n            key = np.asarray(key, dtype=bool)\n\n        self._set_with(key, value)\n\n    def _set_with(self, key, value):\n        # other: fancy integer or otherwise\n        if isinstance(key, slice):\n            indexer = self.ix._convert_to_indexer(key, axis=0)\n            return self._set_values(indexer, value)\n        else:\n            if isinstance(key, tuple):\n                try:\n                    self._set_values(key, value)\n                except Exception:\n                    pass\n\n            if not isinstance(key, (list, np.ndarray)):\n                key = list(key)\n\n            key_type = lib.infer_dtype(key)\n\n            if key_type == 'integer':\n                if self.index.inferred_type == 'integer':\n                    self._set_labels(key, value)\n                else:\n                    return self._set_values(key, value)\n            elif key_type == 'boolean':\n                self._set_values(key, value)\n            else:\n                self._set_labels(key, value)\n\n    def _set_labels(self, key, value):\n        key = _asarray_tuplesafe(key)\n        indexer = self.index.get_indexer(key)\n        mask = indexer == -1\n        if mask.any():\n            raise ValueError('%s not contained in the index'\n                             % str(key[mask]))\n        self._set_values(indexer, value)\n\n    def _get_values(self, indexer):\n        try:\n            return Series(self.values[indexer], index=self.index[indexer],\n                          name=self.name)\n        except Exception:\n            return self.values[indexer]\n\n    def _set_values(self, key, value):\n        self.values[key] = value\n\n    # help out SparseSeries\n    _get_val_at = ndarray.__getitem__\n\n    def __getslice__(self, i, j):\n        if i < 0:\n            i = 0\n        if j < 0:\n            j = 0\n        slobj = slice(i, j)\n        return self.__getitem__(slobj)\n\n    def _check_bool_indexer(self, key):\n        # boolean indexing, need to check that the data are aligned, otherwise\n        # disallowed\n        result = key\n        if isinstance(key, Series) and key.dtype == np.bool_:\n            if not key.index.equals(self.index):\n                result = key.reindex(self.index)\n\n        if isinstance(result, np.ndarray) and result.dtype == np.object_:\n            mask = isnull(result)\n            if mask.any():\n                raise ValueError('cannot index with vector containing '\n                                 'NA / NaN values')\n\n        return result\n\n    def __setslice__(self, i, j, value):\n        \"\"\"Set slice equal to given value(s)\"\"\"\n        ndarray.__setslice__(self, i, j, value)\n\n    def get(self, label, default=None):\n        \"\"\"\n        Returns value occupying requested label, default to specified\n        missing value if not present. Analogous to dict.get\n\n        Parameters\n        ----------\n        label : object\n            Label value looking for\n        default : object, optional\n            Value to return if label not in index\n\n        Returns\n        -------\n        y : scalar\n        \"\"\"\n        try:\n            return self.get_value(label)\n        except KeyError:\n            return default\n\n    def get_value(self, label):\n        \"\"\"\n        Quickly retrieve single value at passed index label\n\n        Parameters\n        ----------\n        index : label\n\n        Returns\n        -------\n        value : scalar value\n        \"\"\"\n        return self.index._engine.get_value(self, label)\n\n    def set_value(self, label, value):\n        \"\"\"\n        Quickly set single value at passed label. If label is not contained, a\n        new object is created with the label placed at the end of the result\n        index\n\n        Parameters\n        ----------\n        label : object\n            Partial indexing with MultiIndex not allowed\n        value : object\n            Scalar value\n\n        Returns\n        -------\n        series : Series\n            If label is contained, will be reference to calling Series,\n            otherwise a new object\n        \"\"\"\n        try:\n            self.index._engine.set_value(self, label, value)\n            return self\n        except KeyError:\n            new_index = np.concatenate([self.index.values, [label]])\n            new_values = np.concatenate([self.values, [value]])\n            return Series(new_values, index=new_index, name=self.name)\n\n    def __repr__(self):\n        \"\"\"Clean string representation of a Series\"\"\"\n        width, height = get_terminal_size()\n        max_rows = (height if com.print_config.max_rows == 0\n                    else com.print_config.max_rows)\n        if len(self.index) > max_rows:\n            result = self._tidy_repr(min(30, max_rows - 4))\n        elif len(self.index) > 0:\n            result = self._get_repr(print_header=True,\n                                    length=len(self) > 50,\n                                    name=True)\n        else:\n            result = '%s' % ndarray.__repr__(self)\n\n        return result\n\n    def _tidy_repr(self, max_vals=20):\n        num = max_vals // 2\n        head = self[:num]._get_repr(print_header=True, length=False,\n                                    name=False)\n        tail = self[-(max_vals - num):]._get_repr(print_header=False,\n                                                  length=False,\n                                                  name=False)\n        result = head + '\\n...\\n' + tail\n        namestr = \"Name: %s, \" % str(self.name) if self.name else \"\"\n        result = '%s\\n%sLength: %d' % (result, namestr, len(self))\n        return result\n\n    def to_string(self, buf=None, na_rep='NaN', float_format=None,\n                  nanRep=None, length=False, name=False):\n        if nanRep is not None:  # pragma: no cover\n            import warnings\n            warnings.warn(\"nanRep is deprecated, use na_rep\",\n                          FutureWarning)\n            na_rep = nanRep\n\n        the_repr = self._get_repr(float_format=float_format, na_rep=na_rep,\n                                  length=length, name=name)\n        if buf is None:\n            return the_repr\n        else:\n            print >> buf, the_repr\n\n    def _get_repr(self, name=False, print_header=False, length=True,\n                  na_rep='NaN', float_format=None):\n        if len(self) == 0:\n            return ''\n\n        vals = self.values\n        index = self.index\n\n        is_multi = isinstance(index, MultiIndex)\n        if is_multi:\n            have_header = any(name for name in index.names)\n            string_index = index.format(names=True)\n            header, string_index = string_index[0], string_index[1:]\n        else:\n            have_header = index.name is not None\n            header = str(index.name)\n            string_index = index.format()\n\n        maxlen = max(len(x) for x in string_index)\n        padSpace = min(maxlen, 60)\n\n        if float_format is None:\n            float_format = com.print_config.float_format\n            if float_format is None:\n                float_format = com._float_format_default\n\n        def _format(k, v, extra=0):\n            # GH #490\n            if not isinstance(v, np.ndarray) and isnull(v):\n                v = na_rep\n            if com.is_float(v):\n                v = float_format(v)\n            strv = ' ' * extra + str(v).replace('\\n', ' ')\n            return '%s    %s' % (str(k).ljust(padSpace), strv)\n\n        # floating point handling\n        if self.dtype == 'O':\n            is_float = (self.map(com.is_float) & self.notnull()).values\n            leading_space = is_float.any()\n\n            res = []\n            for i, (k, v) in enumerate(izip(string_index, vals)):\n                if not is_float[i] and leading_space:\n                    res.append(_format(k, v, extra=1))\n                else:\n                    res.append(_format(k, v))\n        else:\n            res = [_format(idx, v) for idx, v in izip(string_index, vals)]\n\n        if print_header and have_header:\n            res.insert(0, header)\n\n        footer = ''\n        if name:\n            footer += \"Name: %s\" % str(self.name) if self.name else ''\n\n        if length:\n            if footer:\n                footer += ', '\n            footer += 'Length: %d' % len(self)\n\n        if footer:\n            res.append(footer)\n\n        return '\\n'.join(res)\n\n    def __str__(self):\n        return repr(self)\n\n    def __iter__(self):\n        return iter(self.values)\n\n    def iteritems(self):\n        \"\"\"\n        Lazily iterate over (index, value) tuples\n        \"\"\"\n        return izip(iter(self.index), iter(self))\n\n    iterkv = iteritems\n    if py3compat.PY3:  # pragma: no cover\n        items = iteritems\n\n    #----------------------------------------------------------------------\n    #   Arithmetic operators\n\n    __add__ = _arith_method(operator.add, '__add__')\n    __sub__ = _arith_method(operator.sub, '__sub__')\n    __mul__ = _arith_method(operator.mul, '__mul__')\n    __truediv__ = _arith_method(operator.truediv, '__truediv__')\n    __floordiv__ = _arith_method(operator.floordiv, '__floordiv__')\n    __pow__ = _arith_method(operator.pow, '__pow__')\n\n    __radd__ = _arith_method(lambda x, y: y + x, '__add__')\n    __rmul__ = _arith_method(operator.mul, '__mul__')\n    __rsub__ = _arith_method(lambda x, y: y - x, '__sub__')\n    __rtruediv__ = _arith_method(lambda x, y: y / x, '__truediv__')\n    __rfloordiv__ = _arith_method(lambda x, y: y // x, '__floordiv__')\n    __rpow__ = _arith_method(lambda x, y: y ** x, '__pow__')\n\n    # Inplace operators\n    __iadd__ = __add__\n    __isub__ = __sub__\n    __imul__ = __mul__\n    __itruediv__ = __truediv__\n    __ifloordiv__ = __floordiv__\n    __ipow__ = __pow__\n\n    # Python 2 division operators\n    if not py3compat.PY3:\n        __div__ = _arith_method(operator.div, '__div__')\n        __rdiv__ = _arith_method(lambda x, y: y / x, '__div__')\n        __idiv__ = __div__\n\n    #----------------------------------------------------------------------\n    # unbox reductions\n\n    all = _unbox(np.ndarray.all)\n    any = _unbox(np.ndarray.any)\n\n    #----------------------------------------------------------------------\n    # Misc public methods\n\n    def keys(self):\n        \"Alias for index\"\n        return self.index\n\n    # alas, I wish this worked\n    # values = lib.ValuesProperty()\n\n    @property\n    def values(self):\n        \"\"\"\n        Return Series as ndarray\n\n        Returns\n        -------\n        arr : numpy.ndarray\n        \"\"\"\n        return self.view(ndarray)\n\n    def copy(self):\n        \"\"\"\n        Return new Series with copy of underlying values\n\n        Returns\n        -------\n        cp : Series\n        \"\"\"\n        return Series(self.values.copy(), index=self.index, name=self.name)\n\n    def to_dict(self):\n        \"\"\"\n        Convert Series to {label -> value} dict\n\n        Returns\n        -------\n        value_dict : dict\n        \"\"\"\n        return dict(self.iteritems())\n\n    def to_sparse(self, kind='block', fill_value=None):\n        \"\"\"\n        Convert Series to SparseSeries\n\n        Parameters\n        ----------\n        kind : {'block', 'integer'}\n        fill_value : float, defaults to NaN (missing)\n\n        Returns\n        -------\n        sp : SparseSeries\n        \"\"\"\n        from pandas.core.sparse import SparseSeries\n        return SparseSeries(self, kind=kind, fill_value=fill_value,\n                            name=self.name)\n\n    def head(self, n=5):\n        \"\"\"Returns first n rows of Series\n        \"\"\"\n        return self[:n]\n\n    def tail(self, n=5):\n        \"\"\"Returns last n rows of Series\n        \"\"\"\n        return self[-n:]\n\n    #----------------------------------------------------------------------\n    # Statistics, overridden ndarray methods\n\n    # TODO: integrate bottleneck\n\n    def count(self, level=None):\n        \"\"\"\n        Return number of non-NA/null observations in the Series\n\n        Parameters\n        ----------\n        level : int, default None\n            If the axis is a MultiIndex (hierarchical), count along a\n            particular level, collapsing into a smaller Series\n\n        Returns\n        -------\n        nobs : int or Series (if level specified)\n        \"\"\"\n        if level is not None:\n            mask = notnull(self.values)\n            level_index = self.index.levels[level]\n\n            if len(self) == 0:\n                return Series(0, index=level_index)\n\n            # call cython function\n            max_bin = len(level_index)\n            counts = lib.count_level_1d(mask.view(np.uint8),\n                                        self.index.labels[level], max_bin)\n            return Series(counts, index=level_index)\n\n        return notnull(self.values).sum()\n\n    def value_counts(self):\n        \"\"\"\n        Returns Series containing counts of unique values. The resulting Series\n        will be in descending order so that the first element is the most\n        frequently-occurring element. Excludes NA values\n\n        Returns\n        -------\n        counts : Series\n        \"\"\"\n        from collections import defaultdict\n        counter = defaultdict(lambda: 0)\n        for value in self.dropna().values:\n            counter[value] += 1\n        return Series(counter).order(ascending=False)\n\n    def nunique(self):\n        \"\"\"\n        Return count of unique elements in the Series\n\n        Returns\n        -------\n        nunique : int\n        \"\"\"\n        return len(self.value_counts())\n\n    @Substitution(name='sum', shortname='sum', na_action=_doc_exclude_na,\n                  extras=_doc_ndarray_interface)\n    @Appender(_stat_doc)\n    def sum(self, axis=0, dtype=None, out=None, skipna=True, level=None):\n        if level is not None:\n            return self._agg_by_level('sum', level=level, skipna=skipna)\n        return nanops.nansum(self.values, skipna=skipna)\n\n    @Substitution(name='mean', shortname='mean', na_action=_doc_exclude_na,\n                  extras=_doc_ndarray_interface)\n    @Appender(_stat_doc)\n    def mean(self, axis=0, dtype=None, out=None, skipna=True, level=None):\n        if level is not None:\n            return self._agg_by_level('mean', level=level, skipna=skipna)\n        return nanops.nanmean(self.values, skipna=skipna)\n\n    @Substitution(name='mean absolute deviation', shortname='mad',\n                  na_action=_doc_exclude_na, extras='')\n    @Appender(_stat_doc)\n    def mad(self, skipna=True, level=None):\n        if level is not None:\n            return self._agg_by_level('mad', level=level, skipna=skipna)\n\n        demeaned = self - self.mean(skipna=skipna)\n        return np.abs(demeaned).mean(skipna=skipna)\n\n    @Substitution(name='median', shortname='median',\n                  na_action=_doc_exclude_na, extras='')\n    @Appender(_stat_doc)\n    def median(self, skipna=True, level=None):\n        if level is not None:\n            return self._agg_by_level('median', level=level, skipna=skipna)\n        return nanops.nanmedian(self.values, skipna=skipna)\n\n    @Substitution(name='product', shortname='product',\n                  na_action=_doc_exclude_na, extras='')\n    @Appender(_stat_doc)\n    def prod(self, axis=None, dtype=None, out=None, skipna=True, level=None):\n        if level is not None:\n            return self._agg_by_level('prod', level=level, skipna=skipna)\n        return nanops.nanprod(self.values, skipna=skipna)\n\n    @Substitution(name='minimum', shortname='min',\n                  na_action=_doc_exclude_na, extras='')\n    @Appender(_stat_doc)\n    def min(self, axis=None, out=None, skipna=True, level=None):\n        if level is not None:\n            return self._agg_by_level('min', level=level, skipna=skipna)\n        return nanops.nanmin(self.values, skipna=skipna)\n\n    @Substitution(name='maximum', shortname='max',\n                  na_action=_doc_exclude_na, extras='')\n    @Appender(_stat_doc)\n    def max(self, axis=None, out=None, skipna=True, level=None):\n        if level is not None:\n            return self._agg_by_level('max', level=level, skipna=skipna)\n        return nanops.nanmax(self.values, skipna=skipna)\n\n    @Substitution(name='unbiased standard deviation', shortname='stdev',\n                  na_action=_doc_exclude_na, extras='')\n    @Appender(_stat_doc)\n    def std(self, axis=None, dtype=None, out=None, ddof=1, skipna=True,\n            level=None):\n        if level is not None:\n            return self._agg_by_level('std', level=level, skipna=skipna)\n        return np.sqrt(nanops.nanvar(self.values, skipna=skipna))\n\n    @Substitution(name='unbiased variance', shortname='var',\n                  na_action=_doc_exclude_na, extras='')\n    @Appender(_stat_doc)\n    def var(self, axis=None, dtype=None, out=None, ddof=1, skipna=True,\n            level=None):\n        if level is not None:\n            return self._agg_by_level('var', level=level, skipna=skipna)\n        return nanops.nanvar(self.values, skipna=skipna)\n\n    @Substitution(name='unbiased skewness', shortname='skew',\n                  na_action=_doc_exclude_na, extras='')\n    @Appender(_stat_doc)\n    def skew(self, skipna=True, level=None):\n        if level is not None:\n            return self._agg_by_level('skew', level=level, skipna=skipna)\n\n        return nanops.nanskew(self.values, skipna=skipna)\n\n    def _agg_by_level(self, name, level=0, skipna=True):\n        grouped = self.groupby(level=level)\n        if hasattr(grouped, name) and skipna:\n            return getattr(grouped, name)()\n        method = getattr(type(self), name)\n        applyf = lambda x: method(x, skipna=skipna)\n        return grouped.aggregate(applyf)\n\n    def idxmin(self, axis=None, out=None, skipna=True):\n        \"\"\"\n        Index of first occurence of minimum of values.\n\n        Parameters\n        ----------\n        skipna : boolean, default True\n            Exclude NA/null values\n\n        Returns\n        -------\n        idxmin : Index of mimimum of values\n        \"\"\"\n        i = nanops.nanargmin(self.values, skipna=skipna)\n        if i == -1:\n            return np.nan\n        return self.index[i]\n\n    def idxmax(self, axis=None, out=None, skipna=True):\n        \"\"\"\n        Index of first occurence of maximum of values.\n\n        Parameters\n        ----------\n        skipna : boolean, default True\n            Exclude NA/null values\n\n        Returns\n        -------\n        idxmax : Index of mimimum of values\n        \"\"\"\n        i = nanops.nanargmax(self.values, skipna=skipna)\n        if i == -1:\n            return np.nan\n        return self.index[i]\n\n    def cumsum(self, axis=0, dtype=None, out=None, skipna=True):\n        \"\"\"\n        Cumulative sum of values. Preserves locations of NaN values\n\n        Extra parameters are to preserve ndarray interface.\n\n        Parameters\n        ----------\n        skipna : boolean, default True\n            Exclude NA/null values\n\n        Returns\n        -------\n        cumsum : Series\n        \"\"\"\n        arr = self.values.copy()\n\n        do_mask = skipna and not issubclass(self.dtype.type, np.integer)\n        if do_mask:\n            mask = isnull(arr)\n            np.putmask(arr, mask, 0.)\n\n        result = arr.cumsum()\n\n        if do_mask:\n            np.putmask(result, mask, np.nan)\n\n        return Series(result, index=self.index)\n\n    def cumprod(self, axis=0, dtype=None, out=None, skipna=True):\n        \"\"\"\n        Cumulative product of values. Preserves locations of NaN values\n\n        Extra parameters are to preserve ndarray interface.\n\n        Parameters\n        ----------\n        skipna : boolean, default True\n            Exclude NA/null values\n\n        Returns\n        -------\n        cumprod : Series\n        \"\"\"\n        arr = self.values.copy()\n\n        do_mask = skipna and not issubclass(self.dtype.type, np.integer)\n        if do_mask:\n            mask = isnull(arr)\n            np.putmask(arr, mask, 1.)\n\n        result = arr.cumprod()\n\n        if do_mask:\n            np.putmask(result, mask, np.nan)\n\n        return Series(result, index=self.index)\n\n    @Appender(np.ndarray.round.__doc__)\n    def round(self, decimals=0, out=None):\n        \"\"\"\n\n        \"\"\"\n        result = self.values.round(decimals, out=out)\n        if out is None:\n            result = Series(result, index=self.index, name=self.name)\n\n        return result\n\n    def quantile(self, q=0.5):\n        \"\"\"\n        Return value at the given quantile, a la scoreatpercentile in\n        scipy.stats\n\n        Parameters\n        ----------\n        q : quantile\n            0 <= q <= 1\n\n        Returns\n        -------\n        quantile : float\n        \"\"\"\n        from scipy.stats import scoreatpercentile\n        valid_values = self.dropna().values\n        if len(valid_values) == 0:\n            return np.nan\n        return scoreatpercentile(valid_values, q * 100)\n\n    def describe(self):\n        \"\"\"\n        Generate various summary statistics of Series, excluding NaN\n        values. These include: count, mean, std, min, max, and 10%/50%/90%\n        quantiles\n\n        Returns\n        -------\n        desc : Series\n        \"\"\"\n        try:\n            from collections import Counter\n        except ImportError:  # pragma: no cover\n            # For Python < 2.7, we include a local copy of this:\n            from pandas.util.counter import Counter\n\n        if self.dtype == object:\n            names = ['count', 'unique', 'top', 'freq']\n\n            objcounts = Counter(self.dropna().values)\n            top, freq = objcounts.most_common(1)[0]\n            data = [self.count(), len(objcounts), top, freq]\n\n        else:\n            names = ['count', 'mean', 'std', 'min',\n                     '25%', '50%', '75%', 'max']\n\n            data = [self.count(), self.mean(), self.std(), self.min(),\n                    self.quantile(.25), self.median(), self.quantile(.75),\n                    self.max()]\n\n        return Series(data, index=names)\n\n    def corr(self, other, method='pearson'):\n        \"\"\"\n        Compute correlation two Series, excluding missing values\n\n        Parameters\n        ----------\n        other : Series\n        method : {'pearson', 'kendall', 'spearman'}\n            pearson : standard correlation coefficient\n            kendall : Kendall Tau correlation coefficient\n            spearman : Spearman rank correlation\n\n        Returns\n        -------\n        correlation : float\n        \"\"\"\n        this, other = self.align(other, join='inner', copy=False)\n        return nanops.nancorr(this.values, other.values, method=method)\n\n    def cov(self, other):\n        \"\"\"\n        Compute covariance with Series, excluding missing values\n\n        Parameters\n        ----------\n        other : Series\n\n        Returns\n        -------\n        covariance : float\n        \"\"\"\n        this, other = self.align(other, join='inner')\n        if len(this) == 0:\n            return np.nan\n        return nanops.nancov(this.values, other.values)\n\n    def diff(self, periods=1):\n        \"\"\"\n        1st discrete difference of object\n\n        Parameters\n        ----------\n        periods : int, default 1\n            Periods to shift for forming difference\n\n        Returns\n        -------\n        diffed : Series\n        \"\"\"\n        return (self - self.shift(periods))\n\n    def autocorr(self):\n        \"\"\"\n        Lag-1 autocorrelation\n\n        Returns\n        -------\n        autocorr : float\n        \"\"\"\n        return self.corr(self.shift(1))\n\n    def clip(self, lower=None, upper=None, out=None):\n        \"\"\"\n        Trim values at input threshold(s)\n\n        Parameters\n        ----------\n        lower : float, default None\n        upper : float, default None\n\n        Returns\n        -------\n        clipped : Series\n        \"\"\"\n        if out is not None:  # pragma: no cover\n            raise Exception('out argument is not supported yet')\n\n        result = self\n        if lower is not None:\n            result = result.clip_lower(lower)\n        if upper is not None:\n            result = result.clip_upper(upper)\n\n        return result\n\n    def clip_upper(self, threshold):\n        \"\"\"\n        Return copy of series with values above given value truncated\n\n        See also\n        --------\n        clip\n\n        Returns\n        -------\n        clipped : Series\n        \"\"\"\n        return np.where(self > threshold, threshold, self)\n\n    def clip_lower(self, threshold):\n        \"\"\"\n        Return copy of series with values below given value truncated\n\n        See also\n        --------\n        clip\n\n        Returns\n        -------\n        clipped : Series\n        \"\"\"\n        return np.where(self < threshold, threshold, self)\n\n#-------------------------------------------------------------------------------\n# Combination\n\n    def append(self, to_append):\n        \"\"\"\n        Concatenate two or more Series. The indexes must not overlap\n\n        Parameters\n        ----------\n        to_append : Series or list/tuple of Series\n\n        Returns\n        -------\n        appended : Series\n        \"\"\"\n        from pandas.tools.merge import concat\n        if isinstance(to_append, (list, tuple)):\n            to_concat = [self] + to_append\n        else:\n            to_concat = [self, to_append]\n        return concat(to_concat, ignore_index=False, verify_integrity=True)\n\n    def _binop(self, other, func, level=None, fill_value=None):\n        \"\"\"\n        Perform generic binary operation with optional fill value\n\n        Parameters\n        ----------\n        other : Series\n        func : binary operator\n        fill_value : float or object\n            Value to substitute for NA/null values. If both Series are NA in a\n            location, the result will be NA regardless of the passed fill value\n        level : int or name\n            Broadcast across a level, matching Index values on the\n            passed MultiIndex level\n\n        Returns\n        -------\n        combined : Series\n        \"\"\"\n        assert(isinstance(other, Series))\n\n        new_index = self.index\n        this = self\n\n        if not self.index.equals(other.index):\n            this, other = self.align(other, level=level, join='outer')\n            new_index = this.index\n\n        this_vals = this.values\n        other_vals = other.values\n\n        if fill_value is not None:\n            this_mask = isnull(this_vals)\n            other_mask = isnull(other_vals)\n            this_vals = this_vals.copy()\n            other_vals = other_vals.copy()\n\n            # one but not both\n            mask = this_mask ^ other_mask\n            this_vals[this_mask & mask] = fill_value\n            other_vals[other_mask & mask] = fill_value\n\n        result = func(this_vals, other_vals)\n        name = _maybe_match_name(self, other)\n        return Series(result, index=new_index, name=name)\n\n    add = _flex_method(operator.add, 'add')\n    sub = _flex_method(operator.sub, 'subtract')\n    mul = _flex_method(operator.mul, 'multiply')\n    try:\n        div = _flex_method(operator.div, 'divide')\n    except AttributeError:  # pragma: no cover\n        # Python 3\n        div = _flex_method(operator.truediv, 'divide')\n\n    def combine(self, other, func, fill_value=nan):\n        \"\"\"\n        Perform elementwise binary operation on two Series using given function\n        with optional fill value when an index is missing from one Series or\n        the other\n\n        Parameters\n        ----------\n        other : Series or scalar value\n        func : function\n        fill_value : scalar value\n\n        Returns\n        -------\n        result : Series\n        \"\"\"\n        if isinstance(other, Series):\n            new_index = self.index + other.index\n            new_name = _maybe_match_name(self, other)\n            new_values = np.empty(len(new_index), dtype=self.dtype)\n            for i, idx in enumerate(new_index):\n                lv = self.get(idx, fill_value)\n                rv = other.get(idx, fill_value)\n                new_values[i] = func(lv, rv)\n        else:\n            new_index = self.index\n            new_values = func(self.values, other)\n            new_name = self.name\n        return Series(new_values, index=new_index, name=new_name)\n\n    def combine_first(self, other):\n        \"\"\"\n        Combine Series values, choosing the calling Series's values\n        first. Result index will be the union of the two indexes\n\n        Parameters\n        ----------\n        other : Series\n\n        Returns\n        -------\n        y : Series\n        \"\"\"\n        new_index = self.index + other.index\n        this = self.reindex(new_index, copy=False)\n        other = other.reindex(new_index, copy=False)\n        name = _maybe_match_name(self, other)\n        return Series(np.where(isnull(this), other, this), index=new_index,\n                      name=name)\n\n    #----------------------------------------------------------------------\n    # Reindexing, sorting\n\n    def sort(self, axis=0, kind='quicksort', order=None):\n        \"\"\"\n        Sort values and index labels in place, for compatibility with\n        ndarray. No return value\n        \"\"\"\n        sortedSeries = self.order(na_last=True)\n\n        true_base = self\n        while true_base.base is not None:\n            true_base = true_base.base\n\n        if (true_base is not None and\n            (true_base.ndim != 1 or true_base.shape != self.shape)):\n            raise Exception('This Series is a view of some other array, to '\n                            'sort in-place you must create a copy')\n\n        self[:] = sortedSeries\n        self.index = sortedSeries.index\n\n    def sort_index(self, ascending=True):\n        \"\"\"\n        Sort object by labels (along an axis)\n\n        Parameters\n        ----------\n        ascending : boolean, default True\n            Sort ascending vs. descending\n\n        Returns\n        -------\n        sorted_obj : Series\n        \"\"\"\n        labels = self.index\n        sort_index = labels.argsort()\n        if not ascending:\n            sort_index = sort_index[::-1]\n        new_labels = labels.take(sort_index)\n        new_values = self.values.take(sort_index)\n        return Series(new_values, new_labels, name=self.name)\n\n    def argsort(self, axis=0, kind='quicksort', order=None):\n        \"\"\"\n        Overrides ndarray.argsort. Argsorts the value, omitting NA/null values,\n        and places the result in the same locations as the non-NA values\n\n        Returns\n        -------\n        argsorted : Series\n        \"\"\"\n        values = self.values\n        mask = isnull(values)\n\n        if mask.any():\n            result = values.copy()\n            notmask = -mask\n            result[notmask] = np.argsort(values[notmask])\n            return Series(result, index=self.index, name=self.name)\n        else:\n            return Series(np.argsort(values), index=self.index, name=self.name)\n\n    def rank(self):\n        \"\"\"\n        Compute data ranks (1 through n). Equal values are assigned a rank that\n        is the average of the ranks of those values\n\n        Returns\n        -------\n        ranks : Series\n        \"\"\"\n        try:\n            ranks = lib.rank_1d_float64(self.values)\n        except Exception:\n            ranks = lib.rank_1d_generic(self.values)\n        return Series(ranks, index=self.index, name=self.name)\n\n    def order(self, na_last=True, ascending=True):\n        \"\"\"\n        Sorts Series object, by value, maintaining index-value link\n\n        Parameters\n        ----------\n        na_last : boolean (optional, default=True)\n            Put NaN's at beginning or end\n        ascending : boolean, default True\n            Sort ascending. Passing False sorts descending\n\n        Returns\n        -------\n        y : Series\n        \"\"\"\n        def _try_mergesort(arr):\n            # easier to ask forgiveness than permission\n            try:\n                return arr.argsort(kind='mergesort')\n            except TypeError:\n                # stable sort not available for object dtype\n                return arr.argsort()\n\n        arr = self.values\n        sortedIdx = np.empty(len(self), dtype=np.int32)\n\n        bad = isnull(arr)\n\n        good = -bad\n        idx = np.arange(len(self))\n\n        argsorted = _try_mergesort(arr[good])\n\n        if not ascending:\n            argsorted = argsorted[::-1]\n\n        if na_last:\n            n = good.sum()\n            sortedIdx[:n] = idx[good][argsorted]\n            sortedIdx[n:] = idx[bad]\n        else:\n            n = bad.sum()\n            sortedIdx[n:] = idx[good][argsorted]\n            sortedIdx[:n] = idx[bad]\n\n        return Series(arr[sortedIdx], index=self.index[sortedIdx],\n                      name=self.name)\n\n    def sortlevel(self, level=0, ascending=True):\n        \"\"\"\n        Sort Series with MultiIndex by chosen level. Data will be\n        lexicographically sorted by the chosen level followed by the other\n        levels (in order)\n\n        Parameters\n        ----------\n        level : int\n        ascending : bool, default True\n\n        Returns\n        -------\n        sorted : Series\n        \"\"\"\n        if not isinstance(self.index, MultiIndex):\n            raise Exception('can only sort by level with a hierarchical index')\n\n        new_index, indexer = self.index.sortlevel(level, ascending=ascending)\n        new_values = self.values.take(indexer)\n        return Series(new_values, index=new_index, name=self.name)\n\n    def swaplevel(self, i, j, copy=True):\n        \"\"\"\n        Swap levels i and j in a MultiIndex\n\n        Returns\n        -------\n        swapped : Series\n        \"\"\"\n        new_index = self.index.swaplevel(i, j)\n        return Series(self.values, index=new_index, copy=copy, name=self.name)\n\n    def reorder_levels(self, order):\n        \"\"\"\n        Rearrange index levels using input order. May not drop or duplicate\n        levels\n\n        Parameters\n        ----------\n        order: list of int representing new level order.\n               (reference level by number not by key)\n        axis: where to reorder levels\n\n        Returns\n        -------\n        type of caller (new object)\n        \"\"\"\n        if not isinstance(self.index, MultiIndex):  # pragma: no cover\n            raise Exception('Can only reorder levels on a hierarchical axis.')\n\n        result = self.copy()\n        result.index = result.index.reorder_levels(order)\n        return result\n\n    def unstack(self, level=-1):\n        \"\"\"\n        Unstack, a.k.a. pivot, Series with MultiIndex to produce DataFrame\n\n        Parameters\n        ----------\n        level : int, string, or list of these, default last level\n            Level(s) to unstack, can pass level name\n\n        Examples\n        --------\n        >>> s\n        one  a   1.\n        one  b   2.\n        two  a   3.\n        two  b   4.\n\n        >>> s.unstack(level=-1)\n             a   b\n        one  1.  2.\n        two  3.  4.\n\n        >>> s.unstack(level=0)\n           one  two\n        a  1.   2.\n        b  3.   4.\n\n        Returns\n        -------\n        unstacked : DataFrame\n        \"\"\"\n        from pandas.core.reshape import unstack\n        if isinstance(level, (tuple, list)):\n            result = self\n            for lev in level:\n                result = unstack(result, lev)\n            return result\n        else:\n            return unstack(self, level)\n\n    #----------------------------------------------------------------------\n    # function application\n\n    def map(self, arg):\n        \"\"\"\n        Map values of Series using input correspondence (which can be\n        a dict, Series, or function)\n\n        Parameters\n        ----------\n        arg : function, dict, or Series\n\n        Examples\n        --------\n        >>> x\n        one   1\n        two   2\n        three 3\n\n        >>> y\n        1  foo\n        2  bar\n        3  baz\n\n        >>> x.map(y)\n        one   foo\n        two   bar\n        three baz\n\n        Returns\n        -------\n        y : Series\n            same index as caller\n        \"\"\"\n        if isinstance(arg, (dict, Series)):\n            if isinstance(arg, dict):\n                arg = Series(arg)\n\n            indexer = lib.merge_indexer_object(self.values.astype(object),\n                                               arg.index.indexMap)\n\n            new_values = com.take_1d(np.asarray(arg), indexer)\n            return Series(new_values, index=self.index, name=self.name)\n        else:\n            mapped = lib.map_infer(self.values, arg)\n            return Series(mapped, index=self.index, name=self.name)\n\n    def apply(self, func):\n        \"\"\"\n        Invoke function on values of Series. Can be ufunc or Python function\n        expecting only single values\n\n        Parameters\n        ----------\n        func : function\n\n        Returns\n        -------\n        y : Series\n        \"\"\"\n        try:\n            result = func(self)\n            if not isinstance(result, Series):\n                result = Series(result, index=self.index, name=self.name)\n            return result\n        except Exception:\n            mapped = lib.map_infer(self.values, func)\n            return Series(mapped, index=self.index, name=self.name)\n\n    def align(self, other, join='outer', level=None, copy=True):\n        \"\"\"\n        Align two Series object with the specified join method\n\n        Parameters\n        ----------\n        other : Series\n        join : {'outer', 'inner', 'left', 'right'}, default 'outer'\n        level : int or name\n            Broadcast across a level, matching Index values on the\n            passed MultiIndex level\n        copy : boolean, default True\n            Always return new objects. If copy=False and no reindexing is\n            required, the same object will be returned (for better performance)\n\n        Returns\n        -------\n        (left, right) : (Series, Series)\n            Aligned Series\n        \"\"\"\n        join_index, lidx, ridx = self.index.join(other.index, how=join,\n                                                 level=level,\n                                                 return_indexers=True)\n\n        left = self._reindex_indexer(join_index, lidx, copy)\n        right = other._reindex_indexer(join_index, ridx, copy)\n        return left, right\n\n    def _reindex_indexer(self, new_index, indexer, copy):\n        if indexer is not None:\n            new_values = com.take_1d(self.values, indexer)\n        else:\n            if copy:\n                return self.copy()\n            else:\n                return self\n\n        # be subclass-friendly\n        return self._constructor(new_values, new_index, name=self.name)\n\n    def reindex(self, index=None, method=None, level=None, copy=True):\n        \"\"\"Conform Series to new index with optional filling logic, placing\n        NA/NaN in locations having no value in the previous index. A new object\n        is produced unless the new index is equivalent to the current one and\n        copy=False\n\n        Parameters\n        ----------\n        index : array-like or Index\n            New labels / index to conform to. Preferably an Index object to\n            avoid duplicating data\n        method : {'backfill', 'bfill', 'pad', 'ffill', None}\n            Method to use for filling holes in reindexed Series\n            pad / ffill: propagate LAST valid observation forward to next valid\n            backfill / bfill: use NEXT valid observation to fill gap\n        copy : boolean, default True\n            Return a new object, even if the passed indexes are the same\n        level : int or name\n            Broadcast across a level, matching Index values on the\n            passed MultiIndex level\n\n        Returns\n        -------\n        reindexed : Series\n        \"\"\"\n        index = _ensure_index(index)\n        if self.index.equals(index):\n            if copy:\n                return self.copy()\n            else:\n                return self\n\n        if len(self.index) == 0:\n            return Series(nan, index=index, name=self.name)\n\n        new_index, fill_vec = self.index.reindex(index, method=method,\n                                                 level=level)\n        new_values = com.take_1d(self.values, fill_vec)\n        return Series(new_values, index=new_index, name=self.name)\n\n    def reindex_like(self, other, method=None):\n        \"\"\"\n        Reindex Series to match index of another Series, optionally with\n        filling logic\n\n        Parameters\n        ----------\n        other : Series\n        method : string or None\n            See Series.reindex docstring\n\n        Notes\n        -----\n        Like calling s.reindex(other.index, method=...)\n\n        Returns\n        -------\n        reindexed : Series\n        \"\"\"\n        return self.reindex(other.index, method=method)\n\n    def take(self, indices, axis=0):\n        \"\"\"\n        Analogous to ndarray.take, return Series corresponding to requested\n        indices\n\n        Parameters\n        ----------\n        indices : list / array of ints\n\n        Returns\n        -------\n        taken : Series\n        \"\"\"\n        new_index = self.index.take(indices)\n        new_values = self.values.take(indices)\n        return Series(new_values, index=new_index, name=self.name)\n\n    truncate = generic.truncate\n\n    def fillna(self, value=None, method='pad'):\n        \"\"\"\n        Fill NA/NaN values using the specified method\n\n        Parameters\n        ----------\n        value : any kind (should be same type as array)\n            Value to use to fill holes (e.g. 0)\n        method : {'backfill', 'bfill', 'pad', 'ffill', None}, default 'pad'\n            Method to use for filling holes in reindexed Series\n            pad / ffill: propagate last valid observation forward to next valid\n            backfill / bfill: use NEXT valid observation to fill gap\n\n        See also\n        --------\n        reindex, asfreq\n\n        Returns\n        -------\n        filled : Series\n        \"\"\"\n        if value is not None:\n            newSeries = self.copy()\n            newSeries[isnull(newSeries)] = value\n            return newSeries\n        else:\n            if method is None:  # pragma: no cover\n                raise ValueError('must specify a fill method')\n\n            method = method.lower()\n\n            if method == 'ffill':\n                method = 'pad'\n            if method == 'bfill':\n                method = 'backfill'\n\n            mask = isnull(self.values)\n\n            # sadness. for Python 2.5 compatibility\n            mask = mask.astype(np.uint8)\n\n            if method == 'pad':\n                indexer = lib.get_pad_indexer(mask)\n            elif method == 'backfill':\n                indexer = lib.get_backfill_indexer(mask)\n\n            new_values = self.values.take(indexer)\n            return Series(new_values, index=self.index, name=self.name)\n\n    def isin(self, values):\n        \"\"\"\n        Return boolean vector showing whether each element in the Series is\n        exactly contained in the passed sequence of values\n\n        Parameters\n        ----------\n        values : sequence\n\n        Returns\n        -------\n        isin : Series (boolean dtype)\n        \"\"\"\n        value_set = set(values)\n        result = lib.ismember(self, value_set)\n        # return self.map(value_set.__contains__)\n        return Series(result, self.index, name=self.name)\n\n#-------------------------------------------------------------------------------\n# Miscellaneous\n\n    def plot(self, label=None, kind='line', use_index=True, rot=30, ax=None,\n             style='-', grid=True, logy=False, **kwds):\n        \"\"\"\n        Plot the input series with the index on the x-axis using matplotlib\n\n        Parameters\n        ----------\n        label : label argument to provide to plot\n        kind : {'line', 'bar'}\n        rot : int, default 30\n            Rotation for tick labels\n        use_index : boolean, default True\n            Plot index as axis tick labels\n        ax : matplotlib axis object\n            If not passed, uses gca()\n        style : string, default '-'\n            matplotlib line style to use\n        kwds : keywords\n            To be passed to the actual plotting function\n\n        Notes\n        -----\n        See matplotlib documentation online for more on this subject\n        Intended to be used in ipython --pylab mode\n        \"\"\"\n        import matplotlib.pyplot as plt\n\n        if label is not None:\n            kwds = kwds.copy()\n            kwds['label'] = label\n\n        N = len(self)\n\n        if ax is None:\n            ax = plt.gca()\n\n        if kind == 'line':\n            if use_index:\n                x = np.asarray(self.index)\n            else:\n                x = range(len(self))\n\n            if logy:\n                ax.semilogy(x, self.values.astype(float), style, **kwds)\n            else:\n                ax.plot(x, self.values.astype(float), style, **kwds)\n        elif kind == 'bar':\n            xinds = np.arange(N) + 0.25\n            ax.bar(xinds, self.values.astype(float), 0.5,\n                   bottom=np.zeros(N), linewidth=1, **kwds)\n\n            if N < 10:\n                fontsize = 12\n            else:\n                fontsize = 10\n\n            ax.set_xticks(xinds + 0.25)\n            ax.set_xticklabels(self.index, rotation=rot, fontsize=fontsize)\n\n        ax.grid(grid)\n\n        # try to make things prettier\n        try:\n            fig = plt.gcf()\n            fig.autofmt_xdate()\n        except Exception:  # pragma: no cover\n            pass\n\n        plt.draw_if_interactive()\n\n        return ax\n\n    def hist(self, ax=None, grid=True, **kwds):\n        \"\"\"\n        Draw histogram of the input series using matplotlib\n\n        Parameters\n        ----------\n        ax : matplotlib axis object\n            If not passed, uses gca()\n        kwds : keywords\n            To be passed to the actual plotting function\n\n        Notes\n        -----\n        See matplotlib documentation online for more on this\n\n        \"\"\"\n        import matplotlib.pyplot as plt\n\n        if ax is None:\n            ax = plt.gca()\n\n        values = self.dropna().values\n\n        ax.hist(values, **kwds)\n        ax.grid(grid)\n\n        return ax\n\n    @classmethod\n    def from_csv(cls, path, sep=',', parse_dates=True):\n        \"\"\"\n        Read delimited file into Series\n\n        Parameters\n        ----------\n        path : string\n        sep : string, default ','\n            Field delimiter\n        parse_dates : boolean, default True\n            Parse dates. Different default from read_table\n\n        Returns\n        -------\n        y : Series\n        \"\"\"\n        from pandas.core.frame import DataFrame\n        df = DataFrame.from_csv(path, header=None, sep=sep, parse_dates=parse_dates)\n        return df[df.columns[0]]\n\n    def to_csv(self, path):\n        \"\"\"\n        Write the Series to a CSV file\n\n        Parameters\n        ----------\n        path : string or None\n            Output filepath. If None, write to stdout\n        \"\"\"\n        f = open(path, 'w')\n        csvout = csv.writer(f, lineterminator='\\n')\n        csvout.writerows(self.iteritems())\n        f.close()\n\n    def dropna(self):\n        \"\"\"\n        Return Series without null values\n\n        Returns\n        -------\n        valid : Series\n        \"\"\"\n        return remove_na(self)\n\n    valid = dropna\n\n    isnull = isnull\n    notnull = notnull\n\n    def first_valid_index(self):\n        \"\"\"\n        Return label for first non-NA/null value\n        \"\"\"\n        if len(self) == 0:\n            return None\n\n        mask = isnull(self.values)\n        i = mask.argmin()\n        if mask[i]:\n            return None\n        else:\n            return self.index[i]\n\n    def last_valid_index(self):\n        \"\"\"\n        Return label for last non-NA/null value\n        \"\"\"\n        if len(self) == 0:\n            return None\n\n        mask = isnull(self.values[::-1])\n        i = mask.argmin()\n        if mask[i]:\n            return None\n        else:\n            return self.index[len(self) - i - 1]\n\n    #----------------------------------------------------------------------\n    # Time series-oriented methods\n\n    def shift(self, periods, offset=None, **kwds):\n        \"\"\"\n        Shift the index of the Series by desired number of periods with an\n        optional time offset\n\n        Parameters\n        ----------\n        periods : int\n            Number of periods to move, can be positive or negative\n        offset : DateOffset, timedelta, or time rule string, optional\n            Increment to use from datetools module or time rule (e.g. 'EOM')\n\n        Returns\n        -------\n        shifted : Series\n        \"\"\"\n        if periods == 0:\n            return self.copy()\n\n        offset = kwds.get('timeRule', offset)\n        if isinstance(offset, basestring):\n            offset = datetools.getOffset(offset)\n\n        if offset is None:\n            new_values = np.empty(len(self), dtype=self.dtype)\n            new_values = _maybe_upcast(new_values)\n\n            if periods > 0:\n                new_values[periods:] = self.values[:-periods]\n                new_values[:periods] = nan\n            elif periods < 0:\n                new_values[:periods] = self.values[-periods:]\n                new_values[periods:] = nan\n\n            return Series(new_values, index=self.index, name=self.name)\n        else:\n            return Series(self, index=self.index.shift(periods, offset),\n                          name=self.name)\n\n    def asof(self, date):\n        \"\"\"\n        Return last good (non-NaN) value in TimeSeries if value is NaN for\n        requested date.\n\n        If there is no good value, NaN is returned.\n\n        Parameters\n        ----------\n        date : datetime or similar value\n\n        Notes\n        -----\n        Dates are assumed to be sorted\n\n        Returns\n        -------\n        value or NaN\n        \"\"\"\n        if isinstance(date, basestring):\n            date = datetools.to_datetime(date)\n\n        v = self.get(date)\n\n        if isnull(v):\n            candidates = self.index[notnull(self)]\n            index = candidates.searchsorted(date)\n\n            if index > 0:\n                asOfDate = candidates[index - 1]\n            else:\n                return nan\n\n            return self.get(asOfDate)\n        else:\n            return v\n\n    def asfreq(self, freq, method=None):\n        \"\"\"\n        Convert this TimeSeries to the provided frequency using DateOffset\n        object or time rule. Optionally provide fill method to pad/backfill\n        missing values.\n\n        Parameters\n        ----------\n        offset : DateOffset object, or string in {'WEEKDAY', 'EOM'}\n            DateOffset object or subclass (e.g. monthEnd)\n        method : {'backfill', 'pad', None}\n            Method to use for filling holes in new index\n\n        Returns\n        -------\n        converted : TimeSeries\n        \"\"\"\n        if isinstance(freq, datetools.DateOffset):\n            dateRange = DateRange(self.index[0], self.index[-1], offset=freq)\n        else:\n            dateRange = DateRange(self.index[0], self.index[-1], time_rule=freq)\n\n        return self.reindex(dateRange, method=method)\n\n    def interpolate(self, method='linear'):\n        \"\"\"\n        Interpolate missing values (after the first valid value)\n\n        Parameters\n        ----------\n        method : {'linear', 'time'}\n            Interpolation method.\n            Time interpolation works on daily and higher resolution\n            data to interpolate given length of interval\n\n        Returns\n        -------\n        interpolated : Series\n        \"\"\"\n        if method == 'time':\n            if not isinstance(self, TimeSeries):\n                raise Exception('time-weighted interpolation only works'\n                                'on TimeSeries')\n            inds = np.array([d.toordinal() for d in self.index])\n        else:\n            inds = np.arange(len(self))\n\n        values = self.values\n\n        invalid = isnull(values)\n        valid = -invalid\n\n        firstIndex = valid.argmax()\n        valid = valid[firstIndex:]\n        invalid = invalid[firstIndex:]\n        inds = inds[firstIndex:]\n\n        result = values.copy()\n        result[firstIndex:][invalid] = np.interp(inds[invalid], inds[valid],\n                                                 values[firstIndex:][valid])\n\n        return Series(result, index=self.index, name=self.name)\n\n    def rename(self, mapper):\n        \"\"\"\n        Alter Series index using dict or function\n\n        Parameters\n        ----------\n        mapper : dict-like or function\n            Transformation to apply to each index\n\n        Notes\n        -----\n        Function / dict values must be unique (1-to-1)\n\n        Examples\n        --------\n        >>> x\n        foo 1\n        bar 2\n        baz 3\n\n        >>> x.rename(str.upper)\n        FOO 1\n        BAR 2\n        BAZ 3\n\n        >>> x.rename({'foo' : 'a', 'bar' : 'b', 'baz' : 'c'})\n        a 1\n        b 2\n        c 3\n\n        Returns\n        -------\n        renamed : Series (new object)\n        \"\"\"\n        mapper_f = _get_rename_function(mapper)\n        result = self.copy()\n        result.index = [mapper_f(x) for x in self.index]\n\n        return result\n\n    @property\n    def weekday(self):\n        return Series([d.weekday() for d in self.index], index=self.index)\n\n\nclass TimeSeries(Series):\n    pass\n\n_INDEX_TYPES = ndarray, Index, list, tuple\n\n#-------------------------------------------------------------------------------\n# Supplementary functions\n\ndef remove_na(arr):\n    \"\"\"\n    Return array containing only true/non-NaN values, possibly empty.\n    \"\"\"\n    return arr[notnull(arr)]\n\n\ndef _sanitize_array(data, index, dtype=None, copy=False,\n                    raise_cast_failure=False):\n    if isinstance(data, ma.MaskedArray):\n        mask = ma.getmaskarray(data)\n        data = ma.copy(data)\n        data[mask] = np.nan\n\n    try:\n        subarr = np.array(data, dtype=dtype, copy=copy)\n    except (ValueError, TypeError):\n        if dtype and raise_cast_failure:\n            raise\n        else:  # pragma: no cover\n            subarr = np.array(data, dtype=object)\n\n    if subarr.ndim == 0:\n        if isinstance(data, list):  # pragma: no cover\n            subarr = np.array(data, dtype=object)\n        elif index is not None:\n            value = data\n\n            # If we create an empty array using a string to infer\n            # the dtype, NumPy will only allocate one character per entry\n            # so this is kind of bad. Alternately we could use np.repeat\n            # instead of np.empty (but then you still don't want things\n            # coming out as np.str_!\n            if isinstance(value, basestring) and dtype is None:\n                dtype = np.object_\n\n            if dtype is None:\n                subarr = np.empty(len(index), dtype=type(value))\n            else:\n                subarr = np.empty(len(index), dtype=dtype)\n            subarr.fill(value)\n        else:\n            return subarr.item()\n    elif subarr.ndim > 1:\n        if isinstance(data, np.ndarray):\n            raise Exception('Data must be 1-dimensional')\n        else:\n            subarr = _asarray_tuplesafe(data, dtype=dtype)\n\n    # This is to prevent mixed-type Series getting all casted to\n    # NumPy string type, e.g. NaN --> '-1#IND'.\n    if issubclass(subarr.dtype.type, basestring):\n        subarr = np.array(data, dtype=object, copy=copy)\n\n    return subarr\n\ndef _get_rename_function(mapper):\n    if isinstance(mapper, (dict, Series)):\n        def f(x):\n            if x in mapper:\n                return mapper[x]\n            else:\n                return x\n    else:\n        f = mapper\n\n    return f\n",
      "file_patch": "@@ -220,21 +220,6 @@ copy : boolean, default False\n     _index = None\n     index = lib.SeriesIndex()\n \n-    # def _get_index(self):\n-    #     return self._index\n-\n-    # def _set_index(self, index):\n-    #     if not isinstance(index, _INDEX_TYPES):\n-    #         raise TypeError(\"Expected index to be in %s; was %s.\"\n-    #                         % (_INDEX_TYPES, type(index)))\n-\n-    #     if len(self) != len(index):\n-    #         raise AssertionError('Lengths of index and values did not match!')\n-\n-    #     self._index = _ensure_index(index)\n-\n-    # index = property(fget=_get_index, fset=_set_index)\n-\n     def __array_finalize__(self, obj):\n         \"\"\"\n         Gets called after any ufunc or other array operations, necessary\n@@ -315,18 +300,143 @@ copy : boolean, default False\n             key = self._check_bool_indexer(key)\n             key = np.asarray(key, dtype=bool)\n \n-        return self._index_with(key)\n+        return self._get_with(key)\n \n-    def _index_with(self, key):\n+    def _get_with(self, key):\n         # other: fancy integer or otherwise\n-        # [slice(0, 5, None)] will break if you convert to ndarray,\n-        # e.g. as requested by np.median\n+        if isinstance(key, slice):\n+            indexer = self.ix._convert_to_indexer(key, axis=0)\n+            return self._get_values(indexer)\n+        else:\n+            # mpl hackaround\n+            if isinstance(key, tuple):\n+                try:\n+                    return self._get_values(key)\n+                except Exception:\n+                    pass\n+\n+            if not isinstance(key, (list, np.ndarray)):\n+                key = list(key)\n \n+            key_type = lib.infer_dtype(key)\n+\n+            if key_type == 'integer':\n+                if self.index.inferred_type == 'integer':\n+                    return self.reindex(key)\n+                else:\n+                    return self._get_values(key)\n+            elif key_type == 'boolean':\n+                return self._get_values(key)\n+            else:\n+                try:\n+                    return self.reindex(key)\n+                except Exception:\n+                    # [slice(0, 5, None)] will break if you convert to ndarray,\n+                    # e.g. as requested by np.median\n+                    # hack\n+                    if isinstance(key[0], slice):\n+                        return self._get_values(key)\n+                    raise\n+\n+    def __setitem__(self, key, value):\n+        values = self.values\n         try:\n-            return Series(self.values[key], index=self.index[key],\n+            values[self.index.get_loc(key)] = value\n+            return\n+        except KeyError:\n+            if (com.is_integer(key)\n+                and not self.index.inferred_type == 'integer'):\n+\n+                values[key] = value\n+                return\n+\n+            raise KeyError('%s not in this series!' % str(key))\n+        except TypeError:\n+            # Could not hash item\n+            pass\n+\n+        if _is_bool_indexer(key):\n+            key = self._check_bool_indexer(key)\n+            key = np.asarray(key, dtype=bool)\n+\n+        self._set_with(key, value)\n+\n+    def _set_with(self, key, value):\n+        # other: fancy integer or otherwise\n+        if isinstance(key, slice):\n+            indexer = self.ix._convert_to_indexer(key, axis=0)\n+            return self._set_values(indexer, value)\n+        else:\n+            if isinstance(key, tuple):\n+                try:\n+                    self._set_values(key, value)\n+                except Exception:\n+                    pass\n+\n+            if not isinstance(key, (list, np.ndarray)):\n+                key = list(key)\n+\n+            key_type = lib.infer_dtype(key)\n+\n+            if key_type == 'integer':\n+                if self.index.inferred_type == 'integer':\n+                    self._set_labels(key, value)\n+                else:\n+                    return self._set_values(key, value)\n+            elif key_type == 'boolean':\n+                self._set_values(key, value)\n+            else:\n+                self._set_labels(key, value)\n+\n+    def _set_labels(self, key, value):\n+        key = _asarray_tuplesafe(key)\n+        indexer = self.index.get_indexer(key)\n+        mask = indexer == -1\n+        if mask.any():\n+            raise ValueError('%s not contained in the index'\n+                             % str(key[mask]))\n+        self._set_values(indexer, value)\n+\n+    def _get_values(self, indexer):\n+        try:\n+            return Series(self.values[indexer], index=self.index[indexer],\n                           name=self.name)\n         except Exception:\n-            return self.values[key]\n+            return self.values[indexer]\n+\n+    def _set_values(self, key, value):\n+        self.values[key] = value\n+\n+    # help out SparseSeries\n+    _get_val_at = ndarray.__getitem__\n+\n+    def __getslice__(self, i, j):\n+        if i < 0:\n+            i = 0\n+        if j < 0:\n+            j = 0\n+        slobj = slice(i, j)\n+        return self.__getitem__(slobj)\n+\n+    def _check_bool_indexer(self, key):\n+        # boolean indexing, need to check that the data are aligned, otherwise\n+        # disallowed\n+        result = key\n+        if isinstance(key, Series) and key.dtype == np.bool_:\n+            if not key.index.equals(self.index):\n+                result = key.reindex(self.index)\n+\n+        if isinstance(result, np.ndarray) and result.dtype == np.object_:\n+            mask = isnull(result)\n+            if mask.any():\n+                raise ValueError('cannot index with vector containing '\n+                                 'NA / NaN values')\n+\n+        return result\n+\n+    def __setslice__(self, i, j, value):\n+        \"\"\"Set slice equal to given value(s)\"\"\"\n+        ndarray.__setslice__(self, i, j, value)\n \n     def get(self, label, default=None):\n         \"\"\"\n@@ -390,67 +500,6 @@ copy : boolean, default False\n             new_values = np.concatenate([self.values, [value]])\n             return Series(new_values, index=new_index, name=self.name)\n \n-    # help out SparseSeries\n-    _get_val_at = ndarray.__getitem__\n-\n-    def __getslice__(self, i, j):\n-        if i < 0:\n-            i = 0\n-        if j < 0:\n-            j = 0\n-        slobj = slice(i, j)\n-        return self.__getitem__(slobj)\n-\n-    def __setitem__(self, key, value):\n-        values = self.values\n-        try:\n-            values[self.index.get_loc(key)] = value\n-            return\n-        except KeyError:\n-            if (com.is_integer(key)\n-                and not self.index.inferred_type == 'integer'):\n-\n-                values[key] = value\n-                return\n-\n-            raise KeyError('%s not in this series!' % str(key))\n-        except TypeError:\n-            # Could not hash item\n-            pass\n-\n-        key = self._check_bool_indexer(key)\n-\n-        # special handling of boolean data with NAs stored in object\n-        # arrays. Sort of an elaborate hack since we can't represent boolean\n-        # NA. Hmm\n-        if isinstance(key, np.ndarray) and key.dtype == np.object_:\n-            if set([True, False]).issubset(set(key)):\n-                key = np.asarray(key, dtype=bool)\n-                values[key] = value\n-                return\n-\n-        values[key] = value\n-\n-    def _check_bool_indexer(self, key):\n-        # boolean indexing, need to check that the data are aligned, otherwise\n-        # disallowed\n-        result = key\n-        if isinstance(key, Series) and key.dtype == np.bool_:\n-            if not key.index.equals(self.index):\n-                result = key.reindex(self.index)\n-\n-        if isinstance(result, np.ndarray) and result.dtype == np.object_:\n-            mask = isnull(result)\n-            if mask.any():\n-                raise ValueError('cannot index with vector containing '\n-                                 'NA / NaN values')\n-\n-        return result\n-\n-    def __setslice__(self, i, j, value):\n-        \"\"\"Set slice equal to given value(s)\"\"\"\n-        ndarray.__setslice__(self, i, j, value)\n-\n     def __repr__(self):\n         \"\"\"Clean string representation of a Series\"\"\"\n         width, height = get_terminal_size()\n",
      "files_name_in_blame_commit": [
        "groupby.py",
        "series.py",
        "test_series.py",
        "test_moments.py",
        "test_frame.py",
        "test_merge.py",
        "generic.py",
        "test_multilevel.py",
        "inference.pyx",
        "indexing.py"
      ]
    }
  },
  "commits_modify_file_before_fix": {
    "size": 1345
  },
  "recursive_blame_commits": {}
}