{
  "id": "10",
  "blame_commit": {
    "commit": {
      "commit_id": "e41ecf7a4cafaa8abddb948b2c2d53ca0af480a1",
      "commit_message": "Cleans up get_work logic for assistants\n\nSome logic was added to get_work to treat assistants as a special case\nthroughout the function to deal with the fact that assistants aren't necessarily\nstakeholders of the tasks they'll be running. This commit cleans up some of that\nlogic by performing a check that the worker is either in the stakeholders or is\nan assistant and using that in place of all the special cases.\n\nThis also cleans up the greedy logic to treat assistants more like other workers.",
      "commit_author": "Dave Buchfuhrer",
      "commit_date": "2015-03-05 11:44:42",
      "commit_parent": "abbe8ec61179d0a702fef5f5508ff7c00c2fe647"
    },
    "function": {
      "function_name": "get_work",
      "function_code_before": "def get_work(self, worker, host=None, assistant=False, **kwargs):\n    self.update(worker, {'host': host})\n    best_task = None\n    best_task_id = None\n    locally_pending_tasks = 0\n    running_tasks = []\n    used_resources = self._used_resources()\n    greedy_resources = collections.defaultdict(int)\n    n_unique_pending = 0\n    greedy_workers = dict(((worker.id, worker.info.get('workers', 1)) for worker in self._state.get_active_workers()))\n    tasks = list(self._state.get_pending_tasks())\n    tasks.sort(key=self._rank(), reverse=True)\n    for task in tasks:\n        if task.status == 'RUNNING' and worker in task.workers:\n            other_worker = self._state.get_worker(task.worker_running)\n            more_info = {'task_id': task.id, 'worker': str(other_worker)}\n            if other_worker is not None:\n                more_info.update(other_worker.info)\n                running_tasks.append(more_info)\n        if task.status == PENDING and (worker in task.workers or assistant):\n            locally_pending_tasks += 1\n            if len(task.workers) == 1:\n                n_unique_pending += 1\n        if task.status == RUNNING and (task.worker_running in greedy_workers or assistant):\n            greedy_workers[task.worker_running] -= 1\n            for (resource, amount) in six.iteritems(task.resources or {}):\n                greedy_resources[resource] += amount\n        if not best_task and self._schedulable(task) and self._has_resources(task.resources, greedy_resources):\n            if (worker in task.workers or assistant) and self._has_resources(task.resources, used_resources):\n                best_task = task\n                best_task_id = task.id\n            else:\n                for task_worker in task.workers:\n                    if greedy_workers.get(task_worker, 0) > 0:\n                        greedy_workers[task_worker] -= 1\n                        for (resource, amount) in six.iteritems(task.resources or {}):\n                            greedy_resources[resource] += amount\n                        break\n    reply = {'n_pending_tasks': locally_pending_tasks, 'running_tasks': running_tasks, 'task_id': None, 'n_unique_pending': n_unique_pending}\n    if best_task:\n        self._state.set_status(best_task, RUNNING, self._config)\n        best_task.worker_running = worker\n        best_task.time_running = time.time()\n        self._update_task_history(best_task.id, RUNNING, host=host)\n        reply['task_id'] = best_task.id\n        reply['task_family'] = best_task.family\n        reply['task_params'] = best_task.params\n    return reply",
      "function_code_after": "def get_work(self, worker, host=None, assistant=False, **kwargs):\n    self.update(worker, {'host': host})\n    best_task = None\n    locally_pending_tasks = 0\n    running_tasks = []\n    used_resources = self._used_resources()\n    greedy_resources = collections.defaultdict(int)\n    n_unique_pending = 0\n    greedy_workers = dict(((worker.id, worker.info.get('workers', 1)) for worker in self._state.get_active_workers()))\n    tasks = list(self._state.get_pending_tasks())\n    tasks.sort(key=self._rank(), reverse=True)\n    for task in tasks:\n        in_workers = assistant or worker in task.workers\n        if task.status == 'RUNNING' and in_workers:\n            other_worker = self._state.get_worker(task.worker_running)\n            more_info = {'task_id': task.id, 'worker': str(other_worker)}\n            if other_worker is not None:\n                more_info.update(other_worker.info)\n                running_tasks.append(more_info)\n        if task.status == PENDING and in_workers:\n            locally_pending_tasks += 1\n            if len(task.workers) == 1 and (not assistant):\n                n_unique_pending += 1\n        if task.status == RUNNING and task.worker_running in greedy_workers:\n            greedy_workers[task.worker_running] -= 1\n            for (resource, amount) in six.iteritems(task.resources or {}):\n                greedy_resources[resource] += amount\n        if not best_task and self._schedulable(task) and self._has_resources(task.resources, greedy_resources):\n            if in_workers and self._has_resources(task.resources, used_resources):\n                best_task = task\n            else:\n                workers = itertools.chain(task.workers, [worker]) if assistant else task.workers\n                for task_worker in workers:\n                    if greedy_workers.get(task_worker, 0) > 0:\n                        greedy_workers[task_worker] -= 1\n                        for (resource, amount) in six.iteritems(task.resources or {}):\n                            greedy_resources[resource] += amount\n                        break\n    reply = {'n_pending_tasks': locally_pending_tasks, 'running_tasks': running_tasks, 'task_id': None, 'n_unique_pending': n_unique_pending}\n    if best_task:\n        self._state.set_status(best_task, RUNNING, self._config)\n        best_task.worker_running = worker\n        best_task.time_running = time.time()\n        self._update_task_history(best_task.id, RUNNING, host=host)\n        reply['task_id'] = best_task.id\n        reply['task_family'] = best_task.family\n        reply['task_params'] = best_task.params\n    return reply",
      "function_before_start_line": 609,
      "function_before_end_line": 690,
      "function_after_start_line": 609,
      "function_after_end_line": 690,
      "function_before_token_count": 464,
      "function_after_token_count": 464,
      "functions_name_modified_file": [
        "add_failure",
        "get_worker_ids",
        "prune",
        "_schedulable",
        "clear",
        "has_excessive_failures",
        "_upstream_status",
        "_update_task_history",
        "can_disable",
        "ping",
        "inverse_dep_graph",
        "get_pending_tasks",
        "_traverse_inverse_deps",
        "load",
        "num_failures",
        "task_search",
        "_recurse_deps",
        "get_worker",
        "get_active_workers",
        "worker_list",
        "add_task",
        "update_resources",
        "_update_priority",
        "task_list",
        "add_info",
        "get_active_tasks",
        "_has_resources",
        "_used_resources",
        "has_task",
        "fix_time",
        "_serialize_task",
        "task_history",
        "dep_graph",
        "__str__",
        "graph",
        "_get_default",
        "dump",
        "__repr__",
        "re_enable",
        "inactivate_tasks",
        "__init__",
        "update",
        "set_status",
        "add_worker",
        "get_task",
        "get_running_tasks",
        "_rank",
        "get_work",
        "re_enable_task",
        "fetch_error",
        "inactivate_workers"
      ],
      "functions_name_all_files": [
        "add_failure",
        "get_worker_ids",
        "prune",
        "_schedulable",
        "clear",
        "has_excessive_failures",
        "_upstream_status",
        "_update_task_history",
        "can_disable",
        "ping",
        "inverse_dep_graph",
        "get_pending_tasks",
        "_traverse_inverse_deps",
        "load",
        "num_failures",
        "task_search",
        "_recurse_deps",
        "get_worker",
        "get_active_workers",
        "worker_list",
        "add_task",
        "update_resources",
        "_update_priority",
        "task_list",
        "add_info",
        "get_active_tasks",
        "_has_resources",
        "_used_resources",
        "has_task",
        "fix_time",
        "_serialize_task",
        "task_history",
        "dep_graph",
        "__str__",
        "graph",
        "_get_default",
        "dump",
        "__repr__",
        "re_enable",
        "inactivate_tasks",
        "__init__",
        "update",
        "set_status",
        "add_worker",
        "get_task",
        "get_running_tasks",
        "_rank",
        "get_work",
        "re_enable_task",
        "fetch_error",
        "inactivate_workers"
      ],
      "functions_name_co_evolved_modified_file": [],
      "functions_name_co_evolved_all_files": []
    },
    "file": {
      "file_name": "scheduler.py",
      "file_nloc": 636,
      "file_complexity": 231,
      "file_token_count": 4824,
      "file_before": "# -*- coding: utf-8 -*-\n#\n# Copyright 2012-2015 Spotify AB\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\"\"\"\nThe system for scheduling tasks and executing them in order.\nDeals with dependencies, priorities, resources, etc.\nThe :py:class:`~luigi.worker.Worker` pulls tasks from the scheduler (usually over the REST interface) and executes them.\nSee :doc:`/central_scheduler` for more info.\n\"\"\"\n\nimport collections\ntry:\n    import cPickle as pickle\nexcept ImportError:\n    import pickle\nimport datetime\nimport functools\nimport itertools\nimport logging\nimport os\nimport time\n\nfrom luigi import six\n\nfrom luigi import configuration\nfrom luigi import notifications\nfrom luigi import parameter\nfrom luigi import task_history as history\nfrom luigi.task_status import DISABLED, DONE, FAILED, PENDING, RUNNING, SUSPENDED, UNKNOWN\nfrom luigi.task import Config\n\nlogger = logging.getLogger(\"luigi.server\")\n\n\nclass Scheduler(object):\n    \"\"\"\n    Abstract base class.\n\n    Note that the methods all take string arguments, not Task objects...\n    \"\"\"\"\"\n    add_task = NotImplemented\n    get_work = NotImplemented\n    ping = NotImplemented\n\nUPSTREAM_RUNNING = 'UPSTREAM_RUNNING'\nUPSTREAM_MISSING_INPUT = 'UPSTREAM_MISSING_INPUT'\nUPSTREAM_FAILED = 'UPSTREAM_FAILED'\nUPSTREAM_DISABLED = 'UPSTREAM_DISABLED'\n\nUPSTREAM_SEVERITY_ORDER = (\n    '',\n    UPSTREAM_RUNNING,\n    UPSTREAM_MISSING_INPUT,\n    UPSTREAM_FAILED,\n    UPSTREAM_DISABLED,\n)\nUPSTREAM_SEVERITY_KEY = UPSTREAM_SEVERITY_ORDER.index\nSTATUS_TO_UPSTREAM_MAP = {\n    FAILED: UPSTREAM_FAILED,\n    RUNNING: UPSTREAM_RUNNING,\n    PENDING: UPSTREAM_MISSING_INPUT,\n    DISABLED: UPSTREAM_DISABLED,\n}\n\n\nclass scheduler(Config):\n    # TODO(erikbern): the config_path is needed for backwards compatilibity. We should drop the compatibility\n    # at some point (in particular this would force users to replace all dashes with underscores in the config)\n    retry_delay = parameter.FloatParameter(default=900.0,\n                                           config_path=dict(section='scheduler', name='retry-delay'))\n    remove_delay = parameter.FloatParameter(default=600.0,\n                                            config_path=dict(section='scheduler', name='remove-delay'))\n    worker_disconnect_delay = parameter.FloatParameter(default=60.0,\n                                                       config_path=dict(section='scheduler', name='worker-disconnect-delay'))\n    state_path = parameter.Parameter(default='/var/lib/luigi-server/state.pickle',\n                                     config_path=dict(section='scheduler', name='state-path'))\n\n    # Jobs are disabled if we see more than disable_failures failures in disable_window seconds.\n    # These disables last for disable_persist seconds.\n    disable_window = parameter.IntParameter(default=3600,\n                                            config_path=dict(section='scheduler', name='disable-window-seconds'))\n    disable_failures = parameter.IntParameter(default=None,\n                                              config_path=dict(section='scheduler', name='disable-num-failures'))\n    disable_persist = parameter.IntParameter(default=86400,\n                                             config_path=dict(section='scheduler', name='disable-persist-seconds'))\n    max_shown_tasks = parameter.IntParameter(default=100000,\n                                             config_path=dict(section='scheduler', name='max-shown-task'))\n\n    record_task_history = parameter.BoolParameter(default=False,\n                                                  config_path=dict(section='scheduler', name='record_task_history'))\n\n\ndef fix_time(x):\n    # Backwards compatibility for a fix in Dec 2014. Prior to the fix, pickled state might store datetime objects\n    # Let's remove this function soon\n    if isinstance(x, datetime.datetime):\n        return time.mktime(x.timetuple())\n    else:\n        return x\n\n\nclass Failures(object):\n    \"\"\"\n    This class tracks the number of failures in a given time window.\n\n    Failures added are marked with the current timestamp, and this class counts\n    the number of failures in a sliding time window ending at the present.\n    \"\"\"\n\n    def __init__(self, window):\n        \"\"\"\n        Initialize with the given window.\n\n        :param window: how long to track failures for, as a float (number of seconds).\n        \"\"\"\n        self.window = window\n        self.failures = collections.deque()\n\n    def add_failure(self):\n        \"\"\"\n        Add a failure event with the current timestamp.\n        \"\"\"\n        self.failures.append(time.time())\n\n    def num_failures(self):\n        \"\"\"\n        Return the number of failures in the window.\n        \"\"\"\n        min_time = time.time() - self.window\n\n        while self.failures and fix_time(self.failures[0]) < min_time:\n            self.failures.popleft()\n\n        return len(self.failures)\n\n    def clear(self):\n        \"\"\"\n        Clear the failure queue.\n        \"\"\"\n        self.failures.clear()\n\n\ndef _get_default(x, default):\n    if x is not None:\n        return x\n    else:\n        return default\n\n\nclass Task(object):\n\n    def __init__(self, task_id, status, deps, resources=None, priority=0, family='', params=None,\n                 disable_failures=None, disable_window=None):\n        self.id = task_id\n        self.stakeholders = set()  # workers ids that are somehow related to this task (i.e. don't prune while any of these workers are still active)\n        self.workers = set()  # workers ids that can perform task - task is 'BROKEN' if none of these workers are active\n        if deps is None:\n            self.deps = set()\n        else:\n            self.deps = set(deps)\n        self.status = status  # PENDING, RUNNING, FAILED or DONE\n        self.time = time.time()  # Timestamp when task was first added\n        self.retry = None\n        self.remove = None\n        self.worker_running = None  # the worker id that is currently running the task or None\n        self.time_running = None  # Timestamp when picked up by worker\n        self.expl = None\n        self.priority = priority\n        self.resources = _get_default(resources, {})\n        self.family = family\n        self.params = _get_default(params, {})\n        self.disable_failures = disable_failures\n        self.failures = Failures(disable_window)\n        self.scheduler_disable_time = None\n\n    def __repr__(self):\n        return \"Task(%r)\" % vars(self)\n\n    def add_failure(self):\n        self.failures.add_failure()\n\n    def has_excessive_failures(self):\n        return self.failures.num_failures() >= self.disable_failures\n\n    def can_disable(self):\n        return self.disable_failures is not None\n\n\nclass Worker(object):\n    \"\"\"\n    Structure for tracking worker activity and keeping their references.\n    \"\"\"\n\n    def __init__(self, worker_id, last_active=None):\n        self.id = worker_id\n        self.reference = None  # reference to the worker in the real world. (Currently a dict containing just the host)\n        self.last_active = last_active  # seconds since epoch\n        self.started = time.time()  # seconds since epoch\n        self.info = {}\n\n    def add_info(self, info):\n        self.info.update(info)\n\n    def update(self, worker_reference):\n        if worker_reference:\n            self.reference = worker_reference\n        self.last_active = time.time()\n\n    def prune(self, config):\n        # Delete workers that haven't said anything for a while (probably killed)\n        if self.last_active + config.worker_disconnect_delay < time.time():\n            return True\n\n    def __str__(self):\n        return self.id\n\n\nclass SimpleTaskState(object):\n    \"\"\"\n    Keep track of the current state and handle persistance.\n\n    The point of this class is to enable other ways to keep state, eg. by using a database\n    These will be implemented by creating an abstract base class that this and other classes\n    inherit from.\n    \"\"\"\n\n    def __init__(self, state_path):\n        self._state_path = state_path\n        self._tasks = {}  # map from id to a Task object\n        self._status_tasks = collections.defaultdict(dict)\n        self._active_workers = {}  # map from id to a Worker object\n\n    def dump(self):\n        state = (self._tasks, self._active_workers)\n        try:\n            with open(self._state_path, 'w') as fobj:\n                pickle.dump(state, fobj)\n        except IOError:\n            logger.warning(\"Failed saving scheduler state\", exc_info=1)\n        else:\n            logger.info(\"Saved state in %s\", self._state_path)\n\n    # prone to lead to crashes when old state is unpickled with updated code. TODO some kind of version control?\n    def load(self):\n        if os.path.exists(self._state_path):\n            logger.info(\"Attempting to load state from %s\", self._state_path)\n            try:\n                with open(self._state_path, 'rb') as fobj:\n                    state = pickle.load(fobj)\n            except BaseException:\n                logger.exception(\"Error when loading state. Starting from clean slate.\")\n                return\n\n            self._tasks, self._active_workers = state\n            self._status_tasks = collections.defaultdict(dict)\n            for task in six.itervalues(self._tasks):\n                self._status_tasks[task.status][task.id] = task\n\n            # Convert from old format\n            # TODO: this is really ugly, we need something more future-proof\n            # Every time we add an attribute to the Worker class, this code needs to be updated\n            for k, v in six.iteritems(self._active_workers):\n                if isinstance(v, float):\n                    self._active_workers[k] = Worker(worker_id=k, last_active=v)\n        else:\n            logger.info(\"No prior state file exists at %s. Starting with clean slate\", self._state_path)\n\n    def get_active_tasks(self, status=None):\n        if status:\n            for task in six.itervalues(self._status_tasks[status]):\n                yield task\n        else:\n            for task in six.itervalues(self._tasks):\n                yield task\n\n    def get_running_tasks(self):\n        return six.itervalues(self._status_tasks[RUNNING])\n\n    def get_pending_tasks(self):\n        return itertools.chain.from_iterable(six.itervalues(self._status_tasks[status])\n                                             for status in [PENDING, RUNNING])\n\n    def get_task(self, task_id, default=None, setdefault=None):\n        if setdefault:\n            task = self._tasks.setdefault(task_id, setdefault)\n            self._status_tasks[task.status][task.id] = task\n            return task\n        else:\n            return self._tasks.get(task_id, default)\n\n    def has_task(self, task_id):\n        return task_id in self._tasks\n\n    def re_enable(self, task, config=None):\n        task.scheduler_disable_time = None\n        task.failures.clear()\n        if config:\n            self.set_status(task, FAILED, config)\n            task.failures.clear()\n\n    def set_status(self, task, new_status, config=None):\n        if new_status == FAILED:\n            assert config is not None\n\n        # not sure why we have SUSPENDED, as it can never be set\n        if new_status == SUSPENDED:\n            new_status = PENDING\n\n        if new_status == DISABLED and task.status == RUNNING:\n            return\n\n        if task.status == DISABLED:\n            if new_status == DONE:\n                self.re_enable(task)\n\n            # don't allow workers to override a scheduler disable\n            elif task.scheduler_disable_time is not None:\n                return\n\n        if new_status == FAILED and task.can_disable():\n            task.add_failure()\n            if task.has_excessive_failures():\n                task.scheduler_disable_time = time.time()\n                new_status = DISABLED\n                notifications.send_error_email(\n                    'Luigi Scheduler: DISABLED {task} due to excessive failures'.format(task=task.id),\n                    '{task} failed {failures} times in the last {window} seconds, so it is being '\n                    'disabled for {persist} seconds'.format(\n                        failures=config.disable_failures,\n                        task=task.id,\n                        window=config.disable_window,\n                        persist=config.disable_persist,\n                    ))\n        elif new_status == DISABLED:\n            task.scheduler_disable_time = None\n\n        self._status_tasks[task.status].pop(task.id)\n        self._status_tasks[new_status][task.id] = task\n        task.status = new_status\n\n    def prune(self, task, config):\n        remove = False\n\n        # Mark tasks with no remaining active stakeholders for deletion\n        if not task.stakeholders:\n            if task.remove is None:\n                logger.info(\"Task %r has stakeholders %r but none remain connected -> will remove \"\n                            \"task in %s seconds\", task.id, task.stakeholders, config.remove_delay)\n                task.remove = time.time() + config.remove_delay\n\n        # If a running worker disconnects, tag all its jobs as FAILED and subject it to the same retry logic\n        if task.status == RUNNING and task.worker_running and task.worker_running not in task.stakeholders:\n            logger.info(\"Task %r is marked as running by disconnected worker %r -> marking as \"\n                        \"FAILED with retry delay of %rs\", task.id, task.worker_running,\n                        config.retry_delay)\n            task.worker_running = None\n            self.set_status(task, FAILED, config)\n            task.retry = time.time() + config.retry_delay\n\n        # Re-enable task after the disable time expires\n        if task.status == DISABLED and task.scheduler_disable_time:\n            if time.time() - fix_time(task.scheduler_disable_time) > config.disable_persist:\n                self.re_enable(task, config)\n\n        # Remove tasks that have no stakeholders\n        if task.remove and time.time() > task.remove:\n            logger.info(\"Removing task %r (no connected stakeholders)\", task.id)\n            remove = True\n\n        # Reset FAILED tasks to PENDING if max timeout is reached, and retry delay is >= 0\n        if task.status == FAILED and config.retry_delay >= 0 and task.retry < time.time():\n            self.set_status(task, PENDING, config)\n\n        return remove\n\n    def inactivate_tasks(self, delete_tasks):\n        # The terminology is a bit confusing: we used to \"delete\" tasks when they became inactive,\n        # but with a pluggable state storage, you might very well want to keep some history of\n        # older tasks as well. That's why we call it \"inactivate\" (as in the verb)\n        for task in delete_tasks:\n            task_obj = self._tasks.pop(task)\n            self._status_tasks[task_obj.status].pop(task)\n\n    def get_active_workers(self, last_active_lt=None):\n        for worker in six.itervalues(self._active_workers):\n            if last_active_lt is not None and worker.last_active >= last_active_lt:\n                continue\n            yield worker\n\n    def get_worker_ids(self):\n        return self._active_workers.keys()  # only used for unit tests\n\n    def get_worker(self, worker_id):\n        return self._active_workers.setdefault(worker_id, Worker(worker_id))\n\n    def inactivate_workers(self, delete_workers):\n        # Mark workers as inactive\n        for worker in delete_workers:\n            self._active_workers.pop(worker)\n\n        # remove workers from tasks\n        for task in self.get_active_tasks():\n            task.stakeholders.difference_update(delete_workers)\n            task.workers.difference_update(delete_workers)\n\n\nclass CentralPlannerScheduler(Scheduler):\n    \"\"\"\n    Async scheduler that can handle multiple workers, etc.\n\n    Can be run locally or on a server (using RemoteScheduler + server.Server).\n    \"\"\"\n\n    def __init__(self, config=None, resources=None, task_history_impl=None, **kwargs):\n        \"\"\"\n        Keyword Arguments:\n        :param config: an object of class \"scheduler\" or None (in which the global instance will be used)\n        :param resources: a dict of str->int constraints\n        :param task_history_override: ignore config and use this object as the task history\n        \"\"\"\n        self._config = config or scheduler(**kwargs)\n        self._state = SimpleTaskState(self._config.state_path)\n\n        if task_history_impl:\n            self._task_history = task_history_impl\n        elif self._config.record_task_history:\n            import db_task_history  # Needs sqlalchemy, thus imported here\n            self._task_history = db_task_history.DbTaskHistory()\n        else:\n            self._task_history = history.NopHistory()\n        self._resources = resources or configuration.get_config().getintdict('resources')  # TODO: Can we make this a Parameter?\n        self._make_task = functools.partial(\n            Task, disable_failures=self._config.disable_failures,\n            disable_window=self._config.disable_window)\n\n    def load(self):\n        self._state.load()\n\n    def dump(self):\n        self._state.dump()\n\n    def prune(self):\n        logger.info(\"Starting pruning of task graph\")\n        remove_workers = []\n        for worker in self._state.get_active_workers():\n            if worker.prune(self._config):\n                logger.info(\"Worker %s timed out (no contact for >=%ss)\", worker, self._config.worker_disconnect_delay)\n                remove_workers.append(worker.id)\n\n        self._state.inactivate_workers(remove_workers)\n\n        remove_tasks = []\n        for task in self._state.get_active_tasks():\n            if self._state.prune(task, self._config):\n                remove_tasks.append(task.id)\n\n        self._state.inactivate_tasks(remove_tasks)\n\n        logger.info(\"Done pruning task graph\")\n\n    def update(self, worker_id, worker_reference=None):\n        \"\"\"\n        Keep track of whenever the worker was last active.\n        \"\"\"\n        worker = self._state.get_worker(worker_id)\n        worker.update(worker_reference)\n\n    def _update_priority(self, task, prio, worker):\n        \"\"\"\n        Update priority of the given task.\n\n        Priority can only be increased.\n        If the task doesn't exist, a placeholder task is created to preserve priority when the task is later scheduled.\n        \"\"\"\n        task.priority = prio = max(prio, task.priority)\n        for dep in task.deps or []:\n            t = self._state.get_task(dep)\n            if t is not None and prio > t.priority:\n                self._update_priority(t, prio, worker)\n\n    def add_task(self, worker, task_id, status=PENDING, runnable=True,\n                 deps=None, new_deps=None, expl=None, resources=None,\n                 priority=0, family='', params=None, **kwargs):\n        \"\"\"\n        * add task identified by task_id if it doesn't exist\n        * if deps is not None, update dependency list\n        * update status of task\n        * add additional workers/stakeholders\n        * update priority when needed\n        \"\"\"\n        self.update(worker)\n\n        task = self._state.get_task(task_id, setdefault=self._make_task(\n            task_id=task_id, status=PENDING, deps=deps, resources=resources,\n            priority=priority, family=family, params=params))\n\n        # for setting priority, we'll sometimes create tasks with unset family and params\n        if not task.family:\n            task.family = family\n        if not task.params:\n            task.params = _get_default(params, {})\n\n        if task.remove is not None:\n            task.remove = None  # unmark task for removal so it isn't removed after being added\n\n        if not (task.status == RUNNING and status == PENDING):\n            # don't allow re-scheduling of task while it is running, it must either fail or succeed first\n            if status == PENDING or status != task.status:\n                # Update the DB only if there was a acctual change, to prevent noise.\n                # We also check for status == PENDING b/c that's the default value\n                # (so checking for status != task.status woule lie)\n                self._update_task_history(task_id, status)\n            self._state.set_status(task, PENDING if status == SUSPENDED else status, self._config)\n            if status == FAILED:\n                task.retry = time.time() + self._config.retry_delay\n\n        if deps is not None:\n            task.deps = set(deps)\n\n        if new_deps is not None:\n            task.deps.update(new_deps)\n\n        task.stakeholders.add(worker)\n        task.resources = resources\n\n        # Task dependencies might not exist yet. Let's create dummy tasks for them for now.\n        # Otherwise the task dependencies might end up being pruned if scheduling takes a long time\n        for dep in task.deps or []:\n            t = self._state.get_task(dep, setdefault=self._make_task(task_id=dep, status=UNKNOWN, deps=None, priority=priority))\n            t.stakeholders.add(worker)\n\n        self._update_priority(task, priority, worker)\n\n        if runnable:\n            task.workers.add(worker)\n\n        if expl is not None:\n            task.expl = expl\n\n    def add_worker(self, worker, info, **kwargs):\n        self._state.get_worker(worker).add_info(info)\n\n    def update_resources(self, **resources):\n        if self._resources is None:\n            self._resources = {}\n        self._resources.update(resources)\n\n    def _has_resources(self, needed_resources, used_resources):\n        if needed_resources is None:\n            return True\n\n        available_resources = self._resources or {}\n        for resource, amount in six.iteritems(needed_resources):\n            if amount + used_resources[resource] > available_resources.get(resource, 1):\n                return False\n        return True\n\n    def _used_resources(self):\n        used_resources = collections.defaultdict(int)\n        if self._resources is not None:\n            for task in self._state.get_active_tasks():\n                if task.status == RUNNING and task.resources:\n                    for resource, amount in six.iteritems(task.resources):\n                        used_resources[resource] += amount\n        return used_resources\n\n    def _rank(self):\n        \"\"\"\n        Return worker's rank function for task scheduling.\n\n        :return:\n        \"\"\"\n        dependents = collections.defaultdict(int)\n\n        def not_done(t):\n            task = self._state.get_task(t, default=None)\n            return task is None or task.status != DONE\n        for task in self._state.get_pending_tasks():\n            if task.status != DONE:\n                deps = list(filter(not_done, task.deps))\n                inverse_num_deps = 1.0 / max(len(deps), 1)\n                for dep in deps:\n                    dependents[dep] += inverse_num_deps\n\n        return lambda task: (task.priority, dependents[task.id], -task.time)\n\n    def _schedulable(self, task):\n        if task.status != PENDING:\n            return False\n        for dep in task.deps:\n            dep_task = self._state.get_task(dep, default=None)\n            if dep_task is None or dep_task.status != DONE:\n                return False\n        return True\n\n    def get_work(self, worker, host=None, assistant=False, **kwargs):\n        # TODO: remove any expired nodes\n\n        # Algo: iterate over all nodes, find the highest priority node no dependencies and available\n        # resources.\n\n        # Resource checking looks both at currently available resources and at which resources would\n        # be available if all running tasks died and we rescheduled all workers greedily. We do both\n        # checks in order to prevent a worker with many low-priority tasks from starving other\n        # workers with higher priority tasks that share the same resources.\n\n        # TODO: remove tasks that can't be done, figure out if the worker has absolutely\n        # nothing it can wait for\n\n        # Return remaining tasks that have no FAILED descendents\n        self.update(worker, {'host': host})\n        best_task = None\n        best_task_id = None\n        locally_pending_tasks = 0\n        running_tasks = []\n\n        used_resources = self._used_resources()\n        greedy_resources = collections.defaultdict(int)\n        n_unique_pending = 0\n        greedy_workers = dict((worker.id, worker.info.get('workers', 1))\n                              for worker in self._state.get_active_workers())\n\n        tasks = list(self._state.get_pending_tasks())\n        tasks.sort(key=self._rank(), reverse=True)\n\n        for task in tasks:\n            if task.status == 'RUNNING' and worker in task.workers:\n                # Return a list of currently running tasks to the client,\n                # makes it easier to troubleshoot\n                other_worker = self._state.get_worker(task.worker_running)\n                more_info = {'task_id': task.id, 'worker': str(other_worker)}\n                if other_worker is not None:\n                    more_info.update(other_worker.info)\n                    running_tasks.append(more_info)\n\n            if task.status == PENDING and (worker in task.workers or assistant):\n                locally_pending_tasks += 1\n                if len(task.workers) == 1:\n                    n_unique_pending += 1\n\n            if task.status == RUNNING and (task.worker_running in greedy_workers or assistant):\n                greedy_workers[task.worker_running] -= 1\n                for resource, amount in six.iteritems((task.resources or {})):\n                    greedy_resources[resource] += amount\n\n            if not best_task and self._schedulable(task) and self._has_resources(task.resources, greedy_resources):\n                if (worker in task.workers or assistant) and self._has_resources(task.resources, used_resources):\n                    best_task = task\n                    best_task_id = task.id\n                else:\n                    for task_worker in task.workers:\n                        if greedy_workers.get(task_worker, 0) > 0:\n                            # use up a worker\n                            greedy_workers[task_worker] -= 1\n\n                            # keep track of the resources used in greedy scheduling\n                            for resource, amount in six.iteritems((task.resources or {})):\n                                greedy_resources[resource] += amount\n\n                            break\n\n        reply = {'n_pending_tasks': locally_pending_tasks,\n                 'running_tasks': running_tasks,\n                 'task_id': None,\n                 'n_unique_pending': n_unique_pending}\n\n        if best_task:\n            self._state.set_status(best_task, RUNNING, self._config)\n            best_task.worker_running = worker\n            best_task.time_running = time.time()\n            self._update_task_history(best_task.id, RUNNING, host=host)\n\n            reply['task_id'] = best_task.id\n            reply['task_family'] = best_task.family\n            reply['task_params'] = best_task.params\n\n        return reply\n\n    def ping(self, worker, **kwargs):\n        self.update(worker)\n\n    def _upstream_status(self, task_id, upstream_status_table):\n        if task_id in upstream_status_table:\n            return upstream_status_table[task_id]\n        elif self._state.has_task(task_id):\n            task_stack = [task_id]\n\n            while task_stack:\n                dep_id = task_stack.pop()\n                if self._state.has_task(dep_id):\n                    dep = self._state.get_task(dep_id)\n                    if dep_id not in upstream_status_table:\n                        if dep.status == PENDING and dep.deps:\n                            task_stack = task_stack + [dep_id] + list(dep.deps)\n                            upstream_status_table[dep_id] = ''  # will be updated postorder\n                        else:\n                            dep_status = STATUS_TO_UPSTREAM_MAP.get(dep.status, '')\n                            upstream_status_table[dep_id] = dep_status\n                    elif upstream_status_table[dep_id] == '' and dep.deps:\n                        # This is the postorder update step when we set the\n                        # status based on the previously calculated child elements\n                        upstream_status = [upstream_status_table.get(task_id, '') for task_id in dep.deps]\n                        upstream_status.append('')  # to handle empty list\n                        status = max(upstream_status, key=UPSTREAM_SEVERITY_KEY)\n                        upstream_status_table[dep_id] = status\n            return upstream_status_table[dep_id]\n\n    def _serialize_task(self, task_id, include_deps=True):\n        task = self._state.get_task(task_id)\n        ret = {\n            'deps': list(task.deps),\n            'status': task.status,\n            'workers': list(task.workers),\n            'worker_running': task.worker_running,\n            'time_running': getattr(task, \"time_running\", None),\n            'start_time': task.time,\n            'params': task.params,\n            'name': task.family,\n            'priority': task.priority,\n            'resources': task.resources,\n        }\n        if include_deps:\n            ret['deps'] = list(task.deps)\n        return ret\n\n    def graph(self, **kwargs):\n        self.prune()\n        serialized = {}\n        for task in self._state.get_active_tasks():\n            serialized[task.id] = self._serialize_task(task.id)\n        return serialized\n\n    def _recurse_deps(self, task_id, serialized):\n        if task_id not in serialized:\n            task = self._state.get_task(task_id)\n            if task is None or not task.family:\n                logger.warn('Missing task for id [%s]', task_id)\n\n                # try to infer family and params from task_id\n                try:\n                    family, _, param_str = task_id.rstrip(')').partition('(')\n                    params = dict(param.split('=') for param in param_str.split(', '))\n                except BaseException:\n                    family, params = '', {}\n                serialized[task_id] = {\n                    'deps': [],\n                    'status': UNKNOWN,\n                    'workers': [],\n                    'start_time': UNKNOWN,\n                    'params': params,\n                    'name': family,\n                    'priority': 0,\n                }\n            else:\n                serialized[task_id] = self._serialize_task(task_id)\n                for dep in task.deps:\n                    self._recurse_deps(dep, serialized)\n\n    def dep_graph(self, task_id, **kwargs):\n        self.prune()\n        serialized = {}\n        if self._state.has_task(task_id):\n            self._recurse_deps(task_id, serialized)\n        return serialized\n\n    def task_list(self, status, upstream_status, limit=True, **kwargs):\n        \"\"\"\n        Query for a subset of tasks by status.\n        \"\"\"\n        self.prune()\n        result = {}\n        upstream_status_table = {}  # used to memoize upstream status\n        for task in self._state.get_active_tasks(status):\n            if (task.status != PENDING or not upstream_status or\n                    upstream_status == self._upstream_status(task.id, upstream_status_table)):\n                serialized = self._serialize_task(task.id, False)\n                result[task.id] = serialized\n        if limit and len(result) > self._config.max_shown_tasks:\n            return {'num_tasks': len(result)}\n        return result\n\n    def worker_list(self, include_running=True, **kwargs):\n        self.prune()\n        workers = [\n            dict(\n                name=worker.id,\n                last_active=worker.last_active,\n                started=getattr(worker, 'started', None),\n                **worker.info\n            ) for worker in self._state.get_active_workers()]\n        workers.sort(key=lambda worker: worker['started'], reverse=True)\n        if include_running:\n            running = collections.defaultdict(dict)\n            num_pending = collections.defaultdict(int)\n            num_uniques = collections.defaultdict(int)\n            for task in self._state.get_pending_tasks():\n                if task.status == RUNNING and task.worker_running:\n                    running[task.worker_running][task.id] = self._serialize_task(task.id, False)\n                elif task.status == PENDING:\n                    for worker in task.workers:\n                        num_pending[worker] += 1\n                    if len(task.workers) == 1:\n                        num_uniques[list(task.workers)[0]] += 1\n            for worker in workers:\n                tasks = running[worker['name']]\n                worker['num_running'] = len(tasks)\n                worker['num_pending'] = num_pending[worker['name']]\n                worker['num_uniques'] = num_uniques[worker['name']]\n                worker['running'] = tasks\n        return workers\n\n    def inverse_dep_graph(self, task_id, **kwargs):\n        self.prune()\n        serialized = {}\n        if self._state.has_task(task_id):\n            self._traverse_inverse_deps(task_id, serialized)\n        return serialized\n\n    def _traverse_inverse_deps(self, task_id, serialized):\n        stack = [task_id]\n        serialized[task_id] = self._serialize_task(task_id)\n        while len(stack) > 0:\n            curr_id = stack.pop()\n            for task in self._state.get_active_tasks():\n                if curr_id in task.deps:\n                    serialized[curr_id][\"deps\"].append(task.id)\n                    if task.id not in serialized:\n                        serialized[task.id] = self._serialize_task(task.id)\n                        serialized[task.id][\"deps\"] = []\n                        stack.append(task.id)\n\n    def task_search(self, task_str, **kwargs):\n        \"\"\"\n        Query for a subset of tasks by task_id.\n\n        :param task_str:\n        :return:\n        \"\"\"\n        self.prune()\n        result = collections.defaultdict(dict)\n        for task in self._state.get_active_tasks():\n            if task.id.find(task_str) != -1:\n                serialized = self._serialize_task(task.id, False)\n                result[task.status][task.id] = serialized\n        return result\n\n    def re_enable_task(self, task_id):\n        serialized = {}\n        task = self._state.get_task(task_id)\n        if task and task.status == DISABLED and task.scheduler_disable_time:\n            self._state.re_enable(task, self._config)\n            serialized = self._serialize_task(task_id)\n        return serialized\n\n    def fetch_error(self, task_id, **kwargs):\n        if self._state.has_task(task_id):\n            return {\"taskId\": task_id, \"error\": self._state.get_task(task_id).expl}\n        else:\n            return {\"taskId\": task_id, \"error\": \"\"}\n\n    def _update_task_history(self, task_id, status, host=None):\n        try:\n            if status == DONE or status == FAILED:\n                successful = (status == DONE)\n                self._task_history.task_finished(task_id, successful)\n            elif status == PENDING:\n                self._task_history.task_scheduled(task_id)\n            elif status == RUNNING:\n                self._task_history.task_started(task_id, host)\n        except BaseException:\n            logger.warning(\"Error saving Task history\", exc_info=True)\n\n    @property\n    def task_history(self):\n        # Used by server.py to expose the calls\n        return self._task_history\n",
      "file_after": "# -*- coding: utf-8 -*-\n#\n# Copyright 2012-2015 Spotify AB\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\"\"\"\nThe system for scheduling tasks and executing them in order.\nDeals with dependencies, priorities, resources, etc.\nThe :py:class:`~luigi.worker.Worker` pulls tasks from the scheduler (usually over the REST interface) and executes them.\nSee :doc:`/central_scheduler` for more info.\n\"\"\"\n\nimport collections\ntry:\n    import cPickle as pickle\nexcept ImportError:\n    import pickle\nimport datetime\nimport functools\nimport itertools\nimport logging\nimport os\nimport time\n\nfrom luigi import six\n\nfrom luigi import configuration\nfrom luigi import notifications\nfrom luigi import parameter\nfrom luigi import task_history as history\nfrom luigi.task_status import DISABLED, DONE, FAILED, PENDING, RUNNING, SUSPENDED, UNKNOWN\nfrom luigi.task import Config\n\nlogger = logging.getLogger(\"luigi.server\")\n\n\nclass Scheduler(object):\n    \"\"\"\n    Abstract base class.\n\n    Note that the methods all take string arguments, not Task objects...\n    \"\"\"\"\"\n    add_task = NotImplemented\n    get_work = NotImplemented\n    ping = NotImplemented\n\nUPSTREAM_RUNNING = 'UPSTREAM_RUNNING'\nUPSTREAM_MISSING_INPUT = 'UPSTREAM_MISSING_INPUT'\nUPSTREAM_FAILED = 'UPSTREAM_FAILED'\nUPSTREAM_DISABLED = 'UPSTREAM_DISABLED'\n\nUPSTREAM_SEVERITY_ORDER = (\n    '',\n    UPSTREAM_RUNNING,\n    UPSTREAM_MISSING_INPUT,\n    UPSTREAM_FAILED,\n    UPSTREAM_DISABLED,\n)\nUPSTREAM_SEVERITY_KEY = UPSTREAM_SEVERITY_ORDER.index\nSTATUS_TO_UPSTREAM_MAP = {\n    FAILED: UPSTREAM_FAILED,\n    RUNNING: UPSTREAM_RUNNING,\n    PENDING: UPSTREAM_MISSING_INPUT,\n    DISABLED: UPSTREAM_DISABLED,\n}\n\n\nclass scheduler(Config):\n    # TODO(erikbern): the config_path is needed for backwards compatilibity. We should drop the compatibility\n    # at some point (in particular this would force users to replace all dashes with underscores in the config)\n    retry_delay = parameter.FloatParameter(default=900.0,\n                                           config_path=dict(section='scheduler', name='retry-delay'))\n    remove_delay = parameter.FloatParameter(default=600.0,\n                                            config_path=dict(section='scheduler', name='remove-delay'))\n    worker_disconnect_delay = parameter.FloatParameter(default=60.0,\n                                                       config_path=dict(section='scheduler', name='worker-disconnect-delay'))\n    state_path = parameter.Parameter(default='/var/lib/luigi-server/state.pickle',\n                                     config_path=dict(section='scheduler', name='state-path'))\n\n    # Jobs are disabled if we see more than disable_failures failures in disable_window seconds.\n    # These disables last for disable_persist seconds.\n    disable_window = parameter.IntParameter(default=3600,\n                                            config_path=dict(section='scheduler', name='disable-window-seconds'))\n    disable_failures = parameter.IntParameter(default=None,\n                                              config_path=dict(section='scheduler', name='disable-num-failures'))\n    disable_persist = parameter.IntParameter(default=86400,\n                                             config_path=dict(section='scheduler', name='disable-persist-seconds'))\n    max_shown_tasks = parameter.IntParameter(default=100000,\n                                             config_path=dict(section='scheduler', name='max-shown-task'))\n\n    record_task_history = parameter.BoolParameter(default=False,\n                                                  config_path=dict(section='scheduler', name='record_task_history'))\n\n\ndef fix_time(x):\n    # Backwards compatibility for a fix in Dec 2014. Prior to the fix, pickled state might store datetime objects\n    # Let's remove this function soon\n    if isinstance(x, datetime.datetime):\n        return time.mktime(x.timetuple())\n    else:\n        return x\n\n\nclass Failures(object):\n    \"\"\"\n    This class tracks the number of failures in a given time window.\n\n    Failures added are marked with the current timestamp, and this class counts\n    the number of failures in a sliding time window ending at the present.\n    \"\"\"\n\n    def __init__(self, window):\n        \"\"\"\n        Initialize with the given window.\n\n        :param window: how long to track failures for, as a float (number of seconds).\n        \"\"\"\n        self.window = window\n        self.failures = collections.deque()\n\n    def add_failure(self):\n        \"\"\"\n        Add a failure event with the current timestamp.\n        \"\"\"\n        self.failures.append(time.time())\n\n    def num_failures(self):\n        \"\"\"\n        Return the number of failures in the window.\n        \"\"\"\n        min_time = time.time() - self.window\n\n        while self.failures and fix_time(self.failures[0]) < min_time:\n            self.failures.popleft()\n\n        return len(self.failures)\n\n    def clear(self):\n        \"\"\"\n        Clear the failure queue.\n        \"\"\"\n        self.failures.clear()\n\n\ndef _get_default(x, default):\n    if x is not None:\n        return x\n    else:\n        return default\n\n\nclass Task(object):\n\n    def __init__(self, task_id, status, deps, resources=None, priority=0, family='', params=None,\n                 disable_failures=None, disable_window=None):\n        self.id = task_id\n        self.stakeholders = set()  # workers ids that are somehow related to this task (i.e. don't prune while any of these workers are still active)\n        self.workers = set()  # workers ids that can perform task - task is 'BROKEN' if none of these workers are active\n        if deps is None:\n            self.deps = set()\n        else:\n            self.deps = set(deps)\n        self.status = status  # PENDING, RUNNING, FAILED or DONE\n        self.time = time.time()  # Timestamp when task was first added\n        self.retry = None\n        self.remove = None\n        self.worker_running = None  # the worker id that is currently running the task or None\n        self.time_running = None  # Timestamp when picked up by worker\n        self.expl = None\n        self.priority = priority\n        self.resources = _get_default(resources, {})\n        self.family = family\n        self.params = _get_default(params, {})\n        self.disable_failures = disable_failures\n        self.failures = Failures(disable_window)\n        self.scheduler_disable_time = None\n\n    def __repr__(self):\n        return \"Task(%r)\" % vars(self)\n\n    def add_failure(self):\n        self.failures.add_failure()\n\n    def has_excessive_failures(self):\n        return self.failures.num_failures() >= self.disable_failures\n\n    def can_disable(self):\n        return self.disable_failures is not None\n\n\nclass Worker(object):\n    \"\"\"\n    Structure for tracking worker activity and keeping their references.\n    \"\"\"\n\n    def __init__(self, worker_id, last_active=None):\n        self.id = worker_id\n        self.reference = None  # reference to the worker in the real world. (Currently a dict containing just the host)\n        self.last_active = last_active  # seconds since epoch\n        self.started = time.time()  # seconds since epoch\n        self.info = {}\n\n    def add_info(self, info):\n        self.info.update(info)\n\n    def update(self, worker_reference):\n        if worker_reference:\n            self.reference = worker_reference\n        self.last_active = time.time()\n\n    def prune(self, config):\n        # Delete workers that haven't said anything for a while (probably killed)\n        if self.last_active + config.worker_disconnect_delay < time.time():\n            return True\n\n    def __str__(self):\n        return self.id\n\n\nclass SimpleTaskState(object):\n    \"\"\"\n    Keep track of the current state and handle persistance.\n\n    The point of this class is to enable other ways to keep state, eg. by using a database\n    These will be implemented by creating an abstract base class that this and other classes\n    inherit from.\n    \"\"\"\n\n    def __init__(self, state_path):\n        self._state_path = state_path\n        self._tasks = {}  # map from id to a Task object\n        self._status_tasks = collections.defaultdict(dict)\n        self._active_workers = {}  # map from id to a Worker object\n\n    def dump(self):\n        state = (self._tasks, self._active_workers)\n        try:\n            with open(self._state_path, 'w') as fobj:\n                pickle.dump(state, fobj)\n        except IOError:\n            logger.warning(\"Failed saving scheduler state\", exc_info=1)\n        else:\n            logger.info(\"Saved state in %s\", self._state_path)\n\n    # prone to lead to crashes when old state is unpickled with updated code. TODO some kind of version control?\n    def load(self):\n        if os.path.exists(self._state_path):\n            logger.info(\"Attempting to load state from %s\", self._state_path)\n            try:\n                with open(self._state_path, 'rb') as fobj:\n                    state = pickle.load(fobj)\n            except BaseException:\n                logger.exception(\"Error when loading state. Starting from clean slate.\")\n                return\n\n            self._tasks, self._active_workers = state\n            self._status_tasks = collections.defaultdict(dict)\n            for task in six.itervalues(self._tasks):\n                self._status_tasks[task.status][task.id] = task\n\n            # Convert from old format\n            # TODO: this is really ugly, we need something more future-proof\n            # Every time we add an attribute to the Worker class, this code needs to be updated\n            for k, v in six.iteritems(self._active_workers):\n                if isinstance(v, float):\n                    self._active_workers[k] = Worker(worker_id=k, last_active=v)\n        else:\n            logger.info(\"No prior state file exists at %s. Starting with clean slate\", self._state_path)\n\n    def get_active_tasks(self, status=None):\n        if status:\n            for task in six.itervalues(self._status_tasks[status]):\n                yield task\n        else:\n            for task in six.itervalues(self._tasks):\n                yield task\n\n    def get_running_tasks(self):\n        return six.itervalues(self._status_tasks[RUNNING])\n\n    def get_pending_tasks(self):\n        return itertools.chain.from_iterable(six.itervalues(self._status_tasks[status])\n                                             for status in [PENDING, RUNNING])\n\n    def get_task(self, task_id, default=None, setdefault=None):\n        if setdefault:\n            task = self._tasks.setdefault(task_id, setdefault)\n            self._status_tasks[task.status][task.id] = task\n            return task\n        else:\n            return self._tasks.get(task_id, default)\n\n    def has_task(self, task_id):\n        return task_id in self._tasks\n\n    def re_enable(self, task, config=None):\n        task.scheduler_disable_time = None\n        task.failures.clear()\n        if config:\n            self.set_status(task, FAILED, config)\n            task.failures.clear()\n\n    def set_status(self, task, new_status, config=None):\n        if new_status == FAILED:\n            assert config is not None\n\n        # not sure why we have SUSPENDED, as it can never be set\n        if new_status == SUSPENDED:\n            new_status = PENDING\n\n        if new_status == DISABLED and task.status == RUNNING:\n            return\n\n        if task.status == DISABLED:\n            if new_status == DONE:\n                self.re_enable(task)\n\n            # don't allow workers to override a scheduler disable\n            elif task.scheduler_disable_time is not None:\n                return\n\n        if new_status == FAILED and task.can_disable():\n            task.add_failure()\n            if task.has_excessive_failures():\n                task.scheduler_disable_time = time.time()\n                new_status = DISABLED\n                notifications.send_error_email(\n                    'Luigi Scheduler: DISABLED {task} due to excessive failures'.format(task=task.id),\n                    '{task} failed {failures} times in the last {window} seconds, so it is being '\n                    'disabled for {persist} seconds'.format(\n                        failures=config.disable_failures,\n                        task=task.id,\n                        window=config.disable_window,\n                        persist=config.disable_persist,\n                    ))\n        elif new_status == DISABLED:\n            task.scheduler_disable_time = None\n\n        self._status_tasks[task.status].pop(task.id)\n        self._status_tasks[new_status][task.id] = task\n        task.status = new_status\n\n    def prune(self, task, config):\n        remove = False\n\n        # Mark tasks with no remaining active stakeholders for deletion\n        if not task.stakeholders:\n            if task.remove is None:\n                logger.info(\"Task %r has stakeholders %r but none remain connected -> will remove \"\n                            \"task in %s seconds\", task.id, task.stakeholders, config.remove_delay)\n                task.remove = time.time() + config.remove_delay\n\n        # If a running worker disconnects, tag all its jobs as FAILED and subject it to the same retry logic\n        if task.status == RUNNING and task.worker_running and task.worker_running not in task.stakeholders:\n            logger.info(\"Task %r is marked as running by disconnected worker %r -> marking as \"\n                        \"FAILED with retry delay of %rs\", task.id, task.worker_running,\n                        config.retry_delay)\n            task.worker_running = None\n            self.set_status(task, FAILED, config)\n            task.retry = time.time() + config.retry_delay\n\n        # Re-enable task after the disable time expires\n        if task.status == DISABLED and task.scheduler_disable_time:\n            if time.time() - fix_time(task.scheduler_disable_time) > config.disable_persist:\n                self.re_enable(task, config)\n\n        # Remove tasks that have no stakeholders\n        if task.remove and time.time() > task.remove:\n            logger.info(\"Removing task %r (no connected stakeholders)\", task.id)\n            remove = True\n\n        # Reset FAILED tasks to PENDING if max timeout is reached, and retry delay is >= 0\n        if task.status == FAILED and config.retry_delay >= 0 and task.retry < time.time():\n            self.set_status(task, PENDING, config)\n\n        return remove\n\n    def inactivate_tasks(self, delete_tasks):\n        # The terminology is a bit confusing: we used to \"delete\" tasks when they became inactive,\n        # but with a pluggable state storage, you might very well want to keep some history of\n        # older tasks as well. That's why we call it \"inactivate\" (as in the verb)\n        for task in delete_tasks:\n            task_obj = self._tasks.pop(task)\n            self._status_tasks[task_obj.status].pop(task)\n\n    def get_active_workers(self, last_active_lt=None):\n        for worker in six.itervalues(self._active_workers):\n            if last_active_lt is not None and worker.last_active >= last_active_lt:\n                continue\n            yield worker\n\n    def get_worker_ids(self):\n        return self._active_workers.keys()  # only used for unit tests\n\n    def get_worker(self, worker_id):\n        return self._active_workers.setdefault(worker_id, Worker(worker_id))\n\n    def inactivate_workers(self, delete_workers):\n        # Mark workers as inactive\n        for worker in delete_workers:\n            self._active_workers.pop(worker)\n\n        # remove workers from tasks\n        for task in self.get_active_tasks():\n            task.stakeholders.difference_update(delete_workers)\n            task.workers.difference_update(delete_workers)\n\n\nclass CentralPlannerScheduler(Scheduler):\n    \"\"\"\n    Async scheduler that can handle multiple workers, etc.\n\n    Can be run locally or on a server (using RemoteScheduler + server.Server).\n    \"\"\"\n\n    def __init__(self, config=None, resources=None, task_history_impl=None, **kwargs):\n        \"\"\"\n        Keyword Arguments:\n        :param config: an object of class \"scheduler\" or None (in which the global instance will be used)\n        :param resources: a dict of str->int constraints\n        :param task_history_override: ignore config and use this object as the task history\n        \"\"\"\n        self._config = config or scheduler(**kwargs)\n        self._state = SimpleTaskState(self._config.state_path)\n\n        if task_history_impl:\n            self._task_history = task_history_impl\n        elif self._config.record_task_history:\n            import db_task_history  # Needs sqlalchemy, thus imported here\n            self._task_history = db_task_history.DbTaskHistory()\n        else:\n            self._task_history = history.NopHistory()\n        self._resources = resources or configuration.get_config().getintdict('resources')  # TODO: Can we make this a Parameter?\n        self._make_task = functools.partial(\n            Task, disable_failures=self._config.disable_failures,\n            disable_window=self._config.disable_window)\n\n    def load(self):\n        self._state.load()\n\n    def dump(self):\n        self._state.dump()\n\n    def prune(self):\n        logger.info(\"Starting pruning of task graph\")\n        remove_workers = []\n        for worker in self._state.get_active_workers():\n            if worker.prune(self._config):\n                logger.info(\"Worker %s timed out (no contact for >=%ss)\", worker, self._config.worker_disconnect_delay)\n                remove_workers.append(worker.id)\n\n        self._state.inactivate_workers(remove_workers)\n\n        remove_tasks = []\n        for task in self._state.get_active_tasks():\n            if self._state.prune(task, self._config):\n                remove_tasks.append(task.id)\n\n        self._state.inactivate_tasks(remove_tasks)\n\n        logger.info(\"Done pruning task graph\")\n\n    def update(self, worker_id, worker_reference=None):\n        \"\"\"\n        Keep track of whenever the worker was last active.\n        \"\"\"\n        worker = self._state.get_worker(worker_id)\n        worker.update(worker_reference)\n\n    def _update_priority(self, task, prio, worker):\n        \"\"\"\n        Update priority of the given task.\n\n        Priority can only be increased.\n        If the task doesn't exist, a placeholder task is created to preserve priority when the task is later scheduled.\n        \"\"\"\n        task.priority = prio = max(prio, task.priority)\n        for dep in task.deps or []:\n            t = self._state.get_task(dep)\n            if t is not None and prio > t.priority:\n                self._update_priority(t, prio, worker)\n\n    def add_task(self, worker, task_id, status=PENDING, runnable=True,\n                 deps=None, new_deps=None, expl=None, resources=None,\n                 priority=0, family='', params=None, **kwargs):\n        \"\"\"\n        * add task identified by task_id if it doesn't exist\n        * if deps is not None, update dependency list\n        * update status of task\n        * add additional workers/stakeholders\n        * update priority when needed\n        \"\"\"\n        self.update(worker)\n\n        task = self._state.get_task(task_id, setdefault=self._make_task(\n            task_id=task_id, status=PENDING, deps=deps, resources=resources,\n            priority=priority, family=family, params=params))\n\n        # for setting priority, we'll sometimes create tasks with unset family and params\n        if not task.family:\n            task.family = family\n        if not task.params:\n            task.params = _get_default(params, {})\n\n        if task.remove is not None:\n            task.remove = None  # unmark task for removal so it isn't removed after being added\n\n        if not (task.status == RUNNING and status == PENDING):\n            # don't allow re-scheduling of task while it is running, it must either fail or succeed first\n            if status == PENDING or status != task.status:\n                # Update the DB only if there was a acctual change, to prevent noise.\n                # We also check for status == PENDING b/c that's the default value\n                # (so checking for status != task.status woule lie)\n                self._update_task_history(task_id, status)\n            self._state.set_status(task, PENDING if status == SUSPENDED else status, self._config)\n            if status == FAILED:\n                task.retry = time.time() + self._config.retry_delay\n\n        if deps is not None:\n            task.deps = set(deps)\n\n        if new_deps is not None:\n            task.deps.update(new_deps)\n\n        task.stakeholders.add(worker)\n        task.resources = resources\n\n        # Task dependencies might not exist yet. Let's create dummy tasks for them for now.\n        # Otherwise the task dependencies might end up being pruned if scheduling takes a long time\n        for dep in task.deps or []:\n            t = self._state.get_task(dep, setdefault=self._make_task(task_id=dep, status=UNKNOWN, deps=None, priority=priority))\n            t.stakeholders.add(worker)\n\n        self._update_priority(task, priority, worker)\n\n        if runnable:\n            task.workers.add(worker)\n\n        if expl is not None:\n            task.expl = expl\n\n    def add_worker(self, worker, info, **kwargs):\n        self._state.get_worker(worker).add_info(info)\n\n    def update_resources(self, **resources):\n        if self._resources is None:\n            self._resources = {}\n        self._resources.update(resources)\n\n    def _has_resources(self, needed_resources, used_resources):\n        if needed_resources is None:\n            return True\n\n        available_resources = self._resources or {}\n        for resource, amount in six.iteritems(needed_resources):\n            if amount + used_resources[resource] > available_resources.get(resource, 1):\n                return False\n        return True\n\n    def _used_resources(self):\n        used_resources = collections.defaultdict(int)\n        if self._resources is not None:\n            for task in self._state.get_active_tasks():\n                if task.status == RUNNING and task.resources:\n                    for resource, amount in six.iteritems(task.resources):\n                        used_resources[resource] += amount\n        return used_resources\n\n    def _rank(self):\n        \"\"\"\n        Return worker's rank function for task scheduling.\n\n        :return:\n        \"\"\"\n        dependents = collections.defaultdict(int)\n\n        def not_done(t):\n            task = self._state.get_task(t, default=None)\n            return task is None or task.status != DONE\n        for task in self._state.get_pending_tasks():\n            if task.status != DONE:\n                deps = list(filter(not_done, task.deps))\n                inverse_num_deps = 1.0 / max(len(deps), 1)\n                for dep in deps:\n                    dependents[dep] += inverse_num_deps\n\n        return lambda task: (task.priority, dependents[task.id], -task.time)\n\n    def _schedulable(self, task):\n        if task.status != PENDING:\n            return False\n        for dep in task.deps:\n            dep_task = self._state.get_task(dep, default=None)\n            if dep_task is None or dep_task.status != DONE:\n                return False\n        return True\n\n    def get_work(self, worker, host=None, assistant=False, **kwargs):\n        # TODO: remove any expired nodes\n\n        # Algo: iterate over all nodes, find the highest priority node no dependencies and available\n        # resources.\n\n        # Resource checking looks both at currently available resources and at which resources would\n        # be available if all running tasks died and we rescheduled all workers greedily. We do both\n        # checks in order to prevent a worker with many low-priority tasks from starving other\n        # workers with higher priority tasks that share the same resources.\n\n        # TODO: remove tasks that can't be done, figure out if the worker has absolutely\n        # nothing it can wait for\n\n        # Return remaining tasks that have no FAILED descendents\n        self.update(worker, {'host': host})\n        best_task = None\n        locally_pending_tasks = 0\n        running_tasks = []\n\n        used_resources = self._used_resources()\n        greedy_resources = collections.defaultdict(int)\n        n_unique_pending = 0\n        greedy_workers = dict((worker.id, worker.info.get('workers', 1))\n                              for worker in self._state.get_active_workers())\n\n        tasks = list(self._state.get_pending_tasks())\n        tasks.sort(key=self._rank(), reverse=True)\n\n        for task in tasks:\n            in_workers = assistant or worker in task.workers\n            if task.status == 'RUNNING' and in_workers:\n                # Return a list of currently running tasks to the client,\n                # makes it easier to troubleshoot\n                other_worker = self._state.get_worker(task.worker_running)\n                more_info = {'task_id': task.id, 'worker': str(other_worker)}\n                if other_worker is not None:\n                    more_info.update(other_worker.info)\n                    running_tasks.append(more_info)\n\n            if task.status == PENDING and in_workers:\n                locally_pending_tasks += 1\n                if len(task.workers) == 1 and not assistant:\n                    n_unique_pending += 1\n\n            if task.status == RUNNING and (task.worker_running in greedy_workers):\n                greedy_workers[task.worker_running] -= 1\n                for resource, amount in six.iteritems((task.resources or {})):\n                    greedy_resources[resource] += amount\n\n            if not best_task and self._schedulable(task) and self._has_resources(task.resources, greedy_resources):\n                if in_workers and self._has_resources(task.resources, used_resources):\n                    best_task = task\n                else:\n                    workers = itertools.chain(task.workers, [worker]) if assistant else task.workers\n                    for task_worker in workers:\n                        if greedy_workers.get(task_worker, 0) > 0:\n                            # use up a worker\n                            greedy_workers[task_worker] -= 1\n\n                            # keep track of the resources used in greedy scheduling\n                            for resource, amount in six.iteritems((task.resources or {})):\n                                greedy_resources[resource] += amount\n\n                            break\n\n        reply = {'n_pending_tasks': locally_pending_tasks,\n                 'running_tasks': running_tasks,\n                 'task_id': None,\n                 'n_unique_pending': n_unique_pending}\n\n        if best_task:\n            self._state.set_status(best_task, RUNNING, self._config)\n            best_task.worker_running = worker\n            best_task.time_running = time.time()\n            self._update_task_history(best_task.id, RUNNING, host=host)\n\n            reply['task_id'] = best_task.id\n            reply['task_family'] = best_task.family\n            reply['task_params'] = best_task.params\n\n        return reply\n\n    def ping(self, worker, **kwargs):\n        self.update(worker)\n\n    def _upstream_status(self, task_id, upstream_status_table):\n        if task_id in upstream_status_table:\n            return upstream_status_table[task_id]\n        elif self._state.has_task(task_id):\n            task_stack = [task_id]\n\n            while task_stack:\n                dep_id = task_stack.pop()\n                if self._state.has_task(dep_id):\n                    dep = self._state.get_task(dep_id)\n                    if dep_id not in upstream_status_table:\n                        if dep.status == PENDING and dep.deps:\n                            task_stack = task_stack + [dep_id] + list(dep.deps)\n                            upstream_status_table[dep_id] = ''  # will be updated postorder\n                        else:\n                            dep_status = STATUS_TO_UPSTREAM_MAP.get(dep.status, '')\n                            upstream_status_table[dep_id] = dep_status\n                    elif upstream_status_table[dep_id] == '' and dep.deps:\n                        # This is the postorder update step when we set the\n                        # status based on the previously calculated child elements\n                        upstream_status = [upstream_status_table.get(task_id, '') for task_id in dep.deps]\n                        upstream_status.append('')  # to handle empty list\n                        status = max(upstream_status, key=UPSTREAM_SEVERITY_KEY)\n                        upstream_status_table[dep_id] = status\n            return upstream_status_table[dep_id]\n\n    def _serialize_task(self, task_id, include_deps=True):\n        task = self._state.get_task(task_id)\n        ret = {\n            'deps': list(task.deps),\n            'status': task.status,\n            'workers': list(task.workers),\n            'worker_running': task.worker_running,\n            'time_running': getattr(task, \"time_running\", None),\n            'start_time': task.time,\n            'params': task.params,\n            'name': task.family,\n            'priority': task.priority,\n            'resources': task.resources,\n        }\n        if include_deps:\n            ret['deps'] = list(task.deps)\n        return ret\n\n    def graph(self, **kwargs):\n        self.prune()\n        serialized = {}\n        for task in self._state.get_active_tasks():\n            serialized[task.id] = self._serialize_task(task.id)\n        return serialized\n\n    def _recurse_deps(self, task_id, serialized):\n        if task_id not in serialized:\n            task = self._state.get_task(task_id)\n            if task is None or not task.family:\n                logger.warn('Missing task for id [%s]', task_id)\n\n                # try to infer family and params from task_id\n                try:\n                    family, _, param_str = task_id.rstrip(')').partition('(')\n                    params = dict(param.split('=') for param in param_str.split(', '))\n                except BaseException:\n                    family, params = '', {}\n                serialized[task_id] = {\n                    'deps': [],\n                    'status': UNKNOWN,\n                    'workers': [],\n                    'start_time': UNKNOWN,\n                    'params': params,\n                    'name': family,\n                    'priority': 0,\n                }\n            else:\n                serialized[task_id] = self._serialize_task(task_id)\n                for dep in task.deps:\n                    self._recurse_deps(dep, serialized)\n\n    def dep_graph(self, task_id, **kwargs):\n        self.prune()\n        serialized = {}\n        if self._state.has_task(task_id):\n            self._recurse_deps(task_id, serialized)\n        return serialized\n\n    def task_list(self, status, upstream_status, limit=True, **kwargs):\n        \"\"\"\n        Query for a subset of tasks by status.\n        \"\"\"\n        self.prune()\n        result = {}\n        upstream_status_table = {}  # used to memoize upstream status\n        for task in self._state.get_active_tasks(status):\n            if (task.status != PENDING or not upstream_status or\n                    upstream_status == self._upstream_status(task.id, upstream_status_table)):\n                serialized = self._serialize_task(task.id, False)\n                result[task.id] = serialized\n        if limit and len(result) > self._config.max_shown_tasks:\n            return {'num_tasks': len(result)}\n        return result\n\n    def worker_list(self, include_running=True, **kwargs):\n        self.prune()\n        workers = [\n            dict(\n                name=worker.id,\n                last_active=worker.last_active,\n                started=getattr(worker, 'started', None),\n                **worker.info\n            ) for worker in self._state.get_active_workers()]\n        workers.sort(key=lambda worker: worker['started'], reverse=True)\n        if include_running:\n            running = collections.defaultdict(dict)\n            num_pending = collections.defaultdict(int)\n            num_uniques = collections.defaultdict(int)\n            for task in self._state.get_pending_tasks():\n                if task.status == RUNNING and task.worker_running:\n                    running[task.worker_running][task.id] = self._serialize_task(task.id, False)\n                elif task.status == PENDING:\n                    for worker in task.workers:\n                        num_pending[worker] += 1\n                    if len(task.workers) == 1:\n                        num_uniques[list(task.workers)[0]] += 1\n            for worker in workers:\n                tasks = running[worker['name']]\n                worker['num_running'] = len(tasks)\n                worker['num_pending'] = num_pending[worker['name']]\n                worker['num_uniques'] = num_uniques[worker['name']]\n                worker['running'] = tasks\n        return workers\n\n    def inverse_dep_graph(self, task_id, **kwargs):\n        self.prune()\n        serialized = {}\n        if self._state.has_task(task_id):\n            self._traverse_inverse_deps(task_id, serialized)\n        return serialized\n\n    def _traverse_inverse_deps(self, task_id, serialized):\n        stack = [task_id]\n        serialized[task_id] = self._serialize_task(task_id)\n        while len(stack) > 0:\n            curr_id = stack.pop()\n            for task in self._state.get_active_tasks():\n                if curr_id in task.deps:\n                    serialized[curr_id][\"deps\"].append(task.id)\n                    if task.id not in serialized:\n                        serialized[task.id] = self._serialize_task(task.id)\n                        serialized[task.id][\"deps\"] = []\n                        stack.append(task.id)\n\n    def task_search(self, task_str, **kwargs):\n        \"\"\"\n        Query for a subset of tasks by task_id.\n\n        :param task_str:\n        :return:\n        \"\"\"\n        self.prune()\n        result = collections.defaultdict(dict)\n        for task in self._state.get_active_tasks():\n            if task.id.find(task_str) != -1:\n                serialized = self._serialize_task(task.id, False)\n                result[task.status][task.id] = serialized\n        return result\n\n    def re_enable_task(self, task_id):\n        serialized = {}\n        task = self._state.get_task(task_id)\n        if task and task.status == DISABLED and task.scheduler_disable_time:\n            self._state.re_enable(task, self._config)\n            serialized = self._serialize_task(task_id)\n        return serialized\n\n    def fetch_error(self, task_id, **kwargs):\n        if self._state.has_task(task_id):\n            return {\"taskId\": task_id, \"error\": self._state.get_task(task_id).expl}\n        else:\n            return {\"taskId\": task_id, \"error\": \"\"}\n\n    def _update_task_history(self, task_id, status, host=None):\n        try:\n            if status == DONE or status == FAILED:\n                successful = (status == DONE)\n                self._task_history.task_finished(task_id, successful)\n            elif status == PENDING:\n                self._task_history.task_scheduled(task_id)\n            elif status == RUNNING:\n                self._task_history.task_started(task_id, host)\n        except BaseException:\n            logger.warning(\"Error saving Task history\", exc_info=True)\n\n    @property\n    def task_history(self):\n        # Used by server.py to expose the calls\n        return self._task_history\n",
      "file_patch": "@@ -623,7 +623,6 @@ class CentralPlannerScheduler(Scheduler):\n         # Return remaining tasks that have no FAILED descendents\n         self.update(worker, {'host': host})\n         best_task = None\n-        best_task_id = None\n         locally_pending_tasks = 0\n         running_tasks = []\n \n@@ -637,7 +636,8 @@ class CentralPlannerScheduler(Scheduler):\n         tasks.sort(key=self._rank(), reverse=True)\n \n         for task in tasks:\n-            if task.status == 'RUNNING' and worker in task.workers:\n+            in_workers = assistant or worker in task.workers\n+            if task.status == 'RUNNING' and in_workers:\n                 # Return a list of currently running tasks to the client,\n                 # makes it easier to troubleshoot\n                 other_worker = self._state.get_worker(task.worker_running)\n@@ -646,22 +646,22 @@ class CentralPlannerScheduler(Scheduler):\n                     more_info.update(other_worker.info)\n                     running_tasks.append(more_info)\n \n-            if task.status == PENDING and (worker in task.workers or assistant):\n+            if task.status == PENDING and in_workers:\n                 locally_pending_tasks += 1\n-                if len(task.workers) == 1:\n+                if len(task.workers) == 1 and not assistant:\n                     n_unique_pending += 1\n \n-            if task.status == RUNNING and (task.worker_running in greedy_workers or assistant):\n+            if task.status == RUNNING and (task.worker_running in greedy_workers):\n                 greedy_workers[task.worker_running] -= 1\n                 for resource, amount in six.iteritems((task.resources or {})):\n                     greedy_resources[resource] += amount\n \n             if not best_task and self._schedulable(task) and self._has_resources(task.resources, greedy_resources):\n-                if (worker in task.workers or assistant) and self._has_resources(task.resources, used_resources):\n+                if in_workers and self._has_resources(task.resources, used_resources):\n                     best_task = task\n-                    best_task_id = task.id\n                 else:\n-                    for task_worker in task.workers:\n+                    workers = itertools.chain(task.workers, [worker]) if assistant else task.workers\n+                    for task_worker in workers:\n                         if greedy_workers.get(task_worker, 0) > 0:\n                             # use up a worker\n                             greedy_workers[task_worker] -= 1\n",
      "files_name_in_blame_commit": [
        "scheduler.py"
      ]
    }
  },
  "commits_modify_file_before_fix": {
    "size": 112
  },
  "recursive_blame_commits": {
    "recursive_blame_function_lines": {
      "609": {
        "commit_id": "e6c7635a071396e45a64a16d2de4ea62d8bf21bc",
        "line_code": "    def get_work(self, worker, host=None, assistant=False, **kwargs):",
        "commit_date": "2015-03-01 12:38:58",
        "valid": 1
      },
      "610": {
        "commit_id": "9303334c2ea815b8c52401d60a46a5e3d842c1df",
        "line_code": "        # TODO: remove any expired nodes",
        "commit_date": "2012-04-03 12:59:53",
        "valid": 1
      },
      "611": {
        "commit_id": "9303334c2ea815b8c52401d60a46a5e3d842c1df",
        "line_code": "",
        "commit_date": "2012-04-03 12:59:53",
        "valid": 0
      },
      "612": {
        "commit_id": "31c9c7ed4d01441626b57db62cf4d2e1778829d7",
        "line_code": "        # Algo: iterate over all nodes, find the highest priority node no dependencies and available",
        "commit_date": "2014-09-18 13:06:18",
        "valid": 1
      },
      "613": {
        "commit_id": "31c9c7ed4d01441626b57db62cf4d2e1778829d7",
        "line_code": "        # resources.",
        "commit_date": "2014-09-18 13:06:18",
        "valid": 1
      },
      "614": {
        "commit_id": "31c9c7ed4d01441626b57db62cf4d2e1778829d7",
        "line_code": "",
        "commit_date": "2014-09-18 13:06:18",
        "valid": 0
      },
      "615": {
        "commit_id": "31c9c7ed4d01441626b57db62cf4d2e1778829d7",
        "line_code": "        # Resource checking looks both at currently available resources and at which resources would",
        "commit_date": "2014-09-18 13:06:18",
        "valid": 1
      },
      "616": {
        "commit_id": "31c9c7ed4d01441626b57db62cf4d2e1778829d7",
        "line_code": "        # be available if all running tasks died and we rescheduled all workers greedily. We do both",
        "commit_date": "2014-09-18 13:06:18",
        "valid": 1
      },
      "617": {
        "commit_id": "31c9c7ed4d01441626b57db62cf4d2e1778829d7",
        "line_code": "        # checks in order to prevent a worker with many low-priority tasks from starving other",
        "commit_date": "2014-09-18 13:06:18",
        "valid": 1
      },
      "618": {
        "commit_id": "31c9c7ed4d01441626b57db62cf4d2e1778829d7",
        "line_code": "        # workers with higher priority tasks that share the same resources.",
        "commit_date": "2014-09-18 13:06:18",
        "valid": 1
      },
      "619": {
        "commit_id": "aef50abd92da39826958d77ddacef3d323d106a4",
        "line_code": "",
        "commit_date": "2012-07-12 12:06:26",
        "valid": 0
      },
      "620": {
        "commit_id": "9303334c2ea815b8c52401d60a46a5e3d842c1df",
        "line_code": "        # TODO: remove tasks that can't be done, figure out if the worker has absolutely",
        "commit_date": "2012-04-03 12:59:53",
        "valid": 1
      },
      "621": {
        "commit_id": "9303334c2ea815b8c52401d60a46a5e3d842c1df",
        "line_code": "        # nothing it can wait for",
        "commit_date": "2012-04-03 12:59:53",
        "valid": 1
      },
      "622": {
        "commit_id": "aef50abd92da39826958d77ddacef3d323d106a4",
        "line_code": "",
        "commit_date": "2012-07-12 12:06:26",
        "valid": 0
      },
      "623": {
        "commit_id": "aef50abd92da39826958d77ddacef3d323d106a4",
        "line_code": "        # Return remaining tasks that have no FAILED descendents",
        "commit_date": "2012-07-12 12:06:26",
        "valid": 1
      },
      "624": {
        "commit_id": "c75085b3c812c8054ef76d39676ab473e45b9600",
        "line_code": "        self.update(worker, {'host': host})",
        "commit_date": "2014-05-08 16:30:46",
        "valid": 1
      },
      "625": {
        "commit_id": "9303334c2ea815b8c52401d60a46a5e3d842c1df",
        "line_code": "        best_task = None",
        "commit_date": "2012-04-03 12:59:53",
        "valid": 1
      },
      "626": {
        "commit_id": "4bcb2ca8623f807037db8e012950fef867b30357",
        "line_code": "        best_task_id = None",
        "commit_date": "2014-10-07 01:57:27",
        "valid": 1
      },
      "627": {
        "commit_id": "aef50abd92da39826958d77ddacef3d323d106a4",
        "line_code": "        locally_pending_tasks = 0",
        "commit_date": "2012-07-12 12:06:26",
        "valid": 1
      },
      "628": {
        "commit_id": "725172e3d8d970a505931a2556cda9a235dd2373",
        "line_code": "        running_tasks = []",
        "commit_date": "2013-09-24 11:03:14",
        "valid": 1
      },
      "629": {
        "commit_id": "4bcb2ca8623f807037db8e012950fef867b30357",
        "line_code": "",
        "commit_date": "2014-10-07 01:57:27",
        "valid": 0
      },
      "630": {
        "commit_id": "31c9c7ed4d01441626b57db62cf4d2e1778829d7",
        "line_code": "        used_resources = self._used_resources()",
        "commit_date": "2014-09-18 13:06:18",
        "valid": 1
      },
      "631": {
        "commit_id": "bf16c10a29eb5ec530e7e8e0cfa61c3d01196244",
        "line_code": "        greedy_resources = collections.defaultdict(int)",
        "commit_date": "2014-10-13 13:59:26",
        "valid": 1
      },
      "632": {
        "commit_id": "71cd8fd989e08e6a2bd0cde49c0cca1beb953a16",
        "line_code": "        n_unique_pending = 0",
        "commit_date": "2014-09-24 06:47:09",
        "valid": 1
      },
      "633": {
        "commit_id": "92fd2d0a3dfe4656ec84813e5416238f3d6a40f2",
        "line_code": "        greedy_workers = dict((worker.id, worker.info.get('workers', 1))",
        "commit_date": "2014-10-27 14:09:34",
        "valid": 1
      },
      "634": {
        "commit_id": "92fd2d0a3dfe4656ec84813e5416238f3d6a40f2",
        "line_code": "                              for worker in self._state.get_active_workers())",
        "commit_date": "2014-10-27 14:09:34",
        "valid": 1
      },
      "635": {
        "commit_id": "9303334c2ea815b8c52401d60a46a5e3d842c1df",
        "line_code": "",
        "commit_date": "2012-04-03 12:59:53",
        "valid": 0
      },
      "636": {
        "commit_id": "4bcb2ca8623f807037db8e012950fef867b30357",
        "line_code": "        tasks = list(self._state.get_pending_tasks())",
        "commit_date": "2014-10-07 01:57:27",
        "valid": 1
      },
      "637": {
        "commit_id": "4bcb2ca8623f807037db8e012950fef867b30357",
        "line_code": "        tasks.sort(key=self._rank(), reverse=True)",
        "commit_date": "2014-10-07 01:57:27",
        "valid": 1
      },
      "638": {
        "commit_id": "4bcb2ca8623f807037db8e012950fef867b30357",
        "line_code": "",
        "commit_date": "2014-10-07 01:57:27",
        "valid": 0
      },
      "639": {
        "commit_id": "4bcb2ca8623f807037db8e012950fef867b30357",
        "line_code": "        for task in tasks:",
        "commit_date": "2014-10-07 01:57:27",
        "valid": 1
      },
      "640": {
        "commit_id": "4bcb2ca8623f807037db8e012950fef867b30357",
        "line_code": "            if task.status == 'RUNNING' and worker in task.workers:",
        "commit_date": "2014-10-07 01:57:27",
        "valid": 1
      },
      "641": {
        "commit_id": "77e027b52ea8b0645f33ff90b7ad9371a707c13b",
        "line_code": "                # Return a list of currently running tasks to the client,",
        "commit_date": "2014-06-29 22:31:19",
        "valid": 1
      },
      "642": {
        "commit_id": "77e027b52ea8b0645f33ff90b7ad9371a707c13b",
        "line_code": "                # makes it easier to troubleshoot",
        "commit_date": "2014-06-29 22:31:19",
        "valid": 1
      },
      "643": {
        "commit_id": "4bcb2ca8623f807037db8e012950fef867b30357",
        "line_code": "                other_worker = self._state.get_worker(task.worker_running)",
        "commit_date": "2014-10-07 01:57:27",
        "valid": 1
      },
      "644": {
        "commit_id": "4bcb2ca8623f807037db8e012950fef867b30357",
        "line_code": "                more_info = {'task_id': task.id, 'worker': str(other_worker)}",
        "commit_date": "2014-10-07 01:57:27",
        "valid": 1
      },
      "645": {
        "commit_id": "77e027b52ea8b0645f33ff90b7ad9371a707c13b",
        "line_code": "                if other_worker is not None:",
        "commit_date": "2014-06-29 22:31:19",
        "valid": 1
      },
      "646": {
        "commit_id": "77e027b52ea8b0645f33ff90b7ad9371a707c13b",
        "line_code": "                    more_info.update(other_worker.info)",
        "commit_date": "2014-06-29 22:31:19",
        "valid": 1
      },
      "647": {
        "commit_id": "77e027b52ea8b0645f33ff90b7ad9371a707c13b",
        "line_code": "                    running_tasks.append(more_info)",
        "commit_date": "2014-06-29 22:31:19",
        "valid": 1
      },
      "648": {
        "commit_id": "725172e3d8d970a505931a2556cda9a235dd2373",
        "line_code": "",
        "commit_date": "2013-09-24 11:03:14",
        "valid": 0
      },
      "649": {
        "commit_id": "e6c7635a071396e45a64a16d2de4ea62d8bf21bc",
        "line_code": "            if task.status == PENDING and (worker in task.workers or assistant):",
        "commit_date": "2015-03-01 12:38:58",
        "valid": 1
      },
      "650": {
        "commit_id": "31c9c7ed4d01441626b57db62cf4d2e1778829d7",
        "line_code": "                locally_pending_tasks += 1",
        "commit_date": "2014-09-18 13:06:18",
        "valid": 1
      },
      "651": {
        "commit_id": "71cd8fd989e08e6a2bd0cde49c0cca1beb953a16",
        "line_code": "                if len(task.workers) == 1:",
        "commit_date": "2014-09-24 06:47:09",
        "valid": 1
      },
      "652": {
        "commit_id": "71cd8fd989e08e6a2bd0cde49c0cca1beb953a16",
        "line_code": "                    n_unique_pending += 1",
        "commit_date": "2014-09-24 06:47:09",
        "valid": 1
      },
      "653": {
        "commit_id": "31c9c7ed4d01441626b57db62cf4d2e1778829d7",
        "line_code": "",
        "commit_date": "2014-09-18 13:06:18",
        "valid": 0
      },
      "654": {
        "commit_id": "e6c7635a071396e45a64a16d2de4ea62d8bf21bc",
        "line_code": "            if task.status == RUNNING and (task.worker_running in greedy_workers or assistant):",
        "commit_date": "2015-03-01 12:38:58",
        "valid": 1
      },
      "655": {
        "commit_id": "92fd2d0a3dfe4656ec84813e5416238f3d6a40f2",
        "line_code": "                greedy_workers[task.worker_running] -= 1",
        "commit_date": "2014-10-27 14:09:34",
        "valid": 1
      },
      "656": {
        "commit_id": "dbd1662929b530c0df74778ab4b7ecde6ad6457b",
        "line_code": "                for resource, amount in six.iteritems((task.resources or {})):",
        "commit_date": "2015-02-15 11:33:03",
        "valid": 1
      },
      "657": {
        "commit_id": "92fd2d0a3dfe4656ec84813e5416238f3d6a40f2",
        "line_code": "                    greedy_resources[resource] += amount",
        "commit_date": "2014-10-27 14:09:34",
        "valid": 1
      },
      "658": {
        "commit_id": "92fd2d0a3dfe4656ec84813e5416238f3d6a40f2",
        "line_code": "",
        "commit_date": "2014-10-27 14:09:34",
        "valid": 0
      },
      "659": {
        "commit_id": "bf16c10a29eb5ec530e7e8e0cfa61c3d01196244",
        "line_code": "            if not best_task and self._schedulable(task) and self._has_resources(task.resources, greedy_resources):",
        "commit_date": "2014-10-13 13:59:26",
        "valid": 1
      },
      "660": {
        "commit_id": "e6c7635a071396e45a64a16d2de4ea62d8bf21bc",
        "line_code": "                if (worker in task.workers or assistant) and self._has_resources(task.resources, used_resources):",
        "commit_date": "2015-03-01 12:38:58",
        "valid": 1
      },
      "661": {
        "commit_id": "4bcb2ca8623f807037db8e012950fef867b30357",
        "line_code": "                    best_task = task",
        "commit_date": "2014-10-07 01:57:27",
        "valid": 1
      },
      "662": {
        "commit_id": "4bcb2ca8623f807037db8e012950fef867b30357",
        "line_code": "                    best_task_id = task.id",
        "commit_date": "2014-10-07 01:57:27",
        "valid": 1
      },
      "663": {
        "commit_id": "31c9c7ed4d01441626b57db62cf4d2e1778829d7",
        "line_code": "                else:",
        "commit_date": "2014-09-18 13:06:18",
        "valid": 1
      },
      "664": {
        "commit_id": "92fd2d0a3dfe4656ec84813e5416238f3d6a40f2",
        "line_code": "                    for task_worker in task.workers:",
        "commit_date": "2014-10-27 14:09:34",
        "valid": 1
      },
      "665": {
        "commit_id": "92fd2d0a3dfe4656ec84813e5416238f3d6a40f2",
        "line_code": "                        if greedy_workers.get(task_worker, 0) > 0:",
        "commit_date": "2014-10-27 14:09:34",
        "valid": 1
      },
      "666": {
        "commit_id": "92fd2d0a3dfe4656ec84813e5416238f3d6a40f2",
        "line_code": "                            # use up a worker",
        "commit_date": "2014-10-27 14:09:34",
        "valid": 1
      },
      "667": {
        "commit_id": "92fd2d0a3dfe4656ec84813e5416238f3d6a40f2",
        "line_code": "                            greedy_workers[task_worker] -= 1",
        "commit_date": "2014-10-27 14:09:34",
        "valid": 1
      },
      "668": {
        "commit_id": "92fd2d0a3dfe4656ec84813e5416238f3d6a40f2",
        "line_code": "",
        "commit_date": "2014-10-27 14:09:34",
        "valid": 0
      },
      "669": {
        "commit_id": "31c9c7ed4d01441626b57db62cf4d2e1778829d7",
        "line_code": "                            # keep track of the resources used in greedy scheduling",
        "commit_date": "2014-09-18 13:06:18",
        "valid": 1
      },
      "670": {
        "commit_id": "dbd1662929b530c0df74778ab4b7ecde6ad6457b",
        "line_code": "                            for resource, amount in six.iteritems((task.resources or {})):",
        "commit_date": "2015-02-15 11:33:03",
        "valid": 1
      },
      "671": {
        "commit_id": "bf16c10a29eb5ec530e7e8e0cfa61c3d01196244",
        "line_code": "                                greedy_resources[resource] += amount",
        "commit_date": "2014-10-13 13:59:26",
        "valid": 1
      },
      "672": {
        "commit_id": "9303334c2ea815b8c52401d60a46a5e3d842c1df",
        "line_code": "",
        "commit_date": "2012-04-03 12:59:53",
        "valid": 0
      },
      "673": {
        "commit_id": "92fd2d0a3dfe4656ec84813e5416238f3d6a40f2",
        "line_code": "                            break",
        "commit_date": "2014-10-27 14:09:34",
        "valid": 1
      },
      "674": {
        "commit_id": "92fd2d0a3dfe4656ec84813e5416238f3d6a40f2",
        "line_code": "",
        "commit_date": "2014-10-27 14:09:34",
        "valid": 0
      },
      "675": {
        "commit_id": "e6c7635a071396e45a64a16d2de4ea62d8bf21bc",
        "line_code": "        reply = {'n_pending_tasks': locally_pending_tasks,",
        "commit_date": "2015-03-01 12:38:58",
        "valid": 1
      },
      "676": {
        "commit_id": "e6c7635a071396e45a64a16d2de4ea62d8bf21bc",
        "line_code": "                 'running_tasks': running_tasks,",
        "commit_date": "2015-03-01 12:38:58",
        "valid": 1
      },
      "677": {
        "commit_id": "e6c7635a071396e45a64a16d2de4ea62d8bf21bc",
        "line_code": "                 'task_id': None,",
        "commit_date": "2015-03-01 12:38:58",
        "valid": 1
      },
      "678": {
        "commit_id": "e6c7635a071396e45a64a16d2de4ea62d8bf21bc",
        "line_code": "                 'n_unique_pending': n_unique_pending}",
        "commit_date": "2015-03-01 12:38:58",
        "valid": 1
      },
      "679": {
        "commit_id": "e6c7635a071396e45a64a16d2de4ea62d8bf21bc",
        "line_code": "",
        "commit_date": "2015-03-01 12:38:58",
        "valid": 0
      },
      "680": {
        "commit_id": "9303334c2ea815b8c52401d60a46a5e3d842c1df",
        "line_code": "        if best_task:",
        "commit_date": "2012-04-03 12:59:53",
        "valid": 1
      },
      "681": {
        "commit_id": "a5bc6c937c8f02ae0d39c3aac235c9bf7c55d33e",
        "line_code": "            self._state.set_status(best_task, RUNNING, self._config)",
        "commit_date": "2015-01-08 21:26:10",
        "valid": 1
      },
      "682": {
        "commit_id": "4bcb2ca8623f807037db8e012950fef867b30357",
        "line_code": "            best_task.worker_running = worker",
        "commit_date": "2014-10-07 01:57:27",
        "valid": 1
      },
      "683": {
        "commit_id": "4bcb2ca8623f807037db8e012950fef867b30357",
        "line_code": "            best_task.time_running = time.time()",
        "commit_date": "2014-10-07 01:57:27",
        "valid": 1
      },
      "684": {
        "commit_id": "4bcb2ca8623f807037db8e012950fef867b30357",
        "line_code": "            self._update_task_history(best_task.id, RUNNING, host=host)",
        "commit_date": "2014-10-07 01:57:27",
        "valid": 1
      },
      "685": {
        "commit_id": "9303334c2ea815b8c52401d60a46a5e3d842c1df",
        "line_code": "",
        "commit_date": "2012-04-03 12:59:53",
        "valid": 0
      },
      "686": {
        "commit_id": "e6c7635a071396e45a64a16d2de4ea62d8bf21bc",
        "line_code": "            reply['task_id'] = best_task.id",
        "commit_date": "2015-03-01 12:38:58",
        "valid": 1
      },
      "687": {
        "commit_id": "e6c7635a071396e45a64a16d2de4ea62d8bf21bc",
        "line_code": "            reply['task_family'] = best_task.family",
        "commit_date": "2015-03-01 12:38:58",
        "valid": 1
      },
      "688": {
        "commit_id": "e6c7635a071396e45a64a16d2de4ea62d8bf21bc",
        "line_code": "            reply['task_params'] = best_task.params",
        "commit_date": "2015-03-01 12:38:58",
        "valid": 1
      },
      "689": {
        "commit_id": "e6c7635a071396e45a64a16d2de4ea62d8bf21bc",
        "line_code": "",
        "commit_date": "2015-03-01 12:38:58",
        "valid": 0
      },
      "690": {
        "commit_id": "e6c7635a071396e45a64a16d2de4ea62d8bf21bc",
        "line_code": "        return reply",
        "commit_date": "2015-03-01 12:38:58",
        "valid": 1
      }
    },
    "commits": {
      "e6c7635a071396e45a64a16d2de4ea62d8bf21bc": {
        "commit": {
          "commit_id": "e6c7635a071396e45a64a16d2de4ea62d8bf21bc",
          "commit_message": "Initial prototype of a \"assistant\" worker\n\nIf a worker is created using assistant=True, then it will pull *any* tasks from the scheduler,\nnot just tasks that are assigned to it.\n\nThe biggest limitation atm is that the assistant needs to be aware of what those tasks map to,\ni.e. somehow they have to be imported for the worker to know what to do with the tasks.\n\nIt also doesn't handle failure very well.",
          "commit_author": "Erik Bernhardsson",
          "commit_date": "2015-03-01 12:38:58",
          "commit_parent": "d4fc6b7417a6fb5122f8514837470b2d919f280b"
        },
        "function": {
          "function_name": "get_work",
          "function_code_before": "def get_work(self, worker, host=None, **kwargs):\n    self.update(worker, {'host': host})\n    best_task = None\n    best_task_id = None\n    locally_pending_tasks = 0\n    running_tasks = []\n    used_resources = self._used_resources()\n    greedy_resources = collections.defaultdict(int)\n    n_unique_pending = 0\n    greedy_workers = dict(((worker.id, worker.info.get('workers', 1)) for worker in self._state.get_active_workers()))\n    tasks = list(self._state.get_pending_tasks())\n    tasks.sort(key=self._rank(), reverse=True)\n    for task in tasks:\n        if task.status == 'RUNNING' and worker in task.workers:\n            other_worker = self._state.get_worker(task.worker_running)\n            more_info = {'task_id': task.id, 'worker': str(other_worker)}\n            if other_worker is not None:\n                more_info.update(other_worker.info)\n                running_tasks.append(more_info)\n        if task.status == PENDING and worker in task.workers:\n            locally_pending_tasks += 1\n            if len(task.workers) == 1:\n                n_unique_pending += 1\n        if task.status == RUNNING and task.worker_running in greedy_workers:\n            greedy_workers[task.worker_running] -= 1\n            for (resource, amount) in six.iteritems(task.resources or {}):\n                greedy_resources[resource] += amount\n        if not best_task and self._schedulable(task) and self._has_resources(task.resources, greedy_resources):\n            if worker in task.workers and self._has_resources(task.resources, used_resources):\n                best_task = task\n                best_task_id = task.id\n            else:\n                for task_worker in task.workers:\n                    if greedy_workers.get(task_worker, 0) > 0:\n                        greedy_workers[task_worker] -= 1\n                        for (resource, amount) in six.iteritems(task.resources or {}):\n                            greedy_resources[resource] += amount\n                        break\n    if best_task:\n        self._state.set_status(best_task, RUNNING, self._config)\n        best_task.worker_running = worker\n        best_task.time_running = time.time()\n        self._update_task_history(best_task.id, RUNNING, host=host)\n    return {'n_pending_tasks': locally_pending_tasks, 'n_unique_pending': n_unique_pending, 'task_id': best_task_id, 'running_tasks': running_tasks}",
          "function_code_after": "def get_work(self, worker, host=None, assistant=False, **kwargs):\n    self.update(worker, {'host': host})\n    best_task = None\n    best_task_id = None\n    locally_pending_tasks = 0\n    running_tasks = []\n    used_resources = self._used_resources()\n    greedy_resources = collections.defaultdict(int)\n    n_unique_pending = 0\n    greedy_workers = dict(((worker.id, worker.info.get('workers', 1)) for worker in self._state.get_active_workers()))\n    tasks = list(self._state.get_pending_tasks())\n    tasks.sort(key=self._rank(), reverse=True)\n    for task in tasks:\n        if task.status == 'RUNNING' and worker in task.workers:\n            other_worker = self._state.get_worker(task.worker_running)\n            more_info = {'task_id': task.id, 'worker': str(other_worker)}\n            if other_worker is not None:\n                more_info.update(other_worker.info)\n                running_tasks.append(more_info)\n        if task.status == PENDING and (worker in task.workers or assistant):\n            locally_pending_tasks += 1\n            if len(task.workers) == 1:\n                n_unique_pending += 1\n        if task.status == RUNNING and (task.worker_running in greedy_workers or assistant):\n            greedy_workers[task.worker_running] -= 1\n            for (resource, amount) in six.iteritems(task.resources or {}):\n                greedy_resources[resource] += amount\n        if not best_task and self._schedulable(task) and self._has_resources(task.resources, greedy_resources):\n            if (worker in task.workers or assistant) and self._has_resources(task.resources, used_resources):\n                best_task = task\n                best_task_id = task.id\n            else:\n                for task_worker in task.workers:\n                    if greedy_workers.get(task_worker, 0) > 0:\n                        greedy_workers[task_worker] -= 1\n                        for (resource, amount) in six.iteritems(task.resources or {}):\n                            greedy_resources[resource] += amount\n                        break\n    reply = {'n_pending_tasks': locally_pending_tasks, 'running_tasks': running_tasks, 'task_id': None, 'n_unique_pending': n_unique_pending}\n    if best_task:\n        self._state.set_status(best_task, RUNNING, self._config)\n        best_task.worker_running = worker\n        best_task.time_running = time.time()\n        self._update_task_history(best_task.id, RUNNING, host=host)\n        reply['task_id'] = best_task.id\n        reply['task_family'] = best_task.family\n        reply['task_params'] = best_task.params\n    return reply",
          "function_before_start_line": 607,
          "function_before_end_line": 682,
          "function_after_start_line": 607,
          "function_after_end_line": 688,
          "function_before_token_count": 421,
          "function_after_token_count": 464,
          "functions_name_modified_file": [
            "add_failure",
            "get_worker_ids",
            "prune",
            "_schedulable",
            "clear",
            "has_excessive_failures",
            "_upstream_status",
            "_update_task_history",
            "can_disable",
            "ping",
            "inverse_dep_graph",
            "get_pending_tasks",
            "_traverse_inverse_deps",
            "load",
            "num_failures",
            "task_search",
            "_recurse_deps",
            "get_worker",
            "get_active_workers",
            "worker_list",
            "add_task",
            "update_resources",
            "_update_priority",
            "task_list",
            "add_info",
            "get_active_tasks",
            "_has_resources",
            "_used_resources",
            "has_task",
            "fix_time",
            "_serialize_task",
            "task_history",
            "dep_graph",
            "__str__",
            "graph",
            "_get_default",
            "dump",
            "__repr__",
            "re_enable",
            "inactivate_tasks",
            "__init__",
            "update",
            "set_status",
            "add_worker",
            "get_task",
            "get_running_tasks",
            "_rank",
            "get_work",
            "re_enable_task",
            "fetch_error",
            "inactivate_workers"
          ],
          "functions_name_all_files": [
            "add_failure",
            "output",
            "can_disable",
            "check_complete",
            "load",
            "create_worker",
            "test_purge_multiple_workers",
            "add_task",
            "test_task_limit_not_exceeded",
            "add_info",
            "_has_resources",
            "_serialize_task",
            "fix_time",
            "test_run_error",
            "_get_work",
            "_run_task",
            "test_customized_worker",
            "test_no_task_limit",
            "setUp",
            "setup_interface_logging",
            "__init__",
            "test_unfulfilled_dep",
            "test_unknown_dep",
            "test_dynamic_dependencies_with_namespace",
            "stop",
            "test_time_out_hung_worker",
            "inactivate_workers",
            "prune",
            "tearDown",
            "clear",
            "_upstream_status",
            "test_interleaved_workers",
            "test_fail",
            "add_global_parameters",
            "_email_complete_error",
            "ping",
            "_editdistance",
            "inverse_dep_graph",
            "test_dep",
            "_traverse_inverse_deps",
            "test_complete_exception",
            "test_requires_exception",
            "_log_unexpected_error",
            "_log_remote_tasks",
            "_recurse_deps",
            "test_connection_error",
            "test_dynamic_dependencies",
            "update_resources",
            "_update_priority",
            "requires",
            "test_avoid_infinite_reschedule",
            "create_remote_scheduler",
            "run",
            "_add_worker",
            "build",
            "test_multiple_workers",
            "_keep_alive",
            "__str__",
            "parse_task",
            "add_worker",
            "apply_async",
            "test_cmdline_custom_worker",
            "_log_complete_error",
            "_schedulable",
            "has_excessive_failures",
            "_generate_worker_info",
            "load_task",
            "test_no_error",
            "test_complete_error",
            "_email_unexpected_error",
            "email_patch",
            "test_die_for_non_unique_pending",
            "test_complete_return_value",
            "worker_list",
            "_validate_dependency",
            "get_active_tasks",
            "add_task_parameters",
            "test_external_dep",
            "task_history",
            "test_ping_retry",
            "test_ping_thread_shutdown",
            "_handle_next_task",
            "graph",
            "re_enable",
            "dump",
            "__repr__",
            "inactivate_tasks",
            "test_kill_worker",
            "test_system_exit",
            "complete",
            "_check_complete_value",
            "get_task",
            "test_task_limit_exceeded",
            "test_purge_hung_worker_override_timeout_time",
            "_rank",
            "set_global_parameters",
            "get_worker_ids",
            "_update_task_history",
            "test_allow_reschedule_with_many_missing_deps",
            "test_purge_hung_worker_default_timeout_time",
            "_purge_children",
            "get_pending_tasks",
            "num_failures",
            "task_search",
            "get_worker",
            "get_active_workers",
            "task_list",
            "test_interleaved_workers2",
            "_used_resources",
            "add",
            "error_task_names",
            "has_task",
            "_process_args",
            "dep_graph",
            "create_local_scheduler",
            "test_dynamic_dependencies_other_module",
            "get_task_parameters",
            "test_term_worker",
            "_sleeper",
            "_get_default",
            "update",
            "parse",
            "set_status",
            "_validate_task",
            "get_running_tasks",
            "_add",
            "setTime",
            "get_work",
            "re_enable_task",
            "test_interleaved_workers3",
            "fetch_error",
            "test_get_work"
          ],
          "functions_name_co_evolved_modified_file": [],
          "functions_name_co_evolved_all_files": [
            "__init__",
            "create_worker",
            "_get_work",
            "output",
            "load_task",
            "run",
            "test_get_work",
            "setUp"
          ]
        },
        "file": {
          "file_name": "scheduler.py",
          "file_nloc": 634,
          "file_complexity": 231,
          "file_token_count": 4811,
          "file_before": "# -*- coding: utf-8 -*-\n#\n# Copyright 2012-2015 Spotify AB\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\"\"\"\nThe system for scheduling tasks and executing them in order.\nDeals with dependencies, priorities, resources, etc.\nThe :py:class:`~luigi.worker.Worker` pulls tasks from the scheduler (usually over the REST interface) and executes them.\n\"\"\"\n\nimport collections\ntry:\n    import cPickle as pickle\nexcept ImportError:\n    import pickle\nimport datetime\nimport functools\nimport itertools\nimport logging\nimport os\nimport time\n\nfrom luigi import six\n\nfrom luigi import configuration\nfrom luigi import notifications\nfrom luigi import parameter\nfrom luigi import task_history as history\nfrom luigi.task_status import DISABLED, DONE, FAILED, PENDING, RUNNING, SUSPENDED, UNKNOWN\nfrom luigi.task import Config\n\nlogger = logging.getLogger(\"luigi.server\")\n\n\nclass Scheduler(object):\n    \"\"\"\n    Abstract base class.\n\n    Note that the methods all take string arguments, not Task objects...\n    \"\"\"\"\"\n    add_task = NotImplemented\n    get_work = NotImplemented\n    ping = NotImplemented\n\nUPSTREAM_RUNNING = 'UPSTREAM_RUNNING'\nUPSTREAM_MISSING_INPUT = 'UPSTREAM_MISSING_INPUT'\nUPSTREAM_FAILED = 'UPSTREAM_FAILED'\nUPSTREAM_DISABLED = 'UPSTREAM_DISABLED'\n\nUPSTREAM_SEVERITY_ORDER = (\n    '',\n    UPSTREAM_RUNNING,\n    UPSTREAM_MISSING_INPUT,\n    UPSTREAM_FAILED,\n    UPSTREAM_DISABLED,\n)\nUPSTREAM_SEVERITY_KEY = UPSTREAM_SEVERITY_ORDER.index\nSTATUS_TO_UPSTREAM_MAP = {\n    FAILED: UPSTREAM_FAILED,\n    RUNNING: UPSTREAM_RUNNING,\n    PENDING: UPSTREAM_MISSING_INPUT,\n    DISABLED: UPSTREAM_DISABLED,\n}\n\n\nclass scheduler(Config):\n    # TODO(erikbern): the config_path is needed for backwards compatilibity. We should drop the compatibility\n    # at some point (in particular this would force users to replace all dashes with underscores in the config)\n    retry_delay = parameter.FloatParameter(default=900.0,\n                                           config_path=dict(section='scheduler', name='retry-delay'))\n    remove_delay = parameter.FloatParameter(default=600.0,\n                                            config_path=dict(section='scheduler', name='remote-delay'))\n    worker_disconnect_delay = parameter.FloatParameter(default=60.0,\n                                                       config_path=dict(section='scheduler', name='worker-disconnect-delay'))\n    state_path = parameter.Parameter(default='/var/lib/luigi-server/state.pickle',\n                                     config_path=dict(section='scheduler', name='state-path'))\n\n    # Jobs are disabled if we see more than disable_failures failures in disable_window seconds.\n    # These disables last for disable_persist seconds.\n    disable_window = parameter.IntParameter(default=3600,\n                                            config_path=dict(section='scheduler', name='disable-window-seconds'))\n    disable_failures = parameter.IntParameter(default=None,\n                                              config_path=dict(section='scheduler', name='disable-num-failures'))\n    disable_persist = parameter.IntParameter(default=86400,\n                                             config_path=dict(section='scheduler', name='disable-persist-seconds'))\n    max_shown_tasks = parameter.IntParameter(default=100000,\n                                             config_path=dict(section='scheduler', name='max-shown-task'))\n\n    record_task_history = parameter.BoolParameter(default=False)\n\n\ndef fix_time(x):\n    # Backwards compatibility for a fix in Dec 2014. Prior to the fix, pickled state might store datetime objects\n    # Let's remove this function soon\n    if isinstance(x, datetime.datetime):\n        return time.mktime(x.timetuple())\n    else:\n        return x\n\n\nclass Failures(object):\n    \"\"\"\n    This class tracks the number of failures in a given time window.\n\n    Failures added are marked with the current timestamp, and this class counts\n    the number of failures in a sliding time window ending at the present.\n    \"\"\"\n\n    def __init__(self, window):\n        \"\"\"\n        Initialize with the given window.\n\n        :param window: how long to track failures for, as a float (number of seconds).\n        \"\"\"\n        self.window = window\n        self.failures = collections.deque()\n\n    def add_failure(self):\n        \"\"\"\n        Add a failure event with the current timestamp.\n        \"\"\"\n        self.failures.append(time.time())\n\n    def num_failures(self):\n        \"\"\"\n        Return the number of failures in the window.\n        \"\"\"\n        min_time = time.time() - self.window\n\n        while self.failures and fix_time(self.failures[0]) < min_time:\n            self.failures.popleft()\n\n        return len(self.failures)\n\n    def clear(self):\n        \"\"\"\n        Clear the failure queue.\n        \"\"\"\n        self.failures.clear()\n\n\ndef _get_default(x, default):\n    if x is not None:\n        return x\n    else:\n        return default\n\n\nclass Task(object):\n\n    def __init__(self, task_id, status, deps, resources=None, priority=0, family='', params=None,\n                 disable_failures=None, disable_window=None):\n        self.id = task_id\n        self.stakeholders = set()  # workers ids that are somehow related to this task (i.e. don't prune while any of these workers are still active)\n        self.workers = set()  # workers ids that can perform task - task is 'BROKEN' if none of these workers are active\n        if deps is None:\n            self.deps = set()\n        else:\n            self.deps = set(deps)\n        self.status = status  # PENDING, RUNNING, FAILED or DONE\n        self.time = time.time()  # Timestamp when task was first added\n        self.retry = None\n        self.remove = None\n        self.worker_running = None  # the worker id that is currently running the task or None\n        self.time_running = None  # Timestamp when picked up by worker\n        self.expl = None\n        self.priority = priority\n        self.resources = _get_default(resources, {})\n        self.family = family\n        self.params = _get_default(params, {})\n        self.disable_failures = disable_failures\n        self.failures = Failures(disable_window)\n        self.scheduler_disable_time = None\n\n    def __repr__(self):\n        return \"Task(%r)\" % vars(self)\n\n    def add_failure(self):\n        self.failures.add_failure()\n\n    def has_excessive_failures(self):\n        return self.failures.num_failures() >= self.disable_failures\n\n    def can_disable(self):\n        return self.disable_failures is not None\n\n\nclass Worker(object):\n    \"\"\"\n    Structure for tracking worker activity and keeping their references.\n    \"\"\"\n\n    def __init__(self, worker_id, last_active=None):\n        self.id = worker_id\n        self.reference = None  # reference to the worker in the real world. (Currently a dict containing just the host)\n        self.last_active = last_active  # seconds since epoch\n        self.started = time.time()  # seconds since epoch\n        self.info = {}\n\n    def add_info(self, info):\n        self.info.update(info)\n\n    def update(self, worker_reference):\n        if worker_reference:\n            self.reference = worker_reference\n        self.last_active = time.time()\n\n    def prune(self, config):\n        # Delete workers that haven't said anything for a while (probably killed)\n        if self.last_active + config.worker_disconnect_delay < time.time():\n            return True\n\n    def __str__(self):\n        return self.id\n\n\nclass SimpleTaskState(object):\n    \"\"\"\n    Keep track of the current state and handle persistance.\n\n    The point of this class is to enable other ways to keep state, eg. by using a database\n    These will be implemented by creating an abstract base class that this and other classes\n    inherit from.\n    \"\"\"\n\n    def __init__(self, state_path):\n        self._state_path = state_path\n        self._tasks = {}  # map from id to a Task object\n        self._status_tasks = collections.defaultdict(dict)\n        self._active_workers = {}  # map from id to a Worker object\n\n    def dump(self):\n        state = (self._tasks, self._active_workers)\n        try:\n            with open(self._state_path, 'w') as fobj:\n                pickle.dump(state, fobj)\n        except IOError:\n            logger.warning(\"Failed saving scheduler state\", exc_info=1)\n        else:\n            logger.info(\"Saved state in %s\", self._state_path)\n\n    # prone to lead to crashes when old state is unpickled with updated code. TODO some kind of version control?\n    def load(self):\n        if os.path.exists(self._state_path):\n            logger.info(\"Attempting to load state from %s\", self._state_path)\n            try:\n                with open(self._state_path, 'rb') as fobj:\n                    state = pickle.load(fobj)\n            except BaseException:\n                logger.exception(\"Error when loading state. Starting from clean slate.\")\n                return\n\n            self._tasks, self._active_workers = state\n            self._status_tasks = collections.defaultdict(dict)\n            for task in six.itervalues(self._tasks):\n                self._status_tasks[task.status][task.id] = task\n\n            # Convert from old format\n            # TODO: this is really ugly, we need something more future-proof\n            # Every time we add an attribute to the Worker class, this code needs to be updated\n            for k, v in six.iteritems(self._active_workers):\n                if isinstance(v, float):\n                    self._active_workers[k] = Worker(worker_id=k, last_active=v)\n        else:\n            logger.info(\"No prior state file exists at %s. Starting with clean slate\", self._state_path)\n\n    def get_active_tasks(self, status=None):\n        if status:\n            for task in six.itervalues(self._status_tasks[status]):\n                yield task\n        else:\n            for task in six.itervalues(self._tasks):\n                yield task\n\n    def get_running_tasks(self):\n        return six.itervalues(self._status_tasks[RUNNING])\n\n    def get_pending_tasks(self):\n        return itertools.chain.from_iterable(six.itervalues(self._status_tasks[status])\n                                             for status in [PENDING, RUNNING])\n\n    def get_task(self, task_id, default=None, setdefault=None):\n        if setdefault:\n            task = self._tasks.setdefault(task_id, setdefault)\n            self._status_tasks[task.status][task.id] = task\n            return task\n        else:\n            return self._tasks.get(task_id, default)\n\n    def has_task(self, task_id):\n        return task_id in self._tasks\n\n    def re_enable(self, task, config=None):\n        task.scheduler_disable_time = None\n        task.failures.clear()\n        if config:\n            self.set_status(task, FAILED, config)\n            task.failures.clear()\n\n    def set_status(self, task, new_status, config=None):\n        if new_status == FAILED:\n            assert config is not None\n\n        # not sure why we have SUSPENDED, as it can never be set\n        if new_status == SUSPENDED:\n            new_status = PENDING\n\n        if new_status == DISABLED and task.status == RUNNING:\n            return\n\n        if task.status == DISABLED:\n            if new_status == DONE:\n                self.re_enable(task)\n\n            # don't allow workers to override a scheduler disable\n            elif task.scheduler_disable_time is not None:\n                return\n\n        if new_status == FAILED and task.can_disable():\n            task.add_failure()\n            if task.has_excessive_failures():\n                task.scheduler_disable_time = time.time()\n                new_status = DISABLED\n                notifications.send_error_email(\n                    'Luigi Scheduler: DISABLED {task} due to excessive failures'.format(task=task.id),\n                    '{task} failed {failures} times in the last {window} seconds, so it is being '\n                    'disabled for {persist} seconds'.format(\n                        failures=config.disable_failures,\n                        task=task.id,\n                        window=config.disable_window,\n                        persist=config.disable_persist,\n                    ))\n        elif new_status == DISABLED:\n            task.scheduler_disable_time = None\n\n        self._status_tasks[task.status].pop(task.id)\n        self._status_tasks[new_status][task.id] = task\n        task.status = new_status\n\n    def prune(self, task, config):\n        remove = False\n\n        # Mark tasks with no remaining active stakeholders for deletion\n        if not task.stakeholders:\n            if task.remove is None:\n                logger.info(\"Task %r has stakeholders %r but none remain connected -> will remove \"\n                            \"task in %s seconds\", task.id, task.stakeholders, config.remove_delay)\n                task.remove = time.time() + config.remove_delay\n\n        # If a running worker disconnects, tag all its jobs as FAILED and subject it to the same retry logic\n        if task.status == RUNNING and task.worker_running and task.worker_running not in task.stakeholders:\n            logger.info(\"Task %r is marked as running by disconnected worker %r -> marking as \"\n                        \"FAILED with retry delay of %rs\", task.id, task.worker_running,\n                        config.retry_delay)\n            task.worker_running = None\n            self.set_status(task, FAILED, config)\n            task.retry = time.time() + config.retry_delay\n\n        # Re-enable task after the disable time expires\n        if task.status == DISABLED and task.scheduler_disable_time:\n            if time.time() - fix_time(task.scheduler_disable_time) > config.disable_persist:\n                self.re_enable(task, config)\n\n        # Remove tasks that have no stakeholders\n        if task.remove and time.time() > task.remove:\n            logger.info(\"Removing task %r (no connected stakeholders)\", task.id)\n            remove = True\n\n        # Reset FAILED tasks to PENDING if max timeout is reached, and retry delay is >= 0\n        if task.status == FAILED and config.retry_delay >= 0 and task.retry < time.time():\n            self.set_status(task, PENDING, config)\n\n        return remove\n\n    def inactivate_tasks(self, delete_tasks):\n        # The terminology is a bit confusing: we used to \"delete\" tasks when they became inactive,\n        # but with a pluggable state storage, you might very well want to keep some history of\n        # older tasks as well. That's why we call it \"inactivate\" (as in the verb)\n        for task in delete_tasks:\n            task_obj = self._tasks.pop(task)\n            self._status_tasks[task_obj.status].pop(task)\n\n    def get_active_workers(self, last_active_lt=None):\n        for worker in six.itervalues(self._active_workers):\n            if last_active_lt is not None and worker.last_active >= last_active_lt:\n                continue\n            yield worker\n\n    def get_worker_ids(self):\n        return self._active_workers.keys()  # only used for unit tests\n\n    def get_worker(self, worker_id):\n        return self._active_workers.setdefault(worker_id, Worker(worker_id))\n\n    def inactivate_workers(self, delete_workers):\n        # Mark workers as inactive\n        for worker in delete_workers:\n            self._active_workers.pop(worker)\n\n        # remove workers from tasks\n        for task in self.get_active_tasks():\n            task.stakeholders.difference_update(delete_workers)\n            task.workers.difference_update(delete_workers)\n\n\nclass CentralPlannerScheduler(Scheduler):\n    \"\"\"\n    Async scheduler that can handle multiple workers, etc.\n\n    Can be run locally or on a server (using RemoteScheduler + server.Server).\n    \"\"\"\n\n    def __init__(self, config=None, resources=None, task_history_impl=None, **kwargs):\n        \"\"\"\n        Keyword Arguments:\n        :param config: an object of class \"scheduler\" or None (in which the global instance will be used)\n        :param resources: a dict of str->int constraints\n        :param task_history_override: ignore config and use this object as the task history\n        \"\"\"\n        self._config = config or scheduler(**kwargs)\n        self._state = SimpleTaskState(self._config.state_path)\n\n        if task_history_impl:\n            self._task_history = task_history_impl\n        elif self._config.record_task_history:\n            import db_task_history  # Needs sqlalchemy, thus imported here\n            self._task_history = db_task_history.DbTaskHistory()\n        else:\n            self._task_history = history.NopHistory()\n        self._resources = resources or configuration.get_config().getintdict('resources')  # TODO: Can we make this a Parameter?\n        self._make_task = functools.partial(\n            Task, disable_failures=self._config.disable_failures,\n            disable_window=self._config.disable_window)\n\n    def load(self):\n        self._state.load()\n\n    def dump(self):\n        self._state.dump()\n\n    def prune(self):\n        logger.info(\"Starting pruning of task graph\")\n        remove_workers = []\n        for worker in self._state.get_active_workers():\n            if worker.prune(self._config):\n                logger.info(\"Worker %s timed out (no contact for >=%ss)\", worker, self._config.worker_disconnect_delay)\n                remove_workers.append(worker.id)\n\n        self._state.inactivate_workers(remove_workers)\n\n        remove_tasks = []\n        for task in self._state.get_active_tasks():\n            if self._state.prune(task, self._config):\n                remove_tasks.append(task.id)\n\n        self._state.inactivate_tasks(remove_tasks)\n\n        logger.info(\"Done pruning task graph\")\n\n    def update(self, worker_id, worker_reference=None):\n        \"\"\"\n        Keep track of whenever the worker was last active.\n        \"\"\"\n        worker = self._state.get_worker(worker_id)\n        worker.update(worker_reference)\n\n    def _update_priority(self, task, prio, worker):\n        \"\"\"\n        Update priority of the given task.\n\n        Priority can only be increased.\n        If the task doesn't exist, a placeholder task is created to preserve priority when the task is later scheduled.\n        \"\"\"\n        task.priority = prio = max(prio, task.priority)\n        for dep in task.deps or []:\n            t = self._state.get_task(dep)\n            if t is not None and prio > t.priority:\n                self._update_priority(t, prio, worker)\n\n    def add_task(self, worker, task_id, status=PENDING, runnable=True,\n                 deps=None, new_deps=None, expl=None, resources=None,\n                 priority=0, family='', params=None, **kwargs):\n        \"\"\"\n        * add task identified by task_id if it doesn't exist\n        * if deps is not None, update dependency list\n        * update status of task\n        * add additional workers/stakeholders\n        * update priority when needed\n        \"\"\"\n        self.update(worker)\n\n        task = self._state.get_task(task_id, setdefault=self._make_task(\n            task_id=task_id, status=PENDING, deps=deps, resources=resources,\n            priority=priority, family=family, params=params))\n\n        # for setting priority, we'll sometimes create tasks with unset family and params\n        if not task.family:\n            task.family = family\n        if not task.params:\n            task.params = _get_default(params, {})\n\n        if task.remove is not None:\n            task.remove = None  # unmark task for removal so it isn't removed after being added\n\n        if not (task.status == RUNNING and status == PENDING):\n            # don't allow re-scheduling of task while it is running, it must either fail or succeed first\n            if status == PENDING or status != task.status:\n                # Update the DB only if there was a acctual change, to prevent noise.\n                # We also check for status == PENDING b/c that's the default value\n                # (so checking for status != task.status woule lie)\n                self._update_task_history(task_id, status)\n            self._state.set_status(task, PENDING if status == SUSPENDED else status, self._config)\n            if status == FAILED:\n                task.retry = time.time() + self._config.retry_delay\n\n        if deps is not None:\n            task.deps = set(deps)\n\n        if new_deps is not None:\n            task.deps.update(new_deps)\n\n        task.stakeholders.add(worker)\n        task.resources = resources\n\n        # Task dependencies might not exist yet. Let's create dummy tasks for them for now.\n        # Otherwise the task dependencies might end up being pruned if scheduling takes a long time\n        for dep in task.deps or []:\n            t = self._state.get_task(dep, setdefault=self._make_task(task_id=dep, status=UNKNOWN, deps=None, priority=priority))\n            t.stakeholders.add(worker)\n\n        self._update_priority(task, priority, worker)\n\n        if runnable:\n            task.workers.add(worker)\n\n        if expl is not None:\n            task.expl = expl\n\n    def add_worker(self, worker, info, **kwargs):\n        self._state.get_worker(worker).add_info(info)\n\n    def update_resources(self, **resources):\n        if self._resources is None:\n            self._resources = {}\n        self._resources.update(resources)\n\n    def _has_resources(self, needed_resources, used_resources):\n        if needed_resources is None:\n            return True\n\n        available_resources = self._resources or {}\n        for resource, amount in six.iteritems(needed_resources):\n            if amount + used_resources[resource] > available_resources.get(resource, 1):\n                return False\n        return True\n\n    def _used_resources(self):\n        used_resources = collections.defaultdict(int)\n        if self._resources is not None:\n            for task in self._state.get_active_tasks():\n                if task.status == RUNNING and task.resources:\n                    for resource, amount in six.iteritems(task.resources):\n                        used_resources[resource] += amount\n        return used_resources\n\n    def _rank(self):\n        \"\"\"\n        Return worker's rank function for task scheduling.\n\n        :return:\n        \"\"\"\n        dependents = collections.defaultdict(int)\n\n        def not_done(t):\n            task = self._state.get_task(t, default=None)\n            return task is None or task.status != DONE\n        for task in self._state.get_pending_tasks():\n            if task.status != DONE:\n                deps = list(filter(not_done, task.deps))\n                inverse_num_deps = 1.0 / max(len(deps), 1)\n                for dep in deps:\n                    dependents[dep] += inverse_num_deps\n\n        return lambda task: (task.priority, dependents[task.id], -task.time)\n\n    def _schedulable(self, task):\n        if task.status != PENDING:\n            return False\n        for dep in task.deps:\n            dep_task = self._state.get_task(dep, default=None)\n            if dep_task is None or dep_task.status != DONE:\n                return False\n        return True\n\n    def get_work(self, worker, host=None, **kwargs):\n        # TODO: remove any expired nodes\n\n        # Algo: iterate over all nodes, find the highest priority node no dependencies and available\n        # resources.\n\n        # Resource checking looks both at currently available resources and at which resources would\n        # be available if all running tasks died and we rescheduled all workers greedily. We do both\n        # checks in order to prevent a worker with many low-priority tasks from starving other\n        # workers with higher priority tasks that share the same resources.\n\n        # TODO: remove tasks that can't be done, figure out if the worker has absolutely\n        # nothing it can wait for\n\n        # Return remaining tasks that have no FAILED descendents\n        self.update(worker, {'host': host})\n        best_task = None\n        best_task_id = None\n        locally_pending_tasks = 0\n        running_tasks = []\n\n        used_resources = self._used_resources()\n        greedy_resources = collections.defaultdict(int)\n        n_unique_pending = 0\n        greedy_workers = dict((worker.id, worker.info.get('workers', 1))\n                              for worker in self._state.get_active_workers())\n\n        tasks = list(self._state.get_pending_tasks())\n        tasks.sort(key=self._rank(), reverse=True)\n\n        for task in tasks:\n            if task.status == 'RUNNING' and worker in task.workers:\n                # Return a list of currently running tasks to the client,\n                # makes it easier to troubleshoot\n                other_worker = self._state.get_worker(task.worker_running)\n                more_info = {'task_id': task.id, 'worker': str(other_worker)}\n                if other_worker is not None:\n                    more_info.update(other_worker.info)\n                    running_tasks.append(more_info)\n\n            if task.status == PENDING and worker in task.workers:\n                locally_pending_tasks += 1\n                if len(task.workers) == 1:\n                    n_unique_pending += 1\n\n            if task.status == RUNNING and task.worker_running in greedy_workers:\n                greedy_workers[task.worker_running] -= 1\n                for resource, amount in six.iteritems((task.resources or {})):\n                    greedy_resources[resource] += amount\n\n            if not best_task and self._schedulable(task) and self._has_resources(task.resources, greedy_resources):\n                if worker in task.workers and self._has_resources(task.resources, used_resources):\n                    best_task = task\n                    best_task_id = task.id\n                else:\n                    for task_worker in task.workers:\n                        if greedy_workers.get(task_worker, 0) > 0:\n                            # use up a worker\n                            greedy_workers[task_worker] -= 1\n\n                            # keep track of the resources used in greedy scheduling\n                            for resource, amount in six.iteritems((task.resources or {})):\n                                greedy_resources[resource] += amount\n\n                            break\n\n        if best_task:\n            self._state.set_status(best_task, RUNNING, self._config)\n            best_task.worker_running = worker\n            best_task.time_running = time.time()\n            self._update_task_history(best_task.id, RUNNING, host=host)\n\n        return {'n_pending_tasks': locally_pending_tasks,\n                'n_unique_pending': n_unique_pending,\n                'task_id': best_task_id,\n                'running_tasks': running_tasks}\n\n    def ping(self, worker, **kwargs):\n        self.update(worker)\n\n    def _upstream_status(self, task_id, upstream_status_table):\n        if task_id in upstream_status_table:\n            return upstream_status_table[task_id]\n        elif self._state.has_task(task_id):\n            task_stack = [task_id]\n\n            while task_stack:\n                dep_id = task_stack.pop()\n                if self._state.has_task(dep_id):\n                    dep = self._state.get_task(dep_id)\n                    if dep_id not in upstream_status_table:\n                        if dep.status == PENDING and dep.deps:\n                            task_stack = task_stack + [dep_id] + list(dep.deps)\n                            upstream_status_table[dep_id] = ''  # will be updated postorder\n                        else:\n                            dep_status = STATUS_TO_UPSTREAM_MAP.get(dep.status, '')\n                            upstream_status_table[dep_id] = dep_status\n                    elif upstream_status_table[dep_id] == '' and dep.deps:\n                        # This is the postorder update step when we set the\n                        # status based on the previously calculated child elements\n                        upstream_status = [upstream_status_table.get(task_id, '') for task_id in dep.deps]\n                        upstream_status.append('')  # to handle empty list\n                        status = max(upstream_status, key=UPSTREAM_SEVERITY_KEY)\n                        upstream_status_table[dep_id] = status\n            return upstream_status_table[dep_id]\n\n    def _serialize_task(self, task_id, include_deps=True):\n        task = self._state.get_task(task_id)\n        ret = {\n            'deps': list(task.deps),\n            'status': task.status,\n            'workers': list(task.workers),\n            'worker_running': task.worker_running,\n            'time_running': getattr(task, \"time_running\", None),\n            'start_time': task.time,\n            'params': task.params,\n            'name': task.family,\n            'priority': task.priority,\n            'resources': task.resources,\n        }\n        if include_deps:\n            ret['deps'] = list(task.deps)\n        return ret\n\n    def graph(self, **kwargs):\n        self.prune()\n        serialized = {}\n        for task in self._state.get_active_tasks():\n            serialized[task.id] = self._serialize_task(task.id)\n        return serialized\n\n    def _recurse_deps(self, task_id, serialized):\n        if task_id not in serialized:\n            task = self._state.get_task(task_id)\n            if task is None or not task.family:\n                logger.warn('Missing task for id [%s]', task_id)\n\n                # try to infer family and params from task_id\n                try:\n                    family, _, param_str = task_id.rstrip(')').partition('(')\n                    params = dict(param.split('=') for param in param_str.split(', '))\n                except BaseException:\n                    family, params = '', {}\n                serialized[task_id] = {\n                    'deps': [],\n                    'status': UNKNOWN,\n                    'workers': [],\n                    'start_time': UNKNOWN,\n                    'params': params,\n                    'name': family,\n                    'priority': 0,\n                }\n            else:\n                serialized[task_id] = self._serialize_task(task_id)\n                for dep in task.deps:\n                    self._recurse_deps(dep, serialized)\n\n    def dep_graph(self, task_id, **kwargs):\n        self.prune()\n        serialized = {}\n        if self._state.has_task(task_id):\n            self._recurse_deps(task_id, serialized)\n        return serialized\n\n    def task_list(self, status, upstream_status, limit=True, **kwargs):\n        \"\"\"\n        Query for a subset of tasks by status.\n        \"\"\"\n        self.prune()\n        result = {}\n        upstream_status_table = {}  # used to memoize upstream status\n        for task in self._state.get_active_tasks(status):\n            if (task.status != PENDING or not upstream_status or\n                    upstream_status == self._upstream_status(task.id, upstream_status_table)):\n                serialized = self._serialize_task(task.id, False)\n                result[task.id] = serialized\n        if limit and len(result) > self._config.max_shown_tasks:\n            return {'num_tasks': len(result)}\n        return result\n\n    def worker_list(self, include_running=True, **kwargs):\n        self.prune()\n        workers = [\n            dict(\n                name=worker.id,\n                last_active=worker.last_active,\n                started=getattr(worker, 'started', None),\n                **worker.info\n            ) for worker in self._state.get_active_workers()]\n        workers.sort(key=lambda worker: worker['started'], reverse=True)\n        if include_running:\n            running = collections.defaultdict(dict)\n            num_pending = collections.defaultdict(int)\n            num_uniques = collections.defaultdict(int)\n            for task in self._state.get_pending_tasks():\n                if task.status == RUNNING and task.worker_running:\n                    running[task.worker_running][task.id] = self._serialize_task(task.id, False)\n                elif task.status == PENDING:\n                    for worker in task.workers:\n                        num_pending[worker] += 1\n                    if len(task.workers) == 1:\n                        num_uniques[list(task.workers)[0]] += 1\n            for worker in workers:\n                tasks = running[worker['name']]\n                worker['num_running'] = len(tasks)\n                worker['num_pending'] = num_pending[worker['name']]\n                worker['num_uniques'] = num_uniques[worker['name']]\n                worker['running'] = tasks\n        return workers\n\n    def inverse_dep_graph(self, task_id, **kwargs):\n        self.prune()\n        serialized = {}\n        if self._state.has_task(task_id):\n            self._traverse_inverse_deps(task_id, serialized)\n        return serialized\n\n    def _traverse_inverse_deps(self, task_id, serialized):\n        stack = [task_id]\n        serialized[task_id] = self._serialize_task(task_id)\n        while len(stack) > 0:\n            curr_id = stack.pop()\n            for task in self._state.get_active_tasks():\n                if curr_id in task.deps:\n                    serialized[curr_id][\"deps\"].append(task.id)\n                    if task.id not in serialized:\n                        serialized[task.id] = self._serialize_task(task.id)\n                        serialized[task.id][\"deps\"] = []\n                        stack.append(task.id)\n\n    def task_search(self, task_str, **kwargs):\n        \"\"\"\n        Query for a subset of tasks by task_id.\n\n        :param task_str:\n        :return:\n        \"\"\"\n        self.prune()\n        result = collections.defaultdict(dict)\n        for task in self._state.get_active_tasks():\n            if task.id.find(task_str) != -1:\n                serialized = self._serialize_task(task.id, False)\n                result[task.status][task.id] = serialized\n        return result\n\n    def re_enable_task(self, task_id):\n        serialized = {}\n        task = self._state.get_task(task_id)\n        if task and task.status == DISABLED and task.scheduler_disable_time:\n            self._state.re_enable(task, self._config)\n            serialized = self._serialize_task(task_id)\n        return serialized\n\n    def fetch_error(self, task_id, **kwargs):\n        if self._state.has_task(task_id):\n            return {\"taskId\": task_id, \"error\": self._state.get_task(task_id).expl}\n        else:\n            return {\"taskId\": task_id, \"error\": \"\"}\n\n    def _update_task_history(self, task_id, status, host=None):\n        try:\n            if status == DONE or status == FAILED:\n                successful = (status == DONE)\n                self._task_history.task_finished(task_id, successful)\n            elif status == PENDING:\n                self._task_history.task_scheduled(task_id)\n            elif status == RUNNING:\n                self._task_history.task_started(task_id, host)\n        except BaseException:\n            logger.warning(\"Error saving Task history\", exc_info=True)\n\n    @property\n    def task_history(self):\n        # Used by server.py to expose the calls\n        return self._task_history\n",
          "file_after": "# -*- coding: utf-8 -*-\n#\n# Copyright 2012-2015 Spotify AB\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\"\"\"\nThe system for scheduling tasks and executing them in order.\nDeals with dependencies, priorities, resources, etc.\nThe :py:class:`~luigi.worker.Worker` pulls tasks from the scheduler (usually over the REST interface) and executes them.\n\"\"\"\n\nimport collections\ntry:\n    import cPickle as pickle\nexcept ImportError:\n    import pickle\nimport datetime\nimport functools\nimport itertools\nimport logging\nimport os\nimport time\n\nfrom luigi import six\n\nfrom luigi import configuration\nfrom luigi import notifications\nfrom luigi import parameter\nfrom luigi import task_history as history\nfrom luigi.task_status import DISABLED, DONE, FAILED, PENDING, RUNNING, SUSPENDED, UNKNOWN\nfrom luigi.task import Config\n\nlogger = logging.getLogger(\"luigi.server\")\n\n\nclass Scheduler(object):\n    \"\"\"\n    Abstract base class.\n\n    Note that the methods all take string arguments, not Task objects...\n    \"\"\"\"\"\n    add_task = NotImplemented\n    get_work = NotImplemented\n    ping = NotImplemented\n\nUPSTREAM_RUNNING = 'UPSTREAM_RUNNING'\nUPSTREAM_MISSING_INPUT = 'UPSTREAM_MISSING_INPUT'\nUPSTREAM_FAILED = 'UPSTREAM_FAILED'\nUPSTREAM_DISABLED = 'UPSTREAM_DISABLED'\n\nUPSTREAM_SEVERITY_ORDER = (\n    '',\n    UPSTREAM_RUNNING,\n    UPSTREAM_MISSING_INPUT,\n    UPSTREAM_FAILED,\n    UPSTREAM_DISABLED,\n)\nUPSTREAM_SEVERITY_KEY = UPSTREAM_SEVERITY_ORDER.index\nSTATUS_TO_UPSTREAM_MAP = {\n    FAILED: UPSTREAM_FAILED,\n    RUNNING: UPSTREAM_RUNNING,\n    PENDING: UPSTREAM_MISSING_INPUT,\n    DISABLED: UPSTREAM_DISABLED,\n}\n\n\nclass scheduler(Config):\n    # TODO(erikbern): the config_path is needed for backwards compatilibity. We should drop the compatibility\n    # at some point (in particular this would force users to replace all dashes with underscores in the config)\n    retry_delay = parameter.FloatParameter(default=900.0,\n                                           config_path=dict(section='scheduler', name='retry-delay'))\n    remove_delay = parameter.FloatParameter(default=600.0,\n                                            config_path=dict(section='scheduler', name='remote-delay'))\n    worker_disconnect_delay = parameter.FloatParameter(default=60.0,\n                                                       config_path=dict(section='scheduler', name='worker-disconnect-delay'))\n    state_path = parameter.Parameter(default='/var/lib/luigi-server/state.pickle',\n                                     config_path=dict(section='scheduler', name='state-path'))\n\n    # Jobs are disabled if we see more than disable_failures failures in disable_window seconds.\n    # These disables last for disable_persist seconds.\n    disable_window = parameter.IntParameter(default=3600,\n                                            config_path=dict(section='scheduler', name='disable-window-seconds'))\n    disable_failures = parameter.IntParameter(default=None,\n                                              config_path=dict(section='scheduler', name='disable-num-failures'))\n    disable_persist = parameter.IntParameter(default=86400,\n                                             config_path=dict(section='scheduler', name='disable-persist-seconds'))\n    max_shown_tasks = parameter.IntParameter(default=100000,\n                                             config_path=dict(section='scheduler', name='max-shown-task'))\n\n    record_task_history = parameter.BoolParameter(default=False)\n\n\ndef fix_time(x):\n    # Backwards compatibility for a fix in Dec 2014. Prior to the fix, pickled state might store datetime objects\n    # Let's remove this function soon\n    if isinstance(x, datetime.datetime):\n        return time.mktime(x.timetuple())\n    else:\n        return x\n\n\nclass Failures(object):\n    \"\"\"\n    This class tracks the number of failures in a given time window.\n\n    Failures added are marked with the current timestamp, and this class counts\n    the number of failures in a sliding time window ending at the present.\n    \"\"\"\n\n    def __init__(self, window):\n        \"\"\"\n        Initialize with the given window.\n\n        :param window: how long to track failures for, as a float (number of seconds).\n        \"\"\"\n        self.window = window\n        self.failures = collections.deque()\n\n    def add_failure(self):\n        \"\"\"\n        Add a failure event with the current timestamp.\n        \"\"\"\n        self.failures.append(time.time())\n\n    def num_failures(self):\n        \"\"\"\n        Return the number of failures in the window.\n        \"\"\"\n        min_time = time.time() - self.window\n\n        while self.failures and fix_time(self.failures[0]) < min_time:\n            self.failures.popleft()\n\n        return len(self.failures)\n\n    def clear(self):\n        \"\"\"\n        Clear the failure queue.\n        \"\"\"\n        self.failures.clear()\n\n\ndef _get_default(x, default):\n    if x is not None:\n        return x\n    else:\n        return default\n\n\nclass Task(object):\n\n    def __init__(self, task_id, status, deps, resources=None, priority=0, family='', params=None,\n                 disable_failures=None, disable_window=None):\n        self.id = task_id\n        self.stakeholders = set()  # workers ids that are somehow related to this task (i.e. don't prune while any of these workers are still active)\n        self.workers = set()  # workers ids that can perform task - task is 'BROKEN' if none of these workers are active\n        if deps is None:\n            self.deps = set()\n        else:\n            self.deps = set(deps)\n        self.status = status  # PENDING, RUNNING, FAILED or DONE\n        self.time = time.time()  # Timestamp when task was first added\n        self.retry = None\n        self.remove = None\n        self.worker_running = None  # the worker id that is currently running the task or None\n        self.time_running = None  # Timestamp when picked up by worker\n        self.expl = None\n        self.priority = priority\n        self.resources = _get_default(resources, {})\n        self.family = family\n        self.params = _get_default(params, {})\n        self.disable_failures = disable_failures\n        self.failures = Failures(disable_window)\n        self.scheduler_disable_time = None\n\n    def __repr__(self):\n        return \"Task(%r)\" % vars(self)\n\n    def add_failure(self):\n        self.failures.add_failure()\n\n    def has_excessive_failures(self):\n        return self.failures.num_failures() >= self.disable_failures\n\n    def can_disable(self):\n        return self.disable_failures is not None\n\n\nclass Worker(object):\n    \"\"\"\n    Structure for tracking worker activity and keeping their references.\n    \"\"\"\n\n    def __init__(self, worker_id, last_active=None):\n        self.id = worker_id\n        self.reference = None  # reference to the worker in the real world. (Currently a dict containing just the host)\n        self.last_active = last_active  # seconds since epoch\n        self.started = time.time()  # seconds since epoch\n        self.info = {}\n\n    def add_info(self, info):\n        self.info.update(info)\n\n    def update(self, worker_reference):\n        if worker_reference:\n            self.reference = worker_reference\n        self.last_active = time.time()\n\n    def prune(self, config):\n        # Delete workers that haven't said anything for a while (probably killed)\n        if self.last_active + config.worker_disconnect_delay < time.time():\n            return True\n\n    def __str__(self):\n        return self.id\n\n\nclass SimpleTaskState(object):\n    \"\"\"\n    Keep track of the current state and handle persistance.\n\n    The point of this class is to enable other ways to keep state, eg. by using a database\n    These will be implemented by creating an abstract base class that this and other classes\n    inherit from.\n    \"\"\"\n\n    def __init__(self, state_path):\n        self._state_path = state_path\n        self._tasks = {}  # map from id to a Task object\n        self._status_tasks = collections.defaultdict(dict)\n        self._active_workers = {}  # map from id to a Worker object\n\n    def dump(self):\n        state = (self._tasks, self._active_workers)\n        try:\n            with open(self._state_path, 'w') as fobj:\n                pickle.dump(state, fobj)\n        except IOError:\n            logger.warning(\"Failed saving scheduler state\", exc_info=1)\n        else:\n            logger.info(\"Saved state in %s\", self._state_path)\n\n    # prone to lead to crashes when old state is unpickled with updated code. TODO some kind of version control?\n    def load(self):\n        if os.path.exists(self._state_path):\n            logger.info(\"Attempting to load state from %s\", self._state_path)\n            try:\n                with open(self._state_path, 'rb') as fobj:\n                    state = pickle.load(fobj)\n            except BaseException:\n                logger.exception(\"Error when loading state. Starting from clean slate.\")\n                return\n\n            self._tasks, self._active_workers = state\n            self._status_tasks = collections.defaultdict(dict)\n            for task in six.itervalues(self._tasks):\n                self._status_tasks[task.status][task.id] = task\n\n            # Convert from old format\n            # TODO: this is really ugly, we need something more future-proof\n            # Every time we add an attribute to the Worker class, this code needs to be updated\n            for k, v in six.iteritems(self._active_workers):\n                if isinstance(v, float):\n                    self._active_workers[k] = Worker(worker_id=k, last_active=v)\n        else:\n            logger.info(\"No prior state file exists at %s. Starting with clean slate\", self._state_path)\n\n    def get_active_tasks(self, status=None):\n        if status:\n            for task in six.itervalues(self._status_tasks[status]):\n                yield task\n        else:\n            for task in six.itervalues(self._tasks):\n                yield task\n\n    def get_running_tasks(self):\n        return six.itervalues(self._status_tasks[RUNNING])\n\n    def get_pending_tasks(self):\n        return itertools.chain.from_iterable(six.itervalues(self._status_tasks[status])\n                                             for status in [PENDING, RUNNING])\n\n    def get_task(self, task_id, default=None, setdefault=None):\n        if setdefault:\n            task = self._tasks.setdefault(task_id, setdefault)\n            self._status_tasks[task.status][task.id] = task\n            return task\n        else:\n            return self._tasks.get(task_id, default)\n\n    def has_task(self, task_id):\n        return task_id in self._tasks\n\n    def re_enable(self, task, config=None):\n        task.scheduler_disable_time = None\n        task.failures.clear()\n        if config:\n            self.set_status(task, FAILED, config)\n            task.failures.clear()\n\n    def set_status(self, task, new_status, config=None):\n        if new_status == FAILED:\n            assert config is not None\n\n        # not sure why we have SUSPENDED, as it can never be set\n        if new_status == SUSPENDED:\n            new_status = PENDING\n\n        if new_status == DISABLED and task.status == RUNNING:\n            return\n\n        if task.status == DISABLED:\n            if new_status == DONE:\n                self.re_enable(task)\n\n            # don't allow workers to override a scheduler disable\n            elif task.scheduler_disable_time is not None:\n                return\n\n        if new_status == FAILED and task.can_disable():\n            task.add_failure()\n            if task.has_excessive_failures():\n                task.scheduler_disable_time = time.time()\n                new_status = DISABLED\n                notifications.send_error_email(\n                    'Luigi Scheduler: DISABLED {task} due to excessive failures'.format(task=task.id),\n                    '{task} failed {failures} times in the last {window} seconds, so it is being '\n                    'disabled for {persist} seconds'.format(\n                        failures=config.disable_failures,\n                        task=task.id,\n                        window=config.disable_window,\n                        persist=config.disable_persist,\n                    ))\n        elif new_status == DISABLED:\n            task.scheduler_disable_time = None\n\n        self._status_tasks[task.status].pop(task.id)\n        self._status_tasks[new_status][task.id] = task\n        task.status = new_status\n\n    def prune(self, task, config):\n        remove = False\n\n        # Mark tasks with no remaining active stakeholders for deletion\n        if not task.stakeholders:\n            if task.remove is None:\n                logger.info(\"Task %r has stakeholders %r but none remain connected -> will remove \"\n                            \"task in %s seconds\", task.id, task.stakeholders, config.remove_delay)\n                task.remove = time.time() + config.remove_delay\n\n        # If a running worker disconnects, tag all its jobs as FAILED and subject it to the same retry logic\n        if task.status == RUNNING and task.worker_running and task.worker_running not in task.stakeholders:\n            logger.info(\"Task %r is marked as running by disconnected worker %r -> marking as \"\n                        \"FAILED with retry delay of %rs\", task.id, task.worker_running,\n                        config.retry_delay)\n            task.worker_running = None\n            self.set_status(task, FAILED, config)\n            task.retry = time.time() + config.retry_delay\n\n        # Re-enable task after the disable time expires\n        if task.status == DISABLED and task.scheduler_disable_time:\n            if time.time() - fix_time(task.scheduler_disable_time) > config.disable_persist:\n                self.re_enable(task, config)\n\n        # Remove tasks that have no stakeholders\n        if task.remove and time.time() > task.remove:\n            logger.info(\"Removing task %r (no connected stakeholders)\", task.id)\n            remove = True\n\n        # Reset FAILED tasks to PENDING if max timeout is reached, and retry delay is >= 0\n        if task.status == FAILED and config.retry_delay >= 0 and task.retry < time.time():\n            self.set_status(task, PENDING, config)\n\n        return remove\n\n    def inactivate_tasks(self, delete_tasks):\n        # The terminology is a bit confusing: we used to \"delete\" tasks when they became inactive,\n        # but with a pluggable state storage, you might very well want to keep some history of\n        # older tasks as well. That's why we call it \"inactivate\" (as in the verb)\n        for task in delete_tasks:\n            task_obj = self._tasks.pop(task)\n            self._status_tasks[task_obj.status].pop(task)\n\n    def get_active_workers(self, last_active_lt=None):\n        for worker in six.itervalues(self._active_workers):\n            if last_active_lt is not None and worker.last_active >= last_active_lt:\n                continue\n            yield worker\n\n    def get_worker_ids(self):\n        return self._active_workers.keys()  # only used for unit tests\n\n    def get_worker(self, worker_id):\n        return self._active_workers.setdefault(worker_id, Worker(worker_id))\n\n    def inactivate_workers(self, delete_workers):\n        # Mark workers as inactive\n        for worker in delete_workers:\n            self._active_workers.pop(worker)\n\n        # remove workers from tasks\n        for task in self.get_active_tasks():\n            task.stakeholders.difference_update(delete_workers)\n            task.workers.difference_update(delete_workers)\n\n\nclass CentralPlannerScheduler(Scheduler):\n    \"\"\"\n    Async scheduler that can handle multiple workers, etc.\n\n    Can be run locally or on a server (using RemoteScheduler + server.Server).\n    \"\"\"\n\n    def __init__(self, config=None, resources=None, task_history_impl=None, **kwargs):\n        \"\"\"\n        Keyword Arguments:\n        :param config: an object of class \"scheduler\" or None (in which the global instance will be used)\n        :param resources: a dict of str->int constraints\n        :param task_history_override: ignore config and use this object as the task history\n        \"\"\"\n        self._config = config or scheduler(**kwargs)\n        self._state = SimpleTaskState(self._config.state_path)\n\n        if task_history_impl:\n            self._task_history = task_history_impl\n        elif self._config.record_task_history:\n            import db_task_history  # Needs sqlalchemy, thus imported here\n            self._task_history = db_task_history.DbTaskHistory()\n        else:\n            self._task_history = history.NopHistory()\n        self._resources = resources or configuration.get_config().getintdict('resources')  # TODO: Can we make this a Parameter?\n        self._make_task = functools.partial(\n            Task, disable_failures=self._config.disable_failures,\n            disable_window=self._config.disable_window)\n\n    def load(self):\n        self._state.load()\n\n    def dump(self):\n        self._state.dump()\n\n    def prune(self):\n        logger.info(\"Starting pruning of task graph\")\n        remove_workers = []\n        for worker in self._state.get_active_workers():\n            if worker.prune(self._config):\n                logger.info(\"Worker %s timed out (no contact for >=%ss)\", worker, self._config.worker_disconnect_delay)\n                remove_workers.append(worker.id)\n\n        self._state.inactivate_workers(remove_workers)\n\n        remove_tasks = []\n        for task in self._state.get_active_tasks():\n            if self._state.prune(task, self._config):\n                remove_tasks.append(task.id)\n\n        self._state.inactivate_tasks(remove_tasks)\n\n        logger.info(\"Done pruning task graph\")\n\n    def update(self, worker_id, worker_reference=None):\n        \"\"\"\n        Keep track of whenever the worker was last active.\n        \"\"\"\n        worker = self._state.get_worker(worker_id)\n        worker.update(worker_reference)\n\n    def _update_priority(self, task, prio, worker):\n        \"\"\"\n        Update priority of the given task.\n\n        Priority can only be increased.\n        If the task doesn't exist, a placeholder task is created to preserve priority when the task is later scheduled.\n        \"\"\"\n        task.priority = prio = max(prio, task.priority)\n        for dep in task.deps or []:\n            t = self._state.get_task(dep)\n            if t is not None and prio > t.priority:\n                self._update_priority(t, prio, worker)\n\n    def add_task(self, worker, task_id, status=PENDING, runnable=True,\n                 deps=None, new_deps=None, expl=None, resources=None,\n                 priority=0, family='', params=None, **kwargs):\n        \"\"\"\n        * add task identified by task_id if it doesn't exist\n        * if deps is not None, update dependency list\n        * update status of task\n        * add additional workers/stakeholders\n        * update priority when needed\n        \"\"\"\n        self.update(worker)\n\n        task = self._state.get_task(task_id, setdefault=self._make_task(\n            task_id=task_id, status=PENDING, deps=deps, resources=resources,\n            priority=priority, family=family, params=params))\n\n        # for setting priority, we'll sometimes create tasks with unset family and params\n        if not task.family:\n            task.family = family\n        if not task.params:\n            task.params = _get_default(params, {})\n\n        if task.remove is not None:\n            task.remove = None  # unmark task for removal so it isn't removed after being added\n\n        if not (task.status == RUNNING and status == PENDING):\n            # don't allow re-scheduling of task while it is running, it must either fail or succeed first\n            if status == PENDING or status != task.status:\n                # Update the DB only if there was a acctual change, to prevent noise.\n                # We also check for status == PENDING b/c that's the default value\n                # (so checking for status != task.status woule lie)\n                self._update_task_history(task_id, status)\n            self._state.set_status(task, PENDING if status == SUSPENDED else status, self._config)\n            if status == FAILED:\n                task.retry = time.time() + self._config.retry_delay\n\n        if deps is not None:\n            task.deps = set(deps)\n\n        if new_deps is not None:\n            task.deps.update(new_deps)\n\n        task.stakeholders.add(worker)\n        task.resources = resources\n\n        # Task dependencies might not exist yet. Let's create dummy tasks for them for now.\n        # Otherwise the task dependencies might end up being pruned if scheduling takes a long time\n        for dep in task.deps or []:\n            t = self._state.get_task(dep, setdefault=self._make_task(task_id=dep, status=UNKNOWN, deps=None, priority=priority))\n            t.stakeholders.add(worker)\n\n        self._update_priority(task, priority, worker)\n\n        if runnable:\n            task.workers.add(worker)\n\n        if expl is not None:\n            task.expl = expl\n\n    def add_worker(self, worker, info, **kwargs):\n        self._state.get_worker(worker).add_info(info)\n\n    def update_resources(self, **resources):\n        if self._resources is None:\n            self._resources = {}\n        self._resources.update(resources)\n\n    def _has_resources(self, needed_resources, used_resources):\n        if needed_resources is None:\n            return True\n\n        available_resources = self._resources or {}\n        for resource, amount in six.iteritems(needed_resources):\n            if amount + used_resources[resource] > available_resources.get(resource, 1):\n                return False\n        return True\n\n    def _used_resources(self):\n        used_resources = collections.defaultdict(int)\n        if self._resources is not None:\n            for task in self._state.get_active_tasks():\n                if task.status == RUNNING and task.resources:\n                    for resource, amount in six.iteritems(task.resources):\n                        used_resources[resource] += amount\n        return used_resources\n\n    def _rank(self):\n        \"\"\"\n        Return worker's rank function for task scheduling.\n\n        :return:\n        \"\"\"\n        dependents = collections.defaultdict(int)\n\n        def not_done(t):\n            task = self._state.get_task(t, default=None)\n            return task is None or task.status != DONE\n        for task in self._state.get_pending_tasks():\n            if task.status != DONE:\n                deps = list(filter(not_done, task.deps))\n                inverse_num_deps = 1.0 / max(len(deps), 1)\n                for dep in deps:\n                    dependents[dep] += inverse_num_deps\n\n        return lambda task: (task.priority, dependents[task.id], -task.time)\n\n    def _schedulable(self, task):\n        if task.status != PENDING:\n            return False\n        for dep in task.deps:\n            dep_task = self._state.get_task(dep, default=None)\n            if dep_task is None or dep_task.status != DONE:\n                return False\n        return True\n\n    def get_work(self, worker, host=None, assistant=False, **kwargs):\n        # TODO: remove any expired nodes\n\n        # Algo: iterate over all nodes, find the highest priority node no dependencies and available\n        # resources.\n\n        # Resource checking looks both at currently available resources and at which resources would\n        # be available if all running tasks died and we rescheduled all workers greedily. We do both\n        # checks in order to prevent a worker with many low-priority tasks from starving other\n        # workers with higher priority tasks that share the same resources.\n\n        # TODO: remove tasks that can't be done, figure out if the worker has absolutely\n        # nothing it can wait for\n\n        # Return remaining tasks that have no FAILED descendents\n        self.update(worker, {'host': host})\n        best_task = None\n        best_task_id = None\n        locally_pending_tasks = 0\n        running_tasks = []\n\n        used_resources = self._used_resources()\n        greedy_resources = collections.defaultdict(int)\n        n_unique_pending = 0\n        greedy_workers = dict((worker.id, worker.info.get('workers', 1))\n                              for worker in self._state.get_active_workers())\n\n        tasks = list(self._state.get_pending_tasks())\n        tasks.sort(key=self._rank(), reverse=True)\n\n        for task in tasks:\n            if task.status == 'RUNNING' and worker in task.workers:\n                # Return a list of currently running tasks to the client,\n                # makes it easier to troubleshoot\n                other_worker = self._state.get_worker(task.worker_running)\n                more_info = {'task_id': task.id, 'worker': str(other_worker)}\n                if other_worker is not None:\n                    more_info.update(other_worker.info)\n                    running_tasks.append(more_info)\n\n            if task.status == PENDING and (worker in task.workers or assistant):\n                locally_pending_tasks += 1\n                if len(task.workers) == 1:\n                    n_unique_pending += 1\n\n            if task.status == RUNNING and (task.worker_running in greedy_workers or assistant):\n                greedy_workers[task.worker_running] -= 1\n                for resource, amount in six.iteritems((task.resources or {})):\n                    greedy_resources[resource] += amount\n\n            if not best_task and self._schedulable(task) and self._has_resources(task.resources, greedy_resources):\n                if (worker in task.workers or assistant) and self._has_resources(task.resources, used_resources):\n                    best_task = task\n                    best_task_id = task.id\n                else:\n                    for task_worker in task.workers:\n                        if greedy_workers.get(task_worker, 0) > 0:\n                            # use up a worker\n                            greedy_workers[task_worker] -= 1\n\n                            # keep track of the resources used in greedy scheduling\n                            for resource, amount in six.iteritems((task.resources or {})):\n                                greedy_resources[resource] += amount\n\n                            break\n\n        reply = {'n_pending_tasks': locally_pending_tasks,\n                 'running_tasks': running_tasks,\n                 'task_id': None,\n                 'n_unique_pending': n_unique_pending}\n\n        if best_task:\n            self._state.set_status(best_task, RUNNING, self._config)\n            best_task.worker_running = worker\n            best_task.time_running = time.time()\n            self._update_task_history(best_task.id, RUNNING, host=host)\n\n            reply['task_id'] = best_task.id\n            reply['task_family'] = best_task.family\n            reply['task_params'] = best_task.params\n\n        return reply\n\n    def ping(self, worker, **kwargs):\n        self.update(worker)\n\n    def _upstream_status(self, task_id, upstream_status_table):\n        if task_id in upstream_status_table:\n            return upstream_status_table[task_id]\n        elif self._state.has_task(task_id):\n            task_stack = [task_id]\n\n            while task_stack:\n                dep_id = task_stack.pop()\n                if self._state.has_task(dep_id):\n                    dep = self._state.get_task(dep_id)\n                    if dep_id not in upstream_status_table:\n                        if dep.status == PENDING and dep.deps:\n                            task_stack = task_stack + [dep_id] + list(dep.deps)\n                            upstream_status_table[dep_id] = ''  # will be updated postorder\n                        else:\n                            dep_status = STATUS_TO_UPSTREAM_MAP.get(dep.status, '')\n                            upstream_status_table[dep_id] = dep_status\n                    elif upstream_status_table[dep_id] == '' and dep.deps:\n                        # This is the postorder update step when we set the\n                        # status based on the previously calculated child elements\n                        upstream_status = [upstream_status_table.get(task_id, '') for task_id in dep.deps]\n                        upstream_status.append('')  # to handle empty list\n                        status = max(upstream_status, key=UPSTREAM_SEVERITY_KEY)\n                        upstream_status_table[dep_id] = status\n            return upstream_status_table[dep_id]\n\n    def _serialize_task(self, task_id, include_deps=True):\n        task = self._state.get_task(task_id)\n        ret = {\n            'deps': list(task.deps),\n            'status': task.status,\n            'workers': list(task.workers),\n            'worker_running': task.worker_running,\n            'time_running': getattr(task, \"time_running\", None),\n            'start_time': task.time,\n            'params': task.params,\n            'name': task.family,\n            'priority': task.priority,\n            'resources': task.resources,\n        }\n        if include_deps:\n            ret['deps'] = list(task.deps)\n        return ret\n\n    def graph(self, **kwargs):\n        self.prune()\n        serialized = {}\n        for task in self._state.get_active_tasks():\n            serialized[task.id] = self._serialize_task(task.id)\n        return serialized\n\n    def _recurse_deps(self, task_id, serialized):\n        if task_id not in serialized:\n            task = self._state.get_task(task_id)\n            if task is None or not task.family:\n                logger.warn('Missing task for id [%s]', task_id)\n\n                # try to infer family and params from task_id\n                try:\n                    family, _, param_str = task_id.rstrip(')').partition('(')\n                    params = dict(param.split('=') for param in param_str.split(', '))\n                except BaseException:\n                    family, params = '', {}\n                serialized[task_id] = {\n                    'deps': [],\n                    'status': UNKNOWN,\n                    'workers': [],\n                    'start_time': UNKNOWN,\n                    'params': params,\n                    'name': family,\n                    'priority': 0,\n                }\n            else:\n                serialized[task_id] = self._serialize_task(task_id)\n                for dep in task.deps:\n                    self._recurse_deps(dep, serialized)\n\n    def dep_graph(self, task_id, **kwargs):\n        self.prune()\n        serialized = {}\n        if self._state.has_task(task_id):\n            self._recurse_deps(task_id, serialized)\n        return serialized\n\n    def task_list(self, status, upstream_status, limit=True, **kwargs):\n        \"\"\"\n        Query for a subset of tasks by status.\n        \"\"\"\n        self.prune()\n        result = {}\n        upstream_status_table = {}  # used to memoize upstream status\n        for task in self._state.get_active_tasks(status):\n            if (task.status != PENDING or not upstream_status or\n                    upstream_status == self._upstream_status(task.id, upstream_status_table)):\n                serialized = self._serialize_task(task.id, False)\n                result[task.id] = serialized\n        if limit and len(result) > self._config.max_shown_tasks:\n            return {'num_tasks': len(result)}\n        return result\n\n    def worker_list(self, include_running=True, **kwargs):\n        self.prune()\n        workers = [\n            dict(\n                name=worker.id,\n                last_active=worker.last_active,\n                started=getattr(worker, 'started', None),\n                **worker.info\n            ) for worker in self._state.get_active_workers()]\n        workers.sort(key=lambda worker: worker['started'], reverse=True)\n        if include_running:\n            running = collections.defaultdict(dict)\n            num_pending = collections.defaultdict(int)\n            num_uniques = collections.defaultdict(int)\n            for task in self._state.get_pending_tasks():\n                if task.status == RUNNING and task.worker_running:\n                    running[task.worker_running][task.id] = self._serialize_task(task.id, False)\n                elif task.status == PENDING:\n                    for worker in task.workers:\n                        num_pending[worker] += 1\n                    if len(task.workers) == 1:\n                        num_uniques[list(task.workers)[0]] += 1\n            for worker in workers:\n                tasks = running[worker['name']]\n                worker['num_running'] = len(tasks)\n                worker['num_pending'] = num_pending[worker['name']]\n                worker['num_uniques'] = num_uniques[worker['name']]\n                worker['running'] = tasks\n        return workers\n\n    def inverse_dep_graph(self, task_id, **kwargs):\n        self.prune()\n        serialized = {}\n        if self._state.has_task(task_id):\n            self._traverse_inverse_deps(task_id, serialized)\n        return serialized\n\n    def _traverse_inverse_deps(self, task_id, serialized):\n        stack = [task_id]\n        serialized[task_id] = self._serialize_task(task_id)\n        while len(stack) > 0:\n            curr_id = stack.pop()\n            for task in self._state.get_active_tasks():\n                if curr_id in task.deps:\n                    serialized[curr_id][\"deps\"].append(task.id)\n                    if task.id not in serialized:\n                        serialized[task.id] = self._serialize_task(task.id)\n                        serialized[task.id][\"deps\"] = []\n                        stack.append(task.id)\n\n    def task_search(self, task_str, **kwargs):\n        \"\"\"\n        Query for a subset of tasks by task_id.\n\n        :param task_str:\n        :return:\n        \"\"\"\n        self.prune()\n        result = collections.defaultdict(dict)\n        for task in self._state.get_active_tasks():\n            if task.id.find(task_str) != -1:\n                serialized = self._serialize_task(task.id, False)\n                result[task.status][task.id] = serialized\n        return result\n\n    def re_enable_task(self, task_id):\n        serialized = {}\n        task = self._state.get_task(task_id)\n        if task and task.status == DISABLED and task.scheduler_disable_time:\n            self._state.re_enable(task, self._config)\n            serialized = self._serialize_task(task_id)\n        return serialized\n\n    def fetch_error(self, task_id, **kwargs):\n        if self._state.has_task(task_id):\n            return {\"taskId\": task_id, \"error\": self._state.get_task(task_id).expl}\n        else:\n            return {\"taskId\": task_id, \"error\": \"\"}\n\n    def _update_task_history(self, task_id, status, host=None):\n        try:\n            if status == DONE or status == FAILED:\n                successful = (status == DONE)\n                self._task_history.task_finished(task_id, successful)\n            elif status == PENDING:\n                self._task_history.task_scheduled(task_id)\n            elif status == RUNNING:\n                self._task_history.task_started(task_id, host)\n        except BaseException:\n            logger.warning(\"Error saving Task history\", exc_info=True)\n\n    @property\n    def task_history(self):\n        # Used by server.py to expose the calls\n        return self._task_history\n",
          "file_patch": "@@ -604,7 +604,7 @@ class CentralPlannerScheduler(Scheduler):\n                 return False\n         return True\n \n-    def get_work(self, worker, host=None, **kwargs):\n+    def get_work(self, worker, host=None, assistant=False, **kwargs):\n         # TODO: remove any expired nodes\n \n         # Algo: iterate over all nodes, find the highest priority node no dependencies and available\n@@ -644,18 +644,18 @@ class CentralPlannerScheduler(Scheduler):\n                     more_info.update(other_worker.info)\n                     running_tasks.append(more_info)\n \n-            if task.status == PENDING and worker in task.workers:\n+            if task.status == PENDING and (worker in task.workers or assistant):\n                 locally_pending_tasks += 1\n                 if len(task.workers) == 1:\n                     n_unique_pending += 1\n \n-            if task.status == RUNNING and task.worker_running in greedy_workers:\n+            if task.status == RUNNING and (task.worker_running in greedy_workers or assistant):\n                 greedy_workers[task.worker_running] -= 1\n                 for resource, amount in six.iteritems((task.resources or {})):\n                     greedy_resources[resource] += amount\n \n             if not best_task and self._schedulable(task) and self._has_resources(task.resources, greedy_resources):\n-                if worker in task.workers and self._has_resources(task.resources, used_resources):\n+                if (worker in task.workers or assistant) and self._has_resources(task.resources, used_resources):\n                     best_task = task\n                     best_task_id = task.id\n                 else:\n@@ -670,16 +670,22 @@ class CentralPlannerScheduler(Scheduler):\n \n                             break\n \n+        reply = {'n_pending_tasks': locally_pending_tasks,\n+                 'running_tasks': running_tasks,\n+                 'task_id': None,\n+                 'n_unique_pending': n_unique_pending}\n+\n         if best_task:\n             self._state.set_status(best_task, RUNNING, self._config)\n             best_task.worker_running = worker\n             best_task.time_running = time.time()\n             self._update_task_history(best_task.id, RUNNING, host=host)\n \n-        return {'n_pending_tasks': locally_pending_tasks,\n-                'n_unique_pending': n_unique_pending,\n-                'task_id': best_task_id,\n-                'running_tasks': running_tasks}\n+            reply['task_id'] = best_task.id\n+            reply['task_family'] = best_task.family\n+            reply['task_params'] = best_task.params\n+\n+        return reply\n \n     def ping(self, worker, **kwargs):\n         self.update(worker)\n",
          "files_name_in_blame_commit": [
            "scheduler.py",
            "interface.py",
            "worker_test.py",
            "worker.py",
            "customized_run_test.py"
          ]
        }
      },
      "dbd1662929b530c0df74778ab4b7ecde6ad6457b": {
        "commit": {
          "commit_id": "dbd1662929b530c0df74778ab4b7ecde6ad6457b",
          "commit_message": "TopArtist doesn't raise exception anymore\n\nstill doesn't work thou\nalso fix all iteritems on dict",
          "commit_author": "Guillaume Poulin",
          "commit_date": "2015-02-15 11:33:03",
          "commit_parent": "867da22c68dce57bc74da39f17df0cb8eb09e34d"
        },
        "function": {
          "function_name": "get_work",
          "function_code_before": "def get_work(self, worker, host=None, **kwargs):\n    self.update(worker, {'host': host})\n    best_task = None\n    best_task_id = None\n    locally_pending_tasks = 0\n    running_tasks = []\n    used_resources = self._used_resources()\n    greedy_resources = collections.defaultdict(int)\n    n_unique_pending = 0\n    greedy_workers = dict(((worker.id, worker.info.get('workers', 1)) for worker in self._state.get_active_workers()))\n    tasks = list(self._state.get_pending_tasks())\n    tasks.sort(key=self._rank(), reverse=True)\n    for task in tasks:\n        if task.status == 'RUNNING' and worker in task.workers:\n            other_worker = self._state.get_worker(task.worker_running)\n            more_info = {'task_id': task.id, 'worker': str(other_worker)}\n            if other_worker is not None:\n                more_info.update(other_worker.info)\n                running_tasks.append(more_info)\n        if task.status == PENDING and worker in task.workers:\n            locally_pending_tasks += 1\n            if len(task.workers) == 1:\n                n_unique_pending += 1\n        if task.status == RUNNING and task.worker_running in greedy_workers:\n            greedy_workers[task.worker_running] -= 1\n            for (resource, amount) in (task.resources or {}).iteritems():\n                greedy_resources[resource] += amount\n        if not best_task and self._schedulable(task) and self._has_resources(task.resources, greedy_resources):\n            if worker in task.workers and self._has_resources(task.resources, used_resources):\n                best_task = task\n                best_task_id = task.id\n            else:\n                for task_worker in task.workers:\n                    if greedy_workers.get(task_worker, 0) > 0:\n                        greedy_workers[task_worker] -= 1\n                        for (resource, amount) in (task.resources or {}).iteritems():\n                            greedy_resources[resource] += amount\n                        break\n    if best_task:\n        self._state.set_status(best_task, RUNNING, self._config)\n        best_task.worker_running = worker\n        best_task.time_running = time.time()\n        self._update_task_history(best_task.id, RUNNING, host=host)\n    return {'n_pending_tasks': locally_pending_tasks, 'n_unique_pending': n_unique_pending, 'task_id': best_task_id, 'running_tasks': running_tasks}",
          "function_code_after": "def get_work(self, worker, host=None, **kwargs):\n    self.update(worker, {'host': host})\n    best_task = None\n    best_task_id = None\n    locally_pending_tasks = 0\n    running_tasks = []\n    used_resources = self._used_resources()\n    greedy_resources = collections.defaultdict(int)\n    n_unique_pending = 0\n    greedy_workers = dict(((worker.id, worker.info.get('workers', 1)) for worker in self._state.get_active_workers()))\n    tasks = list(self._state.get_pending_tasks())\n    tasks.sort(key=self._rank(), reverse=True)\n    for task in tasks:\n        if task.status == 'RUNNING' and worker in task.workers:\n            other_worker = self._state.get_worker(task.worker_running)\n            more_info = {'task_id': task.id, 'worker': str(other_worker)}\n            if other_worker is not None:\n                more_info.update(other_worker.info)\n                running_tasks.append(more_info)\n        if task.status == PENDING and worker in task.workers:\n            locally_pending_tasks += 1\n            if len(task.workers) == 1:\n                n_unique_pending += 1\n        if task.status == RUNNING and task.worker_running in greedy_workers:\n            greedy_workers[task.worker_running] -= 1\n            for (resource, amount) in six.iteritems(task.resources or {}):\n                greedy_resources[resource] += amount\n        if not best_task and self._schedulable(task) and self._has_resources(task.resources, greedy_resources):\n            if worker in task.workers and self._has_resources(task.resources, used_resources):\n                best_task = task\n                best_task_id = task.id\n            else:\n                for task_worker in task.workers:\n                    if greedy_workers.get(task_worker, 0) > 0:\n                        greedy_workers[task_worker] -= 1\n                        for (resource, amount) in six.iteritems(task.resources or {}):\n                            greedy_resources[resource] += amount\n                        break\n    if best_task:\n        self._state.set_status(best_task, RUNNING, self._config)\n        best_task.worker_running = worker\n        best_task.time_running = time.time()\n        self._update_task_history(best_task.id, RUNNING, host=host)\n    return {'n_pending_tasks': locally_pending_tasks, 'n_unique_pending': n_unique_pending, 'task_id': best_task_id, 'running_tasks': running_tasks}",
          "function_before_start_line": 601,
          "function_before_end_line": 676,
          "function_after_start_line": 603,
          "function_after_end_line": 678,
          "function_before_token_count": 419,
          "function_after_token_count": 421,
          "functions_name_modified_file": [
            "add_failure",
            "get_worker_ids",
            "prune",
            "_schedulable",
            "clear",
            "has_excessive_failures",
            "_upstream_status",
            "_update_task_history",
            "can_disable",
            "ping",
            "get_pending_tasks",
            "_traverse_inverse_deps",
            "load",
            "inverse_dependencies",
            "num_failures",
            "task_search",
            "_recurse_deps",
            "get_worker",
            "get_active_workers",
            "worker_list",
            "add_task",
            "update_resources",
            "_update_priority",
            "task_list",
            "add_info",
            "get_active_tasks",
            "_has_resources",
            "_used_resources",
            "has_task",
            "fix_time",
            "_serialize_task",
            "task_history",
            "dep_graph",
            "__str__",
            "graph",
            "_get_default",
            "dump",
            "__repr__",
            "re_enable",
            "inactivate_tasks",
            "__init__",
            "update",
            "set_status",
            "add_worker",
            "get_task",
            "get_running_tasks",
            "_rank",
            "get_work",
            "re_enable_task",
            "fetch_error",
            "inactivate_workers"
          ],
          "functions_name_all_files": [
            "pipe_writer",
            "get_scala_jars",
            "_get_jars",
            "pig_properties",
            "id_to_name_and_params",
            "tmppath",
            "_list_existing",
            "mkdir",
            "_get_value",
            "test_put_multipart_empty_file",
            "task_started",
            "get_provided_jars",
            "get_all_params",
            "_add_task_event",
            "fix_time",
            "pig_script_path",
            "move",
            "parser_dest",
            "_run_task",
            "run_job",
            "setUp",
            "task_scheduled",
            "_constrain_glob",
            "stop",
            "task_module",
            "init_combiner",
            "pig_command_path",
            "connect",
            "test_init_with_environment_variables",
            "_recurse_deps",
            "get_configured_hdfs_client",
            "rename_dont_move",
            "update_resources",
            "_update_priority",
            "requires",
            "rows",
            "externalize",
            "_build_pig_cmd",
            "run",
            "fs",
            "is_dir",
            "_get_per_location_glob",
            "create_packages_archive",
            "to_str_params",
            "create_marker_table",
            "_flush_buffer",
            "move_dir",
            "_reduce_input",
            "get_configured_hadoop_version",
            "_flush_batch_incr_counter",
            "get_param_values",
            "_schedulable",
            "chmod",
            "_requires",
            "test_exists",
            "_email_unexpected_error",
            "has_value",
            "_validate_dependency",
            "get_default_client",
            "get_active_tasks",
            "list_path",
            "on_success",
            "add_to_cmdline_parser",
            "mapper",
            "get_scalding_core",
            "serialize",
            "_is_root",
            "graph",
            "init_hadoop",
            "__call__",
            "deps",
            "_setup_links",
            "prefix_search",
            "_update_task_history",
            "job_class",
            "incr_counter",
            "table_location",
            "get_active_workers",
            "has_task",
            "get_task_cls",
            "touch",
            "dep_graph",
            "put",
            "clone",
            "update",
            "get_scalding_jars",
            "abort",
            "call_check",
            "test_is_dir",
            "job_args",
            "put_multipart",
            "listdir",
            "load",
            "inverse_dependencies",
            "_add_path_delimiter",
            "_has_resources",
            "_serialize_task",
            "on_failure",
            "glob_exists",
            "from_str_params",
            "test_put",
            "test_put_multipart_less_than_split_size",
            "atomic_output",
            "clear",
            "_incr_counter",
            "getmerge",
            "jar",
            "test_read_iterator",
            "group",
            "test_put_string",
            "_session",
            "__str__",
            "source",
            "extra_modules",
            "add_worker",
            "_emit_metrics",
            "apply_async",
            "disable_instance_cache",
            "process_resources",
            "_generate_worker_info",
            "find_all_by_parameters",
            "_is_writable",
            "test_put_multipart_multiple_parts_exact_fit",
            "writer",
            "worker_list",
            "__hash__",
            "moving_start",
            "reader",
            "trigger_event",
            "__repr__",
            "fetch_task_failures",
            "get_task",
            "_get_s3_config",
            "test_remove",
            "num_failures",
            "pickle_reader",
            "get_key",
            "moving_stop",
            "__iter__",
            "load_hive_cmd",
            "hiverc",
            "exists",
            "close",
            "table_exists",
            "get",
            "_input_iterator",
            "hdfs_reader",
            "chown",
            "parse",
            "pickle_writer",
            "sample",
            "output",
            "task_finished",
            "reducer",
            "can_disable",
            "get_params",
            "list",
            "check_complete",
            "hdfs_writer",
            "add_info",
            "finite_datetimes",
            "path",
            "requires_local",
            "__new__",
            "pig_env_vars",
            "missing_datetimes",
            "__init__",
            "internal_writer",
            "_map_input",
            "set_global",
            "_run_multipart_test",
            "_log_unexpected_error",
            "_log_remote_tasks",
            "graph_url",
            "open",
            "run_reducer",
            "_add_worker",
            "hiveconfs",
            "pig_options",
            "test_write_cleanup_no_close",
            "attach",
            "parse_from_input",
            "task_family",
            "init_local",
            "find_task_by_id",
            "test_init_with_config",
            "finish",
            "_parseIso8601",
            "dereference",
            "input",
            "__enter__",
            "get_libjars",
            "add_link",
            "partition_spec",
            "get_reg",
            "_fetch_json",
            "most_common",
            "is_writable",
            "complete",
            "run_hive_script",
            "kill_job",
            "_format_range",
            "extra_files",
            "internal_reader",
            "remove",
            "get_worker",
            "task_list",
            "_existing_partitions",
            "rename",
            "parameter_to_datetime",
            "put_string",
            "relpath",
            "_setup_remote",
            "_replacer",
            "set_status",
            "get_extra_files",
            "_validate_task",
            "prepare_outputs",
            "get_hive_syntax",
            "set_global_from_args",
            "get_work",
            "add_failure",
            "test_read_no_file",
            "has_task_value",
            "add_task",
            "reset_global",
            "table_schema",
            "_apply_regex",
            "test_write_cleanup_with_error",
            "parse_from_args",
            "track_and_progress",
            "_get_work",
            "bulk_complete",
            "datetime_to_parameter",
            "map_column",
            "test_read",
            "extra_jars",
            "inactivate_workers",
            "prune",
            "tearDown",
            "run_hive_cmd",
            "_upstream_status",
            "create_hadoopcli_client",
            "_email_complete_error",
            "ping",
            "_traverse_inverse_deps",
            "_parseSimple",
            "_add_to_buffer",
            "test_del",
            "infer_bulk_complete_from_fs",
            "load_hadoop_cmd",
            "get_autoconfig_client",
            "initialized",
            "namespace",
            "_keep_alive",
            "clear_instance_cache",
            "count",
            "value",
            "test_put_multipart_multiple_parts_non_exact_fit",
            "tasks_str",
            "_log_complete_error",
            "get_build_dir",
            "find_latest_runs",
            "_find_or_create_task",
            "_get_task",
            "has_excessive_failures",
            "build_job_jar",
            "read",
            "test_close",
            "_path_to_bucket_and_key",
            "run_combiner",
            "task_history",
            "get_job_class",
            "run_and_track_hadoop_job",
            "init_reducer",
            "_handle_next_task",
            "input_local",
            "find_all_by_name",
            "test_gzip",
            "test_get_key",
            "__eq__",
            "re_enable",
            "dump",
            "inactivate_tasks",
            "get_tmp_job_jar",
            "pig_parameters",
            "_check_complete_value",
            "_build_results",
            "copy",
            "getpaths",
            "_rank",
            "run_hive",
            "run_mapper",
            "flatten_output",
            "get_worker_ids",
            "_get_filesystems_and_globs",
            "job_runner",
            "_purge_children",
            "get_pending_tasks",
            "status_search",
            "args",
            "task_search",
            "pig_home",
            "get_bite",
            "task_value",
            "flatten",
            "event_handler",
            "_used_resources",
            "add",
            "query",
            "_get_value_from_config",
            "serialize_to_input",
            "init_mapper",
            "jobconfs",
            "_sleeper",
            "_get_default",
            "_format_datetime",
            "test_gzip_works_and_cleans_up",
            "input_hadoop",
            "__del__",
            "get_running_tasks",
            "requires_hadoop",
            "_add",
            "re_enable_task",
            "fetch_error",
            "__exit__"
          ],
          "functions_name_co_evolved_modified_file": [
            "load",
            "_has_resources",
            "_used_resources"
          ],
          "functions_name_co_evolved_all_files": [
            "_purge_children",
            "find_all_by_parameters",
            "load",
            "args",
            "flatten",
            "__exit__",
            "get_all_params",
            "_has_resources",
            "_used_resources",
            "_build_pig_cmd",
            "run",
            "_apply_regex",
            "partition_spec",
            "track_and_progress",
            "run_job",
            "clone",
            "to_str_params",
            "most_common",
            "setUp",
            "__init__",
            "__call__",
            "map_column",
            "copy",
            "_constrain_glob",
            "getpaths",
            "_flush_batch_incr_counter",
            "_get_s3_config",
            "_find_or_create_task"
          ]
        },
        "file": {
          "file_name": "scheduler.py",
          "file_nloc": 625,
          "file_complexity": 228,
          "file_token_count": 4754,
          "file_before": "# -*- coding: utf-8 -*-\n#\n# Copyright 2012-2015 Spotify AB\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport collections\ntry:\n    import cPickle as pickle\nexcept ImportError:\n    import pickle\nimport datetime\nimport functools\nimport itertools\nimport logging\nimport os\nimport time\n\nfrom luigi import configuration\nfrom luigi import notifications\nfrom luigi import parameter\nfrom luigi import task_history as history\nfrom luigi.task_status import DISABLED, DONE, FAILED, PENDING, RUNNING, SUSPENDED, UNKNOWN\nfrom luigi.task import Config\n\nlogger = logging.getLogger(\"luigi.server\")\n\n\nclass Scheduler(object):\n    \"\"\"\n    Abstract base class.\n\n    Note that the methods all take string arguments, not Task objects...\n    \"\"\"\"\"\n    add_task = NotImplemented\n    get_work = NotImplemented\n    ping = NotImplemented\n\nUPSTREAM_RUNNING = 'UPSTREAM_RUNNING'\nUPSTREAM_MISSING_INPUT = 'UPSTREAM_MISSING_INPUT'\nUPSTREAM_FAILED = 'UPSTREAM_FAILED'\nUPSTREAM_DISABLED = 'UPSTREAM_DISABLED'\n\nUPSTREAM_SEVERITY_ORDER = (\n    '',\n    UPSTREAM_RUNNING,\n    UPSTREAM_MISSING_INPUT,\n    UPSTREAM_FAILED,\n    UPSTREAM_DISABLED,\n)\nUPSTREAM_SEVERITY_KEY = UPSTREAM_SEVERITY_ORDER.index\nSTATUS_TO_UPSTREAM_MAP = {\n    FAILED: UPSTREAM_FAILED,\n    RUNNING: UPSTREAM_RUNNING,\n    PENDING: UPSTREAM_MISSING_INPUT,\n    DISABLED: UPSTREAM_DISABLED,\n}\n\n\nclass scheduler(Config):\n    # TODO(erikbern): the config_path is needed for backwards compatilibity. We should drop the compatibility\n    # at some point (in particular this would force users to replace all dashes with underscores in the config)\n    retry_delay = parameter.FloatParameter(default=900.0,\n                                           config_path=dict(section='scheduler', name='retry-delay'))\n    remove_delay = parameter.FloatParameter(default=600.0,\n                                            config_path=dict(section='scheduler', name='remote-delay'))\n    worker_disconnect_delay = parameter.FloatParameter(default=60.0,\n                                                       config_path=dict(section='scheduler', name='worker-disconnect-delay'))\n    state_path = parameter.Parameter(default='/var/lib/luigi-server/state.pickle',\n                                     config_path=dict(section='scheduler', name='state-path'))\n\n    # Jobs are disabled if we see more than disable_failures failures in disable_window seconds.\n    # These disables last for disable_persist seconds.\n    disable_window = parameter.IntParameter(default=3600,\n                                            config_path=dict(section='scheduler', name='disable-window-seconds'))\n    disable_failures = parameter.IntParameter(default=None,\n                                              config_path=dict(section='scheduler', name='disable-num-failures'))\n    disable_persist = parameter.IntParameter(default=86400,\n                                             config_path=dict(section='scheduler', name='disable-persist-seconds'))\n    max_shown_tasks = parameter.IntParameter(default=100000,\n                                             config_path=dict(section='scheduler', name='max-shown-task'))\n\n    record_task_history = parameter.BoolParameter(default=False)\n\n\ndef fix_time(x):\n    # Backwards compatibility for a fix in Dec 2014. Prior to the fix, pickled state might store datetime objects\n    # Let's remove this function soon\n    if isinstance(x, datetime.datetime):\n        return time.mktime(x.timetuple())\n    else:\n        return x\n\n\nclass Failures(object):\n    \"\"\"\n    This class tracks the number of failures in a given time window.\n\n    Failures added are marked with the current timestamp, and this class counts\n    the number of failures in a sliding time window ending at the present.\n    \"\"\"\n\n    def __init__(self, window):\n        \"\"\"\n        Initialize with the given window.\n\n        :param window: how long to track failures for, as a float (number of seconds).\n        \"\"\"\n        self.window = window\n        self.failures = collections.deque()\n\n    def add_failure(self):\n        \"\"\"\n        Add a failure event with the current timestamp.\n        \"\"\"\n        self.failures.append(time.time())\n\n    def num_failures(self):\n        \"\"\"\n        Return the number of failures in the window.\n        \"\"\"\n        min_time = time.time() - self.window\n\n        while self.failures and fix_time(self.failures[0]) < min_time:\n            self.failures.popleft()\n\n        return len(self.failures)\n\n    def clear(self):\n        \"\"\"\n        Clear the failure queue.\n        \"\"\"\n        self.failures.clear()\n\n\ndef _get_default(x, default):\n    if x is not None:\n        return x\n    else:\n        return default\n\n\nclass Task(object):\n\n    def __init__(self, task_id, status, deps, resources=None, priority=0, family='', params=None,\n                 disable_failures=None, disable_window=None):\n        self.id = task_id\n        self.stakeholders = set()  # workers ids that are somehow related to this task (i.e. don't prune while any of these workers are still active)\n        self.workers = set()  # workers ids that can perform task - task is 'BROKEN' if none of these workers are active\n        if deps is None:\n            self.deps = set()\n        else:\n            self.deps = set(deps)\n        self.status = status  # PENDING, RUNNING, FAILED or DONE\n        self.time = time.time()  # Timestamp when task was first added\n        self.retry = None\n        self.remove = None\n        self.worker_running = None  # the worker id that is currently running the task or None\n        self.time_running = None  # Timestamp when picked up by worker\n        self.expl = None\n        self.priority = priority\n        self.resources = _get_default(resources, {})\n        self.family = family\n        self.params = _get_default(params, {})\n        self.disable_failures = disable_failures\n        self.failures = Failures(disable_window)\n        self.scheduler_disable_time = None\n\n    def __repr__(self):\n        return \"Task(%r)\" % vars(self)\n\n    def add_failure(self):\n        self.failures.add_failure()\n\n    def has_excessive_failures(self):\n        return self.failures.num_failures() >= self.disable_failures\n\n    def can_disable(self):\n        return self.disable_failures is not None\n\n\nclass Worker(object):\n    \"\"\"\n    Structure for tracking worker activity and keeping their references.\n    \"\"\"\n\n    def __init__(self, worker_id, last_active=None):\n        self.id = worker_id\n        self.reference = None  # reference to the worker in the real world. (Currently a dict containing just the host)\n        self.last_active = last_active  # seconds since epoch\n        self.started = time.time()  # seconds since epoch\n        self.info = {}\n\n    def add_info(self, info):\n        self.info.update(info)\n\n    def update(self, worker_reference):\n        if worker_reference:\n            self.reference = worker_reference\n        self.last_active = time.time()\n\n    def prune(self, config):\n        # Delete workers that haven't said anything for a while (probably killed)\n        if self.last_active + config.worker_disconnect_delay < time.time():\n            return True\n\n    def __str__(self):\n        return self.id\n\n\nclass SimpleTaskState(object):\n    \"\"\"\n    Keep track of the current state and handle persistance.\n\n    The point of this class is to enable other ways to keep state, eg. by using a database\n    These will be implemented by creating an abstract base class that this and other classes\n    inherit from.\n    \"\"\"\n\n    def __init__(self, state_path):\n        self._state_path = state_path\n        self._tasks = {}  # map from id to a Task object\n        self._status_tasks = collections.defaultdict(dict)\n        self._active_workers = {}  # map from id to a Worker object\n\n    def dump(self):\n        state = (self._tasks, self._active_workers)\n        try:\n            with open(self._state_path, 'w') as fobj:\n                pickle.dump(state, fobj)\n        except IOError:\n            logger.warning(\"Failed saving scheduler state\", exc_info=1)\n        else:\n            logger.info(\"Saved state in %s\", self._state_path)\n\n    # prone to lead to crashes when old state is unpickled with updated code. TODO some kind of version control?\n    def load(self):\n        if os.path.exists(self._state_path):\n            logger.info(\"Attempting to load state from %s\", self._state_path)\n            try:\n                with open(self._state_path) as fobj:\n                    state = pickle.load(fobj)\n            except BaseException:\n                logger.exception(\"Error when loading state. Starting from clean slate.\")\n                return\n\n            self._tasks, self._active_workers = state\n            self._status_tasks = collections.defaultdict(dict)\n            for task in self._tasks.itervalues():\n                self._status_tasks[task.status][task.id] = task\n\n            # Convert from old format\n            # TODO: this is really ugly, we need something more future-proof\n            # Every time we add an attribute to the Worker class, this code needs to be updated\n            for k, v in self._active_workers.iteritems():\n                if isinstance(v, float):\n                    self._active_workers[k] = Worker(worker_id=k, last_active=v)\n        else:\n            logger.info(\"No prior state file exists at %s. Starting with clean slate\", self._state_path)\n\n    def get_active_tasks(self, status=None):\n        if status:\n            for task in self._status_tasks[status].itervalues():\n                yield task\n        else:\n            for task in self._tasks.itervalues():\n                yield task\n\n    def get_running_tasks(self):\n        return self._status_tasks[RUNNING].itervalues()\n\n    def get_pending_tasks(self):\n        return itertools.chain.from_iterable(self._status_tasks[status].itervalues()\n                                             for status in [PENDING, RUNNING])\n\n    def get_task(self, task_id, default=None, setdefault=None):\n        if setdefault:\n            task = self._tasks.setdefault(task_id, setdefault)\n            self._status_tasks[task.status][task.id] = task\n            return task\n        else:\n            return self._tasks.get(task_id, default)\n\n    def has_task(self, task_id):\n        return task_id in self._tasks\n\n    def re_enable(self, task, config=None):\n        task.scheduler_disable_time = None\n        task.failures.clear()\n        if config:\n            self.set_status(task, FAILED, config)\n            task.failures.clear()\n\n    def set_status(self, task, new_status, config=None):\n        if new_status == FAILED:\n            assert config is not None\n\n        # not sure why we have SUSPENDED, as it can never be set\n        if new_status == SUSPENDED:\n            new_status = PENDING\n\n        if new_status == DISABLED and task.status == RUNNING:\n            return\n\n        if task.status == DISABLED:\n            if new_status == DONE:\n                self.re_enable(task)\n\n            # don't allow workers to override a scheduler disable\n            elif task.scheduler_disable_time is not None:\n                return\n\n        if new_status == FAILED and task.can_disable():\n            task.add_failure()\n            if task.has_excessive_failures():\n                task.scheduler_disable_time = time.time()\n                new_status = DISABLED\n                notifications.send_error_email(\n                    'Luigi Scheduler: DISABLED {task} due to excessive failures'.format(task=task.id),\n                    '{task} failed {failures} times in the last {window} seconds, so it is being '\n                    'disabled for {persist} seconds'.format(\n                        failures=config.disable_failures,\n                        task=task.id,\n                        window=config.disable_window,\n                        persist=config.disable_persist,\n                    ))\n        elif new_status == DISABLED:\n            task.scheduler_disable_time = None\n\n        self._status_tasks[task.status].pop(task.id)\n        self._status_tasks[new_status][task.id] = task\n        task.status = new_status\n\n    def prune(self, task, config):\n        remove = False\n\n        # Mark tasks with no remaining active stakeholders for deletion\n        if not task.stakeholders:\n            if task.remove is None:\n                logger.info(\"Task %r has stakeholders %r but none remain connected -> will remove \"\n                            \"task in %s seconds\", task.id, task.stakeholders, config.remove_delay)\n                task.remove = time.time() + config.remove_delay\n\n        # If a running worker disconnects, tag all its jobs as FAILED and subject it to the same retry logic\n        if task.status == RUNNING and task.worker_running and task.worker_running not in task.stakeholders:\n            logger.info(\"Task %r is marked as running by disconnected worker %r -> marking as \"\n                        \"FAILED with retry delay of %rs\", task.id, task.worker_running,\n                        config.retry_delay)\n            task.worker_running = None\n            self.set_status(task, FAILED, config)\n            task.retry = time.time() + config.retry_delay\n\n        # Re-enable task after the disable time expires\n        if task.status == DISABLED and task.scheduler_disable_time:\n            if time.time() - fix_time(task.scheduler_disable_time) > config.disable_persist:\n                self.re_enable(task, config)\n\n        # Remove tasks that have no stakeholders\n        if task.remove and time.time() > task.remove:\n            logger.info(\"Removing task %r (no connected stakeholders)\", task.id)\n            remove = True\n\n        # Reset FAILED tasks to PENDING if max timeout is reached, and retry delay is >= 0\n        if task.status == FAILED and config.retry_delay >= 0 and task.retry < time.time():\n            self.set_status(task, PENDING, config)\n\n        return remove\n\n    def inactivate_tasks(self, delete_tasks):\n        # The terminology is a bit confusing: we used to \"delete\" tasks when they became inactive,\n        # but with a pluggable state storage, you might very well want to keep some history of\n        # older tasks as well. That's why we call it \"inactivate\" (as in the verb)\n        for task in delete_tasks:\n            task_obj = self._tasks.pop(task)\n            self._status_tasks[task_obj.status].pop(task)\n\n    def get_active_workers(self, last_active_lt=None):\n        for worker in self._active_workers.itervalues():\n            if last_active_lt is not None and worker.last_active >= last_active_lt:\n                continue\n            yield worker\n\n    def get_worker_ids(self):\n        return self._active_workers.keys()  # only used for unit tests\n\n    def get_worker(self, worker_id):\n        return self._active_workers.setdefault(worker_id, Worker(worker_id))\n\n    def inactivate_workers(self, delete_workers):\n        # Mark workers as inactive\n        for worker in delete_workers:\n            self._active_workers.pop(worker)\n\n        # remove workers from tasks\n        for task in self.get_active_tasks():\n            task.stakeholders.difference_update(delete_workers)\n            task.workers.difference_update(delete_workers)\n\n\nclass CentralPlannerScheduler(Scheduler):\n    \"\"\"\n    Async scheduler that can handle multiple workers, etc.\n\n    Can be run locally or on a server (using RemoteScheduler + server.Server).\n    \"\"\"\n\n    def __init__(self, config=None, resources=None, task_history=None, **kwargs):\n        \"\"\"\n        (all arguments are in seconds)\n        Keyword Arguments:\n        :param retry_delay: how long after a Task fails to try it again, or -1 to never retry.\n        :param remove_delay: how long after a Task finishes to remove it from the scheduler.\n        :param state_path: path to state file (tasks and active workers).\n        :param worker_disconnect_delay: if a worker hasn't communicated for this long, remove it from active workers.\n        \"\"\"\n        self._config = config or scheduler(**kwargs)\n        self._state = SimpleTaskState(self._config.state_path)\n        if task_history:\n            self._task_history = task_history\n        elif self._config.record_task_history:\n            import db_task_history  # Needs sqlalchemy, thus imported here\n            self._task_history = db_task_history.DbTaskHistory()\n        else:\n            self._task_history = history.NopHistory()\n        self._resources = resources or configuration.get_config().getintdict('resources')  # TODO: Can we make this a Parameter?\n        self._make_task = functools.partial(\n            Task, disable_failures=self._config.disable_failures,\n            disable_window=self._config.disable_window)\n\n    def load(self):\n        self._state.load()\n\n    def dump(self):\n        self._state.dump()\n\n    def prune(self):\n        logger.info(\"Starting pruning of task graph\")\n        remove_workers = []\n        for worker in self._state.get_active_workers():\n            if worker.prune(self._config):\n                logger.info(\"Worker %s timed out (no contact for >=%ss)\", worker, self._config.worker_disconnect_delay)\n                remove_workers.append(worker.id)\n\n        self._state.inactivate_workers(remove_workers)\n\n        remove_tasks = []\n        for task in self._state.get_active_tasks():\n            if self._state.prune(task, self._config):\n                remove_tasks.append(task.id)\n\n        self._state.inactivate_tasks(remove_tasks)\n\n        logger.info(\"Done pruning task graph\")\n\n    def update(self, worker_id, worker_reference=None):\n        \"\"\"\n        Keep track of whenever the worker was last active.\n        \"\"\"\n        worker = self._state.get_worker(worker_id)\n        worker.update(worker_reference)\n\n    def _update_priority(self, task, prio, worker):\n        \"\"\"\n        Update priority of the given task.\n\n        Priority can only be increased.\n        If the task doesn't exist, a placeholder task is created to preserve priority when the task is later scheduled.\n        \"\"\"\n        task.priority = prio = max(prio, task.priority)\n        for dep in task.deps or []:\n            t = self._state.get_task(dep)\n            if t is not None and prio > t.priority:\n                self._update_priority(t, prio, worker)\n\n    def add_task(self, worker, task_id, status=PENDING, runnable=True,\n                 deps=None, new_deps=None, expl=None, resources=None,\n                 priority=0, family='', params=None, **kwargs):\n        \"\"\"\n        * add task identified by task_id if it doesn't exist\n        * if deps is not None, update dependency list\n        * update status of task\n        * add additional workers/stakeholders\n        * update priority when needed\n        \"\"\"\n        self.update(worker)\n\n        task = self._state.get_task(task_id, setdefault=self._make_task(\n            task_id=task_id, status=PENDING, deps=deps, resources=resources,\n            priority=priority, family=family, params=params))\n\n        # for setting priority, we'll sometimes create tasks with unset family and params\n        if not task.family:\n            task.family = family\n        if not task.params:\n            task.params = _get_default(params, {})\n\n        if task.remove is not None:\n            task.remove = None  # unmark task for removal so it isn't removed after being added\n\n        if not (task.status == RUNNING and status == PENDING):\n            # don't allow re-scheduling of task while it is running, it must either fail or succeed first\n            if status == PENDING or status != task.status:\n                # Update the DB only if there was a acctual change, to prevent noise.\n                # We also check for status == PENDING b/c that's the default value\n                # (so checking for status != task.status woule lie)\n                self._update_task_history(task_id, status)\n            self._state.set_status(task, PENDING if status == SUSPENDED else status, self._config)\n            if status == FAILED:\n                task.retry = time.time() + self._config.retry_delay\n\n        if deps is not None:\n            task.deps = set(deps)\n\n        if new_deps is not None:\n            task.deps.update(new_deps)\n\n        task.stakeholders.add(worker)\n        task.resources = resources\n\n        # Task dependencies might not exist yet. Let's create dummy tasks for them for now.\n        # Otherwise the task dependencies might end up being pruned if scheduling takes a long time\n        for dep in task.deps or []:\n            t = self._state.get_task(dep, setdefault=self._make_task(task_id=dep, status=UNKNOWN, deps=None, priority=priority))\n            t.stakeholders.add(worker)\n\n        self._update_priority(task, priority, worker)\n\n        if runnable:\n            task.workers.add(worker)\n\n        if expl is not None:\n            task.expl = expl\n\n    def add_worker(self, worker, info, **kwargs):\n        self._state.get_worker(worker).add_info(info)\n\n    def update_resources(self, **resources):\n        if self._resources is None:\n            self._resources = {}\n        self._resources.update(resources)\n\n    def _has_resources(self, needed_resources, used_resources):\n        if needed_resources is None:\n            return True\n\n        available_resources = self._resources or {}\n        for resource, amount in needed_resources.iteritems():\n            if amount + used_resources[resource] > available_resources.get(resource, 1):\n                return False\n        return True\n\n    def _used_resources(self):\n        used_resources = collections.defaultdict(int)\n        if self._resources is not None:\n            for task in self._state.get_active_tasks():\n                if task.status == RUNNING and task.resources:\n                    for resource, amount in task.resources.iteritems():\n                        used_resources[resource] += amount\n        return used_resources\n\n    def _rank(self):\n        \"\"\"\n        Return worker's rank function for task scheduling.\n\n        :return:\n        \"\"\"\n        dependents = collections.defaultdict(int)\n\n        def not_done(t):\n            task = self._state.get_task(t, default=None)\n            return task is None or task.status != DONE\n        for task in self._state.get_pending_tasks():\n            if task.status != DONE:\n                deps = filter(not_done, task.deps)\n                inverse_num_deps = 1.0 / max(len(deps), 1)\n                for dep in deps:\n                    dependents[dep] += inverse_num_deps\n\n        return lambda task: (task.priority, dependents[task.id], -task.time)\n\n    def _schedulable(self, task):\n        if task.status != PENDING:\n            return False\n        for dep in task.deps:\n            dep_task = self._state.get_task(dep, default=None)\n            if dep_task is None or dep_task.status != DONE:\n                return False\n        return True\n\n    def get_work(self, worker, host=None, **kwargs):\n        # TODO: remove any expired nodes\n\n        # Algo: iterate over all nodes, find the highest priority node no dependencies and available\n        # resources.\n\n        # Resource checking looks both at currently available resources and at which resources would\n        # be available if all running tasks died and we rescheduled all workers greedily. We do both\n        # checks in order to prevent a worker with many low-priority tasks from starving other\n        # workers with higher priority tasks that share the same resources.\n\n        # TODO: remove tasks that can't be done, figure out if the worker has absolutely\n        # nothing it can wait for\n\n        # Return remaining tasks that have no FAILED descendents\n        self.update(worker, {'host': host})\n        best_task = None\n        best_task_id = None\n        locally_pending_tasks = 0\n        running_tasks = []\n\n        used_resources = self._used_resources()\n        greedy_resources = collections.defaultdict(int)\n        n_unique_pending = 0\n        greedy_workers = dict((worker.id, worker.info.get('workers', 1))\n                              for worker in self._state.get_active_workers())\n\n        tasks = list(self._state.get_pending_tasks())\n        tasks.sort(key=self._rank(), reverse=True)\n\n        for task in tasks:\n            if task.status == 'RUNNING' and worker in task.workers:\n                # Return a list of currently running tasks to the client,\n                # makes it easier to troubleshoot\n                other_worker = self._state.get_worker(task.worker_running)\n                more_info = {'task_id': task.id, 'worker': str(other_worker)}\n                if other_worker is not None:\n                    more_info.update(other_worker.info)\n                    running_tasks.append(more_info)\n\n            if task.status == PENDING and worker in task.workers:\n                locally_pending_tasks += 1\n                if len(task.workers) == 1:\n                    n_unique_pending += 1\n\n            if task.status == RUNNING and task.worker_running in greedy_workers:\n                greedy_workers[task.worker_running] -= 1\n                for resource, amount in (task.resources or {}).iteritems():\n                    greedy_resources[resource] += amount\n\n            if not best_task and self._schedulable(task) and self._has_resources(task.resources, greedy_resources):\n                if worker in task.workers and self._has_resources(task.resources, used_resources):\n                    best_task = task\n                    best_task_id = task.id\n                else:\n                    for task_worker in task.workers:\n                        if greedy_workers.get(task_worker, 0) > 0:\n                            # use up a worker\n                            greedy_workers[task_worker] -= 1\n\n                            # keep track of the resources used in greedy scheduling\n                            for resource, amount in (task.resources or {}).iteritems():\n                                greedy_resources[resource] += amount\n\n                            break\n\n        if best_task:\n            self._state.set_status(best_task, RUNNING, self._config)\n            best_task.worker_running = worker\n            best_task.time_running = time.time()\n            self._update_task_history(best_task.id, RUNNING, host=host)\n\n        return {'n_pending_tasks': locally_pending_tasks,\n                'n_unique_pending': n_unique_pending,\n                'task_id': best_task_id,\n                'running_tasks': running_tasks}\n\n    def ping(self, worker, **kwargs):\n        self.update(worker)\n\n    def _upstream_status(self, task_id, upstream_status_table):\n        if task_id in upstream_status_table:\n            return upstream_status_table[task_id]\n        elif self._state.has_task(task_id):\n            task_stack = [task_id]\n\n            while task_stack:\n                dep_id = task_stack.pop()\n                if self._state.has_task(dep_id):\n                    dep = self._state.get_task(dep_id)\n                    if dep_id not in upstream_status_table:\n                        if dep.status == PENDING and dep.deps:\n                            task_stack = task_stack + [dep_id] + list(dep.deps)\n                            upstream_status_table[dep_id] = ''  # will be updated postorder\n                        else:\n                            dep_status = STATUS_TO_UPSTREAM_MAP.get(dep.status, '')\n                            upstream_status_table[dep_id] = dep_status\n                    elif upstream_status_table[dep_id] == '' and dep.deps:\n                        # This is the postorder update step when we set the\n                        # status based on the previously calculated child elements\n                        upstream_status = [upstream_status_table.get(task_id, '') for task_id in dep.deps]\n                        upstream_status.append('')  # to handle empty list\n                        status = max(upstream_status, key=UPSTREAM_SEVERITY_KEY)\n                        upstream_status_table[dep_id] = status\n            return upstream_status_table[dep_id]\n\n    def _serialize_task(self, task_id, include_deps=True):\n        task = self._state.get_task(task_id)\n        ret = {\n            'deps': list(task.deps),\n            'status': task.status,\n            'workers': list(task.workers),\n            'worker_running': task.worker_running,\n            'time_running': getattr(task, \"time_running\", None),\n            'start_time': task.time,\n            'params': task.params,\n            'name': task.family,\n            'priority': task.priority,\n            'resources': task.resources,\n        }\n        if include_deps:\n            ret['deps'] = list(task.deps)\n        return ret\n\n    def graph(self, **kwargs):\n        self.prune()\n        serialized = {}\n        for task in self._state.get_active_tasks():\n            serialized[task.id] = self._serialize_task(task.id)\n        return serialized\n\n    def _recurse_deps(self, task_id, serialized):\n        if task_id not in serialized:\n            task = self._state.get_task(task_id)\n            if task is None or not task.family:\n                logger.warn('Missing task for id [%s]', task_id)\n\n                # try to infer family and params from task_id\n                try:\n                    family, _, param_str = task_id.rstrip(')').partition('(')\n                    params = dict(param.split('=') for param in param_str.split(', '))\n                except BaseException:\n                    family, params = '', {}\n                serialized[task_id] = {\n                    'deps': [],\n                    'status': UNKNOWN,\n                    'workers': [],\n                    'start_time': UNKNOWN,\n                    'params': params,\n                    'name': family,\n                    'priority': 0,\n                }\n            else:\n                serialized[task_id] = self._serialize_task(task_id)\n                for dep in task.deps:\n                    self._recurse_deps(dep, serialized)\n\n    def dep_graph(self, task_id, **kwargs):\n        self.prune()\n        serialized = {}\n        if self._state.has_task(task_id):\n            self._recurse_deps(task_id, serialized)\n        return serialized\n\n    def task_list(self, status, upstream_status, limit=True, **kwargs):\n        \"\"\"\n        Query for a subset of tasks by status.\n        \"\"\"\n        self.prune()\n        result = {}\n        upstream_status_table = {}  # used to memoize upstream status\n        for task in self._state.get_active_tasks(status):\n            if (task.status != PENDING or not upstream_status or\n                    upstream_status == self._upstream_status(task.id, upstream_status_table)):\n                serialized = self._serialize_task(task.id, False)\n                result[task.id] = serialized\n        if limit and len(result) > self._config.max_shown_tasks:\n            return {'num_tasks': len(result)}\n        return result\n\n    def worker_list(self, include_running=True, **kwargs):\n        self.prune()\n        workers = [\n            dict(\n                name=worker.id,\n                last_active=worker.last_active,\n                started=getattr(worker, 'started', None),\n                **worker.info\n            ) for worker in self._state.get_active_workers()]\n        workers.sort(key=lambda worker: worker['started'], reverse=True)\n        if include_running:\n            running = collections.defaultdict(dict)\n            num_pending = collections.defaultdict(int)\n            num_uniques = collections.defaultdict(int)\n            for task in self._state.get_pending_tasks():\n                if task.status == RUNNING and task.worker_running:\n                    running[task.worker_running][task.id] = self._serialize_task(task.id, False)\n                elif task.status == PENDING:\n                    for worker in task.workers:\n                        num_pending[worker] += 1\n                    if len(task.workers) == 1:\n                        num_uniques[list(task.workers)[0]] += 1\n            for worker in workers:\n                tasks = running[worker['name']]\n                worker['num_running'] = len(tasks)\n                worker['num_pending'] = num_pending[worker['name']]\n                worker['num_uniques'] = num_uniques[worker['name']]\n                worker['running'] = tasks\n        return workers\n\n    def inverse_dependencies(self, task_id, **kwargs):\n        self.prune()\n        serialized = {}\n        if self._state.has_task(task_id):\n            self._traverse_inverse_deps(task_id, serialized)\n        return serialized\n\n    def _traverse_inverse_deps(self, task_id, serialized):\n        stack = [task_id]\n        serialized[task_id] = self._serialize_task(task_id)\n        while len(stack) > 0:\n            curr_id = stack.pop()\n            for task in self._state.get_active_tasks():\n                if curr_id in task.deps:\n                    serialized[curr_id][\"deps\"].append(task.id)\n                    if task.id not in serialized:\n                        serialized[task.id] = self._serialize_task(task.id)\n                        serialized[task.id][\"deps\"] = []\n                        stack.append(task.id)\n\n    def task_search(self, task_str, **kwargs):\n        \"\"\"\n        Query for a subset of tasks by task_id.\n\n        :param task_str:\n        :return:\n        \"\"\"\n        self.prune()\n        result = collections.defaultdict(dict)\n        for task in self._state.get_active_tasks():\n            if task.id.find(task_str) != -1:\n                serialized = self._serialize_task(task.id, False)\n                result[task.status][task.id] = serialized\n        return result\n\n    def re_enable_task(self, task_id):\n        serialized = {}\n        task = self._state.get_task(task_id)\n        if task and task.status == DISABLED and task.scheduler_disable_time:\n            self._state.re_enable(task, self._config)\n            serialized = self._serialize_task(task_id)\n        return serialized\n\n    def fetch_error(self, task_id, **kwargs):\n        if self._state.has_task(task_id):\n            return {\"taskId\": task_id, \"error\": self._state.get_task(task_id).expl}\n        else:\n            return {\"taskId\": task_id, \"error\": \"\"}\n\n    def _update_task_history(self, task_id, status, host=None):\n        try:\n            if status == DONE or status == FAILED:\n                successful = (status == DONE)\n                self._task_history.task_finished(task_id, successful)\n            elif status == PENDING:\n                self._task_history.task_scheduled(task_id)\n            elif status == RUNNING:\n                self._task_history.task_started(task_id, host)\n        except BaseException:\n            logger.warning(\"Error saving Task history\", exc_info=True)\n\n    @property\n    def task_history(self):\n        # Used by server.py to expose the calls\n        return self._task_history\n",
          "file_after": "# -*- coding: utf-8 -*-\n#\n# Copyright 2012-2015 Spotify AB\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport collections\ntry:\n    import cPickle as pickle\nexcept ImportError:\n    import pickle\nimport datetime\nimport functools\nimport itertools\nimport logging\nimport os\nimport time\n\nimport six\n\nfrom luigi import configuration\nfrom luigi import notifications\nfrom luigi import parameter\nfrom luigi import task_history as history\nfrom luigi.task_status import DISABLED, DONE, FAILED, PENDING, RUNNING, SUSPENDED, UNKNOWN\nfrom luigi.task import Config\n\nlogger = logging.getLogger(\"luigi.server\")\n\n\nclass Scheduler(object):\n    \"\"\"\n    Abstract base class.\n\n    Note that the methods all take string arguments, not Task objects...\n    \"\"\"\"\"\n    add_task = NotImplemented\n    get_work = NotImplemented\n    ping = NotImplemented\n\nUPSTREAM_RUNNING = 'UPSTREAM_RUNNING'\nUPSTREAM_MISSING_INPUT = 'UPSTREAM_MISSING_INPUT'\nUPSTREAM_FAILED = 'UPSTREAM_FAILED'\nUPSTREAM_DISABLED = 'UPSTREAM_DISABLED'\n\nUPSTREAM_SEVERITY_ORDER = (\n    '',\n    UPSTREAM_RUNNING,\n    UPSTREAM_MISSING_INPUT,\n    UPSTREAM_FAILED,\n    UPSTREAM_DISABLED,\n)\nUPSTREAM_SEVERITY_KEY = UPSTREAM_SEVERITY_ORDER.index\nSTATUS_TO_UPSTREAM_MAP = {\n    FAILED: UPSTREAM_FAILED,\n    RUNNING: UPSTREAM_RUNNING,\n    PENDING: UPSTREAM_MISSING_INPUT,\n    DISABLED: UPSTREAM_DISABLED,\n}\n\n\nclass scheduler(Config):\n    # TODO(erikbern): the config_path is needed for backwards compatilibity. We should drop the compatibility\n    # at some point (in particular this would force users to replace all dashes with underscores in the config)\n    retry_delay = parameter.FloatParameter(default=900.0,\n                                           config_path=dict(section='scheduler', name='retry-delay'))\n    remove_delay = parameter.FloatParameter(default=600.0,\n                                            config_path=dict(section='scheduler', name='remote-delay'))\n    worker_disconnect_delay = parameter.FloatParameter(default=60.0,\n                                                       config_path=dict(section='scheduler', name='worker-disconnect-delay'))\n    state_path = parameter.Parameter(default='/var/lib/luigi-server/state.pickle',\n                                     config_path=dict(section='scheduler', name='state-path'))\n\n    # Jobs are disabled if we see more than disable_failures failures in disable_window seconds.\n    # These disables last for disable_persist seconds.\n    disable_window = parameter.IntParameter(default=3600,\n                                            config_path=dict(section='scheduler', name='disable-window-seconds'))\n    disable_failures = parameter.IntParameter(default=None,\n                                              config_path=dict(section='scheduler', name='disable-num-failures'))\n    disable_persist = parameter.IntParameter(default=86400,\n                                             config_path=dict(section='scheduler', name='disable-persist-seconds'))\n    max_shown_tasks = parameter.IntParameter(default=100000,\n                                             config_path=dict(section='scheduler', name='max-shown-task'))\n\n    record_task_history = parameter.BoolParameter(default=False)\n\n\ndef fix_time(x):\n    # Backwards compatibility for a fix in Dec 2014. Prior to the fix, pickled state might store datetime objects\n    # Let's remove this function soon\n    if isinstance(x, datetime.datetime):\n        return time.mktime(x.timetuple())\n    else:\n        return x\n\n\nclass Failures(object):\n    \"\"\"\n    This class tracks the number of failures in a given time window.\n\n    Failures added are marked with the current timestamp, and this class counts\n    the number of failures in a sliding time window ending at the present.\n    \"\"\"\n\n    def __init__(self, window):\n        \"\"\"\n        Initialize with the given window.\n\n        :param window: how long to track failures for, as a float (number of seconds).\n        \"\"\"\n        self.window = window\n        self.failures = collections.deque()\n\n    def add_failure(self):\n        \"\"\"\n        Add a failure event with the current timestamp.\n        \"\"\"\n        self.failures.append(time.time())\n\n    def num_failures(self):\n        \"\"\"\n        Return the number of failures in the window.\n        \"\"\"\n        min_time = time.time() - self.window\n\n        while self.failures and fix_time(self.failures[0]) < min_time:\n            self.failures.popleft()\n\n        return len(self.failures)\n\n    def clear(self):\n        \"\"\"\n        Clear the failure queue.\n        \"\"\"\n        self.failures.clear()\n\n\ndef _get_default(x, default):\n    if x is not None:\n        return x\n    else:\n        return default\n\n\nclass Task(object):\n\n    def __init__(self, task_id, status, deps, resources=None, priority=0, family='', params=None,\n                 disable_failures=None, disable_window=None):\n        self.id = task_id\n        self.stakeholders = set()  # workers ids that are somehow related to this task (i.e. don't prune while any of these workers are still active)\n        self.workers = set()  # workers ids that can perform task - task is 'BROKEN' if none of these workers are active\n        if deps is None:\n            self.deps = set()\n        else:\n            self.deps = set(deps)\n        self.status = status  # PENDING, RUNNING, FAILED or DONE\n        self.time = time.time()  # Timestamp when task was first added\n        self.retry = None\n        self.remove = None\n        self.worker_running = None  # the worker id that is currently running the task or None\n        self.time_running = None  # Timestamp when picked up by worker\n        self.expl = None\n        self.priority = priority\n        self.resources = _get_default(resources, {})\n        self.family = family\n        self.params = _get_default(params, {})\n        self.disable_failures = disable_failures\n        self.failures = Failures(disable_window)\n        self.scheduler_disable_time = None\n\n    def __repr__(self):\n        return \"Task(%r)\" % vars(self)\n\n    def add_failure(self):\n        self.failures.add_failure()\n\n    def has_excessive_failures(self):\n        return self.failures.num_failures() >= self.disable_failures\n\n    def can_disable(self):\n        return self.disable_failures is not None\n\n\nclass Worker(object):\n    \"\"\"\n    Structure for tracking worker activity and keeping their references.\n    \"\"\"\n\n    def __init__(self, worker_id, last_active=None):\n        self.id = worker_id\n        self.reference = None  # reference to the worker in the real world. (Currently a dict containing just the host)\n        self.last_active = last_active  # seconds since epoch\n        self.started = time.time()  # seconds since epoch\n        self.info = {}\n\n    def add_info(self, info):\n        self.info.update(info)\n\n    def update(self, worker_reference):\n        if worker_reference:\n            self.reference = worker_reference\n        self.last_active = time.time()\n\n    def prune(self, config):\n        # Delete workers that haven't said anything for a while (probably killed)\n        if self.last_active + config.worker_disconnect_delay < time.time():\n            return True\n\n    def __str__(self):\n        return self.id\n\n\nclass SimpleTaskState(object):\n    \"\"\"\n    Keep track of the current state and handle persistance.\n\n    The point of this class is to enable other ways to keep state, eg. by using a database\n    These will be implemented by creating an abstract base class that this and other classes\n    inherit from.\n    \"\"\"\n\n    def __init__(self, state_path):\n        self._state_path = state_path\n        self._tasks = {}  # map from id to a Task object\n        self._status_tasks = collections.defaultdict(dict)\n        self._active_workers = {}  # map from id to a Worker object\n\n    def dump(self):\n        state = (self._tasks, self._active_workers)\n        try:\n            with open(self._state_path, 'w') as fobj:\n                pickle.dump(state, fobj)\n        except IOError:\n            logger.warning(\"Failed saving scheduler state\", exc_info=1)\n        else:\n            logger.info(\"Saved state in %s\", self._state_path)\n\n    # prone to lead to crashes when old state is unpickled with updated code. TODO some kind of version control?\n    def load(self):\n        if os.path.exists(self._state_path):\n            logger.info(\"Attempting to load state from %s\", self._state_path)\n            try:\n                with open(self._state_path) as fobj:\n                    state = pickle.load(fobj)\n            except BaseException:\n                logger.exception(\"Error when loading state. Starting from clean slate.\")\n                return\n\n            self._tasks, self._active_workers = state\n            self._status_tasks = collections.defaultdict(dict)\n            for task in self._tasks.itervalues():\n                self._status_tasks[task.status][task.id] = task\n\n            # Convert from old format\n            # TODO: this is really ugly, we need something more future-proof\n            # Every time we add an attribute to the Worker class, this code needs to be updated\n            for k, v in six.iteritems(self._active_workers):\n                if isinstance(v, float):\n                    self._active_workers[k] = Worker(worker_id=k, last_active=v)\n        else:\n            logger.info(\"No prior state file exists at %s. Starting with clean slate\", self._state_path)\n\n    def get_active_tasks(self, status=None):\n        if status:\n            for task in self._status_tasks[status].itervalues():\n                yield task\n        else:\n            for task in self._tasks.itervalues():\n                yield task\n\n    def get_running_tasks(self):\n        return self._status_tasks[RUNNING].itervalues()\n\n    def get_pending_tasks(self):\n        return itertools.chain.from_iterable(self._status_tasks[status].itervalues()\n                                             for status in [PENDING, RUNNING])\n\n    def get_task(self, task_id, default=None, setdefault=None):\n        if setdefault:\n            task = self._tasks.setdefault(task_id, setdefault)\n            self._status_tasks[task.status][task.id] = task\n            return task\n        else:\n            return self._tasks.get(task_id, default)\n\n    def has_task(self, task_id):\n        return task_id in self._tasks\n\n    def re_enable(self, task, config=None):\n        task.scheduler_disable_time = None\n        task.failures.clear()\n        if config:\n            self.set_status(task, FAILED, config)\n            task.failures.clear()\n\n    def set_status(self, task, new_status, config=None):\n        if new_status == FAILED:\n            assert config is not None\n\n        # not sure why we have SUSPENDED, as it can never be set\n        if new_status == SUSPENDED:\n            new_status = PENDING\n\n        if new_status == DISABLED and task.status == RUNNING:\n            return\n\n        if task.status == DISABLED:\n            if new_status == DONE:\n                self.re_enable(task)\n\n            # don't allow workers to override a scheduler disable\n            elif task.scheduler_disable_time is not None:\n                return\n\n        if new_status == FAILED and task.can_disable():\n            task.add_failure()\n            if task.has_excessive_failures():\n                task.scheduler_disable_time = time.time()\n                new_status = DISABLED\n                notifications.send_error_email(\n                    'Luigi Scheduler: DISABLED {task} due to excessive failures'.format(task=task.id),\n                    '{task} failed {failures} times in the last {window} seconds, so it is being '\n                    'disabled for {persist} seconds'.format(\n                        failures=config.disable_failures,\n                        task=task.id,\n                        window=config.disable_window,\n                        persist=config.disable_persist,\n                    ))\n        elif new_status == DISABLED:\n            task.scheduler_disable_time = None\n\n        self._status_tasks[task.status].pop(task.id)\n        self._status_tasks[new_status][task.id] = task\n        task.status = new_status\n\n    def prune(self, task, config):\n        remove = False\n\n        # Mark tasks with no remaining active stakeholders for deletion\n        if not task.stakeholders:\n            if task.remove is None:\n                logger.info(\"Task %r has stakeholders %r but none remain connected -> will remove \"\n                            \"task in %s seconds\", task.id, task.stakeholders, config.remove_delay)\n                task.remove = time.time() + config.remove_delay\n\n        # If a running worker disconnects, tag all its jobs as FAILED and subject it to the same retry logic\n        if task.status == RUNNING and task.worker_running and task.worker_running not in task.stakeholders:\n            logger.info(\"Task %r is marked as running by disconnected worker %r -> marking as \"\n                        \"FAILED with retry delay of %rs\", task.id, task.worker_running,\n                        config.retry_delay)\n            task.worker_running = None\n            self.set_status(task, FAILED, config)\n            task.retry = time.time() + config.retry_delay\n\n        # Re-enable task after the disable time expires\n        if task.status == DISABLED and task.scheduler_disable_time:\n            if time.time() - fix_time(task.scheduler_disable_time) > config.disable_persist:\n                self.re_enable(task, config)\n\n        # Remove tasks that have no stakeholders\n        if task.remove and time.time() > task.remove:\n            logger.info(\"Removing task %r (no connected stakeholders)\", task.id)\n            remove = True\n\n        # Reset FAILED tasks to PENDING if max timeout is reached, and retry delay is >= 0\n        if task.status == FAILED and config.retry_delay >= 0 and task.retry < time.time():\n            self.set_status(task, PENDING, config)\n\n        return remove\n\n    def inactivate_tasks(self, delete_tasks):\n        # The terminology is a bit confusing: we used to \"delete\" tasks when they became inactive,\n        # but with a pluggable state storage, you might very well want to keep some history of\n        # older tasks as well. That's why we call it \"inactivate\" (as in the verb)\n        for task in delete_tasks:\n            task_obj = self._tasks.pop(task)\n            self._status_tasks[task_obj.status].pop(task)\n\n    def get_active_workers(self, last_active_lt=None):\n        for worker in self._active_workers.itervalues():\n            if last_active_lt is not None and worker.last_active >= last_active_lt:\n                continue\n            yield worker\n\n    def get_worker_ids(self):\n        return self._active_workers.keys()  # only used for unit tests\n\n    def get_worker(self, worker_id):\n        return self._active_workers.setdefault(worker_id, Worker(worker_id))\n\n    def inactivate_workers(self, delete_workers):\n        # Mark workers as inactive\n        for worker in delete_workers:\n            self._active_workers.pop(worker)\n\n        # remove workers from tasks\n        for task in self.get_active_tasks():\n            task.stakeholders.difference_update(delete_workers)\n            task.workers.difference_update(delete_workers)\n\n\nclass CentralPlannerScheduler(Scheduler):\n    \"\"\"\n    Async scheduler that can handle multiple workers, etc.\n\n    Can be run locally or on a server (using RemoteScheduler + server.Server).\n    \"\"\"\n\n    def __init__(self, config=None, resources=None, task_history=None, **kwargs):\n        \"\"\"\n        (all arguments are in seconds)\n        Keyword Arguments:\n        :param retry_delay: how long after a Task fails to try it again, or -1 to never retry.\n        :param remove_delay: how long after a Task finishes to remove it from the scheduler.\n        :param state_path: path to state file (tasks and active workers).\n        :param worker_disconnect_delay: if a worker hasn't communicated for this long, remove it from active workers.\n        \"\"\"\n        self._config = config or scheduler(**kwargs)\n        self._state = SimpleTaskState(self._config.state_path)\n        if task_history:\n            self._task_history = task_history\n        elif self._config.record_task_history:\n            import db_task_history  # Needs sqlalchemy, thus imported here\n            self._task_history = db_task_history.DbTaskHistory()\n        else:\n            self._task_history = history.NopHistory()\n        self._resources = resources or configuration.get_config().getintdict('resources')  # TODO: Can we make this a Parameter?\n        self._make_task = functools.partial(\n            Task, disable_failures=self._config.disable_failures,\n            disable_window=self._config.disable_window)\n\n    def load(self):\n        self._state.load()\n\n    def dump(self):\n        self._state.dump()\n\n    def prune(self):\n        logger.info(\"Starting pruning of task graph\")\n        remove_workers = []\n        for worker in self._state.get_active_workers():\n            if worker.prune(self._config):\n                logger.info(\"Worker %s timed out (no contact for >=%ss)\", worker, self._config.worker_disconnect_delay)\n                remove_workers.append(worker.id)\n\n        self._state.inactivate_workers(remove_workers)\n\n        remove_tasks = []\n        for task in self._state.get_active_tasks():\n            if self._state.prune(task, self._config):\n                remove_tasks.append(task.id)\n\n        self._state.inactivate_tasks(remove_tasks)\n\n        logger.info(\"Done pruning task graph\")\n\n    def update(self, worker_id, worker_reference=None):\n        \"\"\"\n        Keep track of whenever the worker was last active.\n        \"\"\"\n        worker = self._state.get_worker(worker_id)\n        worker.update(worker_reference)\n\n    def _update_priority(self, task, prio, worker):\n        \"\"\"\n        Update priority of the given task.\n\n        Priority can only be increased.\n        If the task doesn't exist, a placeholder task is created to preserve priority when the task is later scheduled.\n        \"\"\"\n        task.priority = prio = max(prio, task.priority)\n        for dep in task.deps or []:\n            t = self._state.get_task(dep)\n            if t is not None and prio > t.priority:\n                self._update_priority(t, prio, worker)\n\n    def add_task(self, worker, task_id, status=PENDING, runnable=True,\n                 deps=None, new_deps=None, expl=None, resources=None,\n                 priority=0, family='', params=None, **kwargs):\n        \"\"\"\n        * add task identified by task_id if it doesn't exist\n        * if deps is not None, update dependency list\n        * update status of task\n        * add additional workers/stakeholders\n        * update priority when needed\n        \"\"\"\n        self.update(worker)\n\n        task = self._state.get_task(task_id, setdefault=self._make_task(\n            task_id=task_id, status=PENDING, deps=deps, resources=resources,\n            priority=priority, family=family, params=params))\n\n        # for setting priority, we'll sometimes create tasks with unset family and params\n        if not task.family:\n            task.family = family\n        if not task.params:\n            task.params = _get_default(params, {})\n\n        if task.remove is not None:\n            task.remove = None  # unmark task for removal so it isn't removed after being added\n\n        if not (task.status == RUNNING and status == PENDING):\n            # don't allow re-scheduling of task while it is running, it must either fail or succeed first\n            if status == PENDING or status != task.status:\n                # Update the DB only if there was a acctual change, to prevent noise.\n                # We also check for status == PENDING b/c that's the default value\n                # (so checking for status != task.status woule lie)\n                self._update_task_history(task_id, status)\n            self._state.set_status(task, PENDING if status == SUSPENDED else status, self._config)\n            if status == FAILED:\n                task.retry = time.time() + self._config.retry_delay\n\n        if deps is not None:\n            task.deps = set(deps)\n\n        if new_deps is not None:\n            task.deps.update(new_deps)\n\n        task.stakeholders.add(worker)\n        task.resources = resources\n\n        # Task dependencies might not exist yet. Let's create dummy tasks for them for now.\n        # Otherwise the task dependencies might end up being pruned if scheduling takes a long time\n        for dep in task.deps or []:\n            t = self._state.get_task(dep, setdefault=self._make_task(task_id=dep, status=UNKNOWN, deps=None, priority=priority))\n            t.stakeholders.add(worker)\n\n        self._update_priority(task, priority, worker)\n\n        if runnable:\n            task.workers.add(worker)\n\n        if expl is not None:\n            task.expl = expl\n\n    def add_worker(self, worker, info, **kwargs):\n        self._state.get_worker(worker).add_info(info)\n\n    def update_resources(self, **resources):\n        if self._resources is None:\n            self._resources = {}\n        self._resources.update(resources)\n\n    def _has_resources(self, needed_resources, used_resources):\n        if needed_resources is None:\n            return True\n\n        available_resources = self._resources or {}\n        for resource, amount in six.iteritems(needed_resources):\n            if amount + used_resources[resource] > available_resources.get(resource, 1):\n                return False\n        return True\n\n    def _used_resources(self):\n        used_resources = collections.defaultdict(int)\n        if self._resources is not None:\n            for task in self._state.get_active_tasks():\n                if task.status == RUNNING and task.resources:\n                    for resource, amount in six.iteritems(task.resources):\n                        used_resources[resource] += amount\n        return used_resources\n\n    def _rank(self):\n        \"\"\"\n        Return worker's rank function for task scheduling.\n\n        :return:\n        \"\"\"\n        dependents = collections.defaultdict(int)\n\n        def not_done(t):\n            task = self._state.get_task(t, default=None)\n            return task is None or task.status != DONE\n        for task in self._state.get_pending_tasks():\n            if task.status != DONE:\n                deps = filter(not_done, task.deps)\n                inverse_num_deps = 1.0 / max(len(deps), 1)\n                for dep in deps:\n                    dependents[dep] += inverse_num_deps\n\n        return lambda task: (task.priority, dependents[task.id], -task.time)\n\n    def _schedulable(self, task):\n        if task.status != PENDING:\n            return False\n        for dep in task.deps:\n            dep_task = self._state.get_task(dep, default=None)\n            if dep_task is None or dep_task.status != DONE:\n                return False\n        return True\n\n    def get_work(self, worker, host=None, **kwargs):\n        # TODO: remove any expired nodes\n\n        # Algo: iterate over all nodes, find the highest priority node no dependencies and available\n        # resources.\n\n        # Resource checking looks both at currently available resources and at which resources would\n        # be available if all running tasks died and we rescheduled all workers greedily. We do both\n        # checks in order to prevent a worker with many low-priority tasks from starving other\n        # workers with higher priority tasks that share the same resources.\n\n        # TODO: remove tasks that can't be done, figure out if the worker has absolutely\n        # nothing it can wait for\n\n        # Return remaining tasks that have no FAILED descendents\n        self.update(worker, {'host': host})\n        best_task = None\n        best_task_id = None\n        locally_pending_tasks = 0\n        running_tasks = []\n\n        used_resources = self._used_resources()\n        greedy_resources = collections.defaultdict(int)\n        n_unique_pending = 0\n        greedy_workers = dict((worker.id, worker.info.get('workers', 1))\n                              for worker in self._state.get_active_workers())\n\n        tasks = list(self._state.get_pending_tasks())\n        tasks.sort(key=self._rank(), reverse=True)\n\n        for task in tasks:\n            if task.status == 'RUNNING' and worker in task.workers:\n                # Return a list of currently running tasks to the client,\n                # makes it easier to troubleshoot\n                other_worker = self._state.get_worker(task.worker_running)\n                more_info = {'task_id': task.id, 'worker': str(other_worker)}\n                if other_worker is not None:\n                    more_info.update(other_worker.info)\n                    running_tasks.append(more_info)\n\n            if task.status == PENDING and worker in task.workers:\n                locally_pending_tasks += 1\n                if len(task.workers) == 1:\n                    n_unique_pending += 1\n\n            if task.status == RUNNING and task.worker_running in greedy_workers:\n                greedy_workers[task.worker_running] -= 1\n                for resource, amount in six.iteritems((task.resources or {})):\n                    greedy_resources[resource] += amount\n\n            if not best_task and self._schedulable(task) and self._has_resources(task.resources, greedy_resources):\n                if worker in task.workers and self._has_resources(task.resources, used_resources):\n                    best_task = task\n                    best_task_id = task.id\n                else:\n                    for task_worker in task.workers:\n                        if greedy_workers.get(task_worker, 0) > 0:\n                            # use up a worker\n                            greedy_workers[task_worker] -= 1\n\n                            # keep track of the resources used in greedy scheduling\n                            for resource, amount in six.iteritems((task.resources or {})):\n                                greedy_resources[resource] += amount\n\n                            break\n\n        if best_task:\n            self._state.set_status(best_task, RUNNING, self._config)\n            best_task.worker_running = worker\n            best_task.time_running = time.time()\n            self._update_task_history(best_task.id, RUNNING, host=host)\n\n        return {'n_pending_tasks': locally_pending_tasks,\n                'n_unique_pending': n_unique_pending,\n                'task_id': best_task_id,\n                'running_tasks': running_tasks}\n\n    def ping(self, worker, **kwargs):\n        self.update(worker)\n\n    def _upstream_status(self, task_id, upstream_status_table):\n        if task_id in upstream_status_table:\n            return upstream_status_table[task_id]\n        elif self._state.has_task(task_id):\n            task_stack = [task_id]\n\n            while task_stack:\n                dep_id = task_stack.pop()\n                if self._state.has_task(dep_id):\n                    dep = self._state.get_task(dep_id)\n                    if dep_id not in upstream_status_table:\n                        if dep.status == PENDING and dep.deps:\n                            task_stack = task_stack + [dep_id] + list(dep.deps)\n                            upstream_status_table[dep_id] = ''  # will be updated postorder\n                        else:\n                            dep_status = STATUS_TO_UPSTREAM_MAP.get(dep.status, '')\n                            upstream_status_table[dep_id] = dep_status\n                    elif upstream_status_table[dep_id] == '' and dep.deps:\n                        # This is the postorder update step when we set the\n                        # status based on the previously calculated child elements\n                        upstream_status = [upstream_status_table.get(task_id, '') for task_id in dep.deps]\n                        upstream_status.append('')  # to handle empty list\n                        status = max(upstream_status, key=UPSTREAM_SEVERITY_KEY)\n                        upstream_status_table[dep_id] = status\n            return upstream_status_table[dep_id]\n\n    def _serialize_task(self, task_id, include_deps=True):\n        task = self._state.get_task(task_id)\n        ret = {\n            'deps': list(task.deps),\n            'status': task.status,\n            'workers': list(task.workers),\n            'worker_running': task.worker_running,\n            'time_running': getattr(task, \"time_running\", None),\n            'start_time': task.time,\n            'params': task.params,\n            'name': task.family,\n            'priority': task.priority,\n            'resources': task.resources,\n        }\n        if include_deps:\n            ret['deps'] = list(task.deps)\n        return ret\n\n    def graph(self, **kwargs):\n        self.prune()\n        serialized = {}\n        for task in self._state.get_active_tasks():\n            serialized[task.id] = self._serialize_task(task.id)\n        return serialized\n\n    def _recurse_deps(self, task_id, serialized):\n        if task_id not in serialized:\n            task = self._state.get_task(task_id)\n            if task is None or not task.family:\n                logger.warn('Missing task for id [%s]', task_id)\n\n                # try to infer family and params from task_id\n                try:\n                    family, _, param_str = task_id.rstrip(')').partition('(')\n                    params = dict(param.split('=') for param in param_str.split(', '))\n                except BaseException:\n                    family, params = '', {}\n                serialized[task_id] = {\n                    'deps': [],\n                    'status': UNKNOWN,\n                    'workers': [],\n                    'start_time': UNKNOWN,\n                    'params': params,\n                    'name': family,\n                    'priority': 0,\n                }\n            else:\n                serialized[task_id] = self._serialize_task(task_id)\n                for dep in task.deps:\n                    self._recurse_deps(dep, serialized)\n\n    def dep_graph(self, task_id, **kwargs):\n        self.prune()\n        serialized = {}\n        if self._state.has_task(task_id):\n            self._recurse_deps(task_id, serialized)\n        return serialized\n\n    def task_list(self, status, upstream_status, limit=True, **kwargs):\n        \"\"\"\n        Query for a subset of tasks by status.\n        \"\"\"\n        self.prune()\n        result = {}\n        upstream_status_table = {}  # used to memoize upstream status\n        for task in self._state.get_active_tasks(status):\n            if (task.status != PENDING or not upstream_status or\n                    upstream_status == self._upstream_status(task.id, upstream_status_table)):\n                serialized = self._serialize_task(task.id, False)\n                result[task.id] = serialized\n        if limit and len(result) > self._config.max_shown_tasks:\n            return {'num_tasks': len(result)}\n        return result\n\n    def worker_list(self, include_running=True, **kwargs):\n        self.prune()\n        workers = [\n            dict(\n                name=worker.id,\n                last_active=worker.last_active,\n                started=getattr(worker, 'started', None),\n                **worker.info\n            ) for worker in self._state.get_active_workers()]\n        workers.sort(key=lambda worker: worker['started'], reverse=True)\n        if include_running:\n            running = collections.defaultdict(dict)\n            num_pending = collections.defaultdict(int)\n            num_uniques = collections.defaultdict(int)\n            for task in self._state.get_pending_tasks():\n                if task.status == RUNNING and task.worker_running:\n                    running[task.worker_running][task.id] = self._serialize_task(task.id, False)\n                elif task.status == PENDING:\n                    for worker in task.workers:\n                        num_pending[worker] += 1\n                    if len(task.workers) == 1:\n                        num_uniques[list(task.workers)[0]] += 1\n            for worker in workers:\n                tasks = running[worker['name']]\n                worker['num_running'] = len(tasks)\n                worker['num_pending'] = num_pending[worker['name']]\n                worker['num_uniques'] = num_uniques[worker['name']]\n                worker['running'] = tasks\n        return workers\n\n    def inverse_dependencies(self, task_id, **kwargs):\n        self.prune()\n        serialized = {}\n        if self._state.has_task(task_id):\n            self._traverse_inverse_deps(task_id, serialized)\n        return serialized\n\n    def _traverse_inverse_deps(self, task_id, serialized):\n        stack = [task_id]\n        serialized[task_id] = self._serialize_task(task_id)\n        while len(stack) > 0:\n            curr_id = stack.pop()\n            for task in self._state.get_active_tasks():\n                if curr_id in task.deps:\n                    serialized[curr_id][\"deps\"].append(task.id)\n                    if task.id not in serialized:\n                        serialized[task.id] = self._serialize_task(task.id)\n                        serialized[task.id][\"deps\"] = []\n                        stack.append(task.id)\n\n    def task_search(self, task_str, **kwargs):\n        \"\"\"\n        Query for a subset of tasks by task_id.\n\n        :param task_str:\n        :return:\n        \"\"\"\n        self.prune()\n        result = collections.defaultdict(dict)\n        for task in self._state.get_active_tasks():\n            if task.id.find(task_str) != -1:\n                serialized = self._serialize_task(task.id, False)\n                result[task.status][task.id] = serialized\n        return result\n\n    def re_enable_task(self, task_id):\n        serialized = {}\n        task = self._state.get_task(task_id)\n        if task and task.status == DISABLED and task.scheduler_disable_time:\n            self._state.re_enable(task, self._config)\n            serialized = self._serialize_task(task_id)\n        return serialized\n\n    def fetch_error(self, task_id, **kwargs):\n        if self._state.has_task(task_id):\n            return {\"taskId\": task_id, \"error\": self._state.get_task(task_id).expl}\n        else:\n            return {\"taskId\": task_id, \"error\": \"\"}\n\n    def _update_task_history(self, task_id, status, host=None):\n        try:\n            if status == DONE or status == FAILED:\n                successful = (status == DONE)\n                self._task_history.task_finished(task_id, successful)\n            elif status == PENDING:\n                self._task_history.task_scheduled(task_id)\n            elif status == RUNNING:\n                self._task_history.task_started(task_id, host)\n        except BaseException:\n            logger.warning(\"Error saving Task history\", exc_info=True)\n\n    @property\n    def task_history(self):\n        # Used by server.py to expose the calls\n        return self._task_history\n",
          "file_patch": "@@ -27,6 +27,8 @@ import logging\n import os\n import time\n \n+import six\n+\n from luigi import configuration\n from luigi import notifications\n from luigi import parameter\n@@ -263,7 +265,7 @@ class SimpleTaskState(object):\n             # Convert from old format\n             # TODO: this is really ugly, we need something more future-proof\n             # Every time we add an attribute to the Worker class, this code needs to be updated\n-            for k, v in self._active_workers.iteritems():\n+            for k, v in six.iteritems(self._active_workers):\n                 if isinstance(v, float):\n                     self._active_workers[k] = Worker(worker_id=k, last_active=v)\n         else:\n@@ -555,7 +557,7 @@ class CentralPlannerScheduler(Scheduler):\n             return True\n \n         available_resources = self._resources or {}\n-        for resource, amount in needed_resources.iteritems():\n+        for resource, amount in six.iteritems(needed_resources):\n             if amount + used_resources[resource] > available_resources.get(resource, 1):\n                 return False\n         return True\n@@ -565,7 +567,7 @@ class CentralPlannerScheduler(Scheduler):\n         if self._resources is not None:\n             for task in self._state.get_active_tasks():\n                 if task.status == RUNNING and task.resources:\n-                    for resource, amount in task.resources.iteritems():\n+                    for resource, amount in six.iteritems(task.resources):\n                         used_resources[resource] += amount\n         return used_resources\n \n@@ -645,7 +647,7 @@ class CentralPlannerScheduler(Scheduler):\n \n             if task.status == RUNNING and task.worker_running in greedy_workers:\n                 greedy_workers[task.worker_running] -= 1\n-                for resource, amount in (task.resources or {}).iteritems():\n+                for resource, amount in six.iteritems((task.resources or {})):\n                     greedy_resources[resource] += amount\n \n             if not best_task and self._schedulable(task) and self._has_resources(task.resources, greedy_resources):\n@@ -659,7 +661,7 @@ class CentralPlannerScheduler(Scheduler):\n                             greedy_workers[task_worker] -= 1\n \n                             # keep track of the resources used in greedy scheduling\n-                            for resource, amount in (task.resources or {}).iteritems():\n+                            for resource, amount in six.iteritems((task.resources or {})):\n                                 greedy_resources[resource] += amount\n \n                             break\n",
          "files_name_in_blame_commit": [
            "hive.py",
            "hadoop.py",
            "postgres.py",
            "pig.py",
            "task.py",
            "db_task_history.py",
            "parameter.py",
            "hdfs.py",
            "luigi-grep.py",
            "ssh_remote_execution.py",
            "helpers.py",
            "scheduler.py",
            "wordcount.py",
            "scalding.py",
            "_s3_test.py",
            "top_artists.py",
            "worker.py",
            "s3.py",
            "range.py"
          ]
        }
      },
      "a5bc6c937c8f02ae0d39c3aac235c9bf7c55d33e": {
        "commit": {
          "commit_id": "a5bc6c937c8f02ae0d39c3aac235c9bf7c55d33e",
          "commit_message": "Keeps track of tasks in scheduler grouped by status\n\nI've been seeing half-second get_work runtimes recently on my scheduler. The\ncause of this has been a large backfill that scheduled a lot of done tasks. With\nabout 100k done tasks and about 350 pending tasks, a lot of time is wasted\nlooping over done tasks just to ignore them. This essentially caches these\ncomputations by keeping a dictionary to group tasks by status and updating it\nwhenever task statuses are changed. This has sped my scheduling up by an order\nof magnitude, now taking about 20ms per get_work.\n\nIn order to ensure updates happen every time, the task state has to take over\nany functions that modify status. This is a necessary for remote task storage\nanyway, so it had to be done eventually. There are no new tests here because\nthere are no functionality changes, only improvements in efficiency and\ndifferent internal representations.",
          "commit_author": "Dave Buchfuhrer",
          "commit_date": "2015-01-08 21:26:10",
          "commit_parent": "5f21334ae80549b1677b01540c1404cd06e1e1fa"
        },
        "function": {
          "function_name": "get_work",
          "function_code_before": "def get_work(self, worker, host=None):\n    self.update(worker, {'host': host})\n    best_task = None\n    best_task_id = None\n    locally_pending_tasks = 0\n    running_tasks = []\n    used_resources = self._used_resources()\n    greedy_resources = collections.defaultdict(int)\n    n_unique_pending = 0\n    greedy_workers = dict(((worker.id, worker.info.get('workers', 1)) for worker in self._state.get_active_workers()))\n    tasks = list(self._state.get_pending_tasks())\n    tasks.sort(key=self._rank(), reverse=True)\n    for task in tasks:\n        if task.status == 'RUNNING' and worker in task.workers:\n            other_worker = self._state.get_worker(task.worker_running)\n            more_info = {'task_id': task.id, 'worker': str(other_worker)}\n            if other_worker is not None:\n                more_info.update(other_worker.info)\n                running_tasks.append(more_info)\n        if task.status == PENDING and worker in task.workers:\n            locally_pending_tasks += 1\n            if len(task.workers) == 1:\n                n_unique_pending += 1\n        if task.status == RUNNING and task.worker_running in greedy_workers:\n            greedy_workers[task.worker_running] -= 1\n            for (resource, amount) in (task.resources or {}).items():\n                greedy_resources[resource] += amount\n        if not best_task and self._schedulable(task) and self._has_resources(task.resources, greedy_resources):\n            if worker in task.workers and self._has_resources(task.resources, used_resources):\n                best_task = task\n                best_task_id = task.id\n            else:\n                for task_worker in task.workers:\n                    if greedy_workers.get(task_worker, 0) > 0:\n                        greedy_workers[task_worker] -= 1\n                        for (resource, amount) in (task.resources or {}).items():\n                            greedy_resources[resource] += amount\n                        break\n    if best_task:\n        best_task.status = RUNNING\n        best_task.worker_running = worker\n        best_task.time_running = time.time()\n        self._update_task_history(best_task.id, RUNNING, host=host)\n    return {'n_pending_tasks': locally_pending_tasks, 'n_unique_pending': n_unique_pending, 'task_id': best_task_id, 'running_tasks': running_tasks}",
          "function_code_after": "def get_work(self, worker, host=None):\n    self.update(worker, {'host': host})\n    best_task = None\n    best_task_id = None\n    locally_pending_tasks = 0\n    running_tasks = []\n    used_resources = self._used_resources()\n    greedy_resources = collections.defaultdict(int)\n    n_unique_pending = 0\n    greedy_workers = dict(((worker.id, worker.info.get('workers', 1)) for worker in self._state.get_active_workers()))\n    tasks = list(self._state.get_pending_tasks())\n    tasks.sort(key=self._rank(), reverse=True)\n    for task in tasks:\n        if task.status == 'RUNNING' and worker in task.workers:\n            other_worker = self._state.get_worker(task.worker_running)\n            more_info = {'task_id': task.id, 'worker': str(other_worker)}\n            if other_worker is not None:\n                more_info.update(other_worker.info)\n                running_tasks.append(more_info)\n        if task.status == PENDING and worker in task.workers:\n            locally_pending_tasks += 1\n            if len(task.workers) == 1:\n                n_unique_pending += 1\n        if task.status == RUNNING and task.worker_running in greedy_workers:\n            greedy_workers[task.worker_running] -= 1\n            for (resource, amount) in (task.resources or {}).items():\n                greedy_resources[resource] += amount\n        if not best_task and self._schedulable(task) and self._has_resources(task.resources, greedy_resources):\n            if worker in task.workers and self._has_resources(task.resources, used_resources):\n                best_task = task\n                best_task_id = task.id\n            else:\n                for task_worker in task.workers:\n                    if greedy_workers.get(task_worker, 0) > 0:\n                        greedy_workers[task_worker] -= 1\n                        for (resource, amount) in (task.resources or {}).items():\n                            greedy_resources[resource] += amount\n                        break\n    if best_task:\n        self._state.set_status(best_task, RUNNING, self._config)\n        best_task.worker_running = worker\n        best_task.time_running = time.time()\n        self._update_task_history(best_task.id, RUNNING, host=host)\n    return {'n_pending_tasks': locally_pending_tasks, 'n_unique_pending': n_unique_pending, 'task_id': best_task_id, 'running_tasks': running_tasks}",
          "function_before_start_line": 529,
          "function_before_end_line": 604,
          "function_after_start_line": 553,
          "function_after_end_line": 628,
          "function_before_token_count": 407,
          "function_after_token_count": 416,
          "functions_name_modified_file": [
            "add_failure",
            "get_worker_ids",
            "prune",
            "_schedulable",
            "clear",
            "has_excessive_failures",
            "_upstream_status",
            "_update_task_history",
            "can_disable",
            "ping",
            "get_pending_tasks",
            "_traverse_inverse_deps",
            "load",
            "inverse_dependencies",
            "num_failures",
            "task_search",
            "_recurse_deps",
            "get_worker",
            "get_active_workers",
            "worker_list",
            "add_task",
            "update_resources",
            "_update_priority",
            "task_list",
            "add_info",
            "get_active_tasks",
            "_has_resources",
            "_used_resources",
            "has_task",
            "fix_time",
            "_serialize_task",
            "task_history",
            "dep_graph",
            "__str__",
            "graph",
            "re_enable",
            "dump",
            "__repr__",
            "inactivate_tasks",
            "__init__",
            "update",
            "set_status",
            "add_worker",
            "get_task",
            "get_running_tasks",
            "_rank",
            "get_work",
            "re_enable_task",
            "fetch_error",
            "inactivate_workers"
          ],
          "functions_name_all_files": [
            "add_failure",
            "get_worker_ids",
            "prune",
            "_schedulable",
            "clear",
            "has_excessive_failures",
            "_upstream_status",
            "_update_task_history",
            "can_disable",
            "ping",
            "get_pending_tasks",
            "_traverse_inverse_deps",
            "load",
            "inverse_dependencies",
            "num_failures",
            "task_search",
            "_recurse_deps",
            "get_worker",
            "get_active_workers",
            "worker_list",
            "add_task",
            "update_resources",
            "_update_priority",
            "task_list",
            "add_info",
            "get_active_tasks",
            "_has_resources",
            "_used_resources",
            "has_task",
            "fix_time",
            "_serialize_task",
            "task_history",
            "dep_graph",
            "__str__",
            "graph",
            "re_enable",
            "dump",
            "__repr__",
            "inactivate_tasks",
            "__init__",
            "update",
            "set_status",
            "add_worker",
            "get_task",
            "get_running_tasks",
            "_rank",
            "get_work",
            "re_enable_task",
            "fetch_error",
            "inactivate_workers"
          ],
          "functions_name_co_evolved_modified_file": [
            "__init__",
            "prune",
            "set_status",
            "get_task",
            "add_task",
            "get_running_tasks",
            "task_list",
            "_rank",
            "get_active_tasks",
            "re_enable_task",
            "get_pending_tasks",
            "load",
            "re_enable",
            "inactivate_tasks"
          ],
          "functions_name_co_evolved_all_files": [
            "__init__",
            "prune",
            "set_status",
            "get_task",
            "add_task",
            "get_running_tasks",
            "task_list",
            "_rank",
            "get_active_tasks",
            "re_enable_task",
            "get_pending_tasks",
            "load",
            "re_enable",
            "inactivate_tasks"
          ]
        },
        "file": {
          "file_name": "scheduler.py",
          "file_nloc": 602,
          "file_complexity": 224,
          "file_token_count": 4496,
          "file_before": "# Copyright (c) 2012 Spotify AB\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"); you may not\n# use this file except in compliance with the License. You may obtain a copy of\n# the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n# License for the specific language governing permissions and limitations under\n# the License.\n\nimport collections\nimport datetime\nimport functools\nimport notifications\nimport os\nimport logging\nimport time\nimport cPickle as pickle\nimport task_history as history\nlogger = logging.getLogger(\"luigi.server\")\n\nfrom task_status import PENDING, FAILED, DONE, RUNNING, SUSPENDED, UNKNOWN, DISABLED\n\n\nclass Scheduler(object):\n    ''' Abstract base class\n\n    Note that the methods all take string arguments, not Task objects...\n    '''\n    add_task = NotImplemented\n    get_work = NotImplemented\n    ping = NotImplemented\n\nUPSTREAM_RUNNING = 'UPSTREAM_RUNNING'\nUPSTREAM_MISSING_INPUT = 'UPSTREAM_MISSING_INPUT'\nUPSTREAM_FAILED = 'UPSTREAM_FAILED'\nUPSTREAM_DISABLED = 'UPSTREAM_DISABLED'\n\nUPSTREAM_SEVERITY_ORDER = (\n    '',\n    UPSTREAM_RUNNING,\n    UPSTREAM_MISSING_INPUT,\n    UPSTREAM_FAILED,\n    UPSTREAM_DISABLED,\n)\nUPSTREAM_SEVERITY_KEY = lambda st: UPSTREAM_SEVERITY_ORDER.index(st)\nSTATUS_TO_UPSTREAM_MAP = {\n    FAILED: UPSTREAM_FAILED,\n    RUNNING: UPSTREAM_RUNNING,\n    PENDING: UPSTREAM_MISSING_INPUT,\n    DISABLED: UPSTREAM_DISABLED,\n}\n\n\n# We're passing around this config a lot, so let's put it on an object\nSchedulerConfig = collections.namedtuple('SchedulerConfig', [\n        'retry_delay', 'remove_delay', 'worker_disconnect_delay',\n        'disable_failures', 'disable_window', 'disable_persist', 'disable_time',\n        'max_shown_tasks',\n])\n\n\ndef fix_time(x):\n    # Backwards compatibility for a fix in Dec 2014. Prior to the fix, pickled state might store datetime objects\n    # Let's remove this function soon\n    if isinstance(x, datetime.datetime):\n        return time.mktime(x.timetuple())\n    else:\n        return x\n\n\nclass Failures(object):\n    \"\"\" This class tracks the number of failures in a given time window\n\n    Failures added are marked with the current timestamp, and this class counts\n    the number of failures in a sliding time window ending at the present.\n\n    \"\"\"\n\n    def __init__(self, window):\n        \"\"\" Initialize with the given window\n\n        :param window: how long to track failures for, as a float (number of seconds)\n        \"\"\"\n        self.window = window\n        self.failures = collections.deque()\n\n    def add_failure(self):\n        \"\"\" Add a failure event with the current timestamp \"\"\"\n        self.failures.append(time.time())\n\n    def num_failures(self):\n        \"\"\" Return the number of failures in the window \"\"\"\n        min_time = time.time() - self.window\n\n        while self.failures and fix_time(self.failures[0]) < min_time:\n            self.failures.popleft()\n\n        return len(self.failures)\n\n    def clear(self):\n        \"\"\" Clear the failure queue \"\"\"\n        self.failures.clear()\n\n\nclass Task(object):\n    def __init__(self, id, status, deps, resources={}, priority=0, family='', params={},\n                 disable_failures=None, disable_window=None):\n        self.id = id\n        self.stakeholders = set()  # workers ids that are somehow related to this task (i.e. don't prune while any of these workers are still active)\n        self.workers = set()  # workers ids that can perform task - task is 'BROKEN' if none of these workers are active\n        if deps is None:\n            self.deps = set()\n        else:\n            self.deps = set(deps)\n        self.status = status  # PENDING, RUNNING, FAILED or DONE\n        self.time = time.time()  # Timestamp when task was first added\n        self.retry = None\n        self.remove = None\n        self.worker_running = None  # the worker id that is currently running the task or None\n        self.time_running = None  # Timestamp when picked up by worker\n        self.expl = None\n        self.priority = priority\n        self.resources = resources\n        self.family = family\n        self.params = params\n        self.disable_failures = disable_failures\n        self.failures = Failures(disable_window)\n        self.scheduler_disable_time = None\n\n    def __repr__(self):\n        return \"Task(%r)\" % vars(self)\n\n    def add_failure(self):\n        self.failures.add_failure()\n\n    def has_excessive_failures(self):\n        return self.failures.num_failures() >= self.disable_failures\n\n    def can_disable(self):\n        return self.disable_failures is not None\n\n    def re_enable(self):\n        self.scheduler_disable_time = None\n        self.status = FAILED\n        self.failures.clear()\n\n    def set_status(self, new_status, config):\n        # not sure why we have SUSPENDED, as it can never be set\n        if new_status == SUSPENDED:\n            new_status = PENDING\n\n        if new_status == DISABLED and self.status == RUNNING:\n            return\n\n        if self.status == DISABLED:\n            if new_status == DONE:\n                self.re_enable()\n\n            # don't allow workers to override a scheduler disable\n            elif self.scheduler_disable_time is not None:\n                return\n\n        if new_status == FAILED and self.can_disable():\n            self.add_failure()\n            if self.has_excessive_failures():\n                self.scheduler_disable_time = time.time()\n                new_status = DISABLED\n                notifications.send_error_email(\n                    'Luigi Scheduler: DISABLED {task} due to excessive failures'.format(task=self.id),\n                    '{task} failed {failures} times in the last {window} seconds, so it is being '\n                    'disabled for {persist} seconds'.format(\n                        failures=config.disable_failures,\n                        task=self.id,\n                        window=config.disable_window,\n                        persist=config.disable_persist,\n                        ))\n        elif new_status == DISABLED:\n            self.scheduler_disable_time = None\n\n        self.status = new_status\n\n    def prune(self, config):\n        remove = False\n\n        # Mark tasks with no remaining active stakeholders for deletion\n        if not self.stakeholders:\n            if self.remove is None:\n                logger.info(\"Task %r has stakeholders %r but none remain connected -> will remove task in %s seconds\", self.id, self.stakeholders, config.remove_delay)\n                self.remove = time.time() + config.remove_delay\n\n        # If a running worker disconnects, tag all its jobs as FAILED and subject it to the same retry logic\n        if self.status == RUNNING and self.worker_running and self.worker_running not in self.stakeholders:\n            logger.info(\"Task %r is marked as running by disconnected worker %r -> marking as FAILED with retry delay of %rs\", self.id, self.worker_running, config.retry_delay)\n            self.worker_running = None\n            self.set_status(FAILED, config)\n            self.retry = time.time() + config.retry_delay\n\n        # Re-enable task after the disable time expires\n        if self.status == DISABLED and self.scheduler_disable_time:\n            if time.time() - fix_time(self.scheduler_disable_time) > config.disable_time:\n                self.re_enable()\n\n        # Remove tasks that have no stakeholders\n        if self.remove and time.time() > self.remove:\n            logger.info(\"Removing task %r (no connected stakeholders)\", self.id)\n            remove = True\n\n        # Reset FAILED tasks to PENDING if max timeout is reached, and retry delay is >= 0\n        if self.status == FAILED and config.retry_delay >= 0 and self.retry < time.time():\n            self.set_status(PENDING, config)\n\n        return remove\n\n\nclass Worker(object):\n    \"\"\" Structure for tracking worker activity and keeping their references \"\"\"\n    def __init__(self, id, last_active=None):\n        self.id = id\n        self.reference = None  # reference to the worker in the real world. (Currently a dict containing just the host)\n        self.last_active = last_active  # seconds since epoch\n        self.started = time.time()  # seconds since epoch\n        self.info = {}\n\n    def add_info(self, info):\n        self.info.update(info)\n\n    def update(self, worker_reference):\n        if worker_reference:\n            self.reference = worker_reference\n        self.last_active = time.time()\n\n    def prune(self, config):\n        # Delete workers that haven't said anything for a while (probably killed)\n        if self.last_active + config.worker_disconnect_delay < time.time():\n            return True\n\n    def __str__(self):\n        return self.id\n\n\nclass SimpleTaskState(object):\n    ''' Keep track of the current state and handle persistance\n\n    The point of this class is to enable other ways to keep state, eg. by using a database\n    These will be implemented by creating an abstract base class that this and other classes\n    inherit from.\n    '''\n\n    def __init__(self, state_path):\n        self._state_path = state_path\n        self._tasks = {}  # map from id to a Task object\n        self._active_workers = {}  # map from id to a Worker object\n\n    def dump(self):\n        state = (self._tasks, self._active_workers)\n        try:\n            with open(self._state_path, 'w') as fobj:\n                pickle.dump(state, fobj)\n        except IOError:\n            logger.warning(\"Failed saving scheduler state\", exc_info=1)\n        else:\n            logger.info(\"Saved state in %s\", self._state_path)\n\n    # prone to lead to crashes when old state is unpickled with updated code. TODO some kind of version control?\n    def load(self):\n        if os.path.exists(self._state_path):\n            logger.info(\"Attempting to load state from %s\", self._state_path)\n            try:\n                with open(self._state_path) as fobj:\n                    state = pickle.load(fobj)\n            except:\n                logger.exception(\"Error when loading state. Starting from clean slate.\")\n                return\n\n            self._tasks, self._active_workers = state\n\n            # Convert from old format\n            # TODO: this is really ugly, we need something more future-proof\n            # Every time we add an attribute to the Worker class, this code needs to be updated\n            for k, v in self._active_workers.iteritems():\n                if isinstance(v, float):\n                    self._active_workers[k] = Worker(id=k, last_active=v)\n        else:\n            logger.info(\"No prior state file exists at %s. Starting with clean slate\", self._state_path)\n\n    def get_active_tasks(self):\n        for task in self._tasks.itervalues():\n            yield task\n\n    def get_pending_tasks(self):\n        for task in self._tasks.itervalues():\n            if task.status in [PENDING, RUNNING]:\n                yield task\n\n    def get_task(self, task_id, default=None, setdefault=None):\n        if setdefault:\n            return self._tasks.setdefault(task_id, setdefault)\n        else:\n            return self._tasks.get(task_id, default)\n\n    def has_task(self, task_id):\n        return task_id in self._tasks\n\n    def inactivate_tasks(self, delete_tasks):\n        # The terminology is a bit confusing: we used to \"delete\" tasks when they became inactive,\n        # but with a pluggable state storage, you might very well want to keep some history of\n        # older tasks as well. That's why we call it \"inactivate\" (as in the verb)\n        for task in delete_tasks:\n            self._tasks.pop(task)\n\n    def get_active_workers(self, last_active_lt=None):\n        for worker in self._active_workers.itervalues():\n            if last_active_lt is not None and worker.last_active >= last_active_lt:\n                continue\n            yield worker\n\n    def get_worker_ids(self):\n        return self._active_workers.keys() # only used for unit tests\n\n    def get_worker(self, worker_id):\n        return self._active_workers.setdefault(worker_id, Worker(worker_id))\n\n    def inactivate_workers(self, delete_workers):\n        # Mark workers as inactive\n        for worker in delete_workers:\n            self._active_workers.pop(worker)\n\n        # remove workers from tasks\n        for task in self.get_active_tasks():\n            task.stakeholders.difference_update(delete_workers)\n            task.workers.difference_update(delete_workers)\n\n\nclass CentralPlannerScheduler(Scheduler):\n    ''' Async scheduler that can handle multiple workers etc\n\n    Can be run locally or on a server (using RemoteScheduler + server.Server).\n    '''\n\n    def __init__(self, retry_delay=900.0, remove_delay=600.0, worker_disconnect_delay=60.0,\n                 state_path='/var/lib/luigi-server/state.pickle', task_history=None,\n                 resources=None, disable_persist=0, disable_window=0, disable_failures=None,\n                 max_shown_tasks=100000):\n        '''\n        (all arguments are in seconds)\n        Keyword Arguments:\n        retry_delay -- How long after a Task fails to try it again, or -1 to never retry\n        remove_delay -- How long after a Task finishes to remove it from the scheduler\n        state_path -- Path to state file (tasks and active workers)\n        worker_disconnect_delay -- If a worker hasn't communicated for this long, remove it from active workers\n        '''\n        self._config = SchedulerConfig(\n            retry_delay=retry_delay,\n            remove_delay=remove_delay,\n            worker_disconnect_delay=worker_disconnect_delay,\n            disable_failures=disable_failures,\n            disable_window=disable_window,\n            disable_persist=disable_persist,\n            disable_time=disable_persist,\n            max_shown_tasks=max_shown_tasks,\n        )\n\n        self._task_history = task_history or history.NopHistory()\n        self._state = SimpleTaskState(state_path)\n\n        self._task_history = task_history or history.NopHistory()\n        self._resources = resources\n        self._make_task = functools.partial(\n            Task, disable_failures=disable_failures,\n            disable_window=disable_window)\n\n    def load(self):\n        self._state.load()\n\n    def dump(self):\n        self._state.dump()\n\n    def prune(self):\n        logger.info(\"Starting pruning of task graph\")\n        remove_workers = []\n        for worker in self._state.get_active_workers():\n            if worker.prune(self._config):\n                logger.info(\"Worker %s timed out (no contact for >=%ss)\", worker, self._config.worker_disconnect_delay)\n                remove_workers.append(worker.id)\n\n        self._state.inactivate_workers(remove_workers)\n\n        remove_tasks = []\n        for task in self._state.get_active_tasks():\n            if task.prune(self._config):\n                remove_tasks.append(task.id)\n\n        self._state.inactivate_tasks(remove_tasks)\n\n        logger.info(\"Done pruning task graph\")\n\n    def update(self, worker_id, worker_reference=None):\n        \"\"\" Keep track of whenever the worker was last active \"\"\"\n        worker = self._state.get_worker(worker_id)\n        worker.update(worker_reference)\n\n    def _update_priority(self, task, prio, worker):\n        \"\"\" Update priority of the given task\n\n        Priority can only be increased. If the task doesn't exist, a placeholder\n        task is created to preserve priority when the task is later scheduled.\n        \"\"\"\n        task.priority = prio = max(prio, task.priority)\n        for dep in task.deps or []:\n            t = self._state.get_task(dep)\n            if t is not None and prio > t.priority:\n                self._update_priority(t, prio, worker)\n\n    def add_task(self, worker, task_id, status=PENDING, runnable=True,\n                 deps=None, new_deps=None, expl=None, resources=None,\n                 priority=0, family='', params={}):\n        \"\"\"\n        * Add task identified by task_id if it doesn't exist\n        * If deps is not None, update dependency list\n        * Update status of task\n        * Add additional workers/stakeholders\n        * Update priority when needed\n        \"\"\"\n        self.update(worker)\n\n        task = self._state.get_task(task_id, setdefault=self._make_task(\n                id=task_id, status=PENDING, deps=deps, resources=resources,\n                priority=priority, family=family, params=params))\n\n        # for setting priority, we'll sometimes create tasks with unset family and params\n        if not task.family:\n            task.family = family\n        if not task.params:\n            task.params = params\n\n        if task.remove is not None:\n            task.remove = None  # unmark task for removal so it isn't removed after being added\n\n        if not (task.status == RUNNING and status == PENDING):\n            # don't allow re-scheduling of task while it is running, it must either fail or succeed first\n            if status == PENDING or status != task.status:\n                # Update the DB only if there was a acctual change, to prevent noise.\n                # We also check for status == PENDING b/c that's the default value\n                # (so checking for status != task.status woule lie)\n                self._update_task_history(task_id, status)\n            task.set_status(PENDING if status == SUSPENDED else status, self._config)\n            if status == FAILED:\n                task.retry = time.time() + self._config.retry_delay\n\n        if deps is not None:\n            task.deps = set(deps)\n\n        if new_deps is not None:\n            task.deps.update(new_deps)\n\n        task.stakeholders.add(worker)\n        task.resources = resources\n\n        # Task dependencies might not exist yet. Let's create dummy tasks for them for now.\n        # Otherwise the task dependencies might end up being pruned if scheduling takes a long time\n        for dep in task.deps or []:\n            t = self._state.get_task(dep, setdefault=self._make_task(id=dep, status=UNKNOWN, deps=None, priority=priority))\n            t.stakeholders.add(worker)\n\n        self._update_priority(task, priority, worker)\n\n        if runnable:\n            task.workers.add(worker)\n\n        if expl is not None:\n            task.expl = expl\n\n    def add_worker(self, worker, info):\n        self._state.get_worker(worker).add_info(info)\n\n    def update_resources(self, **resources):\n        if self._resources is None:\n            self._resources = {}\n        self._resources.update(resources)\n\n    def _has_resources(self, needed_resources, used_resources):\n        if needed_resources is None:\n            return True\n\n        available_resources = self._resources or {}\n        for resource, amount in needed_resources.items():\n            if amount + used_resources[resource] > available_resources.get(resource, 1):\n                return False\n        return True\n\n    def _used_resources(self):\n        used_resources = collections.defaultdict(int)\n        if self._resources is not None:\n            for task in self._state.get_active_tasks():\n                if task.status == RUNNING and task.resources:\n                    for resource, amount in task.resources.items():\n                        used_resources[resource] += amount\n        return used_resources\n\n    def _rank(self):\n        ''' Return worker's rank function for task scheduling '''\n        dependents = collections.defaultdict(int)\n        def not_done(t):\n            task = self._state.get_task(t, default=None)\n            return task is None or task.status != DONE\n        for task in self._state.get_active_tasks():\n            if task.status != DONE:\n                deps = filter(not_done, task.deps)\n                inverse_num_deps = 1.0 / max(len(deps), 1)\n                for dep in deps:\n                    dependents[dep] += inverse_num_deps\n\n        return lambda task: (task.priority, dependents[task.id], -task.time)\n\n    def _schedulable(self, task):\n        if task.status != PENDING:\n            return False\n        for dep in task.deps:\n            dep_task = self._state.get_task(dep, default=None)\n            if dep_task is None or dep_task.status != DONE:\n                return False\n        return True\n\n    def get_work(self, worker, host=None):\n        # TODO: remove any expired nodes\n\n        # Algo: iterate over all nodes, find the highest priority node no dependencies and available\n        # resources.\n\n        # Resource checking looks both at currently available resources and at which resources would\n        # be available if all running tasks died and we rescheduled all workers greedily. We do both\n        # checks in order to prevent a worker with many low-priority tasks from starving other\n        # workers with higher priority tasks that share the same resources.\n\n        # TODO: remove tasks that can't be done, figure out if the worker has absolutely\n        # nothing it can wait for\n\n        # Return remaining tasks that have no FAILED descendents\n        self.update(worker, {'host': host})\n        best_task = None\n        best_task_id = None\n        locally_pending_tasks = 0\n        running_tasks = []\n\n        used_resources = self._used_resources()\n        greedy_resources = collections.defaultdict(int)\n        n_unique_pending = 0\n        greedy_workers = dict((worker.id, worker.info.get('workers', 1))\n                              for worker in self._state.get_active_workers())\n\n        tasks = list(self._state.get_pending_tasks())\n        tasks.sort(key=self._rank(), reverse=True)\n\n        for task in tasks:\n            if task.status == 'RUNNING' and worker in task.workers:\n                # Return a list of currently running tasks to the client,\n                # makes it easier to troubleshoot\n                other_worker = self._state.get_worker(task.worker_running)\n                more_info = {'task_id': task.id, 'worker': str(other_worker)}\n                if other_worker is not None:\n                    more_info.update(other_worker.info)\n                    running_tasks.append(more_info)\n\n            if task.status == PENDING and worker in task.workers:\n                locally_pending_tasks += 1\n                if len(task.workers) == 1:\n                    n_unique_pending += 1\n\n            if task.status == RUNNING and task.worker_running in greedy_workers:\n                greedy_workers[task.worker_running] -= 1\n                for resource, amount in (task.resources or {}).items():\n                    greedy_resources[resource] += amount\n\n            if not best_task and self._schedulable(task) and self._has_resources(task.resources, greedy_resources):\n                if worker in task.workers and self._has_resources(task.resources, used_resources):\n                    best_task = task\n                    best_task_id = task.id\n                else:\n                    for task_worker in task.workers:\n                        if greedy_workers.get(task_worker, 0) > 0:\n                            # use up a worker\n                            greedy_workers[task_worker] -= 1\n\n                            # keep track of the resources used in greedy scheduling\n                            for resource, amount in (task.resources or {}).items():\n                                greedy_resources[resource] += amount\n\n                            break\n\n        if best_task:\n            best_task.status = RUNNING\n            best_task.worker_running = worker\n            best_task.time_running = time.time()\n            self._update_task_history(best_task.id, RUNNING, host=host)\n\n        return {'n_pending_tasks': locally_pending_tasks,\n                'n_unique_pending': n_unique_pending,\n                'task_id': best_task_id,\n                'running_tasks': running_tasks}\n\n    def ping(self, worker):\n        self.update(worker)\n\n    def _upstream_status(self, task_id, upstream_status_table):\n        if task_id in upstream_status_table:\n            return upstream_status_table[task_id]\n        elif self._state.has_task(task_id):\n            task_stack = [task_id]\n\n            while task_stack:\n                dep_id = task_stack.pop()\n                if self._state.has_task(dep_id):\n                    dep = self._state.get_task(dep_id)\n                    if dep_id not in upstream_status_table:\n                        if dep.status == PENDING and dep.deps:\n                            task_stack = task_stack + [dep_id] + list(dep.deps)\n                            upstream_status_table[dep_id] = ''  # will be updated postorder\n                        else:\n                            dep_status = STATUS_TO_UPSTREAM_MAP.get(dep.status, '')\n                            upstream_status_table[dep_id] = dep_status\n                    elif upstream_status_table[dep_id] == '' and dep.deps:\n                        # This is the postorder update step when we set the\n                        # status based on the previously calculated child elements\n                        upstream_status = [upstream_status_table.get(id, '') for id in dep.deps]\n                        upstream_status.append('')  # to handle empty list\n                        status = max(upstream_status, key=UPSTREAM_SEVERITY_KEY)\n                        upstream_status_table[dep_id] = status\n            return upstream_status_table[dep_id]\n\n    def _serialize_task(self, task_id, include_deps=True):\n        task = self._state.get_task(task_id)\n        ret = {\n            'deps': list(task.deps),\n            'status': task.status,\n            'workers': list(task.workers),\n            'worker_running': task.worker_running,\n            'time_running': getattr(task, \"time_running\", None),\n            'start_time': task.time,\n            'params': task.params,\n            'name': task.family,\n            'priority': task.priority,\n            'resources': task.resources,\n        }\n        if include_deps:\n            ret['deps'] = list(task.deps)\n        return ret\n\n    def graph(self):\n        self.prune()\n        serialized = {}\n        for task in self._state.get_active_tasks():\n            serialized[task.id] = self._serialize_task(task.id)\n        return serialized\n\n    def _recurse_deps(self, task_id, serialized):\n        if task_id not in serialized:\n            task = self._state.get_task(task_id)\n            if task is None or not task.family:\n                logger.warn('Missing task for id [%s]', task_id)\n\n                # try to infer family and params from task_id\n                try:\n                    family, _, param_str = task_id.rstrip(')').partition('(')\n                    params = dict(param.split('=') for param in param_str.split(', '))\n                except:\n                    family, params = '', {}\n                serialized[task_id] = {\n                    'deps': [],\n                    'status': UNKNOWN,\n                    'workers': [],\n                    'start_time': UNKNOWN,\n                    'params': params,\n                    'name': family,\n                    'priority': 0,\n                }\n            else:\n                serialized[task_id] = self._serialize_task(task_id)\n                for dep in task.deps:\n                    self._recurse_deps(dep, serialized)\n\n    def dep_graph(self, task_id):\n        self.prune()\n        serialized = {}\n        if self._state.has_task(task_id):\n            self._recurse_deps(task_id, serialized)\n        return serialized\n\n    def task_list(self, status, upstream_status, limit=True):\n        ''' query for a subset of tasks by status '''\n        self.prune()\n        result = {}\n        upstream_status_table = {}  # used to memoize upstream status\n        for task in self._state.get_active_tasks():\n            if not status or task.status == status:\n                if (task.status != PENDING or not upstream_status or\n                    upstream_status == self._upstream_status(task.id, upstream_status_table)):\n                    serialized = self._serialize_task(task.id, False)\n                    result[task.id] = serialized\n        if limit and len(result) > self._config.max_shown_tasks:\n            return {'num_tasks': len(result)}\n        return result\n\n    def worker_list(self, include_running=True):\n        self.prune()\n        workers = [\n            dict(\n                name=worker.id,\n                last_active=worker.last_active,\n                started=getattr(worker, 'started', None),\n                **worker.info\n            ) for worker in self._state.get_active_workers()]\n        workers.sort(key=lambda worker: worker['started'], reverse=True)\n        if include_running:\n            running = collections.defaultdict(dict)\n            num_pending = collections.defaultdict(int)\n            num_uniques = collections.defaultdict(int)\n            for task in self._state.get_pending_tasks():\n                if task.status == RUNNING and task.worker_running:\n                    running[task.worker_running][task.id] = self._serialize_task(task.id, False)\n                elif task.status == PENDING:\n                    for worker in task.workers:\n                        num_pending[worker] += 1\n                    if len(task.workers) == 1:\n                        num_uniques[list(task.workers)[0]] += 1\n            for worker in workers:\n                tasks = running[worker['name']]\n                worker['num_running'] = len(tasks)\n                worker['num_pending'] = num_pending[worker['name']]\n                worker['num_uniques'] = num_uniques[worker['name']]\n                worker['running'] = tasks\n        return workers\n\n    def inverse_dependencies(self, task_id):\n        self.prune()\n        serialized = {}\n        if self._state.has_task(task_id):\n            self._traverse_inverse_deps(task_id, serialized)\n        return serialized\n\n    def _traverse_inverse_deps(self, task_id, serialized):\n        stack = [task_id]\n        serialized[task_id] = self._serialize_task(task_id)\n        while len(stack) > 0:\n            curr_id = stack.pop()\n            for task in self._state.get_active_tasks():\n                if curr_id in task.deps:\n                    serialized[curr_id][\"deps\"].append(task.id)\n                    if task.id not in serialized:\n                        serialized[task.id] = self._serialize_task(task.id)\n                        serialized[task.id][\"deps\"] = []\n                        stack.append(task.id)\n\n    def task_search(self, task_str):\n        ''' query for a subset of tasks by task_id '''\n        self.prune()\n        result = collections.defaultdict(dict)\n        for task in self._state.get_active_tasks():\n            if task.id.find(task_str) != -1:\n                serialized = self._serialize_task(task.id, False)\n                result[task.status][task.id] = serialized\n        return result\n\n    def re_enable_task(self, task_id):\n        serialized = {}\n        task = self._state.get_task(task_id)\n        if task and task.status == DISABLED and task.scheduler_disable_time:\n            task.re_enable()\n            serialized = self._serialize_task(task_id)\n        return serialized\n\n    def fetch_error(self, task_id):\n        if self._state.has_task(task_id):\n            return {\"taskId\": task_id, \"error\": self._state.get_task(task_id).expl}\n        else:\n            return {\"taskId\": task_id, \"error\": \"\"}\n\n    def _update_task_history(self, task_id, status, host=None):\n        try:\n            if status == DONE or status == FAILED:\n                successful = (status == DONE)\n                self._task_history.task_finished(task_id, successful)\n            elif status == PENDING:\n                self._task_history.task_scheduled(task_id)\n            elif status == RUNNING:\n                self._task_history.task_started(task_id, host)\n        except:\n            logger.warning(\"Error saving Task history\", exc_info=1)\n\n    @property\n    def task_history(self):\n        # Used by server.py to expose the calls\n        return self._task_history\n",
          "file_after": "# Copyright (c) 2012 Spotify AB\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"); you may not\n# use this file except in compliance with the License. You may obtain a copy of\n# the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n# License for the specific language governing permissions and limitations under\n# the License.\n\nimport collections\nimport datetime\nimport functools\nimport itertools\nimport notifications\nimport os\nimport logging\nimport time\nimport cPickle as pickle\nimport task_history as history\nlogger = logging.getLogger(\"luigi.server\")\n\nfrom task_status import PENDING, FAILED, DONE, RUNNING, SUSPENDED, UNKNOWN, DISABLED\n\n\nclass Scheduler(object):\n    ''' Abstract base class\n\n    Note that the methods all take string arguments, not Task objects...\n    '''\n    add_task = NotImplemented\n    get_work = NotImplemented\n    ping = NotImplemented\n\nUPSTREAM_RUNNING = 'UPSTREAM_RUNNING'\nUPSTREAM_MISSING_INPUT = 'UPSTREAM_MISSING_INPUT'\nUPSTREAM_FAILED = 'UPSTREAM_FAILED'\nUPSTREAM_DISABLED = 'UPSTREAM_DISABLED'\n\nUPSTREAM_SEVERITY_ORDER = (\n    '',\n    UPSTREAM_RUNNING,\n    UPSTREAM_MISSING_INPUT,\n    UPSTREAM_FAILED,\n    UPSTREAM_DISABLED,\n)\nUPSTREAM_SEVERITY_KEY = lambda st: UPSTREAM_SEVERITY_ORDER.index(st)\nSTATUS_TO_UPSTREAM_MAP = {\n    FAILED: UPSTREAM_FAILED,\n    RUNNING: UPSTREAM_RUNNING,\n    PENDING: UPSTREAM_MISSING_INPUT,\n    DISABLED: UPSTREAM_DISABLED,\n}\n\n\n# We're passing around this config a lot, so let's put it on an object\nSchedulerConfig = collections.namedtuple('SchedulerConfig', [\n        'retry_delay', 'remove_delay', 'worker_disconnect_delay',\n        'disable_failures', 'disable_window', 'disable_persist', 'disable_time',\n        'max_shown_tasks',\n])\n\n\ndef fix_time(x):\n    # Backwards compatibility for a fix in Dec 2014. Prior to the fix, pickled state might store datetime objects\n    # Let's remove this function soon\n    if isinstance(x, datetime.datetime):\n        return time.mktime(x.timetuple())\n    else:\n        return x\n\n\nclass Failures(object):\n    \"\"\" This class tracks the number of failures in a given time window\n\n    Failures added are marked with the current timestamp, and this class counts\n    the number of failures in a sliding time window ending at the present.\n\n    \"\"\"\n\n    def __init__(self, window):\n        \"\"\" Initialize with the given window\n\n        :param window: how long to track failures for, as a float (number of seconds)\n        \"\"\"\n        self.window = window\n        self.failures = collections.deque()\n\n    def add_failure(self):\n        \"\"\" Add a failure event with the current timestamp \"\"\"\n        self.failures.append(time.time())\n\n    def num_failures(self):\n        \"\"\" Return the number of failures in the window \"\"\"\n        min_time = time.time() - self.window\n\n        while self.failures and fix_time(self.failures[0]) < min_time:\n            self.failures.popleft()\n\n        return len(self.failures)\n\n    def clear(self):\n        \"\"\" Clear the failure queue \"\"\"\n        self.failures.clear()\n\n\nclass Task(object):\n    def __init__(self, id, status, deps, resources={}, priority=0, family='', params={},\n                 disable_failures=None, disable_window=None):\n        self.id = id\n        self.stakeholders = set()  # workers ids that are somehow related to this task (i.e. don't prune while any of these workers are still active)\n        self.workers = set()  # workers ids that can perform task - task is 'BROKEN' if none of these workers are active\n        if deps is None:\n            self.deps = set()\n        else:\n            self.deps = set(deps)\n        self.status = status  # PENDING, RUNNING, FAILED or DONE\n        self.time = time.time()  # Timestamp when task was first added\n        self.retry = None\n        self.remove = None\n        self.worker_running = None  # the worker id that is currently running the task or None\n        self.time_running = None  # Timestamp when picked up by worker\n        self.expl = None\n        self.priority = priority\n        self.resources = resources\n        self.family = family\n        self.params = params\n        self.disable_failures = disable_failures\n        self.failures = Failures(disable_window)\n        self.scheduler_disable_time = None\n\n    def __repr__(self):\n        return \"Task(%r)\" % vars(self)\n\n    def add_failure(self):\n        self.failures.add_failure()\n\n    def has_excessive_failures(self):\n        return self.failures.num_failures() >= self.disable_failures\n\n    def can_disable(self):\n        return self.disable_failures is not None\n\n\nclass Worker(object):\n    \"\"\" Structure for tracking worker activity and keeping their references \"\"\"\n    def __init__(self, id, last_active=None):\n        self.id = id\n        self.reference = None  # reference to the worker in the real world. (Currently a dict containing just the host)\n        self.last_active = last_active  # seconds since epoch\n        self.started = time.time()  # seconds since epoch\n        self.info = {}\n\n    def add_info(self, info):\n        self.info.update(info)\n\n    def update(self, worker_reference):\n        if worker_reference:\n            self.reference = worker_reference\n        self.last_active = time.time()\n\n    def prune(self, config):\n        # Delete workers that haven't said anything for a while (probably killed)\n        if self.last_active + config.worker_disconnect_delay < time.time():\n            return True\n\n    def __str__(self):\n        return self.id\n\n\nclass SimpleTaskState(object):\n    ''' Keep track of the current state and handle persistance\n\n    The point of this class is to enable other ways to keep state, eg. by using a database\n    These will be implemented by creating an abstract base class that this and other classes\n    inherit from.\n    '''\n\n    def __init__(self, state_path):\n        self._state_path = state_path\n        self._tasks = {}  # map from id to a Task object\n        self._status_tasks = collections.defaultdict(dict)\n        self._active_workers = {}  # map from id to a Worker object\n\n    def dump(self):\n        state = (self._tasks, self._active_workers)\n        try:\n            with open(self._state_path, 'w') as fobj:\n                pickle.dump(state, fobj)\n        except IOError:\n            logger.warning(\"Failed saving scheduler state\", exc_info=1)\n        else:\n            logger.info(\"Saved state in %s\", self._state_path)\n\n    # prone to lead to crashes when old state is unpickled with updated code. TODO some kind of version control?\n    def load(self):\n        if os.path.exists(self._state_path):\n            logger.info(\"Attempting to load state from %s\", self._state_path)\n            try:\n                with open(self._state_path) as fobj:\n                    state = pickle.load(fobj)\n            except:\n                logger.exception(\"Error when loading state. Starting from clean slate.\")\n                return\n\n            self._tasks, self._active_workers = state\n            self._status_tasks = collections.defaultdict(dict)\n            for task in self._tasks.itervalues():\n                self._status_tasks[task.status][task.id] = task\n\n            # Convert from old format\n            # TODO: this is really ugly, we need something more future-proof\n            # Every time we add an attribute to the Worker class, this code needs to be updated\n            for k, v in self._active_workers.iteritems():\n                if isinstance(v, float):\n                    self._active_workers[k] = Worker(id=k, last_active=v)\n        else:\n            logger.info(\"No prior state file exists at %s. Starting with clean slate\", self._state_path)\n\n    def get_active_tasks(self, status=None):\n        if status:\n            for task in self._status_tasks[status].itervalues():\n                yield task\n        else:\n            for task in self._tasks.itervalues():\n                yield task\n\n    def get_running_tasks(self):\n        return self._status_tasks[RUNNING].itervalues()\n\n    def get_pending_tasks(self):\n        return itertools.chain.from_iterable(self._status_tasks[status].itervalues()\n                                             for status in [PENDING, RUNNING])\n\n    def get_task(self, task_id, default=None, setdefault=None):\n        if setdefault:\n            task = self._tasks.setdefault(task_id, setdefault)\n            self._status_tasks[task.status][task.id] = task\n            return task\n        else:\n            return self._tasks.get(task_id, default)\n\n    def has_task(self, task_id):\n        return task_id in self._tasks\n\n    def re_enable(self, task, config=None):\n        task.scheduler_disable_time = None\n        task.failures.clear()\n        if config:\n            self.set_status(task, FAILED, config)\n            task.failures.clear()\n\n    def set_status(self, task, new_status, config=None):\n        if new_status == FAILED:\n            assert config is not None\n\n        # not sure why we have SUSPENDED, as it can never be set\n        if new_status == SUSPENDED:\n            new_status = PENDING\n\n        if new_status == DISABLED and task.status == RUNNING:\n            return\n\n        if task.status == DISABLED:\n            if new_status == DONE:\n                self.re_enable(task)\n\n            # don't allow workers to override a scheduler disable\n            elif task.scheduler_disable_time is not None:\n                return\n\n        if new_status == FAILED and task.can_disable():\n            task.add_failure()\n            if task.has_excessive_failures():\n                task.scheduler_disable_time = time.time()\n                new_status = DISABLED\n                notifications.send_error_email(\n                    'Luigi Scheduler: DISABLED {task} due to excessive failures'.format(task=task.id),\n                    '{task} failed {failures} times in the last {window} seconds, so it is being '\n                    'disabled for {persist} seconds'.format(\n                        failures=config.disable_failures,\n                        task=task.id,\n                        window=config.disable_window,\n                        persist=config.disable_persist,\n                    ))\n        elif new_status == DISABLED:\n            task.scheduler_disable_time = None\n\n        self._status_tasks[task.status].pop(task.id)\n        self._status_tasks[new_status][task.id] = task\n        task.status = new_status\n\n    def prune(self, task, config):\n        remove = False\n\n        # Mark tasks with no remaining active stakeholders for deletion\n        if not task.stakeholders:\n            if task.remove is None:\n                logger.info(\"Task %r has stakeholders %r but none remain connected -> will remove \"\n                            \"task in %s seconds\", task.id, task.stakeholders, config.remove_delay)\n                task.remove = time.time() + config.remove_delay\n\n        # If a running worker disconnects, tag all its jobs as FAILED and subject it to the same retry logic\n        if task.status == RUNNING and task.worker_running and task.worker_running not in task.stakeholders:\n            logger.info(\"Task %r is marked as running by disconnected worker %r -> marking as \"\n                        \"FAILED with retry delay of %rs\", task.id, task.worker_running,\n                        config.retry_delay)\n            task.worker_running = None\n            self.set_status(task, FAILED, config)\n            task.retry = time.time() + config.retry_delay\n\n        # Re-enable task after the disable time expires\n        if task.status == DISABLED and task.scheduler_disable_time:\n            if time.time() - fix_time(task.scheduler_disable_time) > config.disable_time:\n                self.re_enable(task, config)\n\n        # Remove tasks that have no stakeholders\n        if task.remove and time.time() > task.remove:\n            logger.info(\"Removing task %r (no connected stakeholders)\", task.id)\n            remove = True\n\n        # Reset FAILED tasks to PENDING if max timeout is reached, and retry delay is >= 0\n        if task.status == FAILED and config.retry_delay >= 0 and task.retry < time.time():\n            self.set_status(task, PENDING, config)\n\n        return remove\n\n    def inactivate_tasks(self, delete_tasks):\n        # The terminology is a bit confusing: we used to \"delete\" tasks when they became inactive,\n        # but with a pluggable state storage, you might very well want to keep some history of\n        # older tasks as well. That's why we call it \"inactivate\" (as in the verb)\n        for task in delete_tasks:\n            task_obj = self._tasks.pop(task)\n            self._status_tasks[task_obj.status].pop(task)\n\n    def get_active_workers(self, last_active_lt=None):\n        for worker in self._active_workers.itervalues():\n            if last_active_lt is not None and worker.last_active >= last_active_lt:\n                continue\n            yield worker\n\n    def get_worker_ids(self):\n        return self._active_workers.keys() # only used for unit tests\n\n    def get_worker(self, worker_id):\n        return self._active_workers.setdefault(worker_id, Worker(worker_id))\n\n    def inactivate_workers(self, delete_workers):\n        # Mark workers as inactive\n        for worker in delete_workers:\n            self._active_workers.pop(worker)\n\n        # remove workers from tasks\n        for task in self.get_active_tasks():\n            task.stakeholders.difference_update(delete_workers)\n            task.workers.difference_update(delete_workers)\n\n\nclass CentralPlannerScheduler(Scheduler):\n    ''' Async scheduler that can handle multiple workers etc\n\n    Can be run locally or on a server (using RemoteScheduler + server.Server).\n    '''\n\n    def __init__(self, retry_delay=900.0, remove_delay=600.0, worker_disconnect_delay=60.0,\n                 state_path='/var/lib/luigi-server/state.pickle', task_history=None,\n                 resources=None, disable_persist=0, disable_window=0, disable_failures=None,\n                 max_shown_tasks=100000):\n        '''\n        (all arguments are in seconds)\n        Keyword Arguments:\n        retry_delay -- How long after a Task fails to try it again, or -1 to never retry\n        remove_delay -- How long after a Task finishes to remove it from the scheduler\n        state_path -- Path to state file (tasks and active workers)\n        worker_disconnect_delay -- If a worker hasn't communicated for this long, remove it from active workers\n        '''\n        self._config = SchedulerConfig(\n            retry_delay=retry_delay,\n            remove_delay=remove_delay,\n            worker_disconnect_delay=worker_disconnect_delay,\n            disable_failures=disable_failures,\n            disable_window=disable_window,\n            disable_persist=disable_persist,\n            disable_time=disable_persist,\n            max_shown_tasks=max_shown_tasks,\n        )\n\n        self._task_history = task_history or history.NopHistory()\n        self._state = SimpleTaskState(state_path)\n\n        self._task_history = task_history or history.NopHistory()\n        self._resources = resources\n        self._make_task = functools.partial(\n            Task, disable_failures=disable_failures,\n            disable_window=disable_window)\n\n    def load(self):\n        self._state.load()\n\n    def dump(self):\n        self._state.dump()\n\n    def prune(self):\n        logger.info(\"Starting pruning of task graph\")\n        remove_workers = []\n        for worker in self._state.get_active_workers():\n            if worker.prune(self._config):\n                logger.info(\"Worker %s timed out (no contact for >=%ss)\", worker, self._config.worker_disconnect_delay)\n                remove_workers.append(worker.id)\n\n        self._state.inactivate_workers(remove_workers)\n\n        remove_tasks = []\n        for task in self._state.get_active_tasks():\n            if self._state.prune(task, self._config):\n                remove_tasks.append(task.id)\n\n        self._state.inactivate_tasks(remove_tasks)\n\n        logger.info(\"Done pruning task graph\")\n\n    def update(self, worker_id, worker_reference=None):\n        \"\"\" Keep track of whenever the worker was last active \"\"\"\n        worker = self._state.get_worker(worker_id)\n        worker.update(worker_reference)\n\n    def _update_priority(self, task, prio, worker):\n        \"\"\" Update priority of the given task\n\n        Priority can only be increased. If the task doesn't exist, a placeholder\n        task is created to preserve priority when the task is later scheduled.\n        \"\"\"\n        task.priority = prio = max(prio, task.priority)\n        for dep in task.deps or []:\n            t = self._state.get_task(dep)\n            if t is not None and prio > t.priority:\n                self._update_priority(t, prio, worker)\n\n    def add_task(self, worker, task_id, status=PENDING, runnable=True,\n                 deps=None, new_deps=None, expl=None, resources=None,\n                 priority=0, family='', params={}):\n        \"\"\"\n        * Add task identified by task_id if it doesn't exist\n        * If deps is not None, update dependency list\n        * Update status of task\n        * Add additional workers/stakeholders\n        * Update priority when needed\n        \"\"\"\n        self.update(worker)\n\n        task = self._state.get_task(task_id, setdefault=self._make_task(\n                id=task_id, status=PENDING, deps=deps, resources=resources,\n                priority=priority, family=family, params=params))\n\n        # for setting priority, we'll sometimes create tasks with unset family and params\n        if not task.family:\n            task.family = family\n        if not task.params:\n            task.params = params\n\n        if task.remove is not None:\n            task.remove = None  # unmark task for removal so it isn't removed after being added\n\n        if not (task.status == RUNNING and status == PENDING):\n            # don't allow re-scheduling of task while it is running, it must either fail or succeed first\n            if status == PENDING or status != task.status:\n                # Update the DB only if there was a acctual change, to prevent noise.\n                # We also check for status == PENDING b/c that's the default value\n                # (so checking for status != task.status woule lie)\n                self._update_task_history(task_id, status)\n            self._state.set_status(task, PENDING if status == SUSPENDED else status, self._config)\n            if status == FAILED:\n                task.retry = time.time() + self._config.retry_delay\n\n        if deps is not None:\n            task.deps = set(deps)\n\n        if new_deps is not None:\n            task.deps.update(new_deps)\n\n        task.stakeholders.add(worker)\n        task.resources = resources\n\n        # Task dependencies might not exist yet. Let's create dummy tasks for them for now.\n        # Otherwise the task dependencies might end up being pruned if scheduling takes a long time\n        for dep in task.deps or []:\n            t = self._state.get_task(dep, setdefault=self._make_task(id=dep, status=UNKNOWN, deps=None, priority=priority))\n            t.stakeholders.add(worker)\n\n        self._update_priority(task, priority, worker)\n\n        if runnable:\n            task.workers.add(worker)\n\n        if expl is not None:\n            task.expl = expl\n\n    def add_worker(self, worker, info):\n        self._state.get_worker(worker).add_info(info)\n\n    def update_resources(self, **resources):\n        if self._resources is None:\n            self._resources = {}\n        self._resources.update(resources)\n\n    def _has_resources(self, needed_resources, used_resources):\n        if needed_resources is None:\n            return True\n\n        available_resources = self._resources or {}\n        for resource, amount in needed_resources.items():\n            if amount + used_resources[resource] > available_resources.get(resource, 1):\n                return False\n        return True\n\n    def _used_resources(self):\n        used_resources = collections.defaultdict(int)\n        if self._resources is not None:\n            for task in self._state.get_active_tasks():\n                if task.status == RUNNING and task.resources:\n                    for resource, amount in task.resources.items():\n                        used_resources[resource] += amount\n        return used_resources\n\n    def _rank(self):\n        ''' Return worker's rank function for task scheduling '''\n        dependents = collections.defaultdict(int)\n        def not_done(t):\n            task = self._state.get_task(t, default=None)\n            return task is None or task.status != DONE\n        for task in self._state.get_pending_tasks():\n            if task.status != DONE:\n                deps = filter(not_done, task.deps)\n                inverse_num_deps = 1.0 / max(len(deps), 1)\n                for dep in deps:\n                    dependents[dep] += inverse_num_deps\n\n        return lambda task: (task.priority, dependents[task.id], -task.time)\n\n    def _schedulable(self, task):\n        if task.status != PENDING:\n            return False\n        for dep in task.deps:\n            dep_task = self._state.get_task(dep, default=None)\n            if dep_task is None or dep_task.status != DONE:\n                return False\n        return True\n\n    def get_work(self, worker, host=None):\n        # TODO: remove any expired nodes\n\n        # Algo: iterate over all nodes, find the highest priority node no dependencies and available\n        # resources.\n\n        # Resource checking looks both at currently available resources and at which resources would\n        # be available if all running tasks died and we rescheduled all workers greedily. We do both\n        # checks in order to prevent a worker with many low-priority tasks from starving other\n        # workers with higher priority tasks that share the same resources.\n\n        # TODO: remove tasks that can't be done, figure out if the worker has absolutely\n        # nothing it can wait for\n\n        # Return remaining tasks that have no FAILED descendents\n        self.update(worker, {'host': host})\n        best_task = None\n        best_task_id = None\n        locally_pending_tasks = 0\n        running_tasks = []\n\n        used_resources = self._used_resources()\n        greedy_resources = collections.defaultdict(int)\n        n_unique_pending = 0\n        greedy_workers = dict((worker.id, worker.info.get('workers', 1))\n                              for worker in self._state.get_active_workers())\n\n        tasks = list(self._state.get_pending_tasks())\n        tasks.sort(key=self._rank(), reverse=True)\n\n        for task in tasks:\n            if task.status == 'RUNNING' and worker in task.workers:\n                # Return a list of currently running tasks to the client,\n                # makes it easier to troubleshoot\n                other_worker = self._state.get_worker(task.worker_running)\n                more_info = {'task_id': task.id, 'worker': str(other_worker)}\n                if other_worker is not None:\n                    more_info.update(other_worker.info)\n                    running_tasks.append(more_info)\n\n            if task.status == PENDING and worker in task.workers:\n                locally_pending_tasks += 1\n                if len(task.workers) == 1:\n                    n_unique_pending += 1\n\n            if task.status == RUNNING and task.worker_running in greedy_workers:\n                greedy_workers[task.worker_running] -= 1\n                for resource, amount in (task.resources or {}).items():\n                    greedy_resources[resource] += amount\n\n            if not best_task and self._schedulable(task) and self._has_resources(task.resources, greedy_resources):\n                if worker in task.workers and self._has_resources(task.resources, used_resources):\n                    best_task = task\n                    best_task_id = task.id\n                else:\n                    for task_worker in task.workers:\n                        if greedy_workers.get(task_worker, 0) > 0:\n                            # use up a worker\n                            greedy_workers[task_worker] -= 1\n\n                            # keep track of the resources used in greedy scheduling\n                            for resource, amount in (task.resources or {}).items():\n                                greedy_resources[resource] += amount\n\n                            break\n\n        if best_task:\n            self._state.set_status(best_task, RUNNING, self._config)\n            best_task.worker_running = worker\n            best_task.time_running = time.time()\n            self._update_task_history(best_task.id, RUNNING, host=host)\n\n        return {'n_pending_tasks': locally_pending_tasks,\n                'n_unique_pending': n_unique_pending,\n                'task_id': best_task_id,\n                'running_tasks': running_tasks}\n\n    def ping(self, worker):\n        self.update(worker)\n\n    def _upstream_status(self, task_id, upstream_status_table):\n        if task_id in upstream_status_table:\n            return upstream_status_table[task_id]\n        elif self._state.has_task(task_id):\n            task_stack = [task_id]\n\n            while task_stack:\n                dep_id = task_stack.pop()\n                if self._state.has_task(dep_id):\n                    dep = self._state.get_task(dep_id)\n                    if dep_id not in upstream_status_table:\n                        if dep.status == PENDING and dep.deps:\n                            task_stack = task_stack + [dep_id] + list(dep.deps)\n                            upstream_status_table[dep_id] = ''  # will be updated postorder\n                        else:\n                            dep_status = STATUS_TO_UPSTREAM_MAP.get(dep.status, '')\n                            upstream_status_table[dep_id] = dep_status\n                    elif upstream_status_table[dep_id] == '' and dep.deps:\n                        # This is the postorder update step when we set the\n                        # status based on the previously calculated child elements\n                        upstream_status = [upstream_status_table.get(id, '') for id in dep.deps]\n                        upstream_status.append('')  # to handle empty list\n                        status = max(upstream_status, key=UPSTREAM_SEVERITY_KEY)\n                        upstream_status_table[dep_id] = status\n            return upstream_status_table[dep_id]\n\n    def _serialize_task(self, task_id, include_deps=True):\n        task = self._state.get_task(task_id)\n        ret = {\n            'deps': list(task.deps),\n            'status': task.status,\n            'workers': list(task.workers),\n            'worker_running': task.worker_running,\n            'time_running': getattr(task, \"time_running\", None),\n            'start_time': task.time,\n            'params': task.params,\n            'name': task.family,\n            'priority': task.priority,\n            'resources': task.resources,\n        }\n        if include_deps:\n            ret['deps'] = list(task.deps)\n        return ret\n\n    def graph(self):\n        self.prune()\n        serialized = {}\n        for task in self._state.get_active_tasks():\n            serialized[task.id] = self._serialize_task(task.id)\n        return serialized\n\n    def _recurse_deps(self, task_id, serialized):\n        if task_id not in serialized:\n            task = self._state.get_task(task_id)\n            if task is None or not task.family:\n                logger.warn('Missing task for id [%s]', task_id)\n\n                # try to infer family and params from task_id\n                try:\n                    family, _, param_str = task_id.rstrip(')').partition('(')\n                    params = dict(param.split('=') for param in param_str.split(', '))\n                except:\n                    family, params = '', {}\n                serialized[task_id] = {\n                    'deps': [],\n                    'status': UNKNOWN,\n                    'workers': [],\n                    'start_time': UNKNOWN,\n                    'params': params,\n                    'name': family,\n                    'priority': 0,\n                }\n            else:\n                serialized[task_id] = self._serialize_task(task_id)\n                for dep in task.deps:\n                    self._recurse_deps(dep, serialized)\n\n    def dep_graph(self, task_id):\n        self.prune()\n        serialized = {}\n        if self._state.has_task(task_id):\n            self._recurse_deps(task_id, serialized)\n        return serialized\n\n    def task_list(self, status, upstream_status, limit=True):\n        ''' query for a subset of tasks by status '''\n        self.prune()\n        result = {}\n        upstream_status_table = {}  # used to memoize upstream status\n        for task in self._state.get_active_tasks(status):\n            if (task.status != PENDING or not upstream_status or\n                upstream_status == self._upstream_status(task.id, upstream_status_table)):\n                serialized = self._serialize_task(task.id, False)\n                result[task.id] = serialized\n        if limit and len(result) > self._config.max_shown_tasks:\n            return {'num_tasks': len(result)}\n        return result\n\n    def worker_list(self, include_running=True):\n        self.prune()\n        workers = [\n            dict(\n                name=worker.id,\n                last_active=worker.last_active,\n                started=getattr(worker, 'started', None),\n                **worker.info\n            ) for worker in self._state.get_active_workers()]\n        workers.sort(key=lambda worker: worker['started'], reverse=True)\n        if include_running:\n            running = collections.defaultdict(dict)\n            num_pending = collections.defaultdict(int)\n            num_uniques = collections.defaultdict(int)\n            for task in self._state.get_pending_tasks():\n                if task.status == RUNNING and task.worker_running:\n                    running[task.worker_running][task.id] = self._serialize_task(task.id, False)\n                elif task.status == PENDING:\n                    for worker in task.workers:\n                        num_pending[worker] += 1\n                    if len(task.workers) == 1:\n                        num_uniques[list(task.workers)[0]] += 1\n            for worker in workers:\n                tasks = running[worker['name']]\n                worker['num_running'] = len(tasks)\n                worker['num_pending'] = num_pending[worker['name']]\n                worker['num_uniques'] = num_uniques[worker['name']]\n                worker['running'] = tasks\n        return workers\n\n    def inverse_dependencies(self, task_id):\n        self.prune()\n        serialized = {}\n        if self._state.has_task(task_id):\n            self._traverse_inverse_deps(task_id, serialized)\n        return serialized\n\n    def _traverse_inverse_deps(self, task_id, serialized):\n        stack = [task_id]\n        serialized[task_id] = self._serialize_task(task_id)\n        while len(stack) > 0:\n            curr_id = stack.pop()\n            for task in self._state.get_active_tasks():\n                if curr_id in task.deps:\n                    serialized[curr_id][\"deps\"].append(task.id)\n                    if task.id not in serialized:\n                        serialized[task.id] = self._serialize_task(task.id)\n                        serialized[task.id][\"deps\"] = []\n                        stack.append(task.id)\n\n    def task_search(self, task_str):\n        ''' query for a subset of tasks by task_id '''\n        self.prune()\n        result = collections.defaultdict(dict)\n        for task in self._state.get_active_tasks():\n            if task.id.find(task_str) != -1:\n                serialized = self._serialize_task(task.id, False)\n                result[task.status][task.id] = serialized\n        return result\n\n    def re_enable_task(self, task_id):\n        serialized = {}\n        task = self._state.get_task(task_id)\n        if task and task.status == DISABLED and task.scheduler_disable_time:\n            self._state.re_enable(task, self._config)\n            serialized = self._serialize_task(task_id)\n        return serialized\n\n    def fetch_error(self, task_id):\n        if self._state.has_task(task_id):\n            return {\"taskId\": task_id, \"error\": self._state.get_task(task_id).expl}\n        else:\n            return {\"taskId\": task_id, \"error\": \"\"}\n\n    def _update_task_history(self, task_id, status, host=None):\n        try:\n            if status == DONE or status == FAILED:\n                successful = (status == DONE)\n                self._task_history.task_finished(task_id, successful)\n            elif status == PENDING:\n                self._task_history.task_scheduled(task_id)\n            elif status == RUNNING:\n                self._task_history.task_started(task_id, host)\n        except:\n            logger.warning(\"Error saving Task history\", exc_info=1)\n\n    @property\n    def task_history(self):\n        # Used by server.py to expose the calls\n        return self._task_history\n",
          "file_patch": "@@ -15,6 +15,7 @@\n import collections\n import datetime\n import functools\n+import itertools\n import notifications\n import os\n import logging\n@@ -144,78 +145,6 @@ class Task(object):\n     def can_disable(self):\n         return self.disable_failures is not None\n \n-    def re_enable(self):\n-        self.scheduler_disable_time = None\n-        self.status = FAILED\n-        self.failures.clear()\n-\n-    def set_status(self, new_status, config):\n-        # not sure why we have SUSPENDED, as it can never be set\n-        if new_status == SUSPENDED:\n-            new_status = PENDING\n-\n-        if new_status == DISABLED and self.status == RUNNING:\n-            return\n-\n-        if self.status == DISABLED:\n-            if new_status == DONE:\n-                self.re_enable()\n-\n-            # don't allow workers to override a scheduler disable\n-            elif self.scheduler_disable_time is not None:\n-                return\n-\n-        if new_status == FAILED and self.can_disable():\n-            self.add_failure()\n-            if self.has_excessive_failures():\n-                self.scheduler_disable_time = time.time()\n-                new_status = DISABLED\n-                notifications.send_error_email(\n-                    'Luigi Scheduler: DISABLED {task} due to excessive failures'.format(task=self.id),\n-                    '{task} failed {failures} times in the last {window} seconds, so it is being '\n-                    'disabled for {persist} seconds'.format(\n-                        failures=config.disable_failures,\n-                        task=self.id,\n-                        window=config.disable_window,\n-                        persist=config.disable_persist,\n-                        ))\n-        elif new_status == DISABLED:\n-            self.scheduler_disable_time = None\n-\n-        self.status = new_status\n-\n-    def prune(self, config):\n-        remove = False\n-\n-        # Mark tasks with no remaining active stakeholders for deletion\n-        if not self.stakeholders:\n-            if self.remove is None:\n-                logger.info(\"Task %r has stakeholders %r but none remain connected -> will remove task in %s seconds\", self.id, self.stakeholders, config.remove_delay)\n-                self.remove = time.time() + config.remove_delay\n-\n-        # If a running worker disconnects, tag all its jobs as FAILED and subject it to the same retry logic\n-        if self.status == RUNNING and self.worker_running and self.worker_running not in self.stakeholders:\n-            logger.info(\"Task %r is marked as running by disconnected worker %r -> marking as FAILED with retry delay of %rs\", self.id, self.worker_running, config.retry_delay)\n-            self.worker_running = None\n-            self.set_status(FAILED, config)\n-            self.retry = time.time() + config.retry_delay\n-\n-        # Re-enable task after the disable time expires\n-        if self.status == DISABLED and self.scheduler_disable_time:\n-            if time.time() - fix_time(self.scheduler_disable_time) > config.disable_time:\n-                self.re_enable()\n-\n-        # Remove tasks that have no stakeholders\n-        if self.remove and time.time() > self.remove:\n-            logger.info(\"Removing task %r (no connected stakeholders)\", self.id)\n-            remove = True\n-\n-        # Reset FAILED tasks to PENDING if max timeout is reached, and retry delay is >= 0\n-        if self.status == FAILED and config.retry_delay >= 0 and self.retry < time.time():\n-            self.set_status(PENDING, config)\n-\n-        return remove\n-\n \n class Worker(object):\n     \"\"\" Structure for tracking worker activity and keeping their references \"\"\"\n@@ -254,6 +183,7 @@ class SimpleTaskState(object):\n     def __init__(self, state_path):\n         self._state_path = state_path\n         self._tasks = {}  # map from id to a Task object\n+        self._status_tasks = collections.defaultdict(dict)\n         self._active_workers = {}  # map from id to a Worker object\n \n     def dump(self):\n@@ -278,6 +208,9 @@ class SimpleTaskState(object):\n                 return\n \n             self._tasks, self._active_workers = state\n+            self._status_tasks = collections.defaultdict(dict)\n+            for task in self._tasks.itervalues():\n+                self._status_tasks[task.status][task.id] = task\n \n             # Convert from old format\n             # TODO: this is really ugly, we need something more future-proof\n@@ -288,30 +221,121 @@ class SimpleTaskState(object):\n         else:\n             logger.info(\"No prior state file exists at %s. Starting with clean slate\", self._state_path)\n \n-    def get_active_tasks(self):\n-        for task in self._tasks.itervalues():\n-            yield task\n+    def get_active_tasks(self, status=None):\n+        if status:\n+            for task in self._status_tasks[status].itervalues():\n+                yield task\n+        else:\n+            for task in self._tasks.itervalues():\n+                yield task\n+\n+    def get_running_tasks(self):\n+        return self._status_tasks[RUNNING].itervalues()\n \n     def get_pending_tasks(self):\n-        for task in self._tasks.itervalues():\n-            if task.status in [PENDING, RUNNING]:\n-                yield task\n+        return itertools.chain.from_iterable(self._status_tasks[status].itervalues()\n+                                             for status in [PENDING, RUNNING])\n \n     def get_task(self, task_id, default=None, setdefault=None):\n         if setdefault:\n-            return self._tasks.setdefault(task_id, setdefault)\n+            task = self._tasks.setdefault(task_id, setdefault)\n+            self._status_tasks[task.status][task.id] = task\n+            return task\n         else:\n             return self._tasks.get(task_id, default)\n \n     def has_task(self, task_id):\n         return task_id in self._tasks\n \n+    def re_enable(self, task, config=None):\n+        task.scheduler_disable_time = None\n+        task.failures.clear()\n+        if config:\n+            self.set_status(task, FAILED, config)\n+            task.failures.clear()\n+\n+    def set_status(self, task, new_status, config=None):\n+        if new_status == FAILED:\n+            assert config is not None\n+\n+        # not sure why we have SUSPENDED, as it can never be set\n+        if new_status == SUSPENDED:\n+            new_status = PENDING\n+\n+        if new_status == DISABLED and task.status == RUNNING:\n+            return\n+\n+        if task.status == DISABLED:\n+            if new_status == DONE:\n+                self.re_enable(task)\n+\n+            # don't allow workers to override a scheduler disable\n+            elif task.scheduler_disable_time is not None:\n+                return\n+\n+        if new_status == FAILED and task.can_disable():\n+            task.add_failure()\n+            if task.has_excessive_failures():\n+                task.scheduler_disable_time = time.time()\n+                new_status = DISABLED\n+                notifications.send_error_email(\n+                    'Luigi Scheduler: DISABLED {task} due to excessive failures'.format(task=task.id),\n+                    '{task} failed {failures} times in the last {window} seconds, so it is being '\n+                    'disabled for {persist} seconds'.format(\n+                        failures=config.disable_failures,\n+                        task=task.id,\n+                        window=config.disable_window,\n+                        persist=config.disable_persist,\n+                    ))\n+        elif new_status == DISABLED:\n+            task.scheduler_disable_time = None\n+\n+        self._status_tasks[task.status].pop(task.id)\n+        self._status_tasks[new_status][task.id] = task\n+        task.status = new_status\n+\n+    def prune(self, task, config):\n+        remove = False\n+\n+        # Mark tasks with no remaining active stakeholders for deletion\n+        if not task.stakeholders:\n+            if task.remove is None:\n+                logger.info(\"Task %r has stakeholders %r but none remain connected -> will remove \"\n+                            \"task in %s seconds\", task.id, task.stakeholders, config.remove_delay)\n+                task.remove = time.time() + config.remove_delay\n+\n+        # If a running worker disconnects, tag all its jobs as FAILED and subject it to the same retry logic\n+        if task.status == RUNNING and task.worker_running and task.worker_running not in task.stakeholders:\n+            logger.info(\"Task %r is marked as running by disconnected worker %r -> marking as \"\n+                        \"FAILED with retry delay of %rs\", task.id, task.worker_running,\n+                        config.retry_delay)\n+            task.worker_running = None\n+            self.set_status(task, FAILED, config)\n+            task.retry = time.time() + config.retry_delay\n+\n+        # Re-enable task after the disable time expires\n+        if task.status == DISABLED and task.scheduler_disable_time:\n+            if time.time() - fix_time(task.scheduler_disable_time) > config.disable_time:\n+                self.re_enable(task, config)\n+\n+        # Remove tasks that have no stakeholders\n+        if task.remove and time.time() > task.remove:\n+            logger.info(\"Removing task %r (no connected stakeholders)\", task.id)\n+            remove = True\n+\n+        # Reset FAILED tasks to PENDING if max timeout is reached, and retry delay is >= 0\n+        if task.status == FAILED and config.retry_delay >= 0 and task.retry < time.time():\n+            self.set_status(task, PENDING, config)\n+\n+        return remove\n+\n     def inactivate_tasks(self, delete_tasks):\n         # The terminology is a bit confusing: we used to \"delete\" tasks when they became inactive,\n         # but with a pluggable state storage, you might very well want to keep some history of\n         # older tasks as well. That's why we call it \"inactivate\" (as in the verb)\n         for task in delete_tasks:\n-            self._tasks.pop(task)\n+            task_obj = self._tasks.pop(task)\n+            self._status_tasks[task_obj.status].pop(task)\n \n     def get_active_workers(self, last_active_lt=None):\n         for worker in self._active_workers.itervalues():\n@@ -392,7 +416,7 @@ class CentralPlannerScheduler(Scheduler):\n \n         remove_tasks = []\n         for task in self._state.get_active_tasks():\n-            if task.prune(self._config):\n+            if self._state.prune(task, self._config):\n                 remove_tasks.append(task.id)\n \n         self._state.inactivate_tasks(remove_tasks)\n@@ -448,7 +472,7 @@ class CentralPlannerScheduler(Scheduler):\n                 # We also check for status == PENDING b/c that's the default value\n                 # (so checking for status != task.status woule lie)\n                 self._update_task_history(task_id, status)\n-            task.set_status(PENDING if status == SUSPENDED else status, self._config)\n+            self._state.set_status(task, PENDING if status == SUSPENDED else status, self._config)\n             if status == FAILED:\n                 task.retry = time.time() + self._config.retry_delay\n \n@@ -508,7 +532,7 @@ class CentralPlannerScheduler(Scheduler):\n         def not_done(t):\n             task = self._state.get_task(t, default=None)\n             return task is None or task.status != DONE\n-        for task in self._state.get_active_tasks():\n+        for task in self._state.get_pending_tasks():\n             if task.status != DONE:\n                 deps = filter(not_done, task.deps)\n                 inverse_num_deps = 1.0 / max(len(deps), 1)\n@@ -593,7 +617,7 @@ class CentralPlannerScheduler(Scheduler):\n                             break\n \n         if best_task:\n-            best_task.status = RUNNING\n+            self._state.set_status(best_task, RUNNING, self._config)\n             best_task.worker_running = worker\n             best_task.time_running = time.time()\n             self._update_task_history(best_task.id, RUNNING, host=host)\n@@ -695,12 +719,11 @@ class CentralPlannerScheduler(Scheduler):\n         self.prune()\n         result = {}\n         upstream_status_table = {}  # used to memoize upstream status\n-        for task in self._state.get_active_tasks():\n-            if not status or task.status == status:\n-                if (task.status != PENDING or not upstream_status or\n-                    upstream_status == self._upstream_status(task.id, upstream_status_table)):\n-                    serialized = self._serialize_task(task.id, False)\n-                    result[task.id] = serialized\n+        for task in self._state.get_active_tasks(status):\n+            if (task.status != PENDING or not upstream_status or\n+                upstream_status == self._upstream_status(task.id, upstream_status_table)):\n+                serialized = self._serialize_task(task.id, False)\n+                result[task.id] = serialized\n         if limit and len(result) > self._config.max_shown_tasks:\n             return {'num_tasks': len(result)}\n         return result\n@@ -769,7 +792,7 @@ class CentralPlannerScheduler(Scheduler):\n         serialized = {}\n         task = self._state.get_task(task_id)\n         if task and task.status == DISABLED and task.scheduler_disable_time:\n-            task.re_enable()\n+            self._state.re_enable(task, self._config)\n             serialized = self._serialize_task(task_id)\n         return serialized\n \n",
          "files_name_in_blame_commit": [
            "scheduler.py"
          ]
        }
      },
      "92fd2d0a3dfe4656ec84813e5416238f3d6a40f2": {
        "commit": {
          "commit_id": "92fd2d0a3dfe4656ec84813e5416238f3d6a40f2",
          "commit_message": "Takes number of workers into account in scheduler resource planning\n\nI've noticed recently that sometimes I'll have a job with thousands of pending\ntasks scheduled all using the same resource, but it's not running anything. All\nof the tasks are ready and nothing else is using the resource.\n\nIt turned out that another worker had several tasks using the same resource\nscheduled with higher priority. It only had one worker and wasn't going to get\nto them anytime soon, but the greedy scheduling assigned all the resources to\nthis worker so it could sit for hours preparing to use them.\n\nIn order to alleviate this issue and allow resources to be used more freely,\nthis commit only allows workers to reserve resources when they are actually\nabout to use them.",
          "commit_author": "Dave Buchfuhrer",
          "commit_date": "2014-10-27 14:09:34",
          "commit_parent": "e16f2a8084dcb9db9bbc534199c98fd263aba093"
        },
        "function": {
          "function_name": "get_work",
          "function_code_before": "def get_work(self, worker, host=None):\n    self.update(worker, {'host': host})\n    best_task = None\n    best_task_id = None\n    locally_pending_tasks = 0\n    running_tasks = []\n    used_resources = self._used_resources()\n    greedy_resources = collections.defaultdict(int)\n    n_unique_pending = 0\n    tasks = list(self._state.get_pending_tasks())\n    tasks.sort(key=self._rank(), reverse=True)\n    for task in tasks:\n        if task.status == 'RUNNING' and worker in task.workers:\n            other_worker = self._state.get_worker(task.worker_running)\n            more_info = {'task_id': task.id, 'worker': str(other_worker)}\n            if other_worker is not None:\n                more_info.update(other_worker.info)\n                running_tasks.append(more_info)\n        if task.status == PENDING and worker in task.workers:\n            locally_pending_tasks += 1\n            if len(task.workers) == 1:\n                n_unique_pending += 1\n        if not best_task and self._schedulable(task) and self._has_resources(task.resources, greedy_resources):\n            if worker in task.workers and self._has_resources(task.resources, used_resources):\n                best_task = task\n                best_task_id = task.id\n            else:\n                for (resource, amount) in (task.resources or {}).items():\n                    greedy_resources[resource] += amount\n    if best_task:\n        best_task.status = RUNNING\n        best_task.worker_running = worker\n        best_task.time_running = time.time()\n        self._update_task_history(best_task.id, RUNNING, host=host)\n    return {'n_pending_tasks': locally_pending_tasks, 'n_unique_pending': n_unique_pending, 'task_id': best_task_id, 'running_tasks': running_tasks}",
          "function_code_after": "def get_work(self, worker, host=None):\n    self.update(worker, {'host': host})\n    best_task = None\n    best_task_id = None\n    locally_pending_tasks = 0\n    running_tasks = []\n    used_resources = self._used_resources()\n    greedy_resources = collections.defaultdict(int)\n    n_unique_pending = 0\n    greedy_workers = dict(((worker.id, worker.info.get('workers', 1)) for worker in self._state.get_active_workers()))\n    tasks = list(self._state.get_pending_tasks())\n    tasks.sort(key=self._rank(), reverse=True)\n    for task in tasks:\n        if task.status == 'RUNNING' and worker in task.workers:\n            other_worker = self._state.get_worker(task.worker_running)\n            more_info = {'task_id': task.id, 'worker': str(other_worker)}\n            if other_worker is not None:\n                more_info.update(other_worker.info)\n                running_tasks.append(more_info)\n        if task.status == PENDING and worker in task.workers:\n            locally_pending_tasks += 1\n            if len(task.workers) == 1:\n                n_unique_pending += 1\n        if task.status == RUNNING and task.worker_running in greedy_workers:\n            greedy_workers[task.worker_running] -= 1\n            for (resource, amount) in (task.resources or {}).items():\n                greedy_resources[resource] += amount\n        if not best_task and self._schedulable(task) and self._has_resources(task.resources, greedy_resources):\n            if worker in task.workers and self._has_resources(task.resources, used_resources):\n                best_task = task\n                best_task_id = task.id\n            else:\n                for task_worker in task.workers:\n                    if greedy_workers.get(task_worker, 0) > 0:\n                        greedy_workers[task_worker] -= 1\n                        for (resource, amount) in (task.resources or {}).items():\n                            greedy_resources[resource] += amount\n                        break\n    if best_task:\n        best_task.status = RUNNING\n        best_task.worker_running = worker\n        best_task.time_running = time.time()\n        self._update_task_history(best_task.id, RUNNING, host=host)\n    return {'n_pending_tasks': locally_pending_tasks, 'n_unique_pending': n_unique_pending, 'task_id': best_task_id, 'running_tasks': running_tasks}",
          "function_before_start_line": 496,
          "function_before_end_line": 557,
          "function_after_start_line": 496,
          "function_after_end_line": 571,
          "function_before_token_count": 305,
          "function_after_token_count": 407,
          "functions_name_modified_file": [
            "add_failure",
            "get_worker_ids",
            "prune",
            "_schedulable",
            "clear",
            "has_excessive_failures",
            "_upstream_status",
            "_update_task_history",
            "can_disable",
            "ping",
            "get_pending_tasks",
            "_traverse_inverse_deps",
            "load",
            "inverse_dependencies",
            "num_failures",
            "task_search",
            "_recurse_deps",
            "get_worker",
            "get_active_workers",
            "worker_list",
            "add_task",
            "update_resources",
            "_update_priority",
            "task_list",
            "add_info",
            "get_active_tasks",
            "_has_resources",
            "_used_resources",
            "has_task",
            "_serialize_task",
            "task_history",
            "dep_graph",
            "__str__",
            "graph",
            "re_enable",
            "dump",
            "__repr__",
            "inactivate_tasks",
            "__init__",
            "update",
            "set_status",
            "add_worker",
            "get_task",
            "_rank",
            "get_work",
            "re_enable_task",
            "fetch_error",
            "inactivate_workers"
          ],
          "functions_name_all_files": [
            "add_failure",
            "test_disable",
            "can_disable",
            "test_can_work_on_lower_priority_while_waiting_for_resources",
            "load",
            "inverse_dependencies",
            "test_priorities_and_dependencies",
            "test_timeout",
            "test_scheduler_overprovisioned_on_other_resource",
            "add_task",
            "add_info",
            "_has_resources",
            "_serialize_task",
            "check_task_order",
            "test_prefer_readier_dependents",
            "test_no_lock_if_too_many_resources_required",
            "test_hendle_multiple_resources",
            "setUp",
            "__init__",
            "test_priority_update_with_pruning",
            "test_scheduler_with_priority_and_competing_resources",
            "test_prefer_more_dependents",
            "test_disconnect_running",
            "inactivate_workers",
            "test_priority_update_dependency_after_scheduling",
            "prune",
            "tearDown",
            "clear",
            "_upstream_status",
            "test_two_workers",
            "test_lock_resources_for_second_worker",
            "ping",
            "test_retry",
            "test_dep",
            "_traverse_inverse_deps",
            "_recurse_deps",
            "test_two_worker_info",
            "update_resources",
            "_update_priority",
            "test_disallowed_state_changes",
            "test_single_resource_lock",
            "test_multiple_resources_lock",
            "test_scheduler_resources_none_allow_one",
            "test_lock_resources_while_running_lower_priority",
            "__str__",
            "test_disable_and_done",
            "add_worker",
            "test_broken_dep",
            "test_priorities_default_and_negative",
            "test_scheduler_with_insufficient_resources",
            "test_update_resources",
            "test_ignore_done_dependents",
            "_schedulable",
            "has_excessive_failures",
            "test_unique_tasks",
            "test_scheduler_with_resources_used",
            "test_priority_no_decrease_with_multiple_updates",
            "worker_list",
            "get_active_tasks",
            "test_disable_and_reenable",
            "test_disable_and_reenable_and_disable_again",
            "task_history",
            "test_disable_by_worker",
            "test_multiple_resources_no_lock",
            "graph",
            "re_enable",
            "dump",
            "__repr__",
            "inactivate_tasks",
            "test_failed_dep",
            "get_task",
            "_rank",
            "test_do_not_lock_resources_while_running_higher_priority",
            "get_worker_ids",
            "_update_task_history",
            "test_remove_dep",
            "test_priority_update_dependency_chain",
            "get_pending_tasks",
            "num_failures",
            "task_search",
            "get_worker",
            "get_active_workers",
            "task_list",
            "_used_resources",
            "has_task",
            "test_do_not_lock_resources_when_not_ready",
            "dep_graph",
            "test_scheduler_with_sufficient_resources",
            "update",
            "test_scheduler_resources_none_disallow_two",
            "set_status",
            "test_lock_resources_when_one_of_multiple_workers_is_ready",
            "test_priorities",
            "setTime",
            "get_work",
            "re_enable_task",
            "fetch_error"
          ],
          "functions_name_co_evolved_modified_file": [],
          "functions_name_co_evolved_all_files": [
            "test_single_resource_lock",
            "test_multiple_resources_lock",
            "test_lock_resources_for_second_worker",
            "test_lock_resources_while_running_lower_priority",
            "test_lock_resources_when_one_of_multiple_workers_is_ready",
            "test_can_work_on_lower_priority_while_waiting_for_resources",
            "test_do_not_lock_resources_when_not_ready",
            "test_do_not_lock_resources_while_running_higher_priority"
          ]
        },
        "file": {
          "file_name": "scheduler.py",
          "file_nloc": 561,
          "file_complexity": 213,
          "file_token_count": 4172,
          "file_before": "# Copyright (c) 2012 Spotify AB\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"); you may not\n# use this file except in compliance with the License. You may obtain a copy of\n# the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n# License for the specific language governing permissions and limitations under\n# the License.\n\nimport collections\nimport datetime\nimport functools\nimport notifications\nimport os\nimport logging\nimport time\nimport cPickle as pickle\nimport bisect\nimport task_history as history\nlogger = logging.getLogger(\"luigi.server\")\n\nfrom task_status import PENDING, FAILED, DONE, RUNNING, SUSPENDED, UNKNOWN, DISABLED\n\n\nclass Scheduler(object):\n    ''' Abstract base class\n\n    Note that the methods all take string arguments, not Task objects...\n    '''\n    add_task = NotImplemented\n    get_work = NotImplemented\n    ping = NotImplemented\n\nUPSTREAM_RUNNING = 'UPSTREAM_RUNNING'\nUPSTREAM_MISSING_INPUT = 'UPSTREAM_MISSING_INPUT'\nUPSTREAM_FAILED = 'UPSTREAM_FAILED'\nUPSTREAM_DISABLED = 'UPSTREAM_DISABLED'\n\nUPSTREAM_SEVERITY_ORDER = (\n    '',\n    UPSTREAM_RUNNING,\n    UPSTREAM_MISSING_INPUT,\n    UPSTREAM_FAILED,\n    UPSTREAM_DISABLED,\n)\nUPSTREAM_SEVERITY_KEY = lambda st: UPSTREAM_SEVERITY_ORDER.index(st)\nSTATUS_TO_UPSTREAM_MAP = {\n    FAILED: UPSTREAM_FAILED,\n    RUNNING: UPSTREAM_RUNNING,\n    PENDING: UPSTREAM_MISSING_INPUT,\n    DISABLED: UPSTREAM_DISABLED,\n}\n\n\nclass Failures(object):\n    \"\"\" This class tracks the number of failures in a given time window\n\n    Failures added are marked with the current timestamp, and this class counts\n    the number of failures in a sliding time window ending at the present.\n\n    \"\"\"\n\n    def __init__(self, window):\n        \"\"\" Initialize with the given window\n\n        :param window: how long to track failures for, as a datetime.timedelta\n        \"\"\"\n        self.window = window\n        self.failures = collections.deque()\n\n    def add_failure(self):\n        \"\"\" Add a failure event with the current timestamp \"\"\"\n        self.failures.append(datetime.datetime.now())\n\n    def num_failures(self):\n        \"\"\" Return the number of failures in the window \"\"\"\n        min_time = datetime.datetime.now() - self.window\n        while self.failures and self.failures[0] < min_time:\n            self.failures.popleft()\n        return len(self.failures)\n\n    def clear(self):\n        \"\"\" Clear the failure queue \"\"\"\n        self.failures.clear()\n\n\nclass Task(object):\n    def __init__(self, id, status, deps, resources={}, priority=0, family='', params={},\n                 disable_failures=None, disable_window=None):\n        self.id = id\n        self.stakeholders = set()  # workers ids that are somehow related to this task (i.e. don't prune while any of these workers are still active)\n        self.workers = set()  # workers ids that can perform task - task is 'BROKEN' if none of these workers are active\n        if deps is None:\n            self.deps = set()\n        else:\n            self.deps = set(deps)\n        self.status = status  # PENDING, RUNNING, FAILED or DONE\n        self.time = time.time()  # Timestamp when task was first added\n        self.retry = None\n        self.remove = None\n        self.worker_running = None  # the worker id that is currently running the task or None\n        self.time_running = None  # Timestamp when picked up by worker\n        self.expl = None\n        self.priority = priority\n        self.resources = resources\n        self.family = family\n        self.params = params\n        self.disable_failures = disable_failures\n        self.failures = Failures(disable_window)\n        self.scheduler_disable_time = None\n\n    def __repr__(self):\n        return \"Task(%r)\" % vars(self)\n\n    def add_failure(self):\n        self.failures.add_failure()\n\n    def has_excessive_failures(self):\n        return self.failures.num_failures() >= self.disable_failures\n\n    def can_disable(self):\n        return self.disable_failures is not None\n\n    def re_enable(self):\n        self.scheduler_disable_time = None\n        self.status = FAILED\n        self.failures.clear()\n\n\nclass Worker(object):\n    \"\"\" Structure for tracking worker activity and keeping their references \"\"\"\n    def __init__(self, id, last_active=None):\n        self.id = id\n        self.reference = None  # reference to the worker in the real world. (Currently a dict containing just the host)\n        self.last_active = last_active  # seconds since epoch\n        self.started = time.time()  # seconds since epoch\n        self.info = {}\n\n    def add_info(self, info):\n        self.info.update(info)\n\n    def __str__(self):\n        return self.id\n\n\nclass SimpleTaskState(object):\n    ''' Keep track of the current state and handle persistance\n\n    The point of this class is to enable other ways to keep state, eg. by using a database\n    These will be implemented by creating an abstract base class that this and other classes\n    inherit from.\n    '''\n\n    def __init__(self, state_path):\n        self._state_path = state_path\n        self._tasks = {}  # map from id to a Task object\n        self._active_workers = {}  # map from id to a Worker object\n\n    def dump(self):\n        state = (self._tasks, self._active_workers)\n        try:\n            with open(self._state_path, 'w') as fobj:\n                pickle.dump(state, fobj)\n        except IOError:\n            logger.warning(\"Failed saving scheduler state\", exc_info=1)\n        else:\n            logger.info(\"Saved state in %s\", self._state_path)\n\n    # prone to lead to crashes when old state is unpickled with updated code. TODO some kind of version control?\n    def load(self):\n        if os.path.exists(self._state_path):\n            logger.info(\"Attempting to load state from %s\", self._state_path)\n            try:\n                with open(self._state_path) as fobj:\n                    state = pickle.load(fobj)\n            except:\n                logger.exception(\"Error when loading state. Starting from clean slate.\")\n                return\n\n            self._tasks, self._active_workers = state\n\n            # Convert from old format\n            # TODO: this is really ugly, we need something more future-proof\n            # Every time we add an attribute to the Worker class, this code needs to be updated\n            for k, v in self._active_workers.iteritems():\n                if isinstance(v, float):\n                    self._active_workers[k] = Worker(id=k, last_active=v)\n        else:\n            logger.info(\"No prior state file exists at %s. Starting with clean slate\", self._state_path)\n\n    def get_active_tasks(self):\n        for task in self._tasks.itervalues():\n            yield task\n\n    def get_pending_tasks(self):\n        for task in self._tasks.itervalues():\n            if task.status in [PENDING, RUNNING]:\n                yield task\n\n    def get_task(self, task_id, default=None, setdefault=None):\n        if setdefault:\n            return self._tasks.setdefault(task_id, setdefault)\n        else:\n            return self._tasks.get(task_id, default)\n\n    def has_task(self, task_id):\n        return task_id in self._tasks\n\n    def inactivate_tasks(self, delete_tasks):\n        # The terminology is a bit confusing: we used to \"delete\" tasks when they became inactive,\n        # but with a pluggable state storage, you might very well want to keep some history of\n        # older tasks as well. That's why we call it \"inactivate\" (as in the verb)\n        for task in delete_tasks:\n            self._tasks.pop(task)\n\n    def get_active_workers(self, last_active_lt=None):\n        for worker in self._active_workers.itervalues():\n            if last_active_lt is not None and worker.last_active >= last_active_lt:\n                continue\n            yield worker\n\n    def get_worker_ids(self):\n        return self._active_workers.keys() # only used for unit tests\n\n    def get_worker(self, worker_id):\n        return self._active_workers.setdefault(worker_id, Worker(worker_id))\n\n    def inactivate_workers(self, delete_workers):\n        # Mark workers as inactive\n        for worker in delete_workers:\n            self._active_workers.pop(worker)\n\n        # remove workers from tasks\n        for task in self.get_active_tasks():\n            task.stakeholders.difference_update(delete_workers)\n            task.workers.difference_update(delete_workers)\n\n\nclass CentralPlannerScheduler(Scheduler):\n    ''' Async scheduler that can handle multiple workers etc\n\n    Can be run locally or on a server (using RemoteScheduler + server.Server).\n    '''\n\n    def __init__(self, retry_delay=900.0, remove_delay=600.0, worker_disconnect_delay=60.0,\n                 state_path='/var/lib/luigi-server/state.pickle', task_history=None,\n                 resources=None, disable_persist=0, disable_window=0, disable_failures=None):\n        '''\n        (all arguments are in seconds)\n        Keyword Arguments:\n        retry_delay -- How long after a Task fails to try it again, or -1 to never retry\n        remove_delay -- How long after a Task finishes to remove it from the scheduler\n        state_path -- Path to state file (tasks and active workers)\n        worker_disconnect_delay -- If a worker hasn't communicated for this long, remove it from active workers\n        '''\n        self._retry_delay = retry_delay\n        self._remove_delay = remove_delay\n        self._worker_disconnect_delay = worker_disconnect_delay\n        self._task_history = task_history or history.NopHistory()\n        self._state = SimpleTaskState(state_path)\n\n        self._task_history = task_history or history.NopHistory()\n        self._resources = resources\n        self._disable_failures = disable_failures\n        self._disable_window = disable_window\n        self._make_task = functools.partial(\n            Task, disable_failures=disable_failures,\n            disable_window=datetime.timedelta(seconds=disable_window))\n        self._disable_persist = disable_persist\n        self._disable_time = datetime.timedelta(seconds=disable_persist)\n\n    def load(self):\n        self._state.load()\n\n    def dump(self):\n        self._state.dump()\n\n    def prune(self):\n        logger.info(\"Starting pruning of task graph\")\n        # Delete workers that haven't said anything for a while (probably killed)\n        delete_workers = []\n        for worker in self._state.get_active_workers(last_active_lt=time.time() - self._worker_disconnect_delay):\n            logger.info(\"Worker %s timed out (no contact for >=%ss)\", worker, self._worker_disconnect_delay)\n            delete_workers.append(worker.id)\n\n        self._state.inactivate_workers(delete_workers)\n\n        delete_workers = set(delete_workers)\n\n        remove_tasks = []\n        for task in self._state.get_active_tasks():\n            # Mark tasks with no remaining active stakeholders for deletion\n            if not task.stakeholders:\n                if task.remove is None:\n                    logger.info(\"Task %r has stakeholders %r but none remain connected -> will remove task in %s seconds\", task.id, task.stakeholders, self._remove_delay)\n                    task.remove = time.time() + self._remove_delay\n\n            # If a running worker disconnects, tag all its jobs as FAILED and subject it to the same retry logic\n            if task.status == RUNNING and task.worker_running and task.worker_running not in task.stakeholders:\n                logger.info(\"Task %r is marked as running by disconnected worker %r -> marking as FAILED with retry delay of %rs\", task.id, task.worker_running, self._retry_delay)\n                task.worker_running = None\n                self.set_status(task, FAILED)\n                task.retry = time.time() + self._retry_delay\n\n            if task.status == DISABLED and task.scheduler_disable_time:\n                # re-enable task after the disable time expires\n                if datetime.datetime.now() - task.scheduler_disable_time > self._disable_time:\n                    task.re_enable()\n\n            # Remove tasks that have no stakeholders\n            if task.remove and time.time() > task.remove:\n                logger.info(\"Removing task %r (no connected stakeholders)\", task.id)\n                remove_tasks.append(task.id)\n\n        # Reset FAILED tasks to PENDING if max timeout is reached, and retry delay is >= 0\n        for task in self._state.get_active_tasks():\n            if task.status == FAILED and self._retry_delay >= 0 and task.retry < time.time():\n                self.set_status(task, PENDING)\n\n        self._state.inactivate_tasks(remove_tasks)\n\n        logger.info(\"Done pruning task graph\")\n\n    def set_status(self, task, new_status):\n        # not sure why we have SUSPENDED, as it can never be set\n        if new_status == SUSPENDED:\n            new_status = PENDING\n\n        if new_status == DISABLED and task.status == RUNNING:\n            return\n\n        if task.status == DISABLED:\n            if new_status == DISABLED:\n                task.scheduler_disable_time = None\n            elif new_status == DONE:\n                task.re_enable()\n                task.status = DONE\n            elif task.scheduler_disable_time is None:\n                # when it is disabled by client, we allow the status change\n                task.status = new_status\n            return\n\n        if new_status == FAILED and task.can_disable():\n            task.add_failure()\n            if task.has_excessive_failures():\n                task.scheduler_disable_time = datetime.datetime.now()\n                new_status = DISABLED\n                notifications.send_error_email(\n                    'Luigi Scheduler: DISABLED {task} due to excessive failures'.format(task=task.id),\n                    '{task} failed {failures} times in the last {window} seconds, so it is being '\n                    'disabled for {persist} seconds'.format(\n                        failures=self._disable_failures,\n                        task=task.id,\n                        window=self._disable_window,\n                        persist=self._disable_persist,\n                        ))\n        elif new_status == DISABLED:\n            task.scheduler_disable_time = None\n\n        task.status = new_status\n\n    def update(self, worker_id, worker_reference=None):\n        \"\"\" Keep track of whenever the worker was last active \"\"\"\n        worker = self._state.get_worker(worker_id)\n        if worker_reference:\n            worker.reference = worker_reference\n        worker.last_active = time.time()\n\n    def _update_priority(self, task, prio, worker):\n        \"\"\" Update priority of the given task\n\n        Priority can only be increased. If the task doesn't exist, a placeholder\n        task is created to preserve priority when the task is later scheduled.\n        \"\"\"\n        task.priority = prio = max(prio, task.priority)\n        for dep in task.deps or []:\n            t = self._state.get_task(dep)\n            if t is not None and prio > t.priority:\n                self._update_priority(t, prio, worker)\n\n    def add_task(self, worker, task_id, status=PENDING, runnable=True,\n                 deps=None, new_deps=None, expl=None, resources=None,\n                 priority=0, family='', params={}):\n        \"\"\"\n        * Add task identified by task_id if it doesn't exist\n        * If deps is not None, update dependency list\n        * Update status of task\n        * Add additional workers/stakeholders\n        * Update priority when needed\n        \"\"\"\n        self.update(worker)\n\n        task = self._state.get_task(task_id, setdefault=self._make_task(\n                id=task_id, status=PENDING, deps=deps, resources=resources,\n                priority=priority, family=family, params=params))\n\n        # for setting priority, we'll sometimes create tasks with unset family and params\n        if not task.family:\n            task.family = family\n        if not task.params:\n            task.params = params\n\n        if task.remove is not None:\n            task.remove = None  # unmark task for removal so it isn't removed after being added\n\n        if not (task.status == RUNNING and status == PENDING):\n            # don't allow re-scheduling of task while it is running, it must either fail or succeed first\n            if status == PENDING or status != task.status:\n                # Update the DB only if there was a acctual change, to prevent noise.\n                # We also check for status == PENDING b/c that's the default value\n                # (so checking for status != task.status woule lie)\n                self._update_task_history(task_id, status)\n            self.set_status(task, PENDING if status == SUSPENDED else status)\n            if status == FAILED:\n                task.retry = time.time() + self._retry_delay\n\n        if deps is not None:\n            task.deps = set(deps)\n\n        if new_deps is not None:\n            task.deps.update(new_deps)\n\n        task.stakeholders.add(worker)\n        task.resources = resources\n\n        # Task dependencies might not exist yet. Let's create dummy tasks for them for now.\n        # Otherwise the task dependencies might end up being pruned if scheduling takes a long time\n        for dep in task.deps or []:\n            t = self._state.get_task(dep, setdefault=self._make_task(id=dep, status=UNKNOWN, deps=None, priority=priority))\n            t.stakeholders.add(worker)\n\n        self._update_priority(task, priority, worker)\n\n        if runnable:\n            task.workers.add(worker)\n\n        if expl is not None:\n            task.expl = expl\n\n    def add_worker(self, worker, info):\n        self._state.get_worker(worker).add_info(info)\n\n    def update_resources(self, **resources):\n        if self._resources is None:\n            self._resources = {}\n        self._resources.update(resources)\n\n    def _has_resources(self, needed_resources, used_resources):\n        if needed_resources is None:\n            return True\n\n        available_resources = self._resources or {}\n        for resource, amount in needed_resources.items():\n            if amount + used_resources[resource] > available_resources.get(resource, 1):\n                return False\n        return True\n\n    def _used_resources(self):\n        used_resources = collections.defaultdict(int)\n        if self._resources is not None:\n            for task in self._state.get_active_tasks():\n                if task.status == RUNNING and task.resources:\n                    for resource, amount in task.resources.items():\n                        used_resources[resource] += amount\n        return used_resources\n\n    def _rank(self):\n        ''' Return worker's rank function for task scheduling '''\n        dependents = collections.defaultdict(int)\n        def not_done(t):\n            task = self._state.get_task(t, default=None)\n            return task is None or task.status != DONE\n        for task in self._state.get_active_tasks():\n            if task.status != DONE:\n                deps = filter(not_done, task.deps)\n                inverse_num_deps = 1.0 / max(len(deps), 1)\n                for dep in deps:\n                    dependents[dep] += inverse_num_deps\n\n        return lambda task: (task.priority, dependents[task.id], -task.time)\n\n    def _schedulable(self, task):\n        if task.status != PENDING:\n            return False\n        for dep in task.deps:\n            dep_task = self._state.get_task(dep, default=None)\n            if dep_task is None or dep_task.status != DONE:\n                return False\n        return True\n\n    def get_work(self, worker, host=None):\n        # TODO: remove any expired nodes\n\n        # Algo: iterate over all nodes, find the highest priority node no dependencies and available\n        # resources.\n\n        # Resource checking looks both at currently available resources and at which resources would\n        # be available if all running tasks died and we rescheduled all workers greedily. We do both\n        # checks in order to prevent a worker with many low-priority tasks from starving other\n        # workers with higher priority tasks that share the same resources.\n\n        # TODO: remove tasks that can't be done, figure out if the worker has absolutely\n        # nothing it can wait for\n\n        # Return remaining tasks that have no FAILED descendents\n        self.update(worker, {'host': host})\n        best_task = None\n        best_task_id = None\n        locally_pending_tasks = 0\n        running_tasks = []\n\n        used_resources = self._used_resources()\n        greedy_resources = collections.defaultdict(int)\n        n_unique_pending = 0\n\n        tasks = list(self._state.get_pending_tasks())\n        tasks.sort(key=self._rank(), reverse=True)\n\n        for task in tasks:\n            if task.status == 'RUNNING' and worker in task.workers:\n                # Return a list of currently running tasks to the client,\n                # makes it easier to troubleshoot\n                other_worker = self._state.get_worker(task.worker_running)\n                more_info = {'task_id': task.id, 'worker': str(other_worker)}\n                if other_worker is not None:\n                    more_info.update(other_worker.info)\n                    running_tasks.append(more_info)\n\n            if task.status == PENDING and worker in task.workers:\n                locally_pending_tasks += 1\n                if len(task.workers) == 1:\n                    n_unique_pending += 1\n\n            if not best_task and self._schedulable(task) and self._has_resources(task.resources, greedy_resources):\n                if worker in task.workers and self._has_resources(task.resources, used_resources):\n                    best_task = task\n                    best_task_id = task.id\n                else:\n                    # keep track of the resources used in greedy scheduling\n                    for resource, amount in (task.resources or {}).items():\n                        greedy_resources[resource] += amount\n\n        if best_task:\n            best_task.status = RUNNING\n            best_task.worker_running = worker\n            best_task.time_running = time.time()\n            self._update_task_history(best_task.id, RUNNING, host=host)\n\n        return {'n_pending_tasks': locally_pending_tasks,\n                'n_unique_pending': n_unique_pending,\n                'task_id': best_task_id,\n                'running_tasks': running_tasks}\n\n    def ping(self, worker):\n        self.update(worker)\n\n    def _upstream_status(self, task_id, upstream_status_table):\n        if task_id in upstream_status_table:\n            return upstream_status_table[task_id]\n        elif self._state.has_task(task_id):\n            task_stack = [task_id]\n\n            while task_stack:\n                dep_id = task_stack.pop()\n                if self._state.has_task(dep_id):\n                    dep = self._state.get_task(dep_id)\n                    if dep_id not in upstream_status_table:\n                        if dep.status == PENDING and dep.deps:\n                            task_stack = task_stack + [dep_id] + list(dep.deps)\n                            upstream_status_table[dep_id] = ''  # will be updated postorder\n                        else:\n                            dep_status = STATUS_TO_UPSTREAM_MAP.get(dep.status, '')\n                            upstream_status_table[dep_id] = dep_status\n                    elif upstream_status_table[dep_id] == '' and dep.deps:\n                        # This is the postorder update step when we set the\n                        # status based on the previously calculated child elements\n                        upstream_status = [upstream_status_table.get(id, '') for id in dep.deps]\n                        upstream_status.append('')  # to handle empty list\n                        status = max(upstream_status, key=UPSTREAM_SEVERITY_KEY)\n                        upstream_status_table[dep_id] = status\n            return upstream_status_table[dep_id]\n\n    def _serialize_task(self, task_id, include_deps=True):\n        task = self._state.get_task(task_id)\n        ret = {\n            'deps': list(task.deps),\n            'status': task.status,\n            'workers': list(task.workers),\n            'worker_running': task.worker_running,\n            'time_running': getattr(task, \"time_running\", None),\n            'start_time': task.time,\n            'params': task.params,\n            'name': task.family,\n            'priority': task.priority,\n            'resources': task.resources,\n        }\n        if include_deps:\n            ret['deps'] = list(task.deps)\n        return ret\n\n    def graph(self):\n        self.prune()\n        serialized = {}\n        for task in self._state.get_active_tasks():\n            serialized[task.id] = self._serialize_task(task.id)\n        return serialized\n\n    def _recurse_deps(self, task_id, serialized):\n        if task_id not in serialized:\n            task = self._state.get_task(task_id)\n            if task is None or not task.family:\n                logger.warn('Missing task for id [%s]', task_id)\n\n                # try to infer family and params from task_id\n                try:\n                    family, _, param_str = task_id.rstrip(')').partition('(')\n                    params = dict(param.split('=') for param in param_str.split(', '))\n                except:\n                    family, params = '', {}\n                serialized[task_id] = {\n                    'deps': [],\n                    'status': UNKNOWN,\n                    'workers': [],\n                    'start_time': UNKNOWN,\n                    'params': params,\n                    'name': family,\n                    'priority': 0,\n                }\n            else:\n                serialized[task_id] = self._serialize_task(task_id)\n                for dep in task.deps:\n                    self._recurse_deps(dep, serialized)\n\n    def dep_graph(self, task_id):\n        self.prune()\n        serialized = {}\n        if self._state.has_task(task_id):\n            self._recurse_deps(task_id, serialized)\n        return serialized\n\n    def task_list(self, status, upstream_status):\n        ''' query for a subset of tasks by status '''\n        self.prune()\n        result = {}\n        upstream_status_table = {}  # used to memoize upstream status\n        for task in self._state.get_active_tasks():\n            if not status or task.status == status:\n                if (task.status != PENDING or not upstream_status or\n                    upstream_status == self._upstream_status(task.id, upstream_status_table)):\n                    serialized = self._serialize_task(task.id, False)\n                    result[task.id] = serialized\n        return result\n\n    def worker_list(self, include_running=True):\n        self.prune()\n        workers = [\n            dict(\n                name=worker.id,\n                last_active=worker.last_active,\n                started=getattr(worker, 'started', None),\n                **worker.info\n            ) for worker in self._state.get_active_workers()]\n        workers.sort(key=lambda worker: worker['started'], reverse=True)\n        if include_running:\n            running = collections.defaultdict(dict)\n            num_pending = collections.defaultdict(int)\n            num_uniques = collections.defaultdict(int)\n            for task in self._state.get_pending_tasks():\n                if task.status == RUNNING and task.worker_running:\n                    running[task.worker_running][task.id] = self._serialize_task(task.id, False)\n                elif task.status == PENDING:\n                    for worker in task.workers:\n                        num_pending[worker] += 1\n                    if len(task.workers) == 1:\n                        num_uniques[list(task.workers)[0]] += 1\n            for worker in workers:\n                tasks = running[worker['name']]\n                worker['num_running'] = len(tasks)\n                worker['num_pending'] = num_pending[worker['name']]\n                worker['num_uniques'] = num_uniques[worker['name']]\n                worker['running'] = tasks\n        return workers\n\n    def inverse_dependencies(self, task_id):\n        self.prune()\n        serialized = {}\n        if self._state.has_task(task_id):\n            self._traverse_inverse_deps(task_id, serialized)\n        return serialized\n\n    def _traverse_inverse_deps(self, task_id, serialized):\n        stack = [task_id]\n        serialized[task_id] = self._serialize_task(task_id)\n        while len(stack) > 0:\n            curr_id = stack.pop()\n            for task in self._state.get_active_tasks():\n                if curr_id in task.deps:\n                    serialized[curr_id][\"deps\"].append(task.id)\n                    if task.id not in serialized:\n                        serialized[task.id] = self._serialize_task(task.id)\n                        serialized[task.id][\"deps\"] = []\n                        stack.append(task.id)\n\n    def task_search(self, task_str):\n        ''' query for a subset of tasks by task_id '''\n        self.prune()\n        result = collections.defaultdict(dict)\n        for task in self._state.get_active_tasks():\n            if task.id.find(task_str) != -1:\n                serialized = self._serialize_task(task.id, False)\n                result[task.status][task.id] = serialized\n        return result\n\n    def re_enable_task(self, task_id):\n        serialized = {}\n        task = self._state.get_task(task_id)\n        if task and task.status == DISABLED and task.scheduler_disable_time:\n            task.re_enable()\n            serialized = self._serialize_task(task_id)\n        return serialized\n\n    def fetch_error(self, task_id):\n        if self._state.has_task(task_id):\n            return {\"taskId\": task_id, \"error\": self._state.get_task(task_id).expl}\n        else:\n            return {\"taskId\": task_id, \"error\": \"\"}\n\n    def _update_task_history(self, task_id, status, host=None):\n        try:\n            if status == DONE or status == FAILED:\n                successful = (status == DONE)\n                self._task_history.task_finished(task_id, successful)\n            elif status == PENDING:\n                self._task_history.task_scheduled(task_id)\n            elif status == RUNNING:\n                self._task_history.task_started(task_id, host)\n        except:\n            logger.warning(\"Error saving Task history\", exc_info=1)\n\n    @property\n    def task_history(self):\n        # Used by server.py to expose the calls\n        return self._task_history\n",
          "file_after": "# Copyright (c) 2012 Spotify AB\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"); you may not\n# use this file except in compliance with the License. You may obtain a copy of\n# the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n# License for the specific language governing permissions and limitations under\n# the License.\n\nimport collections\nimport datetime\nimport functools\nimport notifications\nimport os\nimport logging\nimport time\nimport cPickle as pickle\nimport bisect\nimport task_history as history\nlogger = logging.getLogger(\"luigi.server\")\n\nfrom task_status import PENDING, FAILED, DONE, RUNNING, SUSPENDED, UNKNOWN, DISABLED\n\n\nclass Scheduler(object):\n    ''' Abstract base class\n\n    Note that the methods all take string arguments, not Task objects...\n    '''\n    add_task = NotImplemented\n    get_work = NotImplemented\n    ping = NotImplemented\n\nUPSTREAM_RUNNING = 'UPSTREAM_RUNNING'\nUPSTREAM_MISSING_INPUT = 'UPSTREAM_MISSING_INPUT'\nUPSTREAM_FAILED = 'UPSTREAM_FAILED'\nUPSTREAM_DISABLED = 'UPSTREAM_DISABLED'\n\nUPSTREAM_SEVERITY_ORDER = (\n    '',\n    UPSTREAM_RUNNING,\n    UPSTREAM_MISSING_INPUT,\n    UPSTREAM_FAILED,\n    UPSTREAM_DISABLED,\n)\nUPSTREAM_SEVERITY_KEY = lambda st: UPSTREAM_SEVERITY_ORDER.index(st)\nSTATUS_TO_UPSTREAM_MAP = {\n    FAILED: UPSTREAM_FAILED,\n    RUNNING: UPSTREAM_RUNNING,\n    PENDING: UPSTREAM_MISSING_INPUT,\n    DISABLED: UPSTREAM_DISABLED,\n}\n\n\nclass Failures(object):\n    \"\"\" This class tracks the number of failures in a given time window\n\n    Failures added are marked with the current timestamp, and this class counts\n    the number of failures in a sliding time window ending at the present.\n\n    \"\"\"\n\n    def __init__(self, window):\n        \"\"\" Initialize with the given window\n\n        :param window: how long to track failures for, as a datetime.timedelta\n        \"\"\"\n        self.window = window\n        self.failures = collections.deque()\n\n    def add_failure(self):\n        \"\"\" Add a failure event with the current timestamp \"\"\"\n        self.failures.append(datetime.datetime.now())\n\n    def num_failures(self):\n        \"\"\" Return the number of failures in the window \"\"\"\n        min_time = datetime.datetime.now() - self.window\n        while self.failures and self.failures[0] < min_time:\n            self.failures.popleft()\n        return len(self.failures)\n\n    def clear(self):\n        \"\"\" Clear the failure queue \"\"\"\n        self.failures.clear()\n\n\nclass Task(object):\n    def __init__(self, id, status, deps, resources={}, priority=0, family='', params={},\n                 disable_failures=None, disable_window=None):\n        self.id = id\n        self.stakeholders = set()  # workers ids that are somehow related to this task (i.e. don't prune while any of these workers are still active)\n        self.workers = set()  # workers ids that can perform task - task is 'BROKEN' if none of these workers are active\n        if deps is None:\n            self.deps = set()\n        else:\n            self.deps = set(deps)\n        self.status = status  # PENDING, RUNNING, FAILED or DONE\n        self.time = time.time()  # Timestamp when task was first added\n        self.retry = None\n        self.remove = None\n        self.worker_running = None  # the worker id that is currently running the task or None\n        self.time_running = None  # Timestamp when picked up by worker\n        self.expl = None\n        self.priority = priority\n        self.resources = resources\n        self.family = family\n        self.params = params\n        self.disable_failures = disable_failures\n        self.failures = Failures(disable_window)\n        self.scheduler_disable_time = None\n\n    def __repr__(self):\n        return \"Task(%r)\" % vars(self)\n\n    def add_failure(self):\n        self.failures.add_failure()\n\n    def has_excessive_failures(self):\n        return self.failures.num_failures() >= self.disable_failures\n\n    def can_disable(self):\n        return self.disable_failures is not None\n\n    def re_enable(self):\n        self.scheduler_disable_time = None\n        self.status = FAILED\n        self.failures.clear()\n\n\nclass Worker(object):\n    \"\"\" Structure for tracking worker activity and keeping their references \"\"\"\n    def __init__(self, id, last_active=None):\n        self.id = id\n        self.reference = None  # reference to the worker in the real world. (Currently a dict containing just the host)\n        self.last_active = last_active  # seconds since epoch\n        self.started = time.time()  # seconds since epoch\n        self.info = {}\n\n    def add_info(self, info):\n        self.info.update(info)\n\n    def __str__(self):\n        return self.id\n\n\nclass SimpleTaskState(object):\n    ''' Keep track of the current state and handle persistance\n\n    The point of this class is to enable other ways to keep state, eg. by using a database\n    These will be implemented by creating an abstract base class that this and other classes\n    inherit from.\n    '''\n\n    def __init__(self, state_path):\n        self._state_path = state_path\n        self._tasks = {}  # map from id to a Task object\n        self._active_workers = {}  # map from id to a Worker object\n\n    def dump(self):\n        state = (self._tasks, self._active_workers)\n        try:\n            with open(self._state_path, 'w') as fobj:\n                pickle.dump(state, fobj)\n        except IOError:\n            logger.warning(\"Failed saving scheduler state\", exc_info=1)\n        else:\n            logger.info(\"Saved state in %s\", self._state_path)\n\n    # prone to lead to crashes when old state is unpickled with updated code. TODO some kind of version control?\n    def load(self):\n        if os.path.exists(self._state_path):\n            logger.info(\"Attempting to load state from %s\", self._state_path)\n            try:\n                with open(self._state_path) as fobj:\n                    state = pickle.load(fobj)\n            except:\n                logger.exception(\"Error when loading state. Starting from clean slate.\")\n                return\n\n            self._tasks, self._active_workers = state\n\n            # Convert from old format\n            # TODO: this is really ugly, we need something more future-proof\n            # Every time we add an attribute to the Worker class, this code needs to be updated\n            for k, v in self._active_workers.iteritems():\n                if isinstance(v, float):\n                    self._active_workers[k] = Worker(id=k, last_active=v)\n        else:\n            logger.info(\"No prior state file exists at %s. Starting with clean slate\", self._state_path)\n\n    def get_active_tasks(self):\n        for task in self._tasks.itervalues():\n            yield task\n\n    def get_pending_tasks(self):\n        for task in self._tasks.itervalues():\n            if task.status in [PENDING, RUNNING]:\n                yield task\n\n    def get_task(self, task_id, default=None, setdefault=None):\n        if setdefault:\n            return self._tasks.setdefault(task_id, setdefault)\n        else:\n            return self._tasks.get(task_id, default)\n\n    def has_task(self, task_id):\n        return task_id in self._tasks\n\n    def inactivate_tasks(self, delete_tasks):\n        # The terminology is a bit confusing: we used to \"delete\" tasks when they became inactive,\n        # but with a pluggable state storage, you might very well want to keep some history of\n        # older tasks as well. That's why we call it \"inactivate\" (as in the verb)\n        for task in delete_tasks:\n            self._tasks.pop(task)\n\n    def get_active_workers(self, last_active_lt=None):\n        for worker in self._active_workers.itervalues():\n            if last_active_lt is not None and worker.last_active >= last_active_lt:\n                continue\n            yield worker\n\n    def get_worker_ids(self):\n        return self._active_workers.keys() # only used for unit tests\n\n    def get_worker(self, worker_id):\n        return self._active_workers.setdefault(worker_id, Worker(worker_id))\n\n    def inactivate_workers(self, delete_workers):\n        # Mark workers as inactive\n        for worker in delete_workers:\n            self._active_workers.pop(worker)\n\n        # remove workers from tasks\n        for task in self.get_active_tasks():\n            task.stakeholders.difference_update(delete_workers)\n            task.workers.difference_update(delete_workers)\n\n\nclass CentralPlannerScheduler(Scheduler):\n    ''' Async scheduler that can handle multiple workers etc\n\n    Can be run locally or on a server (using RemoteScheduler + server.Server).\n    '''\n\n    def __init__(self, retry_delay=900.0, remove_delay=600.0, worker_disconnect_delay=60.0,\n                 state_path='/var/lib/luigi-server/state.pickle', task_history=None,\n                 resources=None, disable_persist=0, disable_window=0, disable_failures=None):\n        '''\n        (all arguments are in seconds)\n        Keyword Arguments:\n        retry_delay -- How long after a Task fails to try it again, or -1 to never retry\n        remove_delay -- How long after a Task finishes to remove it from the scheduler\n        state_path -- Path to state file (tasks and active workers)\n        worker_disconnect_delay -- If a worker hasn't communicated for this long, remove it from active workers\n        '''\n        self._retry_delay = retry_delay\n        self._remove_delay = remove_delay\n        self._worker_disconnect_delay = worker_disconnect_delay\n        self._task_history = task_history or history.NopHistory()\n        self._state = SimpleTaskState(state_path)\n\n        self._task_history = task_history or history.NopHistory()\n        self._resources = resources\n        self._disable_failures = disable_failures\n        self._disable_window = disable_window\n        self._make_task = functools.partial(\n            Task, disable_failures=disable_failures,\n            disable_window=datetime.timedelta(seconds=disable_window))\n        self._disable_persist = disable_persist\n        self._disable_time = datetime.timedelta(seconds=disable_persist)\n\n    def load(self):\n        self._state.load()\n\n    def dump(self):\n        self._state.dump()\n\n    def prune(self):\n        logger.info(\"Starting pruning of task graph\")\n        # Delete workers that haven't said anything for a while (probably killed)\n        delete_workers = []\n        for worker in self._state.get_active_workers(last_active_lt=time.time() - self._worker_disconnect_delay):\n            logger.info(\"Worker %s timed out (no contact for >=%ss)\", worker, self._worker_disconnect_delay)\n            delete_workers.append(worker.id)\n\n        self._state.inactivate_workers(delete_workers)\n\n        delete_workers = set(delete_workers)\n\n        remove_tasks = []\n        for task in self._state.get_active_tasks():\n            # Mark tasks with no remaining active stakeholders for deletion\n            if not task.stakeholders:\n                if task.remove is None:\n                    logger.info(\"Task %r has stakeholders %r but none remain connected -> will remove task in %s seconds\", task.id, task.stakeholders, self._remove_delay)\n                    task.remove = time.time() + self._remove_delay\n\n            # If a running worker disconnects, tag all its jobs as FAILED and subject it to the same retry logic\n            if task.status == RUNNING and task.worker_running and task.worker_running not in task.stakeholders:\n                logger.info(\"Task %r is marked as running by disconnected worker %r -> marking as FAILED with retry delay of %rs\", task.id, task.worker_running, self._retry_delay)\n                task.worker_running = None\n                self.set_status(task, FAILED)\n                task.retry = time.time() + self._retry_delay\n\n            if task.status == DISABLED and task.scheduler_disable_time:\n                # re-enable task after the disable time expires\n                if datetime.datetime.now() - task.scheduler_disable_time > self._disable_time:\n                    task.re_enable()\n\n            # Remove tasks that have no stakeholders\n            if task.remove and time.time() > task.remove:\n                logger.info(\"Removing task %r (no connected stakeholders)\", task.id)\n                remove_tasks.append(task.id)\n\n        # Reset FAILED tasks to PENDING if max timeout is reached, and retry delay is >= 0\n        for task in self._state.get_active_tasks():\n            if task.status == FAILED and self._retry_delay >= 0 and task.retry < time.time():\n                self.set_status(task, PENDING)\n\n        self._state.inactivate_tasks(remove_tasks)\n\n        logger.info(\"Done pruning task graph\")\n\n    def set_status(self, task, new_status):\n        # not sure why we have SUSPENDED, as it can never be set\n        if new_status == SUSPENDED:\n            new_status = PENDING\n\n        if new_status == DISABLED and task.status == RUNNING:\n            return\n\n        if task.status == DISABLED:\n            if new_status == DISABLED:\n                task.scheduler_disable_time = None\n            elif new_status == DONE:\n                task.re_enable()\n                task.status = DONE\n            elif task.scheduler_disable_time is None:\n                # when it is disabled by client, we allow the status change\n                task.status = new_status\n            return\n\n        if new_status == FAILED and task.can_disable():\n            task.add_failure()\n            if task.has_excessive_failures():\n                task.scheduler_disable_time = datetime.datetime.now()\n                new_status = DISABLED\n                notifications.send_error_email(\n                    'Luigi Scheduler: DISABLED {task} due to excessive failures'.format(task=task.id),\n                    '{task} failed {failures} times in the last {window} seconds, so it is being '\n                    'disabled for {persist} seconds'.format(\n                        failures=self._disable_failures,\n                        task=task.id,\n                        window=self._disable_window,\n                        persist=self._disable_persist,\n                        ))\n        elif new_status == DISABLED:\n            task.scheduler_disable_time = None\n\n        task.status = new_status\n\n    def update(self, worker_id, worker_reference=None):\n        \"\"\" Keep track of whenever the worker was last active \"\"\"\n        worker = self._state.get_worker(worker_id)\n        if worker_reference:\n            worker.reference = worker_reference\n        worker.last_active = time.time()\n\n    def _update_priority(self, task, prio, worker):\n        \"\"\" Update priority of the given task\n\n        Priority can only be increased. If the task doesn't exist, a placeholder\n        task is created to preserve priority when the task is later scheduled.\n        \"\"\"\n        task.priority = prio = max(prio, task.priority)\n        for dep in task.deps or []:\n            t = self._state.get_task(dep)\n            if t is not None and prio > t.priority:\n                self._update_priority(t, prio, worker)\n\n    def add_task(self, worker, task_id, status=PENDING, runnable=True,\n                 deps=None, new_deps=None, expl=None, resources=None,\n                 priority=0, family='', params={}):\n        \"\"\"\n        * Add task identified by task_id if it doesn't exist\n        * If deps is not None, update dependency list\n        * Update status of task\n        * Add additional workers/stakeholders\n        * Update priority when needed\n        \"\"\"\n        self.update(worker)\n\n        task = self._state.get_task(task_id, setdefault=self._make_task(\n                id=task_id, status=PENDING, deps=deps, resources=resources,\n                priority=priority, family=family, params=params))\n\n        # for setting priority, we'll sometimes create tasks with unset family and params\n        if not task.family:\n            task.family = family\n        if not task.params:\n            task.params = params\n\n        if task.remove is not None:\n            task.remove = None  # unmark task for removal so it isn't removed after being added\n\n        if not (task.status == RUNNING and status == PENDING):\n            # don't allow re-scheduling of task while it is running, it must either fail or succeed first\n            if status == PENDING or status != task.status:\n                # Update the DB only if there was a acctual change, to prevent noise.\n                # We also check for status == PENDING b/c that's the default value\n                # (so checking for status != task.status woule lie)\n                self._update_task_history(task_id, status)\n            self.set_status(task, PENDING if status == SUSPENDED else status)\n            if status == FAILED:\n                task.retry = time.time() + self._retry_delay\n\n        if deps is not None:\n            task.deps = set(deps)\n\n        if new_deps is not None:\n            task.deps.update(new_deps)\n\n        task.stakeholders.add(worker)\n        task.resources = resources\n\n        # Task dependencies might not exist yet. Let's create dummy tasks for them for now.\n        # Otherwise the task dependencies might end up being pruned if scheduling takes a long time\n        for dep in task.deps or []:\n            t = self._state.get_task(dep, setdefault=self._make_task(id=dep, status=UNKNOWN, deps=None, priority=priority))\n            t.stakeholders.add(worker)\n\n        self._update_priority(task, priority, worker)\n\n        if runnable:\n            task.workers.add(worker)\n\n        if expl is not None:\n            task.expl = expl\n\n    def add_worker(self, worker, info):\n        self._state.get_worker(worker).add_info(info)\n\n    def update_resources(self, **resources):\n        if self._resources is None:\n            self._resources = {}\n        self._resources.update(resources)\n\n    def _has_resources(self, needed_resources, used_resources):\n        if needed_resources is None:\n            return True\n\n        available_resources = self._resources or {}\n        for resource, amount in needed_resources.items():\n            if amount + used_resources[resource] > available_resources.get(resource, 1):\n                return False\n        return True\n\n    def _used_resources(self):\n        used_resources = collections.defaultdict(int)\n        if self._resources is not None:\n            for task in self._state.get_active_tasks():\n                if task.status == RUNNING and task.resources:\n                    for resource, amount in task.resources.items():\n                        used_resources[resource] += amount\n        return used_resources\n\n    def _rank(self):\n        ''' Return worker's rank function for task scheduling '''\n        dependents = collections.defaultdict(int)\n        def not_done(t):\n            task = self._state.get_task(t, default=None)\n            return task is None or task.status != DONE\n        for task in self._state.get_active_tasks():\n            if task.status != DONE:\n                deps = filter(not_done, task.deps)\n                inverse_num_deps = 1.0 / max(len(deps), 1)\n                for dep in deps:\n                    dependents[dep] += inverse_num_deps\n\n        return lambda task: (task.priority, dependents[task.id], -task.time)\n\n    def _schedulable(self, task):\n        if task.status != PENDING:\n            return False\n        for dep in task.deps:\n            dep_task = self._state.get_task(dep, default=None)\n            if dep_task is None or dep_task.status != DONE:\n                return False\n        return True\n\n    def get_work(self, worker, host=None):\n        # TODO: remove any expired nodes\n\n        # Algo: iterate over all nodes, find the highest priority node no dependencies and available\n        # resources.\n\n        # Resource checking looks both at currently available resources and at which resources would\n        # be available if all running tasks died and we rescheduled all workers greedily. We do both\n        # checks in order to prevent a worker with many low-priority tasks from starving other\n        # workers with higher priority tasks that share the same resources.\n\n        # TODO: remove tasks that can't be done, figure out if the worker has absolutely\n        # nothing it can wait for\n\n        # Return remaining tasks that have no FAILED descendents\n        self.update(worker, {'host': host})\n        best_task = None\n        best_task_id = None\n        locally_pending_tasks = 0\n        running_tasks = []\n\n        used_resources = self._used_resources()\n        greedy_resources = collections.defaultdict(int)\n        n_unique_pending = 0\n        greedy_workers = dict((worker.id, worker.info.get('workers', 1))\n                              for worker in self._state.get_active_workers())\n\n        tasks = list(self._state.get_pending_tasks())\n        tasks.sort(key=self._rank(), reverse=True)\n\n        for task in tasks:\n            if task.status == 'RUNNING' and worker in task.workers:\n                # Return a list of currently running tasks to the client,\n                # makes it easier to troubleshoot\n                other_worker = self._state.get_worker(task.worker_running)\n                more_info = {'task_id': task.id, 'worker': str(other_worker)}\n                if other_worker is not None:\n                    more_info.update(other_worker.info)\n                    running_tasks.append(more_info)\n\n            if task.status == PENDING and worker in task.workers:\n                locally_pending_tasks += 1\n                if len(task.workers) == 1:\n                    n_unique_pending += 1\n\n            if task.status == RUNNING and task.worker_running in greedy_workers:\n                greedy_workers[task.worker_running] -= 1\n                for resource, amount in (task.resources or {}).items():\n                    greedy_resources[resource] += amount\n\n            if not best_task and self._schedulable(task) and self._has_resources(task.resources, greedy_resources):\n                if worker in task.workers and self._has_resources(task.resources, used_resources):\n                    best_task = task\n                    best_task_id = task.id\n                else:\n                    for task_worker in task.workers:\n                        if greedy_workers.get(task_worker, 0) > 0:\n                            # use up a worker\n                            greedy_workers[task_worker] -= 1\n\n                            # keep track of the resources used in greedy scheduling\n                            for resource, amount in (task.resources or {}).items():\n                                greedy_resources[resource] += amount\n\n                            break\n\n        if best_task:\n            best_task.status = RUNNING\n            best_task.worker_running = worker\n            best_task.time_running = time.time()\n            self._update_task_history(best_task.id, RUNNING, host=host)\n\n        return {'n_pending_tasks': locally_pending_tasks,\n                'n_unique_pending': n_unique_pending,\n                'task_id': best_task_id,\n                'running_tasks': running_tasks}\n\n    def ping(self, worker):\n        self.update(worker)\n\n    def _upstream_status(self, task_id, upstream_status_table):\n        if task_id in upstream_status_table:\n            return upstream_status_table[task_id]\n        elif self._state.has_task(task_id):\n            task_stack = [task_id]\n\n            while task_stack:\n                dep_id = task_stack.pop()\n                if self._state.has_task(dep_id):\n                    dep = self._state.get_task(dep_id)\n                    if dep_id not in upstream_status_table:\n                        if dep.status == PENDING and dep.deps:\n                            task_stack = task_stack + [dep_id] + list(dep.deps)\n                            upstream_status_table[dep_id] = ''  # will be updated postorder\n                        else:\n                            dep_status = STATUS_TO_UPSTREAM_MAP.get(dep.status, '')\n                            upstream_status_table[dep_id] = dep_status\n                    elif upstream_status_table[dep_id] == '' and dep.deps:\n                        # This is the postorder update step when we set the\n                        # status based on the previously calculated child elements\n                        upstream_status = [upstream_status_table.get(id, '') for id in dep.deps]\n                        upstream_status.append('')  # to handle empty list\n                        status = max(upstream_status, key=UPSTREAM_SEVERITY_KEY)\n                        upstream_status_table[dep_id] = status\n            return upstream_status_table[dep_id]\n\n    def _serialize_task(self, task_id, include_deps=True):\n        task = self._state.get_task(task_id)\n        ret = {\n            'deps': list(task.deps),\n            'status': task.status,\n            'workers': list(task.workers),\n            'worker_running': task.worker_running,\n            'time_running': getattr(task, \"time_running\", None),\n            'start_time': task.time,\n            'params': task.params,\n            'name': task.family,\n            'priority': task.priority,\n            'resources': task.resources,\n        }\n        if include_deps:\n            ret['deps'] = list(task.deps)\n        return ret\n\n    def graph(self):\n        self.prune()\n        serialized = {}\n        for task in self._state.get_active_tasks():\n            serialized[task.id] = self._serialize_task(task.id)\n        return serialized\n\n    def _recurse_deps(self, task_id, serialized):\n        if task_id not in serialized:\n            task = self._state.get_task(task_id)\n            if task is None or not task.family:\n                logger.warn('Missing task for id [%s]', task_id)\n\n                # try to infer family and params from task_id\n                try:\n                    family, _, param_str = task_id.rstrip(')').partition('(')\n                    params = dict(param.split('=') for param in param_str.split(', '))\n                except:\n                    family, params = '', {}\n                serialized[task_id] = {\n                    'deps': [],\n                    'status': UNKNOWN,\n                    'workers': [],\n                    'start_time': UNKNOWN,\n                    'params': params,\n                    'name': family,\n                    'priority': 0,\n                }\n            else:\n                serialized[task_id] = self._serialize_task(task_id)\n                for dep in task.deps:\n                    self._recurse_deps(dep, serialized)\n\n    def dep_graph(self, task_id):\n        self.prune()\n        serialized = {}\n        if self._state.has_task(task_id):\n            self._recurse_deps(task_id, serialized)\n        return serialized\n\n    def task_list(self, status, upstream_status):\n        ''' query for a subset of tasks by status '''\n        self.prune()\n        result = {}\n        upstream_status_table = {}  # used to memoize upstream status\n        for task in self._state.get_active_tasks():\n            if not status or task.status == status:\n                if (task.status != PENDING or not upstream_status or\n                    upstream_status == self._upstream_status(task.id, upstream_status_table)):\n                    serialized = self._serialize_task(task.id, False)\n                    result[task.id] = serialized\n        return result\n\n    def worker_list(self, include_running=True):\n        self.prune()\n        workers = [\n            dict(\n                name=worker.id,\n                last_active=worker.last_active,\n                started=getattr(worker, 'started', None),\n                **worker.info\n            ) for worker in self._state.get_active_workers()]\n        workers.sort(key=lambda worker: worker['started'], reverse=True)\n        if include_running:\n            running = collections.defaultdict(dict)\n            num_pending = collections.defaultdict(int)\n            num_uniques = collections.defaultdict(int)\n            for task in self._state.get_pending_tasks():\n                if task.status == RUNNING and task.worker_running:\n                    running[task.worker_running][task.id] = self._serialize_task(task.id, False)\n                elif task.status == PENDING:\n                    for worker in task.workers:\n                        num_pending[worker] += 1\n                    if len(task.workers) == 1:\n                        num_uniques[list(task.workers)[0]] += 1\n            for worker in workers:\n                tasks = running[worker['name']]\n                worker['num_running'] = len(tasks)\n                worker['num_pending'] = num_pending[worker['name']]\n                worker['num_uniques'] = num_uniques[worker['name']]\n                worker['running'] = tasks\n        return workers\n\n    def inverse_dependencies(self, task_id):\n        self.prune()\n        serialized = {}\n        if self._state.has_task(task_id):\n            self._traverse_inverse_deps(task_id, serialized)\n        return serialized\n\n    def _traverse_inverse_deps(self, task_id, serialized):\n        stack = [task_id]\n        serialized[task_id] = self._serialize_task(task_id)\n        while len(stack) > 0:\n            curr_id = stack.pop()\n            for task in self._state.get_active_tasks():\n                if curr_id in task.deps:\n                    serialized[curr_id][\"deps\"].append(task.id)\n                    if task.id not in serialized:\n                        serialized[task.id] = self._serialize_task(task.id)\n                        serialized[task.id][\"deps\"] = []\n                        stack.append(task.id)\n\n    def task_search(self, task_str):\n        ''' query for a subset of tasks by task_id '''\n        self.prune()\n        result = collections.defaultdict(dict)\n        for task in self._state.get_active_tasks():\n            if task.id.find(task_str) != -1:\n                serialized = self._serialize_task(task.id, False)\n                result[task.status][task.id] = serialized\n        return result\n\n    def re_enable_task(self, task_id):\n        serialized = {}\n        task = self._state.get_task(task_id)\n        if task and task.status == DISABLED and task.scheduler_disable_time:\n            task.re_enable()\n            serialized = self._serialize_task(task_id)\n        return serialized\n\n    def fetch_error(self, task_id):\n        if self._state.has_task(task_id):\n            return {\"taskId\": task_id, \"error\": self._state.get_task(task_id).expl}\n        else:\n            return {\"taskId\": task_id, \"error\": \"\"}\n\n    def _update_task_history(self, task_id, status, host=None):\n        try:\n            if status == DONE or status == FAILED:\n                successful = (status == DONE)\n                self._task_history.task_finished(task_id, successful)\n            elif status == PENDING:\n                self._task_history.task_scheduled(task_id)\n            elif status == RUNNING:\n                self._task_history.task_started(task_id, host)\n        except:\n            logger.warning(\"Error saving Task history\", exc_info=1)\n\n    @property\n    def task_history(self):\n        # Used by server.py to expose the calls\n        return self._task_history\n",
          "file_patch": "@@ -517,6 +517,8 @@ class CentralPlannerScheduler(Scheduler):\n         used_resources = self._used_resources()\n         greedy_resources = collections.defaultdict(int)\n         n_unique_pending = 0\n+        greedy_workers = dict((worker.id, worker.info.get('workers', 1))\n+                              for worker in self._state.get_active_workers())\n \n         tasks = list(self._state.get_pending_tasks())\n         tasks.sort(key=self._rank(), reverse=True)\n@@ -536,14 +538,26 @@ class CentralPlannerScheduler(Scheduler):\n                 if len(task.workers) == 1:\n                     n_unique_pending += 1\n \n+            if task.status == RUNNING and task.worker_running in greedy_workers:\n+                greedy_workers[task.worker_running] -= 1\n+                for resource, amount in (task.resources or {}).items():\n+                    greedy_resources[resource] += amount\n+\n             if not best_task and self._schedulable(task) and self._has_resources(task.resources, greedy_resources):\n                 if worker in task.workers and self._has_resources(task.resources, used_resources):\n                     best_task = task\n                     best_task_id = task.id\n                 else:\n-                    # keep track of the resources used in greedy scheduling\n-                    for resource, amount in (task.resources or {}).items():\n-                        greedy_resources[resource] += amount\n+                    for task_worker in task.workers:\n+                        if greedy_workers.get(task_worker, 0) > 0:\n+                            # use up a worker\n+                            greedy_workers[task_worker] -= 1\n+\n+                            # keep track of the resources used in greedy scheduling\n+                            for resource, amount in (task.resources or {}).items():\n+                                greedy_resources[resource] += amount\n+\n+                            break\n \n         if best_task:\n             best_task.status = RUNNING\n",
          "files_name_in_blame_commit": [
            "scheduler.py",
            "central_planner_test.py"
          ]
        }
      },
      "bf16c10a29eb5ec530e7e8e0cfa61c3d01196244": {
        "commit": {
          "commit_id": "bf16c10a29eb5ec530e7e8e0cfa61c3d01196244",
          "commit_message": "Improves resource management in scheduler.\n\nI've noticed recently that my scheduler has been limiting a lot of my tasks to\nrun 1 at a time when the resource limitations should allow 4 simultaneous jobs.\nThis happens when tasks have multiple resources. Some of my tasks would have\na resource for writing to mysql and one for making a hive query, while others\nwould only have one for making a hive query. I have 1 write resource, and 4\nhive resources, so I should be able to run 1 mysql write job and 3 other hive\njobs that don't write to mysql. Unfortunately, the mysql write jobs have higher\npriority, so the end up getting scheduled in the greedy scheduling and take all\nof the hive resources without being able to actually run more than one.\n\nI added a test case for this as well as a few other similar issues and re-wrote\nget_work to keep track of the greedy resources and the actual resources\nseparately. A task must fit in both of these to be scheduled, but this gives\nmore flexibility to allow tasks to run that don't interfere with the running of\nhigher priority tasks. Running 3 of the hive-only resource jobs doesn't\ninterfere with running the next hive plus mysql write task when the current one\nis finished.",
          "commit_author": "Dave Buchfuhrer",
          "commit_date": "2014-10-13 13:59:26",
          "commit_parent": "db90a2a0370bc268246de170125893df2f098fb6"
        },
        "function": {
          "function_name": "get_work",
          "function_code_before": "def get_work(self, worker, host=None):\n    self.update(worker, {'host': host})\n    best_task = None\n    best_task_id = None\n    locally_pending_tasks = 0\n    running_tasks = []\n    used_resources = self._used_resources()\n    n_unique_pending = 0\n    tasks = list(self._state.get_pending_tasks())\n    tasks.sort(key=self._rank(), reverse=True)\n    for task in tasks:\n        if task.status == 'RUNNING' and worker in task.workers:\n            other_worker = self._state.get_worker(task.worker_running)\n            more_info = {'task_id': task.id, 'worker': str(other_worker)}\n            if other_worker is not None:\n                more_info.update(other_worker.info)\n                running_tasks.append(more_info)\n        if task.status == PENDING and worker in task.workers:\n            locally_pending_tasks += 1\n            if len(task.workers) == 1:\n                n_unique_pending += 1\n        if not best_task and self._schedulable(task):\n            if worker in task.workers and self._has_resources(task.resources, used_resources):\n                best_task = task\n                best_task_id = task.id\n            else:\n                for (resource, amount) in (task.resources or {}).items():\n                    used_resources[resource] += amount\n    if best_task:\n        best_task.status = RUNNING\n        best_task.worker_running = worker\n        best_task.time_running = time.time()\n        self._update_task_history(best_task.id, RUNNING, host=host)\n    return {'n_pending_tasks': locally_pending_tasks, 'n_unique_pending': n_unique_pending, 'task_id': best_task_id, 'running_tasks': running_tasks}",
          "function_code_after": "def get_work(self, worker, host=None):\n    self.update(worker, {'host': host})\n    best_task = None\n    best_task_id = None\n    locally_pending_tasks = 0\n    running_tasks = []\n    used_resources = self._used_resources()\n    greedy_resources = collections.defaultdict(int)\n    n_unique_pending = 0\n    tasks = list(self._state.get_pending_tasks())\n    tasks.sort(key=self._rank(), reverse=True)\n    for task in tasks:\n        if task.status == 'RUNNING' and worker in task.workers:\n            other_worker = self._state.get_worker(task.worker_running)\n            more_info = {'task_id': task.id, 'worker': str(other_worker)}\n            if other_worker is not None:\n                more_info.update(other_worker.info)\n                running_tasks.append(more_info)\n        if task.status == PENDING and worker in task.workers:\n            locally_pending_tasks += 1\n            if len(task.workers) == 1:\n                n_unique_pending += 1\n        if not best_task and self._schedulable(task) and self._has_resources(task.resources, greedy_resources):\n            if worker in task.workers and self._has_resources(task.resources, used_resources):\n                best_task = task\n                best_task_id = task.id\n            else:\n                for (resource, amount) in (task.resources or {}).items():\n                    greedy_resources[resource] += amount\n    if best_task:\n        best_task.status = RUNNING\n        best_task.worker_running = worker\n        best_task.time_running = time.time()\n        self._update_task_history(best_task.id, RUNNING, host=host)\n    return {'n_pending_tasks': locally_pending_tasks, 'n_unique_pending': n_unique_pending, 'task_id': best_task_id, 'running_tasks': running_tasks}",
          "function_before_start_line": 389,
          "function_before_end_line": 449,
          "function_after_start_line": 389,
          "function_after_end_line": 450,
          "function_before_token_count": 286,
          "function_after_token_count": 305,
          "functions_name_modified_file": [
            "get_worker_ids",
            "_update_task_history",
            "prune",
            "_schedulable",
            "_upstream_status",
            "ping",
            "get_pending_tasks",
            "_traverse_inverse_deps",
            "load",
            "inverse_dependencies",
            "task_search",
            "_recurse_deps",
            "get_worker",
            "get_active_workers",
            "worker_list",
            "add_task",
            "update_resources",
            "_update_priority",
            "task_list",
            "add_info",
            "get_active_tasks",
            "_has_resources",
            "_used_resources",
            "has_task",
            "_serialize_task",
            "task_history",
            "dep_graph",
            "__str__",
            "graph",
            "dump",
            "inactivate_tasks",
            "__repr__",
            "__init__",
            "update",
            "add_worker",
            "get_task",
            "_rank",
            "get_work",
            "fetch_error",
            "inactivate_workers"
          ],
          "functions_name_all_files": [
            "load",
            "inverse_dependencies",
            "test_priorities_and_dependencies",
            "test_timeout",
            "test_scheduler_overprovisioned_on_other_resource",
            "add_task",
            "add_info",
            "_has_resources",
            "_serialize_task",
            "check_task_order",
            "test_prefer_readier_dependents",
            "test_no_lock_if_too_many_resources_required",
            "test_hendle_multiple_resources",
            "setUp",
            "__init__",
            "test_scheduler_with_priority_and_competing_resources",
            "test_prefer_more_dependents",
            "test_disconnect_running",
            "inactivate_workers",
            "test_priority_update_dependency_after_scheduling",
            "prune",
            "tearDown",
            "_upstream_status",
            "test_two_workers",
            "ping",
            "test_retry",
            "test_dep",
            "_traverse_inverse_deps",
            "_recurse_deps",
            "test_two_worker_info",
            "update_resources",
            "_update_priority",
            "test_disallowed_state_changes",
            "test_single_resource_lock",
            "test_multiple_resources_lock",
            "test_scheduler_resources_none_allow_one",
            "__str__",
            "add_worker",
            "test_broken_dep",
            "test_priorities_default_and_negative",
            "test_scheduler_with_insufficient_resources",
            "test_update_resources",
            "test_ignore_done_dependents",
            "_schedulable",
            "test_unique_tasks",
            "test_scheduler_with_resources_used",
            "test_priority_no_decrease_with_multiple_updates",
            "worker_list",
            "get_active_tasks",
            "task_history",
            "test_multiple_resources_no_lock",
            "graph",
            "dump",
            "inactivate_tasks",
            "__repr__",
            "test_failed_dep",
            "get_task",
            "_rank",
            "get_worker_ids",
            "_update_task_history",
            "test_remove_dep",
            "test_priority_update_dependency_chain",
            "get_pending_tasks",
            "task_search",
            "get_worker",
            "get_active_workers",
            "task_list",
            "_used_resources",
            "has_task",
            "dep_graph",
            "test_scheduler_with_sufficient_resources",
            "update",
            "test_scheduler_resources_none_disallow_two",
            "test_priorities",
            "setTime",
            "get_work",
            "fetch_error"
          ],
          "functions_name_co_evolved_modified_file": [],
          "functions_name_co_evolved_all_files": [
            "test_multiple_resources_lock",
            "test_single_resource_lock",
            "test_multiple_resources_no_lock",
            "test_no_lock_if_too_many_resources_required",
            "test_hendle_multiple_resources"
          ]
        },
        "file": {
          "file_name": "scheduler.py",
          "file_nloc": 457,
          "file_complexity": 176,
          "file_token_count": 3529,
          "file_before": "# Copyright (c) 2012 Spotify AB\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"); you may not\n# use this file except in compliance with the License. You may obtain a copy of\n# the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n# License for the specific language governing permissions and limitations under\n# the License.\n\nimport collections\nimport datetime\nimport os\nimport logging\nimport time\nimport cPickle as pickle\nimport bisect\nimport task_history as history\nlogger = logging.getLogger(\"luigi.server\")\n\nfrom task_status import PENDING, FAILED, DONE, RUNNING, SUSPENDED, UNKNOWN\n\n\nclass Scheduler(object):\n    ''' Abstract base class\n\n    Note that the methods all take string arguments, not Task objects...\n    '''\n    add_task = NotImplemented\n    get_work = NotImplemented\n    ping = NotImplemented\n\nUPSTREAM_RUNNING = 'UPSTREAM_RUNNING'\nUPSTREAM_MISSING_INPUT = 'UPSTREAM_MISSING_INPUT'\nUPSTREAM_FAILED = 'UPSTREAM_FAILED'\n\nUPSTREAM_SEVERITY_ORDER = ('', UPSTREAM_RUNNING, UPSTREAM_MISSING_INPUT, UPSTREAM_FAILED)\nUPSTREAM_SEVERITY_KEY = lambda st: UPSTREAM_SEVERITY_ORDER.index(st)\nSTATUS_TO_UPSTREAM_MAP = {FAILED: UPSTREAM_FAILED, RUNNING: UPSTREAM_RUNNING, PENDING: UPSTREAM_MISSING_INPUT}\n\n\nclass Task(object):\n    def __init__(self, id, status, deps, resources={}, priority=0, family='', params={}):\n        self.id = id\n        self.stakeholders = set()  # workers ids that are somehow related to this task (i.e. don't prune while any of these workers are still active)\n        self.workers = set()  # workers ids that can perform task - task is 'BROKEN' if none of these workers are active\n        if deps is None:\n            self.deps = set()\n        else:\n            self.deps = set(deps)\n        self.status = status  # PENDING, RUNNING, FAILED or DONE\n        self.time = time.time()  # Timestamp when task was first added\n        self.retry = None\n        self.remove = None\n        self.worker_running = None  # the worker id that is currently running the task or None\n        self.time_running = None  # Timestamp when picked up by worker\n        self.expl = None\n        self.priority = priority\n        self.resources = resources\n        self.family = family\n        self.params = params\n\n    def __repr__(self):\n        return \"Task(%r)\" % vars(self)\n\n\nclass Worker(object):\n    \"\"\" Structure for tracking worker activity and keeping their references \"\"\"\n    def __init__(self, id, last_active=None):\n        self.id = id\n        self.reference = None  # reference to the worker in the real world. (Currently a dict containing just the host)\n        self.last_active = last_active  # seconds since epoch\n        self.started = time.time()  # seconds since epoch\n        self.info = {}\n\n    def add_info(self, info):\n        self.info.update(info)\n\n    def __str__(self):\n        return self.id\n\n\nclass SimpleTaskState(object):\n    ''' Keep track of the current state and handle persistance\n\n    The point of this class is to enable other ways to keep state, eg. by using a database\n    These will be implemented by creating an abstract base class that this and other classes\n    inherit from.\n    '''\n\n    def __init__(self, retry_delay=900.0, remove_delay=600.0, worker_disconnect_delay=60.0,\n                 state_path='/var/lib/luigi-server/state.pickle', task_history=None,\n                 resources=None):\n        '''\n        (all arguments are in seconds)\n        Keyword Arguments:\n        retry_delay -- How long after a Task fails to try it again, or -1 to never retry\n        remove_delay -- How long after a Task finishes to remove it from the scheduler\n        state_path -- Path to state file (tasks and active workers)\n        worker_disconnect_delay -- If a worker hasn't communicated for this long, remove it from active workers\n        '''\n    def __init__(self, state_path):\n        self._state_path = state_path\n        self._tasks = {}  # map from id to a Task object\n        self._active_workers = {}  # map from id to a Worker object\n\n    def dump(self):\n        state = (self._tasks, self._active_workers)\n        try:\n            with open(self._state_path, 'w') as fobj:\n                pickle.dump(state, fobj)\n        except IOError:\n            logger.warning(\"Failed saving scheduler state\", exc_info=1)\n        else:\n            logger.info(\"Saved state in %s\", self._state_path)\n\n    # prone to lead to crashes when old state is unpickled with updated code. TODO some kind of version control?\n    def load(self):\n        if os.path.exists(self._state_path):\n            logger.info(\"Attempting to load state from %s\", self._state_path)\n            try:\n                with open(self._state_path) as fobj:\n                    state = pickle.load(fobj)\n            except:\n                logger.exception(\"Error when loading state. Starting from clean slate.\")\n                return\n\n            self._tasks, self._active_workers = state\n\n            # Convert from old format\n            # TODO: this is really ugly, we need something more future-proof\n            # Every time we add an attribute to the Worker class, this code needs to be updated\n            for k, v in self._active_workers.iteritems():\n                if isinstance(v, float):\n                    self._active_workers[k] = Worker(id=k, last_active=v)\n        else:\n            logger.info(\"No prior state file exists at %s. Starting with clean slate\", self._state_path)\n\n    def get_active_tasks(self):\n        for task in self._tasks.itervalues():\n            yield task\n\n    def get_pending_tasks(self):\n        for task in self._tasks.itervalues():\n            if task.status in [PENDING, RUNNING]:\n                yield task\n\n    def get_task(self, task_id, default=None, setdefault=None):\n        if setdefault:\n            return self._tasks.setdefault(task_id, setdefault)\n        else:\n            return self._tasks.get(task_id, default)\n\n    def has_task(self, task_id):\n        return task_id in self._tasks\n\n    def inactivate_tasks(self, delete_tasks):\n        # The terminology is a bit confusing: we used to \"delete\" tasks when they became inactive,\n        # but with a pluggable state storage, you might very well want to keep some history of\n        # older tasks as well. That's why we call it \"inactivate\" (as in the verb)\n        for task in delete_tasks:\n            self._tasks.pop(task)\n\n    def get_active_workers(self, last_active_lt=None):\n        for worker in self._active_workers.itervalues():\n            if last_active_lt is not None and worker.last_active >= last_active_lt:\n                continue\n            yield worker\n\n    def get_worker_ids(self):\n        return self._active_workers.keys() # only used for unit tests\n\n    def get_worker(self, worker_id):\n        return self._active_workers.setdefault(worker_id, Worker(worker_id))\n\n    def inactivate_workers(self, delete_workers):\n        # Mark workers as inactive\n        for worker in delete_workers:\n            self._active_workers.pop(worker)\n        \n\nclass CentralPlannerScheduler(Scheduler):\n    ''' Async scheduler that can handle multiple workers etc\n\n    Can be run locally or on a server (using RemoteScheduler + server.Server).\n    '''\n\n    def __init__(self, retry_delay=900.0, remove_delay=600.0, worker_disconnect_delay=60.0,\n                 state_path='/var/lib/luigi-server/state.pickle', task_history=None,\n                 resources=None):\n        '''\n        (all arguments are in seconds)\n        Keyword Arguments:\n        retry_delay -- How long after a Task fails to try it again, or -1 to never retry\n        remove_delay -- How long after a Task finishes to remove it from the scheduler\n        state_path -- Path to state file (tasks and active workers)\n        worker_disconnect_delay -- If a worker hasn't communicated for this long, remove it from active workers\n        '''\n        self._retry_delay = retry_delay\n        self._remove_delay = remove_delay\n        self._worker_disconnect_delay = worker_disconnect_delay\n        self._task_history = task_history or history.NopHistory()\n        self._state = SimpleTaskState(state_path)\n\n        self._task_history = task_history or history.NopHistory()\n        self._resources = resources\n\n    def load(self):\n        self._state.load()\n\n    def dump(self):\n        self._state.dump()\n\n    def prune(self):\n        logger.info(\"Starting pruning of task graph\")\n        # Delete workers that haven't said anything for a while (probably killed)\n        delete_workers = []\n        for worker in self._state.get_active_workers(last_active_lt=time.time() - self._worker_disconnect_delay):\n            logger.info(\"Worker %s timed out (no contact for >=%ss)\", worker, self._worker_disconnect_delay)\n            delete_workers.append(worker.id)\n\n        self._state.inactivate_workers(delete_workers)\n\n        delete_workers = set(delete_workers)\n\n        remove_tasks = []\n        for task in self._state.get_active_tasks():\n            # Mark tasks with no remaining active stakeholders for deletion\n            task.stakeholders.difference_update(delete_workers)\n            if not task.stakeholders:\n                if task.remove is None:\n                    logger.info(\"Task %r has stakeholders %r but none remain connected -> will remove task in %s seconds\", task.id, task.stakeholders, self._remove_delay)\n                    task.remove = time.time() + self._remove_delay\n\n            # If a running worker disconnects, tag all its jobs as FAILED and subject it to the same retry logic\n            if task.status == RUNNING and task.worker_running and task.worker_running not in task.stakeholders:\n                logger.info(\"Task %r is marked as running by disconnected worker %r -> marking as FAILED with retry delay of %rs\", task.id, task.worker_running, self._retry_delay)\n                task.worker_running = None\n                task.status = FAILED\n                task.retry = time.time() + self._retry_delay\n\n            # Remove tasks that have no stakeholders\n            if task.remove and time.time() > task.remove:\n                logger.info(\"Removing task %r (no connected stakeholders)\", task.id)\n                remove_tasks.append(task.id)\n\n        # Reset FAILED tasks to PENDING if max timeout is reached, and retry delay is >= 0\n        for task in self._state.get_active_tasks():\n            if task.status == FAILED and self._retry_delay >= 0 and task.retry < time.time():\n                task.status = PENDING\n\n        self._state.inactivate_tasks(remove_tasks)\n\n        logger.info(\"Done pruning task graph\")\n\n    def update(self, worker_id, worker_reference=None):\n        \"\"\" Keep track of whenever the worker was last active \"\"\"\n        worker = self._state.get_worker(worker_id)\n        if worker_reference:\n            worker.reference = worker_reference\n        worker.last_active = time.time()\n\n    def _update_priority(self, task, prio, worker):\n        \"\"\" Update priority of the given task\n\n        Priority can only be increased. If the task doesn't exist, a placeholder\n        task is created to preserve priority when the task is later scheduled.\n        \"\"\"\n        task.priority = prio = max(prio, task.priority)\n        for dep in task.deps or []:\n            t = self._state.get_task(dep) # This should always exist, see add_task\n            if prio > t.priority:\n                self._update_priority(t, prio, worker)\n\n    def add_task(self, worker, task_id, status=PENDING, runnable=True,\n                 deps=None, new_deps=None, expl=None, resources=None,\n                 priority=0, family='', params={}):\n        \"\"\"\n        * Add task identified by task_id if it doesn't exist\n        * If deps is not None, update dependency list\n        * Update status of task\n        * Add additional workers/stakeholders\n        * Update priority when needed\n        \"\"\"\n        self.update(worker)\n\n        task = self._state.get_task(task_id, setdefault=Task(\n                id=task_id, status=PENDING, deps=deps, resources=resources,\n                priority=priority, family=family, params=params))\n        \n        # for setting priority, we'll sometimes create tasks with unset family and params\n        if not task.family:\n            task.family = family\n        if not task.params:\n            task.params = params\n\n        if task.remove is not None:\n            task.remove = None  # unmark task for removal so it isn't removed after being added\n\n        if not (task.status == RUNNING and status == PENDING):\n            # don't allow re-scheduling of task while it is running, it must either fail or succeed first\n            if status == PENDING or status != task.status:\n                # Update the DB only if there was a acctual change, to prevent noise.\n                # We also check for status == PENDING b/c that's the default value\n                # (so checking for status != task.status woule lie)\n                self._update_task_history(task_id, status)\n            task.status = PENDING if status == SUSPENDED else status\n            if status == FAILED:\n                task.retry = time.time() + self._retry_delay\n\n        if deps is not None:\n            task.deps = set(deps)\n\n        if new_deps is not None:\n            task.deps.update(new_deps)\n\n        task.stakeholders.add(worker)\n        task.resources = resources\n\n        # Task dependencies might not exist yet. Let's create dummy tasks for them for now.\n        # Otherwise the task dependencies might end up being pruned if scheduling takes a long time\n        for dep in task.deps or []:\n            t = self._state.get_task(dep, setdefault=Task(id=dep, status=UNKNOWN, deps=None, priority=priority))\n            t.stakeholders.add(worker)\n\n        self._update_priority(task, priority, worker)\n\n        if runnable:\n            task.workers.add(worker)\n\n        if expl is not None:\n            task.expl = expl\n\n    def add_worker(self, worker, info):\n        self._state.get_worker(worker).add_info(info)\n\n    def update_resources(self, **resources):\n        if self._resources is None:\n            self._resources = {}\n        self._resources.update(resources)\n\n    def _has_resources(self, needed_resources, used_resources):\n        if needed_resources is None:\n            return True\n\n        available_resources = self._resources or {}\n        for resource, amount in needed_resources.items():\n            if amount + used_resources[resource] > available_resources.get(resource, 1):\n                return False\n        return True\n\n    def _used_resources(self):\n        used_resources = collections.defaultdict(int)\n        if self._resources is not None:\n            for task in self._state.get_active_tasks():\n                if task.status == RUNNING and task.resources:\n                    for resource, amount in task.resources.items():\n                        used_resources[resource] += amount\n        return used_resources\n\n    def _rank(self):\n        ''' Return worker's rank function for task scheduling '''\n        dependents = collections.defaultdict(int)\n        def not_done(t):\n            task = self._state.get_task(t, default=None)\n            return task is None or task.status != DONE\n        for task in self._state.get_active_tasks():\n            if task.status != DONE:\n                deps = filter(not_done, task.deps)\n                inverse_num_deps = 1.0 / max(len(deps), 1)\n                for dep in deps:\n                    dependents[dep] += inverse_num_deps\n\n        return lambda task: (task.priority, dependents[task.id], -task.time)\n\n    def _schedulable(self, task):\n        if task.status != PENDING:\n            return False\n        for dep in task.deps:\n            dep_task = self._state.get_task(dep, default=None)\n            if dep_task is None or dep_task.status != DONE:\n                return False\n        return True\n\n    def get_work(self, worker, host=None):\n        # TODO: remove any expired nodes\n\n        # Algo: iterate over all nodes, find the highest priority node no dependencies and available\n        # resources.\n\n        # Resource checking looks both at currently available resources and at which resources would\n        # be available if all running tasks died and we rescheduled all workers greedily. We do both\n        # checks in order to prevent a worker with many low-priority tasks from starving other\n        # workers with higher priority tasks that share the same resources.\n\n        # TODO: remove tasks that can't be done, figure out if the worker has absolutely\n        # nothing it can wait for\n\n        # Return remaining tasks that have no FAILED descendents\n        self.update(worker, {'host': host})\n        best_task = None\n        best_task_id = None\n        locally_pending_tasks = 0\n        running_tasks = []\n\n        used_resources = self._used_resources()\n        n_unique_pending = 0\n\n        tasks = list(self._state.get_pending_tasks())\n        tasks.sort(key=self._rank(), reverse=True)\n\n        for task in tasks:\n            if task.status == 'RUNNING' and worker in task.workers:\n                # Return a list of currently running tasks to the client,\n                # makes it easier to troubleshoot\n                other_worker = self._state.get_worker(task.worker_running)\n                more_info = {'task_id': task.id, 'worker': str(other_worker)}\n                if other_worker is not None:\n                    more_info.update(other_worker.info)\n                    running_tasks.append(more_info)\n\n            if task.status == PENDING and worker in task.workers:\n                locally_pending_tasks += 1\n                if len(task.workers) == 1:\n                    n_unique_pending += 1\n\n            if not best_task and self._schedulable(task):\n                if worker in task.workers and self._has_resources(task.resources, used_resources):\n                    best_task = task\n                    best_task_id = task.id\n                else:\n                    # keep track of the resources used in greedy scheduling\n                    for resource, amount in (task.resources or {}).items():\n                        used_resources[resource] += amount\n\n        if best_task:\n            best_task.status = RUNNING\n            best_task.worker_running = worker\n            best_task.time_running = time.time()\n            self._update_task_history(best_task.id, RUNNING, host=host)\n\n        return {'n_pending_tasks': locally_pending_tasks,\n                'n_unique_pending': n_unique_pending,\n                'task_id': best_task_id,\n                'running_tasks': running_tasks}\n\n    def ping(self, worker):\n        self.update(worker)\n\n    def _upstream_status(self, task_id, upstream_status_table):\n        if task_id in upstream_status_table:\n            return upstream_status_table[task_id]\n        elif self._state.has_task(task_id):\n            task_stack = [task_id]\n\n            while task_stack:\n                dep_id = task_stack.pop()\n                if self._state.has_task(dep_id):\n                    dep = self._state.get_task(dep_id)\n                    if dep_id not in upstream_status_table:\n                        if dep.status == PENDING and dep.deps:\n                            task_stack = task_stack + [dep_id] + list(dep.deps)\n                            upstream_status_table[dep_id] = ''  # will be updated postorder\n                        else:\n                            dep_status = STATUS_TO_UPSTREAM_MAP.get(dep.status, '')\n                            upstream_status_table[dep_id] = dep_status\n                    elif upstream_status_table[dep_id] == '' and dep.deps:\n                        # This is the postorder update step when we set the\n                        # status based on the previously calculated child elements\n                        upstream_status = [upstream_status_table.get(id, '') for id in dep.deps]\n                        upstream_status.append('')  # to handle empty list\n                        status = max(upstream_status, key=UPSTREAM_SEVERITY_KEY)\n                        upstream_status_table[dep_id] = status\n            return upstream_status_table[dep_id]\n\n    def _serialize_task(self, task_id, include_deps=True):\n        task = self._state.get_task(task_id)\n        ret = {\n            'deps': list(task.deps),\n            'status': task.status,\n            'workers': list(task.workers),\n            'worker_running': task.worker_running,\n            'time_running': getattr(task, \"time_running\", None),\n            'start_time': task.time,\n            'params': task.params,\n            'name': task.family,\n            'priority': task.priority,\n            'resources': task.resources,\n        }\n        if include_deps:\n            ret['deps'] = list(task.deps)\n        return ret\n\n    def graph(self):\n        self.prune()\n        serialized = {}\n        for task in self._state.get_active_tasks():\n            serialized[task.id] = self._serialize_task(task.id)\n        return serialized\n\n    def _recurse_deps(self, task_id, serialized):\n        if task_id not in serialized:\n            task = self._state.get_task(task_id)\n            if task is None or not task.family:\n                logger.warn('Missing task for id [%s]', task_id)\n\n                # try to infer family and params from task_id\n                try:\n                    family, _, param_str = task_id.rstrip(')').partition('(')\n                    params = dict(param.split('=') for param in param_str.split(', '))\n                except:\n                    family, params = '', {}\n                serialized[task_id] = {\n                    'deps': [],\n                    'status': UNKNOWN,\n                    'workers': [],\n                    'start_time': UNKNOWN,\n                    'params': params,\n                    'name': family,\n                    'priority': 0,\n                }\n            else:\n                serialized[task_id] = self._serialize_task(task_id)\n                for dep in task.deps:\n                    self._recurse_deps(dep, serialized)\n\n    def dep_graph(self, task_id):\n        self.prune()\n        serialized = {}\n        if self._state.has_task(task_id):\n            self._recurse_deps(task_id, serialized)\n        return serialized\n\n    def task_list(self, status, upstream_status):\n        ''' query for a subset of tasks by status '''\n        self.prune()\n        result = {}\n        upstream_status_table = {}  # used to memoize upstream status\n        for task in self._state.get_active_tasks():\n            if not status or task.status == status:\n                if (task.status != PENDING or not upstream_status or\n                    upstream_status == self._upstream_status(task.id, upstream_status_table)):\n                    serialized = self._serialize_task(task.id, False)\n                    result[task.id] = serialized\n        return result\n\n    def worker_list(self, include_running=True):\n        self.prune()\n        workers = [\n            dict(\n                name=worker.id,\n                last_active=worker.last_active,\n                started=getattr(worker, 'started', None),\n                **worker.info\n            ) for worker in self._state.get_active_workers()]\n        workers.sort(key=lambda worker: worker['started'], reverse=True)\n        if include_running:\n            running = collections.defaultdict(dict)\n            num_pending = collections.defaultdict(int)\n            num_uniques = collections.defaultdict(int)\n            for task in self._state.get_pending_tasks():\n                if task.status == RUNNING and task.worker_running:\n                    running[task.worker_running][task.id] = self._serialize_task(task.id, False)\n                elif task.status == PENDING:\n                    for worker in task.workers:\n                        num_pending[worker] += 1\n                    if len(task.workers) == 1:\n                        num_uniques[list(task.workers)[0]] += 1\n            for worker in workers:\n                tasks = running[worker['name']]\n                worker['num_running'] = len(tasks)\n                worker['num_pending'] = num_pending[worker['name']]\n                worker['num_uniques'] = num_uniques[worker['name']]\n                worker['running'] = tasks\n        return workers\n\n    def inverse_dependencies(self, task_id):\n        self.prune()\n        serialized = {}\n        if self._state.has_task(task_id):\n            self._traverse_inverse_deps(task_id, serialized)\n        return serialized\n\n    def _traverse_inverse_deps(self, task_id, serialized):\n        stack = [task_id]\n        serialized[task_id] = self._serialize_task(task_id)\n        while len(stack) > 0:\n            curr_id = stack.pop()\n            for task in self._state.get_active_tasks():\n                if curr_id in task.deps:\n                    serialized[curr_id][\"deps\"].append(task.id)\n                    if task.id not in serialized:\n                        serialized[task.id] = self._serialize_task(task.id)\n                        serialized[task.id][\"deps\"] = []\n                        stack.append(task.id)\n\n    def task_search(self, task_str):\n        ''' query for a subset of tasks by task_id '''\n        self.prune()\n        result = collections.defaultdict(dict)\n        for task in self._state.get_active_tasks():\n            if task.id.find(task_str) != -1:\n                serialized = self._serialize_task(task.id, False)\n                result[task.status][task.id] = serialized\n        return result\n\n    def fetch_error(self, task_id):\n        if self._state.has_task(task_id):\n            return {\"taskId\": task_id, \"error\": self._state.get_task(task_id).expl}\n        else:\n            return {\"taskId\": task_id, \"error\": \"\"}\n\n    def _update_task_history(self, task_id, status, host=None):\n        try:\n            if status == DONE or status == FAILED:\n                successful = (status == DONE)\n                self._task_history.task_finished(task_id, successful)\n            elif status == PENDING:\n                self._task_history.task_scheduled(task_id)\n            elif status == RUNNING:\n                self._task_history.task_started(task_id, host)\n        except:\n            logger.warning(\"Error saving Task history\", exc_info=1)\n\n    @property\n    def task_history(self):\n        # Used by server.py to expose the calls\n        return self._task_history\n",
          "file_after": "# Copyright (c) 2012 Spotify AB\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"); you may not\n# use this file except in compliance with the License. You may obtain a copy of\n# the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n# License for the specific language governing permissions and limitations under\n# the License.\n\nimport collections\nimport datetime\nimport os\nimport logging\nimport time\nimport cPickle as pickle\nimport bisect\nimport task_history as history\nlogger = logging.getLogger(\"luigi.server\")\n\nfrom task_status import PENDING, FAILED, DONE, RUNNING, SUSPENDED, UNKNOWN\n\n\nclass Scheduler(object):\n    ''' Abstract base class\n\n    Note that the methods all take string arguments, not Task objects...\n    '''\n    add_task = NotImplemented\n    get_work = NotImplemented\n    ping = NotImplemented\n\nUPSTREAM_RUNNING = 'UPSTREAM_RUNNING'\nUPSTREAM_MISSING_INPUT = 'UPSTREAM_MISSING_INPUT'\nUPSTREAM_FAILED = 'UPSTREAM_FAILED'\n\nUPSTREAM_SEVERITY_ORDER = ('', UPSTREAM_RUNNING, UPSTREAM_MISSING_INPUT, UPSTREAM_FAILED)\nUPSTREAM_SEVERITY_KEY = lambda st: UPSTREAM_SEVERITY_ORDER.index(st)\nSTATUS_TO_UPSTREAM_MAP = {FAILED: UPSTREAM_FAILED, RUNNING: UPSTREAM_RUNNING, PENDING: UPSTREAM_MISSING_INPUT}\n\n\nclass Task(object):\n    def __init__(self, id, status, deps, resources={}, priority=0, family='', params={}):\n        self.id = id\n        self.stakeholders = set()  # workers ids that are somehow related to this task (i.e. don't prune while any of these workers are still active)\n        self.workers = set()  # workers ids that can perform task - task is 'BROKEN' if none of these workers are active\n        if deps is None:\n            self.deps = set()\n        else:\n            self.deps = set(deps)\n        self.status = status  # PENDING, RUNNING, FAILED or DONE\n        self.time = time.time()  # Timestamp when task was first added\n        self.retry = None\n        self.remove = None\n        self.worker_running = None  # the worker id that is currently running the task or None\n        self.time_running = None  # Timestamp when picked up by worker\n        self.expl = None\n        self.priority = priority\n        self.resources = resources\n        self.family = family\n        self.params = params\n\n    def __repr__(self):\n        return \"Task(%r)\" % vars(self)\n\n\nclass Worker(object):\n    \"\"\" Structure for tracking worker activity and keeping their references \"\"\"\n    def __init__(self, id, last_active=None):\n        self.id = id\n        self.reference = None  # reference to the worker in the real world. (Currently a dict containing just the host)\n        self.last_active = last_active  # seconds since epoch\n        self.started = time.time()  # seconds since epoch\n        self.info = {}\n\n    def add_info(self, info):\n        self.info.update(info)\n\n    def __str__(self):\n        return self.id\n\n\nclass SimpleTaskState(object):\n    ''' Keep track of the current state and handle persistance\n\n    The point of this class is to enable other ways to keep state, eg. by using a database\n    These will be implemented by creating an abstract base class that this and other classes\n    inherit from.\n    '''\n\n    def __init__(self, retry_delay=900.0, remove_delay=600.0, worker_disconnect_delay=60.0,\n                 state_path='/var/lib/luigi-server/state.pickle', task_history=None,\n                 resources=None):\n        '''\n        (all arguments are in seconds)\n        Keyword Arguments:\n        retry_delay -- How long after a Task fails to try it again, or -1 to never retry\n        remove_delay -- How long after a Task finishes to remove it from the scheduler\n        state_path -- Path to state file (tasks and active workers)\n        worker_disconnect_delay -- If a worker hasn't communicated for this long, remove it from active workers\n        '''\n    def __init__(self, state_path):\n        self._state_path = state_path\n        self._tasks = {}  # map from id to a Task object\n        self._active_workers = {}  # map from id to a Worker object\n\n    def dump(self):\n        state = (self._tasks, self._active_workers)\n        try:\n            with open(self._state_path, 'w') as fobj:\n                pickle.dump(state, fobj)\n        except IOError:\n            logger.warning(\"Failed saving scheduler state\", exc_info=1)\n        else:\n            logger.info(\"Saved state in %s\", self._state_path)\n\n    # prone to lead to crashes when old state is unpickled with updated code. TODO some kind of version control?\n    def load(self):\n        if os.path.exists(self._state_path):\n            logger.info(\"Attempting to load state from %s\", self._state_path)\n            try:\n                with open(self._state_path) as fobj:\n                    state = pickle.load(fobj)\n            except:\n                logger.exception(\"Error when loading state. Starting from clean slate.\")\n                return\n\n            self._tasks, self._active_workers = state\n\n            # Convert from old format\n            # TODO: this is really ugly, we need something more future-proof\n            # Every time we add an attribute to the Worker class, this code needs to be updated\n            for k, v in self._active_workers.iteritems():\n                if isinstance(v, float):\n                    self._active_workers[k] = Worker(id=k, last_active=v)\n        else:\n            logger.info(\"No prior state file exists at %s. Starting with clean slate\", self._state_path)\n\n    def get_active_tasks(self):\n        for task in self._tasks.itervalues():\n            yield task\n\n    def get_pending_tasks(self):\n        for task in self._tasks.itervalues():\n            if task.status in [PENDING, RUNNING]:\n                yield task\n\n    def get_task(self, task_id, default=None, setdefault=None):\n        if setdefault:\n            return self._tasks.setdefault(task_id, setdefault)\n        else:\n            return self._tasks.get(task_id, default)\n\n    def has_task(self, task_id):\n        return task_id in self._tasks\n\n    def inactivate_tasks(self, delete_tasks):\n        # The terminology is a bit confusing: we used to \"delete\" tasks when they became inactive,\n        # but with a pluggable state storage, you might very well want to keep some history of\n        # older tasks as well. That's why we call it \"inactivate\" (as in the verb)\n        for task in delete_tasks:\n            self._tasks.pop(task)\n\n    def get_active_workers(self, last_active_lt=None):\n        for worker in self._active_workers.itervalues():\n            if last_active_lt is not None and worker.last_active >= last_active_lt:\n                continue\n            yield worker\n\n    def get_worker_ids(self):\n        return self._active_workers.keys() # only used for unit tests\n\n    def get_worker(self, worker_id):\n        return self._active_workers.setdefault(worker_id, Worker(worker_id))\n\n    def inactivate_workers(self, delete_workers):\n        # Mark workers as inactive\n        for worker in delete_workers:\n            self._active_workers.pop(worker)\n        \n\nclass CentralPlannerScheduler(Scheduler):\n    ''' Async scheduler that can handle multiple workers etc\n\n    Can be run locally or on a server (using RemoteScheduler + server.Server).\n    '''\n\n    def __init__(self, retry_delay=900.0, remove_delay=600.0, worker_disconnect_delay=60.0,\n                 state_path='/var/lib/luigi-server/state.pickle', task_history=None,\n                 resources=None):\n        '''\n        (all arguments are in seconds)\n        Keyword Arguments:\n        retry_delay -- How long after a Task fails to try it again, or -1 to never retry\n        remove_delay -- How long after a Task finishes to remove it from the scheduler\n        state_path -- Path to state file (tasks and active workers)\n        worker_disconnect_delay -- If a worker hasn't communicated for this long, remove it from active workers\n        '''\n        self._retry_delay = retry_delay\n        self._remove_delay = remove_delay\n        self._worker_disconnect_delay = worker_disconnect_delay\n        self._task_history = task_history or history.NopHistory()\n        self._state = SimpleTaskState(state_path)\n\n        self._task_history = task_history or history.NopHistory()\n        self._resources = resources\n\n    def load(self):\n        self._state.load()\n\n    def dump(self):\n        self._state.dump()\n\n    def prune(self):\n        logger.info(\"Starting pruning of task graph\")\n        # Delete workers that haven't said anything for a while (probably killed)\n        delete_workers = []\n        for worker in self._state.get_active_workers(last_active_lt=time.time() - self._worker_disconnect_delay):\n            logger.info(\"Worker %s timed out (no contact for >=%ss)\", worker, self._worker_disconnect_delay)\n            delete_workers.append(worker.id)\n\n        self._state.inactivate_workers(delete_workers)\n\n        delete_workers = set(delete_workers)\n\n        remove_tasks = []\n        for task in self._state.get_active_tasks():\n            # Mark tasks with no remaining active stakeholders for deletion\n            task.stakeholders.difference_update(delete_workers)\n            if not task.stakeholders:\n                if task.remove is None:\n                    logger.info(\"Task %r has stakeholders %r but none remain connected -> will remove task in %s seconds\", task.id, task.stakeholders, self._remove_delay)\n                    task.remove = time.time() + self._remove_delay\n\n            # If a running worker disconnects, tag all its jobs as FAILED and subject it to the same retry logic\n            if task.status == RUNNING and task.worker_running and task.worker_running not in task.stakeholders:\n                logger.info(\"Task %r is marked as running by disconnected worker %r -> marking as FAILED with retry delay of %rs\", task.id, task.worker_running, self._retry_delay)\n                task.worker_running = None\n                task.status = FAILED\n                task.retry = time.time() + self._retry_delay\n\n            # Remove tasks that have no stakeholders\n            if task.remove and time.time() > task.remove:\n                logger.info(\"Removing task %r (no connected stakeholders)\", task.id)\n                remove_tasks.append(task.id)\n\n        # Reset FAILED tasks to PENDING if max timeout is reached, and retry delay is >= 0\n        for task in self._state.get_active_tasks():\n            if task.status == FAILED and self._retry_delay >= 0 and task.retry < time.time():\n                task.status = PENDING\n\n        self._state.inactivate_tasks(remove_tasks)\n\n        logger.info(\"Done pruning task graph\")\n\n    def update(self, worker_id, worker_reference=None):\n        \"\"\" Keep track of whenever the worker was last active \"\"\"\n        worker = self._state.get_worker(worker_id)\n        if worker_reference:\n            worker.reference = worker_reference\n        worker.last_active = time.time()\n\n    def _update_priority(self, task, prio, worker):\n        \"\"\" Update priority of the given task\n\n        Priority can only be increased. If the task doesn't exist, a placeholder\n        task is created to preserve priority when the task is later scheduled.\n        \"\"\"\n        task.priority = prio = max(prio, task.priority)\n        for dep in task.deps or []:\n            t = self._state.get_task(dep) # This should always exist, see add_task\n            if prio > t.priority:\n                self._update_priority(t, prio, worker)\n\n    def add_task(self, worker, task_id, status=PENDING, runnable=True,\n                 deps=None, new_deps=None, expl=None, resources=None,\n                 priority=0, family='', params={}):\n        \"\"\"\n        * Add task identified by task_id if it doesn't exist\n        * If deps is not None, update dependency list\n        * Update status of task\n        * Add additional workers/stakeholders\n        * Update priority when needed\n        \"\"\"\n        self.update(worker)\n\n        task = self._state.get_task(task_id, setdefault=Task(\n                id=task_id, status=PENDING, deps=deps, resources=resources,\n                priority=priority, family=family, params=params))\n        \n        # for setting priority, we'll sometimes create tasks with unset family and params\n        if not task.family:\n            task.family = family\n        if not task.params:\n            task.params = params\n\n        if task.remove is not None:\n            task.remove = None  # unmark task for removal so it isn't removed after being added\n\n        if not (task.status == RUNNING and status == PENDING):\n            # don't allow re-scheduling of task while it is running, it must either fail or succeed first\n            if status == PENDING or status != task.status:\n                # Update the DB only if there was a acctual change, to prevent noise.\n                # We also check for status == PENDING b/c that's the default value\n                # (so checking for status != task.status woule lie)\n                self._update_task_history(task_id, status)\n            task.status = PENDING if status == SUSPENDED else status\n            if status == FAILED:\n                task.retry = time.time() + self._retry_delay\n\n        if deps is not None:\n            task.deps = set(deps)\n\n        if new_deps is not None:\n            task.deps.update(new_deps)\n\n        task.stakeholders.add(worker)\n        task.resources = resources\n\n        # Task dependencies might not exist yet. Let's create dummy tasks for them for now.\n        # Otherwise the task dependencies might end up being pruned if scheduling takes a long time\n        for dep in task.deps or []:\n            t = self._state.get_task(dep, setdefault=Task(id=dep, status=UNKNOWN, deps=None, priority=priority))\n            t.stakeholders.add(worker)\n\n        self._update_priority(task, priority, worker)\n\n        if runnable:\n            task.workers.add(worker)\n\n        if expl is not None:\n            task.expl = expl\n\n    def add_worker(self, worker, info):\n        self._state.get_worker(worker).add_info(info)\n\n    def update_resources(self, **resources):\n        if self._resources is None:\n            self._resources = {}\n        self._resources.update(resources)\n\n    def _has_resources(self, needed_resources, used_resources):\n        if needed_resources is None:\n            return True\n\n        available_resources = self._resources or {}\n        for resource, amount in needed_resources.items():\n            if amount + used_resources[resource] > available_resources.get(resource, 1):\n                return False\n        return True\n\n    def _used_resources(self):\n        used_resources = collections.defaultdict(int)\n        if self._resources is not None:\n            for task in self._state.get_active_tasks():\n                if task.status == RUNNING and task.resources:\n                    for resource, amount in task.resources.items():\n                        used_resources[resource] += amount\n        return used_resources\n\n    def _rank(self):\n        ''' Return worker's rank function for task scheduling '''\n        dependents = collections.defaultdict(int)\n        def not_done(t):\n            task = self._state.get_task(t, default=None)\n            return task is None or task.status != DONE\n        for task in self._state.get_active_tasks():\n            if task.status != DONE:\n                deps = filter(not_done, task.deps)\n                inverse_num_deps = 1.0 / max(len(deps), 1)\n                for dep in deps:\n                    dependents[dep] += inverse_num_deps\n\n        return lambda task: (task.priority, dependents[task.id], -task.time)\n\n    def _schedulable(self, task):\n        if task.status != PENDING:\n            return False\n        for dep in task.deps:\n            dep_task = self._state.get_task(dep, default=None)\n            if dep_task is None or dep_task.status != DONE:\n                return False\n        return True\n\n    def get_work(self, worker, host=None):\n        # TODO: remove any expired nodes\n\n        # Algo: iterate over all nodes, find the highest priority node no dependencies and available\n        # resources.\n\n        # Resource checking looks both at currently available resources and at which resources would\n        # be available if all running tasks died and we rescheduled all workers greedily. We do both\n        # checks in order to prevent a worker with many low-priority tasks from starving other\n        # workers with higher priority tasks that share the same resources.\n\n        # TODO: remove tasks that can't be done, figure out if the worker has absolutely\n        # nothing it can wait for\n\n        # Return remaining tasks that have no FAILED descendents\n        self.update(worker, {'host': host})\n        best_task = None\n        best_task_id = None\n        locally_pending_tasks = 0\n        running_tasks = []\n\n        used_resources = self._used_resources()\n        greedy_resources = collections.defaultdict(int)\n        n_unique_pending = 0\n\n        tasks = list(self._state.get_pending_tasks())\n        tasks.sort(key=self._rank(), reverse=True)\n\n        for task in tasks:\n            if task.status == 'RUNNING' and worker in task.workers:\n                # Return a list of currently running tasks to the client,\n                # makes it easier to troubleshoot\n                other_worker = self._state.get_worker(task.worker_running)\n                more_info = {'task_id': task.id, 'worker': str(other_worker)}\n                if other_worker is not None:\n                    more_info.update(other_worker.info)\n                    running_tasks.append(more_info)\n\n            if task.status == PENDING and worker in task.workers:\n                locally_pending_tasks += 1\n                if len(task.workers) == 1:\n                    n_unique_pending += 1\n\n            if not best_task and self._schedulable(task) and self._has_resources(task.resources, greedy_resources):\n                if worker in task.workers and self._has_resources(task.resources, used_resources):\n                    best_task = task\n                    best_task_id = task.id\n                else:\n                    # keep track of the resources used in greedy scheduling\n                    for resource, amount in (task.resources or {}).items():\n                        greedy_resources[resource] += amount\n\n        if best_task:\n            best_task.status = RUNNING\n            best_task.worker_running = worker\n            best_task.time_running = time.time()\n            self._update_task_history(best_task.id, RUNNING, host=host)\n\n        return {'n_pending_tasks': locally_pending_tasks,\n                'n_unique_pending': n_unique_pending,\n                'task_id': best_task_id,\n                'running_tasks': running_tasks}\n\n    def ping(self, worker):\n        self.update(worker)\n\n    def _upstream_status(self, task_id, upstream_status_table):\n        if task_id in upstream_status_table:\n            return upstream_status_table[task_id]\n        elif self._state.has_task(task_id):\n            task_stack = [task_id]\n\n            while task_stack:\n                dep_id = task_stack.pop()\n                if self._state.has_task(dep_id):\n                    dep = self._state.get_task(dep_id)\n                    if dep_id not in upstream_status_table:\n                        if dep.status == PENDING and dep.deps:\n                            task_stack = task_stack + [dep_id] + list(dep.deps)\n                            upstream_status_table[dep_id] = ''  # will be updated postorder\n                        else:\n                            dep_status = STATUS_TO_UPSTREAM_MAP.get(dep.status, '')\n                            upstream_status_table[dep_id] = dep_status\n                    elif upstream_status_table[dep_id] == '' and dep.deps:\n                        # This is the postorder update step when we set the\n                        # status based on the previously calculated child elements\n                        upstream_status = [upstream_status_table.get(id, '') for id in dep.deps]\n                        upstream_status.append('')  # to handle empty list\n                        status = max(upstream_status, key=UPSTREAM_SEVERITY_KEY)\n                        upstream_status_table[dep_id] = status\n            return upstream_status_table[dep_id]\n\n    def _serialize_task(self, task_id, include_deps=True):\n        task = self._state.get_task(task_id)\n        ret = {\n            'deps': list(task.deps),\n            'status': task.status,\n            'workers': list(task.workers),\n            'worker_running': task.worker_running,\n            'time_running': getattr(task, \"time_running\", None),\n            'start_time': task.time,\n            'params': task.params,\n            'name': task.family,\n            'priority': task.priority,\n            'resources': task.resources,\n        }\n        if include_deps:\n            ret['deps'] = list(task.deps)\n        return ret\n\n    def graph(self):\n        self.prune()\n        serialized = {}\n        for task in self._state.get_active_tasks():\n            serialized[task.id] = self._serialize_task(task.id)\n        return serialized\n\n    def _recurse_deps(self, task_id, serialized):\n        if task_id not in serialized:\n            task = self._state.get_task(task_id)\n            if task is None or not task.family:\n                logger.warn('Missing task for id [%s]', task_id)\n\n                # try to infer family and params from task_id\n                try:\n                    family, _, param_str = task_id.rstrip(')').partition('(')\n                    params = dict(param.split('=') for param in param_str.split(', '))\n                except:\n                    family, params = '', {}\n                serialized[task_id] = {\n                    'deps': [],\n                    'status': UNKNOWN,\n                    'workers': [],\n                    'start_time': UNKNOWN,\n                    'params': params,\n                    'name': family,\n                    'priority': 0,\n                }\n            else:\n                serialized[task_id] = self._serialize_task(task_id)\n                for dep in task.deps:\n                    self._recurse_deps(dep, serialized)\n\n    def dep_graph(self, task_id):\n        self.prune()\n        serialized = {}\n        if self._state.has_task(task_id):\n            self._recurse_deps(task_id, serialized)\n        return serialized\n\n    def task_list(self, status, upstream_status):\n        ''' query for a subset of tasks by status '''\n        self.prune()\n        result = {}\n        upstream_status_table = {}  # used to memoize upstream status\n        for task in self._state.get_active_tasks():\n            if not status or task.status == status:\n                if (task.status != PENDING or not upstream_status or\n                    upstream_status == self._upstream_status(task.id, upstream_status_table)):\n                    serialized = self._serialize_task(task.id, False)\n                    result[task.id] = serialized\n        return result\n\n    def worker_list(self, include_running=True):\n        self.prune()\n        workers = [\n            dict(\n                name=worker.id,\n                last_active=worker.last_active,\n                started=getattr(worker, 'started', None),\n                **worker.info\n            ) for worker in self._state.get_active_workers()]\n        workers.sort(key=lambda worker: worker['started'], reverse=True)\n        if include_running:\n            running = collections.defaultdict(dict)\n            num_pending = collections.defaultdict(int)\n            num_uniques = collections.defaultdict(int)\n            for task in self._state.get_pending_tasks():\n                if task.status == RUNNING and task.worker_running:\n                    running[task.worker_running][task.id] = self._serialize_task(task.id, False)\n                elif task.status == PENDING:\n                    for worker in task.workers:\n                        num_pending[worker] += 1\n                    if len(task.workers) == 1:\n                        num_uniques[list(task.workers)[0]] += 1\n            for worker in workers:\n                tasks = running[worker['name']]\n                worker['num_running'] = len(tasks)\n                worker['num_pending'] = num_pending[worker['name']]\n                worker['num_uniques'] = num_uniques[worker['name']]\n                worker['running'] = tasks\n        return workers\n\n    def inverse_dependencies(self, task_id):\n        self.prune()\n        serialized = {}\n        if self._state.has_task(task_id):\n            self._traverse_inverse_deps(task_id, serialized)\n        return serialized\n\n    def _traverse_inverse_deps(self, task_id, serialized):\n        stack = [task_id]\n        serialized[task_id] = self._serialize_task(task_id)\n        while len(stack) > 0:\n            curr_id = stack.pop()\n            for task in self._state.get_active_tasks():\n                if curr_id in task.deps:\n                    serialized[curr_id][\"deps\"].append(task.id)\n                    if task.id not in serialized:\n                        serialized[task.id] = self._serialize_task(task.id)\n                        serialized[task.id][\"deps\"] = []\n                        stack.append(task.id)\n\n    def task_search(self, task_str):\n        ''' query for a subset of tasks by task_id '''\n        self.prune()\n        result = collections.defaultdict(dict)\n        for task in self._state.get_active_tasks():\n            if task.id.find(task_str) != -1:\n                serialized = self._serialize_task(task.id, False)\n                result[task.status][task.id] = serialized\n        return result\n\n    def fetch_error(self, task_id):\n        if self._state.has_task(task_id):\n            return {\"taskId\": task_id, \"error\": self._state.get_task(task_id).expl}\n        else:\n            return {\"taskId\": task_id, \"error\": \"\"}\n\n    def _update_task_history(self, task_id, status, host=None):\n        try:\n            if status == DONE or status == FAILED:\n                successful = (status == DONE)\n                self._task_history.task_finished(task_id, successful)\n            elif status == PENDING:\n                self._task_history.task_scheduled(task_id)\n            elif status == RUNNING:\n                self._task_history.task_started(task_id, host)\n        except:\n            logger.warning(\"Error saving Task history\", exc_info=1)\n\n    @property\n    def task_history(self):\n        # Used by server.py to expose the calls\n        return self._task_history\n",
          "file_patch": "@@ -408,6 +408,7 @@ class CentralPlannerScheduler(Scheduler):\n         running_tasks = []\n \n         used_resources = self._used_resources()\n+        greedy_resources = collections.defaultdict(int)\n         n_unique_pending = 0\n \n         tasks = list(self._state.get_pending_tasks())\n@@ -428,14 +429,14 @@ class CentralPlannerScheduler(Scheduler):\n                 if len(task.workers) == 1:\n                     n_unique_pending += 1\n \n-            if not best_task and self._schedulable(task):\n+            if not best_task and self._schedulable(task) and self._has_resources(task.resources, greedy_resources):\n                 if worker in task.workers and self._has_resources(task.resources, used_resources):\n                     best_task = task\n                     best_task_id = task.id\n                 else:\n                     # keep track of the resources used in greedy scheduling\n                     for resource, amount in (task.resources or {}).items():\n-                        used_resources[resource] += amount\n+                        greedy_resources[resource] += amount\n \n         if best_task:\n             best_task.status = RUNNING\n",
          "files_name_in_blame_commit": [
            "scheduler.py",
            "central_planner_test.py"
          ]
        }
      },
      "4bcb2ca8623f807037db8e012950fef867b30357": {
        "commit": {
          "commit_id": "4bcb2ca8623f807037db8e012950fef867b30357",
          "commit_message": "This refactoring is functionally equivalent, but separates the state of the scheduler into its own class. The interface between the scheduler and the state is defined in a way so that it can be easily replaced with a SQL database or some other storage engine.",
          "commit_author": "Erik Bernhardsson",
          "commit_date": "2014-10-07 01:57:27",
          "commit_parent": "ade5c78ff42359f1456df09bc80dc7851504e0b8"
        },
        "function": {
          "function_name": "get_work",
          "function_code_before": "",
          "function_code_after": "def get_work(self, worker, host=None):\n    self.update(worker, {'host': host})\n    best_task = None\n    best_task_id = None\n    locally_pending_tasks = 0\n    running_tasks = []\n    used_resources = self._used_resources()\n    n_unique_pending = 0\n    tasks = list(self._state.get_pending_tasks())\n    tasks.sort(key=self._rank(), reverse=True)\n    for task in tasks:\n        if task.status == 'RUNNING' and worker in task.workers:\n            other_worker = self._state.get_worker(task.worker_running)\n            more_info = {'task_id': task.id, 'worker': str(other_worker)}\n            if other_worker is not None:\n                more_info.update(other_worker.info)\n                running_tasks.append(more_info)\n        if task.status == PENDING and worker in task.workers:\n            locally_pending_tasks += 1\n            if len(task.workers) == 1:\n                n_unique_pending += 1\n        if not best_task and self._schedulable(task):\n            if worker in task.workers and self._has_resources(task.resources, used_resources):\n                best_task = task\n                best_task_id = task.id\n            else:\n                for (resource, amount) in (task.resources or {}).items():\n                    used_resources[resource] += amount\n    if best_task:\n        best_task.status = RUNNING\n        best_task.worker_running = worker\n        best_task.time_running = time.time()\n        self._update_task_history(best_task.id, RUNNING, host=host)\n    return {'n_pending_tasks': locally_pending_tasks, 'n_unique_pending': n_unique_pending, 'task_id': best_task_id, 'running_tasks': running_tasks}",
          "function_before_start_line": "",
          "function_before_end_line": "",
          "function_after_start_line": 388,
          "function_after_end_line": 448,
          "function_before_token_count": 0,
          "function_after_token_count": 286,
          "functions_name_modified_file": [
            "get_worker_ids",
            "_update_task_history",
            "prune",
            "_schedulable",
            "_upstream_status",
            "ping",
            "get_pending_tasks",
            "_traverse_inverse_deps",
            "load",
            "inverse_dependencies",
            "task_search",
            "_recurse_deps",
            "get_worker",
            "get_active_workers",
            "add_task",
            "update_resources",
            "_update_priority",
            "task_list",
            "add_info",
            "get_active_tasks",
            "_has_resources",
            "_used_resources",
            "has_task",
            "_serialize_task",
            "task_history",
            "dep_graph",
            "__str__",
            "graph",
            "dump",
            "inactivate_tasks",
            "__repr__",
            "__init__",
            "update",
            "add_worker",
            "get_task",
            "_rank",
            "get_work",
            "fetch_error",
            "inactivate_workers"
          ],
          "functions_name_all_files": [
            "get_worker_ids",
            "_update_task_history",
            "prune",
            "_schedulable",
            "_upstream_status",
            "ping",
            "get_pending_tasks",
            "_traverse_inverse_deps",
            "load",
            "inverse_dependencies",
            "task_search",
            "_recurse_deps",
            "get_worker",
            "test_load_old_state",
            "get_active_workers",
            "add_task",
            "update_resources",
            "_update_priority",
            "task_list",
            "add_info",
            "get_active_tasks",
            "_has_resources",
            "_used_resources",
            "has_task",
            "_serialize_task",
            "task_history",
            "dep_graph",
            "test_load_broken_state",
            "__str__",
            "graph",
            "dump",
            "inactivate_tasks",
            "__repr__",
            "__init__",
            "update",
            "add_worker",
            "get_task",
            "_rank",
            "get_work",
            "fetch_error",
            "inactivate_workers"
          ],
          "functions_name_co_evolved_modified_file": [
            "get_worker_ids",
            "prune",
            "_schedulable",
            "_upstream_status",
            "_traverse_inverse_deps",
            "get_pending_tasks",
            "inverse_dependencies",
            "load",
            "task_search",
            "_recurse_deps",
            "get_worker",
            "get_active_workers",
            "add_task",
            "_update_priority",
            "task_list",
            "get_active_tasks",
            "_used_resources",
            "has_task",
            "_serialize_task",
            "dep_graph",
            "graph",
            "dump",
            "inactivate_tasks",
            "__init__",
            "update",
            "_not_schedulable",
            "add_worker",
            "get_task",
            "_rank",
            "fetch_error",
            "inactivate_workers"
          ],
          "functions_name_co_evolved_all_files": [
            "get_worker_ids",
            "prune",
            "_schedulable",
            "_upstream_status",
            "_traverse_inverse_deps",
            "get_pending_tasks",
            "inverse_dependencies",
            "load",
            "task_search",
            "_recurse_deps",
            "get_worker",
            "test_load_old_state",
            "get_active_workers",
            "add_task",
            "_update_priority",
            "task_list",
            "get_active_tasks",
            "_used_resources",
            "has_task",
            "_serialize_task",
            "dep_graph",
            "test_load_broken_state",
            "graph",
            "dump",
            "inactivate_tasks",
            "__init__",
            "update",
            "_not_schedulable",
            "add_worker",
            "get_task",
            "_rank",
            "fetch_error",
            "inactivate_workers"
          ]
        },
        "file": {
          "file_name": "scheduler.py",
          "file_nloc": 426,
          "file_complexity": 165,
          "file_token_count": 3254,
          "file_before": "# Copyright (c) 2012 Spotify AB\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"); you may not\n# use this file except in compliance with the License. You may obtain a copy of\n# the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n# License for the specific language governing permissions and limitations under\n# the License.\n\nimport collections\nimport datetime\nimport os\nimport logging\nimport time\nimport cPickle as pickle\nimport task_history as history\nlogger = logging.getLogger(\"luigi.server\")\n\nfrom task_status import PENDING, FAILED, DONE, RUNNING, SUSPENDED, UNKNOWN\n\n\nclass Scheduler(object):\n    ''' Abstract base class\n\n    Note that the methods all take string arguments, not Task objects...\n    '''\n    add_task = NotImplemented\n    get_work = NotImplemented\n    ping = NotImplemented\n\nUPSTREAM_RUNNING = 'UPSTREAM_RUNNING'\nUPSTREAM_MISSING_INPUT = 'UPSTREAM_MISSING_INPUT'\nUPSTREAM_FAILED = 'UPSTREAM_FAILED'\n\nUPSTREAM_SEVERITY_ORDER = ('', UPSTREAM_RUNNING, UPSTREAM_MISSING_INPUT, UPSTREAM_FAILED)\nUPSTREAM_SEVERITY_KEY = lambda st: UPSTREAM_SEVERITY_ORDER.index(st)\nSTATUS_TO_UPSTREAM_MAP = {FAILED: UPSTREAM_FAILED, RUNNING: UPSTREAM_RUNNING, PENDING: UPSTREAM_MISSING_INPUT}\n\n\nclass Task(object):\n    def __init__(self, status, deps, resources={}, priority=0, family='', params={}):\n        self.stakeholders = set()  # workers ids that are somehow related to this task (i.e. don't prune while any of these workers are still active)\n        self.workers = set()  # workers ids that can perform task - task is 'BROKEN' if none of these workers are active\n        if deps is None:\n            self.deps = set()\n        else:\n            self.deps = set(deps)\n        self.status = status  # PENDING, RUNNING, FAILED or DONE\n        self.time = time.time()  # Timestamp when task was first added\n        self.retry = None\n        self.remove = None\n        self.worker_running = None  # the worker id that is currently running the task or None\n        self.time_running = None  # Timestamp when picked up by worker\n        self.expl = None\n        self.priority = priority\n        self.resources = resources\n        self.family = family\n        self.params = params\n\n    def __repr__(self):\n        return \"Task(%r)\" % vars(self)\n\n\nclass Worker(object):\n    \"\"\" Structure for tracking worker activity and keeping their references \"\"\"\n    def __init__(self, id, last_active=None):\n        self.id = id\n        self.reference = None  # reference to the worker in the real world. (Currently a dict containing just the host)\n        self.last_active = last_active  # seconds since epoch\n        self.info = {}\n\n    def add_info(self, info):\n        self.info.update(info)\n\n    def __str__(self):\n        return self.id\n\n\nclass CentralPlannerScheduler(Scheduler):\n    ''' Async scheduler that can handle multiple workers etc\n\n    Can be run locally or on a server (using RemoteScheduler + server.Server).\n    '''\n\n    def __init__(self, retry_delay=900.0, remove_delay=600.0, worker_disconnect_delay=60.0,\n                 state_path='/var/lib/luigi-server/state.pickle', task_history=None,\n                 resources=None):\n        '''\n        (all arguments are in seconds)\n        Keyword Arguments:\n        retry_delay -- How long after a Task fails to try it again, or -1 to never retry\n        remove_delay -- How long after a Task finishes to remove it from the scheduler\n        state_path -- Path to state file (tasks and active workers)\n        worker_disconnect_delay -- If a worker hasn't communicated for this long, remove it from active workers\n        '''\n        self._state_path = state_path\n        self._tasks = {}  # map from id to a Task object\n        self._retry_delay = retry_delay\n        self._remove_delay = remove_delay\n        self._worker_disconnect_delay = worker_disconnect_delay\n        self._active_workers = {}  # map from id to a Worker object\n        self._task_history = task_history or history.NopHistory()\n        self._resources = resources\n\n    def dump(self):\n        state = (self._tasks, self._active_workers)\n        try:\n            with open(self._state_path, 'w') as fobj:\n                pickle.dump(state, fobj)\n        except IOError:\n            logger.warning(\"Failed saving scheduler state\", exc_info=1)\n        else:\n            logger.info(\"Saved state in %s\", self._state_path)\n\n    # prone to lead to crashes when old state is unpickled with updated code. TODO some kind of version control?\n    def load(self):\n        if os.path.exists(self._state_path):\n            logger.info(\"Attempting to load state from %s\", self._state_path)\n            try:\n                with open(self._state_path) as fobj:\n                    state = pickle.load(fobj)\n            except:\n                logger.exception(\"Error when loading state. Starting from clean slate.\")\n                return\n\n            self._tasks, self._active_workers = state\n\n            # Convert from old format\n            # TODO: this is really ugly, we need something more future-proof\n            # Every time we add an attribute to the Worker class, this code needs to be updated\n            for k, v in self._active_workers.iteritems():\n                if isinstance(v, float):\n                    self._active_workers[k] = Worker(id=k, last_active=v)\n        else:\n            logger.info(\"No prior state file exists at %s. Starting with clean slate\", self._state_path)\n\n    def prune(self):\n        logger.info(\"Starting pruning of task graph\")\n        # Delete workers that haven't said anything for a while (probably killed)\n        delete_workers = []\n        for worker in self._active_workers.values():\n            if worker.last_active < time.time() - self._worker_disconnect_delay:\n                logger.info(\"Worker %s timed out (no contact for >=%ss)\", worker, self._worker_disconnect_delay)\n                delete_workers.append(worker.id)\n\n        for worker in delete_workers:\n            self._active_workers.pop(worker)\n\n        remaining_workers = set(self._active_workers.keys())\n\n        # Mark tasks with no remaining active stakeholders for deletion\n        for task_id, task in self._tasks.iteritems():\n            if not task.stakeholders.intersection(remaining_workers):\n                if task.remove is None:\n                    logger.info(\"Task %r has stakeholders %r but none remain connected -> will remove task in %s seconds\", task_id, task.stakeholders, self._remove_delay)\n                    task.remove = time.time() + self._remove_delay\n\n            if task.status == RUNNING and task.worker_running and task.worker_running not in remaining_workers:\n                # If a running worker disconnects, tag all its jobs as FAILED and subject it to the same retry logic\n                logger.info(\"Task %r is marked as running by disconnected worker %r -> marking as FAILED with retry delay of %rs\", task_id, task.worker_running, self._retry_delay)\n                task.worker_running = None\n                task.status = FAILED\n                task.retry = time.time() + self._retry_delay\n\n        # Remove tasks that have no stakeholders\n        remove_tasks = []\n        for task_id, task in self._tasks.iteritems():\n            if task.remove and time.time() > task.remove:\n                logger.info(\"Removing task %r (no connected stakeholders)\", task_id)\n                remove_tasks.append(task_id)\n\n        for task_id in remove_tasks:\n            self._tasks.pop(task_id)\n\n        # Reset FAILED tasks to PENDING if max timeout is reached, and retry delay is >= 0\n        for task in self._tasks.values():\n            if task.status == FAILED and self._retry_delay >= 0 and task.retry < time.time():\n                task.status = PENDING\n        logger.info(\"Done pruning task graph\")\n\n    def update(self, worker_id, worker_reference=None):\n        \"\"\" Keep track of whenever the worker was last active \"\"\"\n        worker = self._active_workers.setdefault(worker_id, Worker(worker_id))\n        if worker_reference:\n            worker.reference = worker_reference\n        worker.last_active = time.time()\n\n    def _update_priority(self, task, prio, worker):\n        \"\"\" Update priority of the given task\n\n        Priority can only be increased. If the task doesn't exist, a placeholder\n        task is created to preserve priority when the task is later scheduled.\n        \"\"\"\n        task.priority = prio = max(prio, task.priority)\n        for dep in task.deps or []:\n            t = self._tasks[dep] # This should always exist, see add_task\n            if prio > t.priority:\n                self._update_priority(t, prio, worker)\n\n    def add_task(self, worker, task_id, status=PENDING, runnable=True,\n                 deps=None, new_deps=None, expl=None, resources=None,\n                 priority=0, family='', params={}):\n        \"\"\"\n        * Add task identified by task_id if it doesn't exist\n        * If deps is not None, update dependency list\n        * Update status of task\n        * Add additional workers/stakeholders\n        * Update priority when needed\n        \"\"\"\n        self.update(worker)\n\n        task = self._tasks.setdefault(task_id, Task(\n            status=PENDING, deps=deps, resources=resources, priority=priority, family=family,\n            params=params))\n\n        # for setting priority, we'll sometimes create tasks with unset family and params\n        if not task.family:\n            task.family = family\n        if not task.params:\n            task.params = params\n\n        if task.remove is not None:\n            task.remove = None  # unmark task for removal so it isn't removed after being added\n\n        if not (task.status == RUNNING and status == PENDING):\n            # don't allow re-scheduling of task while it is running, it must either fail or succeed first\n            if status == PENDING or status != task.status:\n                # Update the DB only if there was a acctual change, to prevent noise.\n                # We also check for status == PENDING b/c that's the default value\n                # (so checking for status != task.status woule lie)\n                self._update_task_history(task_id, status)\n            task.status = PENDING if status == SUSPENDED else status\n            if status == FAILED:\n                task.retry = time.time() + self._retry_delay\n\n        if deps is not None:\n            task.deps = set(deps)\n\n        if new_deps is not None:\n            task.deps.update(new_deps)\n\n        task.stakeholders.add(worker)\n        task.resources = resources\n\n        # Task dependencies might not exist yet. Let's create dummy tasks for them for now.\n        # Otherwise the task dependencies might end up being pruned if scheduling takes a long time\n        for dep in task.deps or []:\n            t = self._tasks.setdefault(dep, Task(status=UNKNOWN, deps=None, priority=priority))\n            t.stakeholders.add(worker)\n\n        self._update_priority(task, priority, worker)\n\n        if runnable:\n            task.workers.add(worker)\n\n        if expl is not None:\n            task.expl = expl\n\n    def add_worker(self, worker, info):\n        self._active_workers[worker].add_info(info)\n\n    def update_resources(self, **resources):\n        if self._resources is None:\n            self._resources = {}\n        self._resources.update(resources)\n\n    def _has_resources(self, needed_resources, used_resources):\n        if needed_resources is None:\n            return True\n\n        available_resources = self._resources or {}\n        for resource, amount in needed_resources.items():\n            if amount + used_resources[resource] > available_resources.get(resource, 1):\n                return False\n        return True\n\n    def _used_resources(self):\n        used_resources = collections.defaultdict(int)\n        if self._resources is not None:\n            for task in self._tasks.itervalues():\n                if task.status == RUNNING and task.resources:\n                    for resource, amount in task.resources.items():\n                        used_resources[resource] += amount\n        return used_resources\n\n    def _rank(self, worker):\n        ''' Return worker's rank function for task scheduling '''\n        dependents = collections.defaultdict(int)\n        not_done = lambda t: t not in self._tasks or self._tasks[t].status != DONE\n        for task_id, task in self._tasks.iteritems():\n            if task.status != DONE:\n                deps = filter(not_done, task.deps)\n                inverse_num_deps = 1.0 / max(len(deps), 1)\n                for dep in deps:\n                    dependents[dep] += inverse_num_deps\n\n        return lambda (task_id, task): (task.priority, worker in task.workers, dependents[task_id], -task.time)\n\n    def _not_schedulable(self, task, used_resources):\n        return any((\n            task.status != PENDING,\n            any(dep not in self._tasks or self._tasks[dep].status != DONE for dep in task.deps),\n            not self._has_resources(task.resources, used_resources)\n        ))\n\n    def get_work(self, worker, host=None):\n        # TODO: remove any expired nodes\n\n        # Algo: iterate over all nodes, find the highest priority node no dependencies and available\n        # resources.\n\n        # Resource checking looks both at currently available resources and at which resources would\n        # be available if all running tasks died and we rescheduled all workers greedily. We do both\n        # checks in order to prevent a worker with many low-priority tasks from starving other\n        # workers with higher priority tasks that share the same resources.\n\n        # TODO: remove tasks that can't be done, figure out if the worker has absolutely\n        # nothing it can wait for\n\n        # Return remaining tasks that have no FAILED descendents\n        self.update(worker, {'host': host})\n        best_task = None\n        locally_pending_tasks = 0\n        running_tasks = []\n        used_resources = self._used_resources()\n        potential_resources = collections.defaultdict(int)\n        potential_workers = set([worker])\n        n_unique_pending = 0\n\n        for task_id, task in sorted(self._tasks.iteritems(), key=self._rank(worker), reverse=True):\n            if task.status == RUNNING and worker in task.workers:\n                # Return a list of currently running tasks to the client,\n                # makes it easier to troubleshoot\n                other_worker = self._active_workers[task.worker_running]\n                more_info = {'task_id': task_id, 'worker': str(other_worker)}\n                if other_worker is not None:\n                    more_info.update(other_worker.info)\n                running_tasks.append(more_info)\n\n            if task.status == PENDING and worker in task.workers:\n                locally_pending_tasks += 1\n                if len(task.workers) == 1:\n                    n_unique_pending += 1\n\n            if self._not_schedulable(task, potential_resources) or best_task:\n                continue\n\n            if worker in task.workers and self._has_resources(task.resources, used_resources):\n                best_task = task_id\n            else:\n                # keep track of the resources used in greedy scheduling\n                for w in filter(lambda w: w not in potential_workers, task.workers):\n                    for resource, amount in (task.resources or {}).items():\n                        potential_resources[resource] += amount\n                    potential_workers.add(w)\n\n        if best_task:\n            t = self._tasks[best_task]\n            t.status = RUNNING\n            t.worker_running = worker\n            t.time_running = time.time()\n            self._update_task_history(best_task, RUNNING, host=host)\n\n        return {'n_pending_tasks': locally_pending_tasks,\n                'n_unique_pending': n_unique_pending,\n                'task_id': best_task,\n                'running_tasks': running_tasks}\n\n    def ping(self, worker):\n        self.update(worker)\n\n    def _upstream_status(self, task_id, upstream_status_table):\n        if task_id in upstream_status_table:\n            return upstream_status_table[task_id]\n        elif task_id in self._tasks:\n            task_stack = [task_id]\n\n            while task_stack:\n                dep_id = task_stack.pop()\n                if dep_id in self._tasks:\n                    dep = self._tasks[dep_id]\n                    if dep_id not in upstream_status_table:\n                        if dep.status == PENDING and dep.deps:\n                            task_stack = task_stack + [dep_id] + list(dep.deps)\n                            upstream_status_table[dep_id] = ''  # will be updated postorder\n                        else:\n                            dep_status = STATUS_TO_UPSTREAM_MAP.get(dep.status, '')\n                            upstream_status_table[dep_id] = dep_status\n                    elif upstream_status_table[dep_id] == '' and dep.deps:\n                        # This is the postorder update step when we set the\n                        # status based on the previously calculated child elements\n                        upstream_status = [upstream_status_table.get(id, '') for id in dep.deps]\n                        upstream_status.append('')  # to handle empty list\n                        status = max(upstream_status, key=UPSTREAM_SEVERITY_KEY)\n                        upstream_status_table[dep_id] = status\n            return upstream_status_table[dep_id]\n\n    def _serialize_task(self, task_id, include_deps=True):\n        task = self._tasks[task_id]\n        ret = {\n            'status': task.status,\n            'workers': list(task.workers),\n            'worker_running': task.worker_running,\n            'time_running': getattr(task, \"time_running\", None),\n            'start_time': task.time,\n            'params': task.params,\n            'name': task.family,\n            'priority': task.priority,\n            'resources': task.resources,\n        }\n        if include_deps:\n            ret['deps'] = list(task.deps)\n        return ret\n\n    def graph(self):\n        self.prune()\n        serialized = {}\n        for task_id, task in self._tasks.iteritems():\n            serialized[task_id] = self._serialize_task(task_id)\n        return serialized\n\n    def _recurse_deps(self, task_id, serialized):\n        if task_id not in serialized:\n            task = self._tasks.get(task_id)\n            if task is None or not task.family:\n                logger.warn('Missing task for id [%s]', task_id)\n\n                # try to infer family and params from task_id\n                try:\n                    family, _, param_str = task_id.rstrip(')').partition('(')\n                    params = dict(param.split('=') for param in param_str.split(', '))\n                except:\n                    family, params = '', {}\n                serialized[task_id] = {\n                    'deps': [],\n                    'status': UNKNOWN,\n                    'workers': [],\n                    'start_time': UNKNOWN,\n                    'params': params,\n                    'name': family,\n                    'priority': 0,\n                }\n            else:\n                serialized[task_id] = self._serialize_task(task_id)\n                for dep in task.deps:\n                    self._recurse_deps(dep, serialized)\n\n    def dep_graph(self, task_id):\n        self.prune()\n        serialized = {}\n        if task_id in self._tasks:\n            self._recurse_deps(task_id, serialized)\n        return serialized\n\n    def task_list(self, status, upstream_status):\n        ''' query for a subset of tasks by status '''\n        self.prune()\n        result = {}\n        upstream_status_table = {}  # used to memoize upstream status\n        for task_id, task in self._tasks.iteritems():\n            if not status or task.status == status:\n                if (task.status != PENDING or not upstream_status or\n                    upstream_status == self._upstream_status(task_id, upstream_status_table)):\n                    serialized = self._serialize_task(task_id, False)\n                    result[task_id] = serialized\n        return result\n\n    def inverse_dependencies(self, task_id):\n        self.prune()\n        serialized = {}\n        if task_id in self._tasks:\n            self._traverse_inverse_deps(task_id, serialized)\n        return serialized\n\n    def _traverse_inverse_deps(self, task_id, serialized):\n        stack = [task_id]\n        serialized[task_id] = self._serialize_task(task_id)\n        while len(stack) > 0:\n            curr_id = stack.pop()\n            for id, task in self._tasks.iteritems():\n                if curr_id in task.deps:\n                    serialized[curr_id][\"deps\"].append(id)\n                    if id not in serialized:\n                        serialized[id] = self._serialize_task(id)\n                        serialized[id][\"deps\"] = []\n                        stack.append(id)\n\n    def task_search(self, task_str):\n        ''' query for a subset of tasks by task_id '''\n        self.prune()\n        result = collections.defaultdict(dict)\n        for task_id, task in self._tasks.iteritems():\n            if task_id.find(task_str) != -1:\n                serialized = self._serialize_task(task_id, False)\n                result[task.status][task_id] = serialized\n        return result\n\n    def fetch_error(self, task_id):\n        if self._tasks[task_id].expl is not None:\n            return {\"taskId\": task_id, \"error\": self._tasks[task_id].expl}\n        else:\n            return {\"taskId\": task_id, \"error\": \"\"}\n\n    def _update_task_history(self, task_id, status, host=None):\n        try:\n            if status == DONE or status == FAILED:\n                successful = (status == DONE)\n                self._task_history.task_finished(task_id, successful)\n            elif status == PENDING:\n                self._task_history.task_scheduled(task_id)\n            elif status == RUNNING:\n                self._task_history.task_started(task_id, host)\n        except:\n            logger.warning(\"Error saving Task history\", exc_info=1)\n\n    @property\n    def task_history(self):\n        # Used by server.py to expose the calls\n        return self._task_history\n",
          "file_after": "# Copyright (c) 2012 Spotify AB\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"); you may not\n# use this file except in compliance with the License. You may obtain a copy of\n# the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n# License for the specific language governing permissions and limitations under\n# the License.\n\nimport collections\nimport datetime\nimport os\nimport logging\nimport time\nimport cPickle as pickle\nimport bisect\nimport task_history as history\nlogger = logging.getLogger(\"luigi.server\")\n\nfrom task_status import PENDING, FAILED, DONE, RUNNING, SUSPENDED, UNKNOWN\n\n\nclass Scheduler(object):\n    ''' Abstract base class\n\n    Note that the methods all take string arguments, not Task objects...\n    '''\n    add_task = NotImplemented\n    get_work = NotImplemented\n    ping = NotImplemented\n\nUPSTREAM_RUNNING = 'UPSTREAM_RUNNING'\nUPSTREAM_MISSING_INPUT = 'UPSTREAM_MISSING_INPUT'\nUPSTREAM_FAILED = 'UPSTREAM_FAILED'\n\nUPSTREAM_SEVERITY_ORDER = ('', UPSTREAM_RUNNING, UPSTREAM_MISSING_INPUT, UPSTREAM_FAILED)\nUPSTREAM_SEVERITY_KEY = lambda st: UPSTREAM_SEVERITY_ORDER.index(st)\nSTATUS_TO_UPSTREAM_MAP = {FAILED: UPSTREAM_FAILED, RUNNING: UPSTREAM_RUNNING, PENDING: UPSTREAM_MISSING_INPUT}\n\n\nclass Task(object):\n    def __init__(self, id, status, deps, resources={}, priority=0, family='', params={}):\n        self.id = id\n        self.stakeholders = set()  # workers ids that are somehow related to this task (i.e. don't prune while any of these workers are still active)\n        self.workers = set()  # workers ids that can perform task - task is 'BROKEN' if none of these workers are active\n        if deps is None:\n            self.deps = set()\n        else:\n            self.deps = set(deps)\n        self.status = status  # PENDING, RUNNING, FAILED or DONE\n        self.time = time.time()  # Timestamp when task was first added\n        self.retry = None\n        self.remove = None\n        self.worker_running = None  # the worker id that is currently running the task or None\n        self.time_running = None  # Timestamp when picked up by worker\n        self.expl = None\n        self.priority = priority\n        self.resources = resources\n        self.family = family\n        self.params = params\n\n    def __repr__(self):\n        return \"Task(%r)\" % vars(self)\n\n\nclass Worker(object):\n    \"\"\" Structure for tracking worker activity and keeping their references \"\"\"\n    def __init__(self, id, last_active=None):\n        self.id = id\n        self.reference = None  # reference to the worker in the real world. (Currently a dict containing just the host)\n        self.last_active = last_active  # seconds since epoch\n        self.info = {}\n\n    def add_info(self, info):\n        self.info.update(info)\n\n    def __str__(self):\n        return self.id\n\n\nclass SimpleTaskState(object):\n    ''' Keep track of the current state and handle persistance\n\n    The point of this class is to enable other ways to keep state, eg. by using a database\n    These will be implemented by creating an abstract base class that this and other classes\n    inherit from.\n    '''\n\n    def __init__(self, retry_delay=900.0, remove_delay=600.0, worker_disconnect_delay=60.0,\n                 state_path='/var/lib/luigi-server/state.pickle', task_history=None,\n                 resources=None):\n        '''\n        (all arguments are in seconds)\n        Keyword Arguments:\n        retry_delay -- How long after a Task fails to try it again, or -1 to never retry\n        remove_delay -- How long after a Task finishes to remove it from the scheduler\n        state_path -- Path to state file (tasks and active workers)\n        worker_disconnect_delay -- If a worker hasn't communicated for this long, remove it from active workers\n        '''\n    def __init__(self, state_path):\n        self._state_path = state_path\n        self._tasks = {}  # map from id to a Task object\n        self._active_workers = {}  # map from id to a Worker object\n\n    def dump(self):\n        state = (self._tasks, self._active_workers)\n        try:\n            with open(self._state_path, 'w') as fobj:\n                pickle.dump(state, fobj)\n        except IOError:\n            logger.warning(\"Failed saving scheduler state\", exc_info=1)\n        else:\n            logger.info(\"Saved state in %s\", self._state_path)\n\n    # prone to lead to crashes when old state is unpickled with updated code. TODO some kind of version control?\n    def load(self):\n        if os.path.exists(self._state_path):\n            logger.info(\"Attempting to load state from %s\", self._state_path)\n            try:\n                with open(self._state_path) as fobj:\n                    state = pickle.load(fobj)\n            except:\n                logger.exception(\"Error when loading state. Starting from clean slate.\")\n                return\n\n            self._tasks, self._active_workers = state\n\n            # Convert from old format\n            # TODO: this is really ugly, we need something more future-proof\n            # Every time we add an attribute to the Worker class, this code needs to be updated\n            for k, v in self._active_workers.iteritems():\n                if isinstance(v, float):\n                    self._active_workers[k] = Worker(id=k, last_active=v)\n        else:\n            logger.info(\"No prior state file exists at %s. Starting with clean slate\", self._state_path)\n\n    def get_active_tasks(self):\n        for task in self._tasks.itervalues():\n            yield task\n\n    def get_pending_tasks(self):\n        for task in self._tasks.itervalues():\n            if task.status in [PENDING, RUNNING]:\n                yield task\n\n    def get_task(self, task_id, default=None, setdefault=None):\n        if setdefault:\n            return self._tasks.setdefault(task_id, setdefault)\n        else:\n            return self._tasks.get(task_id, default)\n\n    def has_task(self, task_id):\n        return task_id in self._tasks\n\n    def inactivate_tasks(self, delete_tasks):\n        # The terminology is a bit confusing: we used to \"delete\" tasks when they became inactive,\n        # but with a pluggable state storage, you might very well want to keep some history of\n        # older tasks as well. That's why we call it \"inactivate\" (as in the verb)\n        for task in delete_tasks:\n            self._tasks.pop(task)\n\n    def get_active_workers(self, last_active_lt=None):\n        for worker in self._active_workers.itervalues():\n            if last_active_lt is not None and worker.last_active >= last_active_lt:\n                continue\n            yield worker\n\n    def get_worker_ids(self):\n        return self._active_workers.keys() # only used for unit tests\n\n    def get_worker(self, worker_id):\n        return self._active_workers.setdefault(worker_id, Worker(worker_id))\n\n    def inactivate_workers(self, delete_workers):\n        # Mark workers as inactive\n        for worker in delete_workers:\n            self._active_workers.pop(worker)\n        \n\nclass CentralPlannerScheduler(Scheduler):\n    ''' Async scheduler that can handle multiple workers etc\n\n    Can be run locally or on a server (using RemoteScheduler + server.Server).\n    '''\n\n    def __init__(self, retry_delay=900.0, remove_delay=600.0, worker_disconnect_delay=60.0,\n                 state_path='/var/lib/luigi-server/state.pickle', task_history=None,\n                 resources=None):\n        '''\n        (all arguments are in seconds)\n        Keyword Arguments:\n        retry_delay -- How long after a Task fails to try it again, or -1 to never retry\n        remove_delay -- How long after a Task finishes to remove it from the scheduler\n        state_path -- Path to state file (tasks and active workers)\n        worker_disconnect_delay -- If a worker hasn't communicated for this long, remove it from active workers\n        '''\n        self._retry_delay = retry_delay\n        self._remove_delay = remove_delay\n        self._worker_disconnect_delay = worker_disconnect_delay\n        self._task_history = task_history or history.NopHistory()\n        self._state = SimpleTaskState(state_path)\n\n        self._task_history = task_history or history.NopHistory()\n        self._resources = resources\n\n    def load(self):\n        self._state.load()\n\n    def dump(self):\n        self._state.dump()\n\n    def prune(self):\n        logger.info(\"Starting pruning of task graph\")\n        # Delete workers that haven't said anything for a while (probably killed)\n        delete_workers = []\n        for worker in self._state.get_active_workers(last_active_lt=time.time() - self._worker_disconnect_delay):\n            logger.info(\"Worker %s timed out (no contact for >=%ss)\", worker, self._worker_disconnect_delay)\n            delete_workers.append(worker.id)\n\n        self._state.inactivate_workers(delete_workers)\n\n        delete_workers = set(delete_workers)\n\n        remove_tasks = []\n        for task in self._state.get_active_tasks():\n            # Mark tasks with no remaining active stakeholders for deletion\n            task.stakeholders.difference_update(delete_workers)\n            if not task.stakeholders:\n                if task.remove is None:\n                    logger.info(\"Task %r has stakeholders %r but none remain connected -> will remove task in %s seconds\", task.id, task.stakeholders, self._remove_delay)\n                    task.remove = time.time() + self._remove_delay\n\n            # If a running worker disconnects, tag all its jobs as FAILED and subject it to the same retry logic\n            if task.status == RUNNING and task.worker_running and task.worker_running not in task.stakeholders:\n                logger.info(\"Task %r is marked as running by disconnected worker %r -> marking as FAILED with retry delay of %rs\", task.id, task.worker_running, self._retry_delay)\n                task.worker_running = None\n                task.status = FAILED\n                task.retry = time.time() + self._retry_delay\n\n            # Remove tasks that have no stakeholders\n            if task.remove and time.time() > task.remove:\n                logger.info(\"Removing task %r (no connected stakeholders)\", task.id)\n                remove_tasks.append(task.id)\n\n        # Reset FAILED tasks to PENDING if max timeout is reached, and retry delay is >= 0\n        for task in self._state.get_active_tasks():\n            if task.status == FAILED and self._retry_delay >= 0 and task.retry < time.time():\n                task.status = PENDING\n\n        self._state.inactivate_tasks(remove_tasks)\n\n        logger.info(\"Done pruning task graph\")\n\n    def update(self, worker_id, worker_reference=None):\n        \"\"\" Keep track of whenever the worker was last active \"\"\"\n        worker = self._state.get_worker(worker_id)\n        if worker_reference:\n            worker.reference = worker_reference\n        worker.last_active = time.time()\n\n    def _update_priority(self, task, prio, worker):\n        \"\"\" Update priority of the given task\n\n        Priority can only be increased. If the task doesn't exist, a placeholder\n        task is created to preserve priority when the task is later scheduled.\n        \"\"\"\n        task.priority = prio = max(prio, task.priority)\n        for dep in task.deps or []:\n            t = self._state.get_task(dep) # This should always exist, see add_task\n            if prio > t.priority:\n                self._update_priority(t, prio, worker)\n\n    def add_task(self, worker, task_id, status=PENDING, runnable=True,\n                 deps=None, new_deps=None, expl=None, resources=None,\n                 priority=0, family='', params={}):\n        \"\"\"\n        * Add task identified by task_id if it doesn't exist\n        * If deps is not None, update dependency list\n        * Update status of task\n        * Add additional workers/stakeholders\n        * Update priority when needed\n        \"\"\"\n        self.update(worker)\n\n        task = self._state.get_task(task_id, setdefault=Task(\n                id=task_id, status=PENDING, deps=deps, resources=resources,\n                priority=priority, family=family, params=params))\n        \n        # for setting priority, we'll sometimes create tasks with unset family and params\n        if not task.family:\n            task.family = family\n        if not task.params:\n            task.params = params\n\n        if task.remove is not None:\n            task.remove = None  # unmark task for removal so it isn't removed after being added\n\n        if not (task.status == RUNNING and status == PENDING):\n            # don't allow re-scheduling of task while it is running, it must either fail or succeed first\n            if status == PENDING or status != task.status:\n                # Update the DB only if there was a acctual change, to prevent noise.\n                # We also check for status == PENDING b/c that's the default value\n                # (so checking for status != task.status woule lie)\n                self._update_task_history(task_id, status)\n            task.status = PENDING if status == SUSPENDED else status\n            if status == FAILED:\n                task.retry = time.time() + self._retry_delay\n\n        if deps is not None:\n            task.deps = set(deps)\n\n        if new_deps is not None:\n            task.deps.update(new_deps)\n\n        task.stakeholders.add(worker)\n        task.resources = resources\n\n        # Task dependencies might not exist yet. Let's create dummy tasks for them for now.\n        # Otherwise the task dependencies might end up being pruned if scheduling takes a long time\n        for dep in task.deps or []:\n            t = self._state.get_task(dep, setdefault=Task(id=dep, status=UNKNOWN, deps=None, priority=priority))\n            t.stakeholders.add(worker)\n\n        self._update_priority(task, priority, worker)\n\n        if runnable:\n            task.workers.add(worker)\n\n        if expl is not None:\n            task.expl = expl\n\n    def add_worker(self, worker, info):\n        self._state.get_worker(worker).add_info(info)\n\n    def update_resources(self, **resources):\n        if self._resources is None:\n            self._resources = {}\n        self._resources.update(resources)\n\n    def _has_resources(self, needed_resources, used_resources):\n        if needed_resources is None:\n            return True\n\n        available_resources = self._resources or {}\n        for resource, amount in needed_resources.items():\n            if amount + used_resources[resource] > available_resources.get(resource, 1):\n                return False\n        return True\n\n    def _used_resources(self):\n        used_resources = collections.defaultdict(int)\n        if self._resources is not None:\n            for task in self._state.get_active_tasks():\n                if task.status == RUNNING and task.resources:\n                    for resource, amount in task.resources.items():\n                        used_resources[resource] += amount\n        return used_resources\n\n    def _rank(self):\n        ''' Return worker's rank function for task scheduling '''\n        dependents = collections.defaultdict(int)\n        def not_done(t):\n            task = self._state.get_task(t, default=None)\n            return task is None or task.status != DONE\n        for task in self._state.get_active_tasks():\n            if task.status != DONE:\n                deps = filter(not_done, task.deps)\n                inverse_num_deps = 1.0 / max(len(deps), 1)\n                for dep in deps:\n                    dependents[dep] += inverse_num_deps\n\n        return lambda task: (task.priority, dependents[task.id], -task.time)\n\n    def _schedulable(self, task):\n        if task.status != PENDING:\n            return False\n        for dep in task.deps:\n            dep_task = self._state.get_task(dep, default=None)\n            if dep_task is None or dep_task.status != DONE:\n                return False\n        return True\n\n    def get_work(self, worker, host=None):\n        # TODO: remove any expired nodes\n\n        # Algo: iterate over all nodes, find the highest priority node no dependencies and available\n        # resources.\n\n        # Resource checking looks both at currently available resources and at which resources would\n        # be available if all running tasks died and we rescheduled all workers greedily. We do both\n        # checks in order to prevent a worker with many low-priority tasks from starving other\n        # workers with higher priority tasks that share the same resources.\n\n        # TODO: remove tasks that can't be done, figure out if the worker has absolutely\n        # nothing it can wait for\n\n        # Return remaining tasks that have no FAILED descendents\n        self.update(worker, {'host': host})\n        best_task = None\n        best_task_id = None\n        locally_pending_tasks = 0\n        running_tasks = []\n\n        used_resources = self._used_resources()\n        n_unique_pending = 0\n\n        tasks = list(self._state.get_pending_tasks())\n        tasks.sort(key=self._rank(), reverse=True)\n\n        for task in tasks:\n            if task.status == 'RUNNING' and worker in task.workers:\n                # Return a list of currently running tasks to the client,\n                # makes it easier to troubleshoot\n                other_worker = self._state.get_worker(task.worker_running)\n                more_info = {'task_id': task.id, 'worker': str(other_worker)}\n                if other_worker is not None:\n                    more_info.update(other_worker.info)\n                    running_tasks.append(more_info)\n\n            if task.status == PENDING and worker in task.workers:\n                locally_pending_tasks += 1\n                if len(task.workers) == 1:\n                    n_unique_pending += 1\n\n            if not best_task and self._schedulable(task):\n                if worker in task.workers and self._has_resources(task.resources, used_resources):\n                    best_task = task\n                    best_task_id = task.id\n                else:\n                    # keep track of the resources used in greedy scheduling\n                    for resource, amount in (task.resources or {}).items():\n                        used_resources[resource] += amount\n\n        if best_task:\n            best_task.status = RUNNING\n            best_task.worker_running = worker\n            best_task.time_running = time.time()\n            self._update_task_history(best_task.id, RUNNING, host=host)\n\n        return {'n_pending_tasks': locally_pending_tasks,\n                'n_unique_pending': n_unique_pending,\n                'task_id': best_task_id,\n                'running_tasks': running_tasks}\n\n    def ping(self, worker):\n        self.update(worker)\n\n    def _upstream_status(self, task_id, upstream_status_table):\n        if task_id in upstream_status_table:\n            return upstream_status_table[task_id]\n        elif self._state.has_task(task_id):\n            task_stack = [task_id]\n\n            while task_stack:\n                dep_id = task_stack.pop()\n                if self._state.has_task(dep_id):\n                    dep = self._state.get_task(dep_id)\n                    if dep_id not in upstream_status_table:\n                        if dep.status == PENDING and dep.deps:\n                            task_stack = task_stack + [dep_id] + list(dep.deps)\n                            upstream_status_table[dep_id] = ''  # will be updated postorder\n                        else:\n                            dep_status = STATUS_TO_UPSTREAM_MAP.get(dep.status, '')\n                            upstream_status_table[dep_id] = dep_status\n                    elif upstream_status_table[dep_id] == '' and dep.deps:\n                        # This is the postorder update step when we set the\n                        # status based on the previously calculated child elements\n                        upstream_status = [upstream_status_table.get(id, '') for id in dep.deps]\n                        upstream_status.append('')  # to handle empty list\n                        status = max(upstream_status, key=UPSTREAM_SEVERITY_KEY)\n                        upstream_status_table[dep_id] = status\n            return upstream_status_table[dep_id]\n\n    def _serialize_task(self, task_id, include_deps=True):\n        task = self._state.get_task(task_id)\n        ret = {\n            'deps': list(task.deps),\n            'status': task.status,\n            'workers': list(task.workers),\n            'worker_running': task.worker_running,\n            'time_running': getattr(task, \"time_running\", None),\n            'start_time': task.time,\n            'params': task.params,\n            'name': task.family,\n            'priority': task.priority,\n            'resources': task.resources,\n        }\n        if include_deps:\n            ret['deps'] = list(task.deps)\n        return ret\n\n    def graph(self):\n        self.prune()\n        serialized = {}\n        for task in self._state.get_active_tasks():\n            serialized[task.id] = self._serialize_task(task.id)\n        return serialized\n\n    def _recurse_deps(self, task_id, serialized):\n        if task_id not in serialized:\n            task = self._state.get_task(task_id)\n            if task is None or not task.family:\n                logger.warn('Missing task for id [%s]', task_id)\n\n                # try to infer family and params from task_id\n                try:\n                    family, _, param_str = task_id.rstrip(')').partition('(')\n                    params = dict(param.split('=') for param in param_str.split(', '))\n                except:\n                    family, params = '', {}\n                serialized[task_id] = {\n                    'deps': [],\n                    'status': UNKNOWN,\n                    'workers': [],\n                    'start_time': UNKNOWN,\n                    'params': params,\n                    'name': family,\n                    'priority': 0,\n                }\n            else:\n                serialized[task_id] = self._serialize_task(task_id)\n                for dep in task.deps:\n                    self._recurse_deps(dep, serialized)\n\n    def dep_graph(self, task_id):\n        self.prune()\n        serialized = {}\n        if self._state.has_task(task_id):\n            self._recurse_deps(task_id, serialized)\n        return serialized\n\n    def task_list(self, status, upstream_status):\n        ''' query for a subset of tasks by status '''\n        self.prune()\n        result = {}\n        upstream_status_table = {}  # used to memoize upstream status\n        for task in self._state.get_active_tasks():\n            if not status or task.status == status:\n                if (task.status != PENDING or not upstream_status or\n                    upstream_status == self._upstream_status(task.id, upstream_status_table)):\n                    serialized = self._serialize_task(task.id, False)\n                    result[task.id] = serialized\n        return result\n\n    def inverse_dependencies(self, task_id):\n        self.prune()\n        serialized = {}\n        if self._state.has_task(task_id):\n            self._traverse_inverse_deps(task_id, serialized)\n        return serialized\n\n    def _traverse_inverse_deps(self, task_id, serialized):\n        stack = [task_id]\n        serialized[task_id] = self._serialize_task(task_id)\n        while len(stack) > 0:\n            curr_id = stack.pop()\n            for task in self._state.get_active_tasks():\n                if curr_id in task.deps:\n                    serialized[curr_id][\"deps\"].append(task.id)\n                    if task.id not in serialized:\n                        serialized[task.id] = self._serialize_task(task.id)\n                        serialized[task.id][\"deps\"] = []\n                        stack.append(task.id)\n\n    def task_search(self, task_str):\n        ''' query for a subset of tasks by task_id '''\n        self.prune()\n        result = collections.defaultdict(dict)\n        for task in self._state.get_active_tasks():\n            if task.id.find(task_str) != -1:\n                serialized = self._serialize_task(task.id, False)\n                result[task.status][task.id] = serialized\n        return result\n\n    def fetch_error(self, task_id):\n        if self._state.has_task(task_id):\n            return {\"taskId\": task_id, \"error\": self._state.get_task(task_id).expl}\n        else:\n            return {\"taskId\": task_id, \"error\": \"\"}\n\n    def _update_task_history(self, task_id, status, host=None):\n        try:\n            if status == DONE or status == FAILED:\n                successful = (status == DONE)\n                self._task_history.task_finished(task_id, successful)\n            elif status == PENDING:\n                self._task_history.task_scheduled(task_id)\n            elif status == RUNNING:\n                self._task_history.task_started(task_id, host)\n        except:\n            logger.warning(\"Error saving Task history\", exc_info=1)\n\n    @property\n    def task_history(self):\n        # Used by server.py to expose the calls\n        return self._task_history\n",
          "file_patch": "@@ -18,6 +18,7 @@ import os\n import logging\n import time\n import cPickle as pickle\n+import bisect\n import task_history as history\n logger = logging.getLogger(\"luigi.server\")\n \n@@ -43,7 +44,8 @@ STATUS_TO_UPSTREAM_MAP = {FAILED: UPSTREAM_FAILED, RUNNING: UPSTREAM_RUNNING, PE\n \n \n class Task(object):\n-    def __init__(self, status, deps, resources={}, priority=0, family='', params={}):\n+    def __init__(self, id, status, deps, resources={}, priority=0, family='', params={}):\n+        self.id = id\n         self.stakeholders = set()  # workers ids that are somehow related to this task (i.e. don't prune while any of these workers are still active)\n         self.workers = set()  # workers ids that can perform task - task is 'BROKEN' if none of these workers are active\n         if deps is None:\n@@ -81,10 +83,12 @@ class Worker(object):\n         return self.id\n \n \n-class CentralPlannerScheduler(Scheduler):\n-    ''' Async scheduler that can handle multiple workers etc\n+class SimpleTaskState(object):\n+    ''' Keep track of the current state and handle persistance\n \n-    Can be run locally or on a server (using RemoteScheduler + server.Server).\n+    The point of this class is to enable other ways to keep state, eg. by using a database\n+    These will be implemented by creating an abstract base class that this and other classes\n+    inherit from.\n     '''\n \n     def __init__(self, retry_delay=900.0, remove_delay=600.0, worker_disconnect_delay=60.0,\n@@ -98,14 +102,10 @@ class CentralPlannerScheduler(Scheduler):\n         state_path -- Path to state file (tasks and active workers)\n         worker_disconnect_delay -- If a worker hasn't communicated for this long, remove it from active workers\n         '''\n+    def __init__(self, state_path):\n         self._state_path = state_path\n         self._tasks = {}  # map from id to a Task object\n-        self._retry_delay = retry_delay\n-        self._remove_delay = remove_delay\n-        self._worker_disconnect_delay = worker_disconnect_delay\n         self._active_workers = {}  # map from id to a Worker object\n-        self._task_history = task_history or history.NopHistory()\n-        self._resources = resources\n \n     def dump(self):\n         state = (self._tasks, self._active_workers)\n@@ -139,53 +139,126 @@ class CentralPlannerScheduler(Scheduler):\n         else:\n             logger.info(\"No prior state file exists at %s. Starting with clean slate\", self._state_path)\n \n+    def get_active_tasks(self):\n+        for task in self._tasks.itervalues():\n+            yield task\n+\n+    def get_pending_tasks(self):\n+        for task in self._tasks.itervalues():\n+            if task.status in [PENDING, RUNNING]:\n+                yield task\n+\n+    def get_task(self, task_id, default=None, setdefault=None):\n+        if setdefault:\n+            return self._tasks.setdefault(task_id, setdefault)\n+        else:\n+            return self._tasks.get(task_id, default)\n+\n+    def has_task(self, task_id):\n+        return task_id in self._tasks\n+\n+    def inactivate_tasks(self, delete_tasks):\n+        # The terminology is a bit confusing: we used to \"delete\" tasks when they became inactive,\n+        # but with a pluggable state storage, you might very well want to keep some history of\n+        # older tasks as well. That's why we call it \"inactivate\" (as in the verb)\n+        for task in delete_tasks:\n+            self._tasks.pop(task)\n+\n+    def get_active_workers(self, last_active_lt=None):\n+        for worker in self._active_workers.itervalues():\n+            if last_active_lt is not None and worker.last_active >= last_active_lt:\n+                continue\n+            yield worker\n+\n+    def get_worker_ids(self):\n+        return self._active_workers.keys() # only used for unit tests\n+\n+    def get_worker(self, worker_id):\n+        return self._active_workers.setdefault(worker_id, Worker(worker_id))\n+\n+    def inactivate_workers(self, delete_workers):\n+        # Mark workers as inactive\n+        for worker in delete_workers:\n+            self._active_workers.pop(worker)\n+        \n+\n+class CentralPlannerScheduler(Scheduler):\n+    ''' Async scheduler that can handle multiple workers etc\n+\n+    Can be run locally or on a server (using RemoteScheduler + server.Server).\n+    '''\n+\n+    def __init__(self, retry_delay=900.0, remove_delay=600.0, worker_disconnect_delay=60.0,\n+                 state_path='/var/lib/luigi-server/state.pickle', task_history=None,\n+                 resources=None):\n+        '''\n+        (all arguments are in seconds)\n+        Keyword Arguments:\n+        retry_delay -- How long after a Task fails to try it again, or -1 to never retry\n+        remove_delay -- How long after a Task finishes to remove it from the scheduler\n+        state_path -- Path to state file (tasks and active workers)\n+        worker_disconnect_delay -- If a worker hasn't communicated for this long, remove it from active workers\n+        '''\n+        self._retry_delay = retry_delay\n+        self._remove_delay = remove_delay\n+        self._worker_disconnect_delay = worker_disconnect_delay\n+        self._task_history = task_history or history.NopHistory()\n+        self._state = SimpleTaskState(state_path)\n+\n+        self._task_history = task_history or history.NopHistory()\n+        self._resources = resources\n+\n+    def load(self):\n+        self._state.load()\n+\n+    def dump(self):\n+        self._state.dump()\n+\n     def prune(self):\n         logger.info(\"Starting pruning of task graph\")\n         # Delete workers that haven't said anything for a while (probably killed)\n         delete_workers = []\n-        for worker in self._active_workers.values():\n-            if worker.last_active < time.time() - self._worker_disconnect_delay:\n-                logger.info(\"Worker %s timed out (no contact for >=%ss)\", worker, self._worker_disconnect_delay)\n-                delete_workers.append(worker.id)\n+        for worker in self._state.get_active_workers(last_active_lt=time.time() - self._worker_disconnect_delay):\n+            logger.info(\"Worker %s timed out (no contact for >=%ss)\", worker, self._worker_disconnect_delay)\n+            delete_workers.append(worker.id)\n \n-        for worker in delete_workers:\n-            self._active_workers.pop(worker)\n+        self._state.inactivate_workers(delete_workers)\n \n-        remaining_workers = set(self._active_workers.keys())\n+        delete_workers = set(delete_workers)\n \n-        # Mark tasks with no remaining active stakeholders for deletion\n-        for task_id, task in self._tasks.iteritems():\n-            if not task.stakeholders.intersection(remaining_workers):\n+        remove_tasks = []\n+        for task in self._state.get_active_tasks():\n+            # Mark tasks with no remaining active stakeholders for deletion\n+            task.stakeholders.difference_update(delete_workers)\n+            if not task.stakeholders:\n                 if task.remove is None:\n-                    logger.info(\"Task %r has stakeholders %r but none remain connected -> will remove task in %s seconds\", task_id, task.stakeholders, self._remove_delay)\n+                    logger.info(\"Task %r has stakeholders %r but none remain connected -> will remove task in %s seconds\", task.id, task.stakeholders, self._remove_delay)\n                     task.remove = time.time() + self._remove_delay\n \n-            if task.status == RUNNING and task.worker_running and task.worker_running not in remaining_workers:\n-                # If a running worker disconnects, tag all its jobs as FAILED and subject it to the same retry logic\n-                logger.info(\"Task %r is marked as running by disconnected worker %r -> marking as FAILED with retry delay of %rs\", task_id, task.worker_running, self._retry_delay)\n+            # If a running worker disconnects, tag all its jobs as FAILED and subject it to the same retry logic\n+            if task.status == RUNNING and task.worker_running and task.worker_running not in task.stakeholders:\n+                logger.info(\"Task %r is marked as running by disconnected worker %r -> marking as FAILED with retry delay of %rs\", task.id, task.worker_running, self._retry_delay)\n                 task.worker_running = None\n                 task.status = FAILED\n                 task.retry = time.time() + self._retry_delay\n \n-        # Remove tasks that have no stakeholders\n-        remove_tasks = []\n-        for task_id, task in self._tasks.iteritems():\n+            # Remove tasks that have no stakeholders\n             if task.remove and time.time() > task.remove:\n-                logger.info(\"Removing task %r (no connected stakeholders)\", task_id)\n-                remove_tasks.append(task_id)\n-\n-        for task_id in remove_tasks:\n-            self._tasks.pop(task_id)\n+                logger.info(\"Removing task %r (no connected stakeholders)\", task.id)\n+                remove_tasks.append(task.id)\n \n         # Reset FAILED tasks to PENDING if max timeout is reached, and retry delay is >= 0\n-        for task in self._tasks.values():\n+        for task in self._state.get_active_tasks():\n             if task.status == FAILED and self._retry_delay >= 0 and task.retry < time.time():\n                 task.status = PENDING\n+\n+        self._state.inactivate_tasks(remove_tasks)\n+\n         logger.info(\"Done pruning task graph\")\n \n     def update(self, worker_id, worker_reference=None):\n         \"\"\" Keep track of whenever the worker was last active \"\"\"\n-        worker = self._active_workers.setdefault(worker_id, Worker(worker_id))\n+        worker = self._state.get_worker(worker_id)\n         if worker_reference:\n             worker.reference = worker_reference\n         worker.last_active = time.time()\n@@ -198,7 +271,7 @@ class CentralPlannerScheduler(Scheduler):\n         \"\"\"\n         task.priority = prio = max(prio, task.priority)\n         for dep in task.deps or []:\n-            t = self._tasks[dep] # This should always exist, see add_task\n+            t = self._state.get_task(dep) # This should always exist, see add_task\n             if prio > t.priority:\n                 self._update_priority(t, prio, worker)\n \n@@ -214,10 +287,10 @@ class CentralPlannerScheduler(Scheduler):\n         \"\"\"\n         self.update(worker)\n \n-        task = self._tasks.setdefault(task_id, Task(\n-            status=PENDING, deps=deps, resources=resources, priority=priority, family=family,\n-            params=params))\n-\n+        task = self._state.get_task(task_id, setdefault=Task(\n+                id=task_id, status=PENDING, deps=deps, resources=resources,\n+                priority=priority, family=family, params=params))\n+        \n         # for setting priority, we'll sometimes create tasks with unset family and params\n         if not task.family:\n             task.family = family\n@@ -250,7 +323,7 @@ class CentralPlannerScheduler(Scheduler):\n         # Task dependencies might not exist yet. Let's create dummy tasks for them for now.\n         # Otherwise the task dependencies might end up being pruned if scheduling takes a long time\n         for dep in task.deps or []:\n-            t = self._tasks.setdefault(dep, Task(status=UNKNOWN, deps=None, priority=priority))\n+            t = self._state.get_task(dep, setdefault=Task(id=dep, status=UNKNOWN, deps=None, priority=priority))\n             t.stakeholders.add(worker)\n \n         self._update_priority(task, priority, worker)\n@@ -262,7 +335,7 @@ class CentralPlannerScheduler(Scheduler):\n             task.expl = expl\n \n     def add_worker(self, worker, info):\n-        self._active_workers[worker].add_info(info)\n+        self._state.get_worker(worker).add_info(info)\n \n     def update_resources(self, **resources):\n         if self._resources is None:\n@@ -282,31 +355,35 @@ class CentralPlannerScheduler(Scheduler):\n     def _used_resources(self):\n         used_resources = collections.defaultdict(int)\n         if self._resources is not None:\n-            for task in self._tasks.itervalues():\n+            for task in self._state.get_active_tasks():\n                 if task.status == RUNNING and task.resources:\n                     for resource, amount in task.resources.items():\n                         used_resources[resource] += amount\n         return used_resources\n \n-    def _rank(self, worker):\n+    def _rank(self):\n         ''' Return worker's rank function for task scheduling '''\n         dependents = collections.defaultdict(int)\n-        not_done = lambda t: t not in self._tasks or self._tasks[t].status != DONE\n-        for task_id, task in self._tasks.iteritems():\n+        def not_done(t):\n+            task = self._state.get_task(t, default=None)\n+            return task is None or task.status != DONE\n+        for task in self._state.get_active_tasks():\n             if task.status != DONE:\n                 deps = filter(not_done, task.deps)\n                 inverse_num_deps = 1.0 / max(len(deps), 1)\n                 for dep in deps:\n                     dependents[dep] += inverse_num_deps\n \n-        return lambda (task_id, task): (task.priority, worker in task.workers, dependents[task_id], -task.time)\n+        return lambda task: (task.priority, dependents[task.id], -task.time)\n \n-    def _not_schedulable(self, task, used_resources):\n-        return any((\n-            task.status != PENDING,\n-            any(dep not in self._tasks or self._tasks[dep].status != DONE for dep in task.deps),\n-            not self._has_resources(task.resources, used_resources)\n-        ))\n+    def _schedulable(self, task):\n+        if task.status != PENDING:\n+            return False\n+        for dep in task.deps:\n+            dep_task = self._state.get_task(dep, default=None)\n+            if dep_task is None or dep_task.status != DONE:\n+                return False\n+        return True\n \n     def get_work(self, worker, host=None):\n         # TODO: remove any expired nodes\n@@ -325,50 +402,49 @@ class CentralPlannerScheduler(Scheduler):\n         # Return remaining tasks that have no FAILED descendents\n         self.update(worker, {'host': host})\n         best_task = None\n+        best_task_id = None\n         locally_pending_tasks = 0\n         running_tasks = []\n+\n         used_resources = self._used_resources()\n-        potential_resources = collections.defaultdict(int)\n-        potential_workers = set([worker])\n         n_unique_pending = 0\n \n-        for task_id, task in sorted(self._tasks.iteritems(), key=self._rank(worker), reverse=True):\n-            if task.status == RUNNING and worker in task.workers:\n+        tasks = list(self._state.get_pending_tasks())\n+        tasks.sort(key=self._rank(), reverse=True)\n+\n+        for task in tasks:\n+            if task.status == 'RUNNING' and worker in task.workers:\n                 # Return a list of currently running tasks to the client,\n                 # makes it easier to troubleshoot\n-                other_worker = self._active_workers[task.worker_running]\n-                more_info = {'task_id': task_id, 'worker': str(other_worker)}\n+                other_worker = self._state.get_worker(task.worker_running)\n+                more_info = {'task_id': task.id, 'worker': str(other_worker)}\n                 if other_worker is not None:\n                     more_info.update(other_worker.info)\n-                running_tasks.append(more_info)\n+                    running_tasks.append(more_info)\n \n             if task.status == PENDING and worker in task.workers:\n                 locally_pending_tasks += 1\n                 if len(task.workers) == 1:\n                     n_unique_pending += 1\n \n-            if self._not_schedulable(task, potential_resources) or best_task:\n-                continue\n-\n-            if worker in task.workers and self._has_resources(task.resources, used_resources):\n-                best_task = task_id\n-            else:\n-                # keep track of the resources used in greedy scheduling\n-                for w in filter(lambda w: w not in potential_workers, task.workers):\n+            if not best_task and self._schedulable(task):\n+                if worker in task.workers and self._has_resources(task.resources, used_resources):\n+                    best_task = task\n+                    best_task_id = task.id\n+                else:\n+                    # keep track of the resources used in greedy scheduling\n                     for resource, amount in (task.resources or {}).items():\n-                        potential_resources[resource] += amount\n-                    potential_workers.add(w)\n+                        used_resources[resource] += amount\n \n         if best_task:\n-            t = self._tasks[best_task]\n-            t.status = RUNNING\n-            t.worker_running = worker\n-            t.time_running = time.time()\n-            self._update_task_history(best_task, RUNNING, host=host)\n+            best_task.status = RUNNING\n+            best_task.worker_running = worker\n+            best_task.time_running = time.time()\n+            self._update_task_history(best_task.id, RUNNING, host=host)\n \n         return {'n_pending_tasks': locally_pending_tasks,\n                 'n_unique_pending': n_unique_pending,\n-                'task_id': best_task,\n+                'task_id': best_task_id,\n                 'running_tasks': running_tasks}\n \n     def ping(self, worker):\n@@ -377,13 +453,13 @@ class CentralPlannerScheduler(Scheduler):\n     def _upstream_status(self, task_id, upstream_status_table):\n         if task_id in upstream_status_table:\n             return upstream_status_table[task_id]\n-        elif task_id in self._tasks:\n+        elif self._state.has_task(task_id):\n             task_stack = [task_id]\n \n             while task_stack:\n                 dep_id = task_stack.pop()\n-                if dep_id in self._tasks:\n-                    dep = self._tasks[dep_id]\n+                if self._state.has_task(dep_id):\n+                    dep = self._state.get_task(dep_id)\n                     if dep_id not in upstream_status_table:\n                         if dep.status == PENDING and dep.deps:\n                             task_stack = task_stack + [dep_id] + list(dep.deps)\n@@ -401,8 +477,9 @@ class CentralPlannerScheduler(Scheduler):\n             return upstream_status_table[dep_id]\n \n     def _serialize_task(self, task_id, include_deps=True):\n-        task = self._tasks[task_id]\n+        task = self._state.get_task(task_id)\n         ret = {\n+            'deps': list(task.deps),\n             'status': task.status,\n             'workers': list(task.workers),\n             'worker_running': task.worker_running,\n@@ -420,13 +497,13 @@ class CentralPlannerScheduler(Scheduler):\n     def graph(self):\n         self.prune()\n         serialized = {}\n-        for task_id, task in self._tasks.iteritems():\n-            serialized[task_id] = self._serialize_task(task_id)\n+        for task in self._state.get_active_tasks():\n+            serialized[task.id] = self._serialize_task(task.id)\n         return serialized\n \n     def _recurse_deps(self, task_id, serialized):\n         if task_id not in serialized:\n-            task = self._tasks.get(task_id)\n+            task = self._state.get_task(task_id)\n             if task is None or not task.family:\n                 logger.warn('Missing task for id [%s]', task_id)\n \n@@ -453,7 +530,7 @@ class CentralPlannerScheduler(Scheduler):\n     def dep_graph(self, task_id):\n         self.prune()\n         serialized = {}\n-        if task_id in self._tasks:\n+        if self._state.has_task(task_id):\n             self._recurse_deps(task_id, serialized)\n         return serialized\n \n@@ -462,18 +539,18 @@ class CentralPlannerScheduler(Scheduler):\n         self.prune()\n         result = {}\n         upstream_status_table = {}  # used to memoize upstream status\n-        for task_id, task in self._tasks.iteritems():\n+        for task in self._state.get_active_tasks():\n             if not status or task.status == status:\n                 if (task.status != PENDING or not upstream_status or\n-                    upstream_status == self._upstream_status(task_id, upstream_status_table)):\n-                    serialized = self._serialize_task(task_id, False)\n-                    result[task_id] = serialized\n+                    upstream_status == self._upstream_status(task.id, upstream_status_table)):\n+                    serialized = self._serialize_task(task.id, False)\n+                    result[task.id] = serialized\n         return result\n \n     def inverse_dependencies(self, task_id):\n         self.prune()\n         serialized = {}\n-        if task_id in self._tasks:\n+        if self._state.has_task(task_id):\n             self._traverse_inverse_deps(task_id, serialized)\n         return serialized\n \n@@ -482,27 +559,27 @@ class CentralPlannerScheduler(Scheduler):\n         serialized[task_id] = self._serialize_task(task_id)\n         while len(stack) > 0:\n             curr_id = stack.pop()\n-            for id, task in self._tasks.iteritems():\n+            for task in self._state.get_active_tasks():\n                 if curr_id in task.deps:\n-                    serialized[curr_id][\"deps\"].append(id)\n-                    if id not in serialized:\n-                        serialized[id] = self._serialize_task(id)\n-                        serialized[id][\"deps\"] = []\n-                        stack.append(id)\n+                    serialized[curr_id][\"deps\"].append(task.id)\n+                    if task.id not in serialized:\n+                        serialized[task.id] = self._serialize_task(task.id)\n+                        serialized[task.id][\"deps\"] = []\n+                        stack.append(task.id)\n \n     def task_search(self, task_str):\n         ''' query for a subset of tasks by task_id '''\n         self.prune()\n         result = collections.defaultdict(dict)\n-        for task_id, task in self._tasks.iteritems():\n-            if task_id.find(task_str) != -1:\n-                serialized = self._serialize_task(task_id, False)\n-                result[task.status][task_id] = serialized\n+        for task in self._state.get_active_tasks():\n+            if task.id.find(task_str) != -1:\n+                serialized = self._serialize_task(task.id, False)\n+                result[task.status][task.id] = serialized\n         return result\n \n     def fetch_error(self, task_id):\n-        if self._tasks[task_id].expl is not None:\n-            return {\"taskId\": task_id, \"error\": self._tasks[task_id].expl}\n+        if self._state.has_task(task_id):\n+            return {\"taskId\": task_id, \"error\": self._state.get_task(task_id).expl}\n         else:\n             return {\"taskId\": task_id, \"error\": \"\"}\n \n",
          "files_name_in_blame_commit": [
            "scheduler.py",
            "scheduler_test.py"
          ]
        }
      },
      "71cd8fd989e08e6a2bd0cde49c0cca1beb953a16": {
        "commit": {
          "commit_id": "71cd8fd989e08e6a2bd0cde49c0cca1beb953a16",
          "commit_message": "Allows workers to keep alive only for unique pending tasks\n\nWith resource limitations, it's very easy to have a lot of workers\nsitting around just waiting for resources. This is problematic for\npeople using locks to prevent scheduling, as it will stop new tasks from\ngetting scheduled. To limit this behavior, this commit adds an option to\nkeep alive only if there is a pending task with all requirements done\nwhich no other worker can work on. So older workers whose tasks are all\nbeing waited on by newer workers can just die. I require the unique\ntasks to be ready because all of my workers have a unique WrapperTask at\nthe root, and I don't want to wait for those. Having a unique ready task\ntends to mean the same thing as having a unique pending task other than\nthe root one for me, so this works well for my scheduling patterns.\n\nSince this won't work well for all scheduling patterns, it has to be\nactivated via a config parameter, worker-require-unique-ready as well as\nenabling worker-keep-alive.",
          "commit_author": "Dave Buchfuhrer",
          "commit_date": "2014-09-24 06:47:09",
          "commit_parent": "d7fc6f4f8c264cbe1fc3cfd556a596f5502da550"
        },
        "function": {
          "function_name": "get_work",
          "function_code_before": "",
          "function_code_after": "",
          "function_before_start_line": "",
          "function_before_end_line": "",
          "function_after_start_line": "",
          "function_after_end_line": "",
          "function_before_token_count": 0,
          "function_after_token_count": 0,
          "functions_name_modified_file": [
            "_update_task_history",
            "prune",
            "_upstream_status",
            "ping",
            "_traverse_inverse_deps",
            "load",
            "inverse_dependencies",
            "task_search",
            "_recurse_deps",
            "add_task",
            "update_resources",
            "_update_priority",
            "task_list",
            "add_info",
            "_has_resources",
            "_used_resources",
            "_serialize_task",
            "task_history",
            "dep_graph",
            "__str__",
            "graph",
            "dump",
            "__repr__",
            "__init__",
            "update",
            "_not_schedulable",
            "add_worker",
            "_rank",
            "get_work",
            "fetch_error"
          ],
          "functions_name_all_files": [
            "output",
            "load",
            "inverse_dependencies",
            "test_priorities_and_dependencies",
            "test_timeout",
            "test_scheduler_overprovisioned_on_other_resource",
            "add_task",
            "add_info",
            "_has_resources",
            "_serialize_task",
            "check_task_order",
            "test_run_error",
            "_get_work",
            "_run_task",
            "setUp",
            "__init__",
            "test_unfulfilled_dep",
            "test_unknown_dep",
            "test_scheduler_with_priority_and_competing_resources",
            "stop",
            "test_disconnect_running",
            "test_priority_update_dependency_after_scheduling",
            "prune",
            "tearDown",
            "test_interleaved_workers",
            "_upstream_status",
            "test_two_workers",
            "test_fail",
            "_email_complete_error",
            "ping",
            "test_retry",
            "test_dep",
            "test_complete_exception",
            "_traverse_inverse_deps",
            "test_requires_exception",
            "_log_unexpected_error",
            "_log_remote_tasks",
            "_recurse_deps",
            "test_connection_error",
            "test_dynamic_dependencies",
            "test_two_worker_info",
            "update_resources",
            "_update_priority",
            "test_avoid_infinite_reschedule",
            "run",
            "test_disallowed_state_changes",
            "_add_worker",
            "test_multiple_workers",
            "test_scheduler_resources_none_allow_one",
            "_keep_alive",
            "_check_complete",
            "__str__",
            "add_worker",
            "test_broken_dep",
            "test_priorities_default_and_negative",
            "test_scheduler_with_insufficient_resources",
            "_log_complete_error",
            "test_update_resources",
            "_generate_worker_info",
            "test_no_error",
            "test_unique_tasks",
            "test_complete_error",
            "test_scheduler_with_resources_used",
            "_email_unexpected_error",
            "test_die_for_non_unique_pending",
            "test_complete_return_value",
            "test_priority_no_decrease_with_multiple_updates",
            "_validate_dependency",
            "test_external_dep",
            "task_history",
            "test_ping_retry",
            "test_ping_thread_shutdown",
            "_handle_next_task",
            "graph",
            "test_kill_worker",
            "dump",
            "__repr__",
            "test_system_exit",
            "test_failed_dep",
            "complete",
            "_check_complete_value",
            "_rank",
            "_update_task_history",
            "test_remove_dep",
            "test_priority_update_dependency_chain",
            "_purge_children",
            "task_search",
            "task_list",
            "test_interleaved_workers2",
            "_used_resources",
            "add",
            "dep_graph",
            "test_scheduler_with_sufficient_resources",
            "test_term_worker",
            "_sleeper",
            "update",
            "_not_schedulable",
            "test_scheduler_resources_none_disallow_two",
            "_validate_task",
            "test_priorities",
            "_add",
            "setTime",
            "get_work",
            "test_interleaved_workers3",
            "fetch_error"
          ],
          "functions_name_co_evolved_modified_file": [],
          "functions_name_co_evolved_all_files": [
            "__init__",
            "test_die_for_non_unique_pending",
            "_get_work",
            "_keep_alive",
            "test_unique_tasks",
            "test_interleaved_workers3",
            "run",
            "_log_remote_tasks"
          ]
        },
        "file": {
          "file_name": "scheduler.py",
          "file_nloc": 365,
          "file_complexity": 139,
          "file_token_count": 2813,
          "file_before": "# Copyright (c) 2012 Spotify AB\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"); you may not\n# use this file except in compliance with the License. You may obtain a copy of\n# the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n# License for the specific language governing permissions and limitations under\n# the License.\n\nimport collections\nimport datetime\nimport os\nimport logging\nimport time\nimport cPickle as pickle\nimport task_history as history\nlogger = logging.getLogger(\"luigi.server\")\n\nfrom task_status import PENDING, FAILED, DONE, RUNNING, SUSPENDED, UNKNOWN\n\n\nclass Scheduler(object):\n    ''' Abstract base class\n\n    Note that the methods all take string arguments, not Task objects...\n    '''\n    add_task = NotImplemented\n    get_work = NotImplemented\n    ping = NotImplemented\n\nUPSTREAM_RUNNING = 'UPSTREAM_RUNNING'\nUPSTREAM_MISSING_INPUT = 'UPSTREAM_MISSING_INPUT'\nUPSTREAM_FAILED = 'UPSTREAM_FAILED'\n\nUPSTREAM_SEVERITY_ORDER = ('', UPSTREAM_RUNNING, UPSTREAM_MISSING_INPUT, UPSTREAM_FAILED)\nUPSTREAM_SEVERITY_KEY = lambda st: UPSTREAM_SEVERITY_ORDER.index(st)\nSTATUS_TO_UPSTREAM_MAP = {FAILED: UPSTREAM_FAILED, RUNNING: UPSTREAM_RUNNING, PENDING: UPSTREAM_MISSING_INPUT}\n\n\nclass Task(object):\n    def __init__(self, status, deps, resources={}, priority=0, family='', params={}):\n        self.stakeholders = set()  # workers ids that are somehow related to this task (i.e. don't prune while any of these workers are still active)\n        self.workers = set()  # workers ids that can perform task - task is 'BROKEN' if none of these workers are active\n        if deps is None:\n            self.deps = set()\n        else:\n            self.deps = set(deps)\n        self.status = status  # PENDING, RUNNING, FAILED or DONE\n        self.time = time.time()  # Timestamp when task was first added\n        self.retry = None\n        self.remove = None\n        self.worker_running = None  # the worker id that is currently running the task or None\n        self.time_running = None  # Timestamp when picked up by worker\n        self.expl = None\n        self.priority = priority\n        self.resources = resources\n        self.family = family\n        self.params = params\n\n    def __repr__(self):\n        return \"Task(%r)\" % vars(self)\n\n\nclass Worker(object):\n    \"\"\" Structure for tracking worker activity and keeping their references \"\"\"\n    def __init__(self, id, last_active=None):\n        self.id = id\n        self.reference = None  # reference to the worker in the real world. (Currently a dict containing just the host)\n        self.last_active = last_active  # seconds since epoch\n        self.info = {}\n\n    def add_info(self, info):\n        self.info.update(info)\n\n    def __str__(self):\n        return self.id\n\n\nclass CentralPlannerScheduler(Scheduler):\n    ''' Async scheduler that can handle multiple workers etc\n\n    Can be run locally or on a server (using RemoteScheduler + server.Server).\n    '''\n\n    def __init__(self, retry_delay=900.0, remove_delay=600.0, worker_disconnect_delay=60.0,\n                 state_path='/var/lib/luigi-server/state.pickle', task_history=None,\n                 resources=None):\n        '''\n        (all arguments are in seconds)\n        Keyword Arguments:\n        retry_delay -- How long after a Task fails to try it again, or -1 to never retry\n        remove_delay -- How long after a Task finishes to remove it from the scheduler\n        state_path -- Path to state file (tasks and active workers)\n        worker_disconnect_delay -- If a worker hasn't communicated for this long, remove it from active workers\n        '''\n        self._state_path = state_path\n        self._tasks = {}  # map from id to a Task object\n        self._retry_delay = retry_delay\n        self._remove_delay = remove_delay\n        self._worker_disconnect_delay = worker_disconnect_delay\n        self._active_workers = {}  # map from id to a Worker object\n        self._task_history = task_history or history.NopHistory()\n        self._resources = resources\n\n    def dump(self):\n        state = (self._tasks, self._active_workers)\n        try:\n            with open(self._state_path, 'w') as fobj:\n                pickle.dump(state, fobj)\n        except IOError:\n            logger.warning(\"Failed saving scheduler state\", exc_info=1)\n        else:\n            logger.info(\"Saved state in %s\", self._state_path)\n\n    # prone to lead to crashes when old state is unpickled with updated code. TODO some kind of version control?\n    def load(self):\n        if os.path.exists(self._state_path):\n            logger.info(\"Attempting to load state from %s\", self._state_path)\n            try:\n                with open(self._state_path) as fobj:\n                    state = pickle.load(fobj)\n            except:\n                logger.exception(\"Error when loading state. Starting from clean slate.\")\n                return\n\n            self._tasks, self._active_workers = state\n\n            # Convert from old format\n            # TODO: this is really ugly, we need something more future-proof\n            # Every time we add an attribute to the Worker class, this code needs to be updated\n            for k, v in self._active_workers.iteritems():\n                if isinstance(v, float):\n                    self._active_workers[k] = Worker(id=k, last_active=v)\n        else:\n            logger.info(\"No prior state file exists at %s. Starting with clean slate\", self._state_path)\n\n    def prune(self):\n        logger.info(\"Starting pruning of task graph\")\n        # Delete workers that haven't said anything for a while (probably killed)\n        delete_workers = []\n        for worker in self._active_workers.values():\n            if worker.last_active < time.time() - self._worker_disconnect_delay:\n                logger.info(\"Worker %s timed out (no contact for >=%ss)\", worker, self._worker_disconnect_delay)\n                delete_workers.append(worker.id)\n\n        for worker in delete_workers:\n            self._active_workers.pop(worker)\n\n        remaining_workers = set(self._active_workers.keys())\n\n        # Mark tasks with no remaining active stakeholders for deletion\n        for task_id, task in self._tasks.iteritems():\n            if not task.stakeholders.intersection(remaining_workers):\n                if task.remove is None:\n                    logger.info(\"Task %r has stakeholders %r but none remain connected -> will remove task in %s seconds\", task_id, task.stakeholders, self._remove_delay)\n                    task.remove = time.time() + self._remove_delay\n\n            if task.status == RUNNING and task.worker_running and task.worker_running not in remaining_workers:\n                # If a running worker disconnects, tag all its jobs as FAILED and subject it to the same retry logic\n                logger.info(\"Task %r is marked as running by disconnected worker %r -> marking as FAILED with retry delay of %rs\", task_id, task.worker_running, self._retry_delay)\n                task.worker_running = None\n                task.status = FAILED\n                task.retry = time.time() + self._retry_delay\n\n        # Remove tasks that have no stakeholders\n        remove_tasks = []\n        for task_id, task in self._tasks.iteritems():\n            if task.remove and time.time() > task.remove:\n                logger.info(\"Removing task %r (no connected stakeholders)\", task_id)\n                remove_tasks.append(task_id)\n\n        for task_id in remove_tasks:\n            self._tasks.pop(task_id)\n\n        # Reset FAILED tasks to PENDING if max timeout is reached, and retry delay is >= 0\n        for task in self._tasks.values():\n            if task.status == FAILED and self._retry_delay >= 0 and task.retry < time.time():\n                task.status = PENDING\n        logger.info(\"Done pruning task graph\")\n\n    def update(self, worker_id, worker_reference=None):\n        \"\"\" Keep track of whenever the worker was last active \"\"\"\n        worker = self._active_workers.setdefault(worker_id, Worker(worker_id))\n        if worker_reference:\n            worker.reference = worker_reference\n        worker.last_active = time.time()\n\n    def _update_priority(self, task, prio, worker):\n        \"\"\" Update priority of the given task\n\n        Priority can only be increased. If the task doesn't exist, a placeholder\n        task is created to preserve priority when the task is later scheduled.\n        \"\"\"\n        task.priority = prio = max(prio, task.priority)\n        for dep in task.deps or []:\n            t = self._tasks[dep] # This should always exist, see add_task\n            if prio > t.priority:\n                self._update_priority(t, prio, worker)\n\n    def add_task(self, worker, task_id, status=PENDING, runnable=True,\n                 deps=None, new_deps=None, expl=None, resources=None,\n                 priority=0, family='', params={}):\n        \"\"\"\n        * Add task identified by task_id if it doesn't exist\n        * If deps is not None, update dependency list\n        * Update status of task\n        * Add additional workers/stakeholders\n        * Update priority when needed\n        \"\"\"\n        self.update(worker)\n\n        task = self._tasks.setdefault(task_id, Task(\n            status=PENDING, deps=deps, resources=resources, priority=priority, family=family,\n            params=params))\n\n        # for setting priority, we'll sometimes create tasks with unset family and params\n        if not task.family:\n            task.family = family\n        if not task.params:\n            task.params = params\n\n        if task.remove is not None:\n            task.remove = None  # unmark task for removal so it isn't removed after being added\n\n        if not (task.status == RUNNING and status == PENDING):\n            # don't allow re-scheduling of task while it is running, it must either fail or succeed first\n            if status == PENDING or status != task.status:\n                # Update the DB only if there was a acctual change, to prevent noise.\n                # We also check for status == PENDING b/c that's the default value\n                # (so checking for status != task.status woule lie)\n                self._update_task_history(task_id, status)\n            task.status = PENDING if status == SUSPENDED else status\n            if status == FAILED:\n                task.retry = time.time() + self._retry_delay\n\n        if deps is not None:\n            task.deps = set(deps)\n\n        if new_deps is not None:\n            task.deps.update(new_deps)\n\n        task.stakeholders.add(worker)\n        task.resources = resources\n\n        # Task dependencies might not exist yet. Let's create dummy tasks for them for now.\n        # Otherwise the task dependencies might end up being pruned if scheduling takes a long time\n        for dep in task.deps or []:\n            t = self._tasks.setdefault(dep, Task(status=UNKNOWN, deps=None, priority=priority))\n            t.stakeholders.add(worker)\n\n        self._update_priority(task, priority, worker)\n\n        if runnable:\n            task.workers.add(worker)\n\n        if expl is not None:\n            task.expl = expl\n\n    def add_worker(self, worker, info):\n        self._active_workers[worker].add_info(info)\n\n    def update_resources(self, **resources):\n        if self._resources is None:\n            self._resources = {}\n        self._resources.update(resources)\n\n    def _has_resources(self, needed_resources, used_resources):\n        if needed_resources is None:\n            return True\n\n        available_resources = self._resources or {}\n        for resource, amount in needed_resources.items():\n            if amount + used_resources[resource] > available_resources.get(resource, 1):\n                return False\n        return True\n\n    def _used_resources(self):\n        used_resources = collections.defaultdict(int)\n        if self._resources is not None:\n            for task in self._tasks.itervalues():\n                if task.status == RUNNING and task.resources:\n                    for resource, amount in task.resources.items():\n                        used_resources[resource] += amount\n        return used_resources\n\n    def _rank(self, worker):\n        ''' Return worker's rank function for task scheduling '''\n        return lambda (task_id, task): (task.priority, worker in task.workers, -task.time)\n\n    def _not_schedulable(self, task, used_resources):\n        return any((\n            task.status != PENDING,\n            any(dep not in self._tasks or self._tasks[dep].status != DONE for dep in task.deps),\n            not self._has_resources(task.resources, used_resources)\n        ))\n\n    def get_work(self, worker, host=None):\n        # TODO: remove any expired nodes\n\n        # Algo: iterate over all nodes, find the highest priority node no dependencies and available\n        # resources.\n\n        # Resource checking looks both at currently available resources and at which resources would\n        # be available if all running tasks died and we rescheduled all workers greedily. We do both\n        # checks in order to prevent a worker with many low-priority tasks from starving other\n        # workers with higher priority tasks that share the same resources.\n\n        # TODO: remove tasks that can't be done, figure out if the worker has absolutely\n        # nothing it can wait for\n\n        # Return remaining tasks that have no FAILED descendents\n        self.update(worker, {'host': host})\n        best_task = None\n        locally_pending_tasks = 0\n        running_tasks = []\n        used_resources = self._used_resources()\n        potential_resources = collections.defaultdict(int)\n        potential_workers = set([worker])\n\n        for task_id, task in sorted(self._tasks.iteritems(), key=self._rank(worker), reverse=True):\n            if task.status == RUNNING and worker in task.workers:\n                # Return a list of currently running tasks to the client,\n                # makes it easier to troubleshoot\n                other_worker = self._active_workers[task.worker_running]\n                more_info = {'task_id': task_id, 'worker': str(other_worker)}\n                if other_worker is not None:\n                    more_info.update(other_worker.info)\n                running_tasks.append(more_info)\n\n            if task.status == PENDING and worker in task.workers:\n                locally_pending_tasks += 1\n\n            if self._not_schedulable(task, potential_resources) or best_task:\n                continue\n\n            if worker in task.workers and self._has_resources(task.resources, used_resources):\n                best_task = task_id\n            else:\n                # keep track of the resources used in greedy scheduling\n                for w in filter(lambda w: w not in potential_workers, task.workers):\n                    for resource, amount in (task.resources or {}).items():\n                        potential_resources[resource] += amount\n                    potential_workers.add(w)\n\n        if best_task:\n            t = self._tasks[best_task]\n            t.status = RUNNING\n            t.worker_running = worker\n            t.time_running = time.time()\n            self._update_task_history(best_task, RUNNING, host=host)\n\n        return {'n_pending_tasks': locally_pending_tasks,\n                'task_id': best_task,\n                'running_tasks': running_tasks}\n\n    def ping(self, worker):\n        self.update(worker)\n\n    def _upstream_status(self, task_id, upstream_status_table):\n        if task_id in upstream_status_table:\n            return upstream_status_table[task_id]\n        elif task_id in self._tasks:\n            task_stack = [task_id]\n\n            while task_stack:\n                dep_id = task_stack.pop()\n                if dep_id in self._tasks:\n                    dep = self._tasks[dep_id]\n                    if dep_id not in upstream_status_table:\n                        if dep.status == PENDING and dep.deps:\n                            task_stack = task_stack + [dep_id] + list(dep.deps)\n                            upstream_status_table[dep_id] = ''  # will be updated postorder\n                        else:\n                            dep_status = STATUS_TO_UPSTREAM_MAP.get(dep.status, '')\n                            upstream_status_table[dep_id] = dep_status\n                    elif upstream_status_table[dep_id] == '' and dep.deps:\n                        # This is the postorder update step when we set the\n                        # status based on the previously calculated child elements\n                        upstream_status = [upstream_status_table.get(id, '') for id in dep.deps]\n                        upstream_status.append('')  # to handle empty list\n                        status = max(upstream_status, key=UPSTREAM_SEVERITY_KEY)\n                        upstream_status_table[dep_id] = status\n            return upstream_status_table[dep_id]\n\n    def _serialize_task(self, task_id):\n        task = self._tasks[task_id]\n        return {\n            'deps': list(task.deps),\n            'status': task.status,\n            'workers': list(task.workers),\n            'worker_running': task.worker_running,\n            'time_running': getattr(task, \"time_running\", None),\n            'start_time': task.time,\n            'params': task.params,\n            'name': task.family\n        }\n\n    def graph(self):\n        self.prune()\n        serialized = {}\n        for task_id, task in self._tasks.iteritems():\n            serialized[task_id] = self._serialize_task(task_id)\n        return serialized\n\n    def _recurse_deps(self, task_id, serialized):\n        if task_id not in serialized:\n            task = self._tasks.get(task_id)\n            if task is None or not task.family:\n                logger.warn('Missing task for id [%s]', task_id)\n\n                # try to infer family and params from task_id\n                try:\n                    family, _, param_str = task_id.rstrip(')').partition('(')\n                    params = dict(param.split('=') for param in param_str.split(', '))\n                except:\n                    family, params = '', {}\n                serialized[task_id] = {\n                    'deps': [],\n                    'status': UNKNOWN,\n                    'workers': [],\n                    'start_time': UNKNOWN,\n                    'params': params,\n                    'name': family\n                }\n            else:\n                serialized[task_id] = self._serialize_task(task_id)\n                for dep in task.deps:\n                    self._recurse_deps(dep, serialized)\n\n    def dep_graph(self, task_id):\n        self.prune()\n        serialized = {}\n        if task_id in self._tasks:\n            self._recurse_deps(task_id, serialized)\n        return serialized\n\n    def task_list(self, status, upstream_status):\n        ''' query for a subset of tasks by status '''\n        self.prune()\n        result = {}\n        upstream_status_table = {}  # used to memoize upstream status\n        for task_id, task in self._tasks.iteritems():\n            if not status or task.status == status:\n                if (task.status != PENDING or not upstream_status or\n                    upstream_status == self._upstream_status(task_id, upstream_status_table)):\n                    serialized = self._serialize_task(task_id)\n                    result[task_id] = serialized\n        return result\n\n    def inverse_dependencies(self, task_id):\n        self.prune()\n        serialized = {}\n        if task_id in self._tasks:\n            self._traverse_inverse_deps(task_id, serialized)\n        return serialized\n\n    def _traverse_inverse_deps(self, task_id, serialized):\n        stack = [task_id]\n        serialized[task_id] = self._serialize_task(task_id)\n        while len(stack) > 0:\n            curr_id = stack.pop()\n            for id, task in self._tasks.iteritems():\n                if curr_id in task.deps:\n                    serialized[curr_id][\"deps\"].append(id)\n                    if id not in serialized:\n                        serialized[id] = self._serialize_task(id)\n                        serialized[id][\"deps\"] = []\n                        stack.append(id)\n\n    def task_search(self, task_str):\n        ''' query for a subset of tasks by task_id '''\n        self.prune()\n        result = collections.defaultdict(dict)\n        for task_id, task in self._tasks.iteritems():\n            if task_id.find(task_str) != -1:\n                serialized = self._serialize_task(task_id)\n                result[task.status][task_id] = serialized\n        return result\n\n    def fetch_error(self, task_id):\n        if self._tasks[task_id].expl is not None:\n            return {\"taskId\": task_id, \"error\": self._tasks[task_id].expl}\n        else:\n            return {\"taskId\": task_id, \"error\": \"\"}\n\n    def _update_task_history(self, task_id, status, host=None):\n        try:\n            if status == DONE or status == FAILED:\n                successful = (status == DONE)\n                self._task_history.task_finished(task_id, successful)\n            elif status == PENDING:\n                self._task_history.task_scheduled(task_id)\n            elif status == RUNNING:\n                self._task_history.task_started(task_id, host)\n        except:\n            logger.warning(\"Error saving Task history\", exc_info=1)\n\n    @property\n    def task_history(self):\n        # Used by server.py to expose the calls\n        return self._task_history\n",
          "file_after": "# Copyright (c) 2012 Spotify AB\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"); you may not\n# use this file except in compliance with the License. You may obtain a copy of\n# the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n# License for the specific language governing permissions and limitations under\n# the License.\n\nimport collections\nimport datetime\nimport os\nimport logging\nimport time\nimport cPickle as pickle\nimport task_history as history\nlogger = logging.getLogger(\"luigi.server\")\n\nfrom task_status import PENDING, FAILED, DONE, RUNNING, SUSPENDED, UNKNOWN\n\n\nclass Scheduler(object):\n    ''' Abstract base class\n\n    Note that the methods all take string arguments, not Task objects...\n    '''\n    add_task = NotImplemented\n    get_work = NotImplemented\n    ping = NotImplemented\n\nUPSTREAM_RUNNING = 'UPSTREAM_RUNNING'\nUPSTREAM_MISSING_INPUT = 'UPSTREAM_MISSING_INPUT'\nUPSTREAM_FAILED = 'UPSTREAM_FAILED'\n\nUPSTREAM_SEVERITY_ORDER = ('', UPSTREAM_RUNNING, UPSTREAM_MISSING_INPUT, UPSTREAM_FAILED)\nUPSTREAM_SEVERITY_KEY = lambda st: UPSTREAM_SEVERITY_ORDER.index(st)\nSTATUS_TO_UPSTREAM_MAP = {FAILED: UPSTREAM_FAILED, RUNNING: UPSTREAM_RUNNING, PENDING: UPSTREAM_MISSING_INPUT}\n\n\nclass Task(object):\n    def __init__(self, status, deps, resources={}, priority=0, family='', params={}):\n        self.stakeholders = set()  # workers ids that are somehow related to this task (i.e. don't prune while any of these workers are still active)\n        self.workers = set()  # workers ids that can perform task - task is 'BROKEN' if none of these workers are active\n        if deps is None:\n            self.deps = set()\n        else:\n            self.deps = set(deps)\n        self.status = status  # PENDING, RUNNING, FAILED or DONE\n        self.time = time.time()  # Timestamp when task was first added\n        self.retry = None\n        self.remove = None\n        self.worker_running = None  # the worker id that is currently running the task or None\n        self.time_running = None  # Timestamp when picked up by worker\n        self.expl = None\n        self.priority = priority\n        self.resources = resources\n        self.family = family\n        self.params = params\n\n    def __repr__(self):\n        return \"Task(%r)\" % vars(self)\n\n\nclass Worker(object):\n    \"\"\" Structure for tracking worker activity and keeping their references \"\"\"\n    def __init__(self, id, last_active=None):\n        self.id = id\n        self.reference = None  # reference to the worker in the real world. (Currently a dict containing just the host)\n        self.last_active = last_active  # seconds since epoch\n        self.info = {}\n\n    def add_info(self, info):\n        self.info.update(info)\n\n    def __str__(self):\n        return self.id\n\n\nclass CentralPlannerScheduler(Scheduler):\n    ''' Async scheduler that can handle multiple workers etc\n\n    Can be run locally or on a server (using RemoteScheduler + server.Server).\n    '''\n\n    def __init__(self, retry_delay=900.0, remove_delay=600.0, worker_disconnect_delay=60.0,\n                 state_path='/var/lib/luigi-server/state.pickle', task_history=None,\n                 resources=None):\n        '''\n        (all arguments are in seconds)\n        Keyword Arguments:\n        retry_delay -- How long after a Task fails to try it again, or -1 to never retry\n        remove_delay -- How long after a Task finishes to remove it from the scheduler\n        state_path -- Path to state file (tasks and active workers)\n        worker_disconnect_delay -- If a worker hasn't communicated for this long, remove it from active workers\n        '''\n        self._state_path = state_path\n        self._tasks = {}  # map from id to a Task object\n        self._retry_delay = retry_delay\n        self._remove_delay = remove_delay\n        self._worker_disconnect_delay = worker_disconnect_delay\n        self._active_workers = {}  # map from id to a Worker object\n        self._task_history = task_history or history.NopHistory()\n        self._resources = resources\n\n    def dump(self):\n        state = (self._tasks, self._active_workers)\n        try:\n            with open(self._state_path, 'w') as fobj:\n                pickle.dump(state, fobj)\n        except IOError:\n            logger.warning(\"Failed saving scheduler state\", exc_info=1)\n        else:\n            logger.info(\"Saved state in %s\", self._state_path)\n\n    # prone to lead to crashes when old state is unpickled with updated code. TODO some kind of version control?\n    def load(self):\n        if os.path.exists(self._state_path):\n            logger.info(\"Attempting to load state from %s\", self._state_path)\n            try:\n                with open(self._state_path) as fobj:\n                    state = pickle.load(fobj)\n            except:\n                logger.exception(\"Error when loading state. Starting from clean slate.\")\n                return\n\n            self._tasks, self._active_workers = state\n\n            # Convert from old format\n            # TODO: this is really ugly, we need something more future-proof\n            # Every time we add an attribute to the Worker class, this code needs to be updated\n            for k, v in self._active_workers.iteritems():\n                if isinstance(v, float):\n                    self._active_workers[k] = Worker(id=k, last_active=v)\n        else:\n            logger.info(\"No prior state file exists at %s. Starting with clean slate\", self._state_path)\n\n    def prune(self):\n        logger.info(\"Starting pruning of task graph\")\n        # Delete workers that haven't said anything for a while (probably killed)\n        delete_workers = []\n        for worker in self._active_workers.values():\n            if worker.last_active < time.time() - self._worker_disconnect_delay:\n                logger.info(\"Worker %s timed out (no contact for >=%ss)\", worker, self._worker_disconnect_delay)\n                delete_workers.append(worker.id)\n\n        for worker in delete_workers:\n            self._active_workers.pop(worker)\n\n        remaining_workers = set(self._active_workers.keys())\n\n        # Mark tasks with no remaining active stakeholders for deletion\n        for task_id, task in self._tasks.iteritems():\n            if not task.stakeholders.intersection(remaining_workers):\n                if task.remove is None:\n                    logger.info(\"Task %r has stakeholders %r but none remain connected -> will remove task in %s seconds\", task_id, task.stakeholders, self._remove_delay)\n                    task.remove = time.time() + self._remove_delay\n\n            if task.status == RUNNING and task.worker_running and task.worker_running not in remaining_workers:\n                # If a running worker disconnects, tag all its jobs as FAILED and subject it to the same retry logic\n                logger.info(\"Task %r is marked as running by disconnected worker %r -> marking as FAILED with retry delay of %rs\", task_id, task.worker_running, self._retry_delay)\n                task.worker_running = None\n                task.status = FAILED\n                task.retry = time.time() + self._retry_delay\n\n        # Remove tasks that have no stakeholders\n        remove_tasks = []\n        for task_id, task in self._tasks.iteritems():\n            if task.remove and time.time() > task.remove:\n                logger.info(\"Removing task %r (no connected stakeholders)\", task_id)\n                remove_tasks.append(task_id)\n\n        for task_id in remove_tasks:\n            self._tasks.pop(task_id)\n\n        # Reset FAILED tasks to PENDING if max timeout is reached, and retry delay is >= 0\n        for task in self._tasks.values():\n            if task.status == FAILED and self._retry_delay >= 0 and task.retry < time.time():\n                task.status = PENDING\n        logger.info(\"Done pruning task graph\")\n\n    def update(self, worker_id, worker_reference=None):\n        \"\"\" Keep track of whenever the worker was last active \"\"\"\n        worker = self._active_workers.setdefault(worker_id, Worker(worker_id))\n        if worker_reference:\n            worker.reference = worker_reference\n        worker.last_active = time.time()\n\n    def _update_priority(self, task, prio, worker):\n        \"\"\" Update priority of the given task\n\n        Priority can only be increased. If the task doesn't exist, a placeholder\n        task is created to preserve priority when the task is later scheduled.\n        \"\"\"\n        task.priority = prio = max(prio, task.priority)\n        for dep in task.deps or []:\n            t = self._tasks[dep] # This should always exist, see add_task\n            if prio > t.priority:\n                self._update_priority(t, prio, worker)\n\n    def add_task(self, worker, task_id, status=PENDING, runnable=True,\n                 deps=None, new_deps=None, expl=None, resources=None,\n                 priority=0, family='', params={}):\n        \"\"\"\n        * Add task identified by task_id if it doesn't exist\n        * If deps is not None, update dependency list\n        * Update status of task\n        * Add additional workers/stakeholders\n        * Update priority when needed\n        \"\"\"\n        self.update(worker)\n\n        task = self._tasks.setdefault(task_id, Task(\n            status=PENDING, deps=deps, resources=resources, priority=priority, family=family,\n            params=params))\n\n        # for setting priority, we'll sometimes create tasks with unset family and params\n        if not task.family:\n            task.family = family\n        if not task.params:\n            task.params = params\n\n        if task.remove is not None:\n            task.remove = None  # unmark task for removal so it isn't removed after being added\n\n        if not (task.status == RUNNING and status == PENDING):\n            # don't allow re-scheduling of task while it is running, it must either fail or succeed first\n            if status == PENDING or status != task.status:\n                # Update the DB only if there was a acctual change, to prevent noise.\n                # We also check for status == PENDING b/c that's the default value\n                # (so checking for status != task.status woule lie)\n                self._update_task_history(task_id, status)\n            task.status = PENDING if status == SUSPENDED else status\n            if status == FAILED:\n                task.retry = time.time() + self._retry_delay\n\n        if deps is not None:\n            task.deps = set(deps)\n\n        if new_deps is not None:\n            task.deps.update(new_deps)\n\n        task.stakeholders.add(worker)\n        task.resources = resources\n\n        # Task dependencies might not exist yet. Let's create dummy tasks for them for now.\n        # Otherwise the task dependencies might end up being pruned if scheduling takes a long time\n        for dep in task.deps or []:\n            t = self._tasks.setdefault(dep, Task(status=UNKNOWN, deps=None, priority=priority))\n            t.stakeholders.add(worker)\n\n        self._update_priority(task, priority, worker)\n\n        if runnable:\n            task.workers.add(worker)\n\n        if expl is not None:\n            task.expl = expl\n\n    def add_worker(self, worker, info):\n        self._active_workers[worker].add_info(info)\n\n    def update_resources(self, **resources):\n        if self._resources is None:\n            self._resources = {}\n        self._resources.update(resources)\n\n    def _has_resources(self, needed_resources, used_resources):\n        if needed_resources is None:\n            return True\n\n        available_resources = self._resources or {}\n        for resource, amount in needed_resources.items():\n            if amount + used_resources[resource] > available_resources.get(resource, 1):\n                return False\n        return True\n\n    def _used_resources(self):\n        used_resources = collections.defaultdict(int)\n        if self._resources is not None:\n            for task in self._tasks.itervalues():\n                if task.status == RUNNING and task.resources:\n                    for resource, amount in task.resources.items():\n                        used_resources[resource] += amount\n        return used_resources\n\n    def _rank(self, worker):\n        ''' Return worker's rank function for task scheduling '''\n        return lambda (task_id, task): (task.priority, worker in task.workers, -task.time)\n\n    def _not_schedulable(self, task, used_resources):\n        return any((\n            task.status != PENDING,\n            any(dep not in self._tasks or self._tasks[dep].status != DONE for dep in task.deps),\n            not self._has_resources(task.resources, used_resources)\n        ))\n\n    def get_work(self, worker, host=None):\n        # TODO: remove any expired nodes\n\n        # Algo: iterate over all nodes, find the highest priority node no dependencies and available\n        # resources.\n\n        # Resource checking looks both at currently available resources and at which resources would\n        # be available if all running tasks died and we rescheduled all workers greedily. We do both\n        # checks in order to prevent a worker with many low-priority tasks from starving other\n        # workers with higher priority tasks that share the same resources.\n\n        # TODO: remove tasks that can't be done, figure out if the worker has absolutely\n        # nothing it can wait for\n\n        # Return remaining tasks that have no FAILED descendents\n        self.update(worker, {'host': host})\n        best_task = None\n        locally_pending_tasks = 0\n        running_tasks = []\n        used_resources = self._used_resources()\n        potential_resources = collections.defaultdict(int)\n        potential_workers = set([worker])\n        n_unique_pending = 0\n\n        for task_id, task in sorted(self._tasks.iteritems(), key=self._rank(worker), reverse=True):\n            if task.status == RUNNING and worker in task.workers:\n                # Return a list of currently running tasks to the client,\n                # makes it easier to troubleshoot\n                other_worker = self._active_workers[task.worker_running]\n                more_info = {'task_id': task_id, 'worker': str(other_worker)}\n                if other_worker is not None:\n                    more_info.update(other_worker.info)\n                running_tasks.append(more_info)\n\n            if task.status == PENDING and worker in task.workers:\n                locally_pending_tasks += 1\n                if len(task.workers) == 1:\n                    n_unique_pending += 1\n\n            if self._not_schedulable(task, potential_resources) or best_task:\n                continue\n\n            if worker in task.workers and self._has_resources(task.resources, used_resources):\n                best_task = task_id\n            else:\n                # keep track of the resources used in greedy scheduling\n                for w in filter(lambda w: w not in potential_workers, task.workers):\n                    for resource, amount in (task.resources or {}).items():\n                        potential_resources[resource] += amount\n                    potential_workers.add(w)\n\n        if best_task:\n            t = self._tasks[best_task]\n            t.status = RUNNING\n            t.worker_running = worker\n            t.time_running = time.time()\n            self._update_task_history(best_task, RUNNING, host=host)\n\n        return {'n_pending_tasks': locally_pending_tasks,\n                'n_unique_pending': n_unique_pending,\n                'task_id': best_task,\n                'running_tasks': running_tasks}\n\n    def ping(self, worker):\n        self.update(worker)\n\n    def _upstream_status(self, task_id, upstream_status_table):\n        if task_id in upstream_status_table:\n            return upstream_status_table[task_id]\n        elif task_id in self._tasks:\n            task_stack = [task_id]\n\n            while task_stack:\n                dep_id = task_stack.pop()\n                if dep_id in self._tasks:\n                    dep = self._tasks[dep_id]\n                    if dep_id not in upstream_status_table:\n                        if dep.status == PENDING and dep.deps:\n                            task_stack = task_stack + [dep_id] + list(dep.deps)\n                            upstream_status_table[dep_id] = ''  # will be updated postorder\n                        else:\n                            dep_status = STATUS_TO_UPSTREAM_MAP.get(dep.status, '')\n                            upstream_status_table[dep_id] = dep_status\n                    elif upstream_status_table[dep_id] == '' and dep.deps:\n                        # This is the postorder update step when we set the\n                        # status based on the previously calculated child elements\n                        upstream_status = [upstream_status_table.get(id, '') for id in dep.deps]\n                        upstream_status.append('')  # to handle empty list\n                        status = max(upstream_status, key=UPSTREAM_SEVERITY_KEY)\n                        upstream_status_table[dep_id] = status\n            return upstream_status_table[dep_id]\n\n    def _serialize_task(self, task_id):\n        task = self._tasks[task_id]\n        return {\n            'deps': list(task.deps),\n            'status': task.status,\n            'workers': list(task.workers),\n            'worker_running': task.worker_running,\n            'time_running': getattr(task, \"time_running\", None),\n            'start_time': task.time,\n            'params': task.params,\n            'name': task.family\n        }\n\n    def graph(self):\n        self.prune()\n        serialized = {}\n        for task_id, task in self._tasks.iteritems():\n            serialized[task_id] = self._serialize_task(task_id)\n        return serialized\n\n    def _recurse_deps(self, task_id, serialized):\n        if task_id not in serialized:\n            task = self._tasks.get(task_id)\n            if task is None or not task.family:\n                logger.warn('Missing task for id [%s]', task_id)\n\n                # try to infer family and params from task_id\n                try:\n                    family, _, param_str = task_id.rstrip(')').partition('(')\n                    params = dict(param.split('=') for param in param_str.split(', '))\n                except:\n                    family, params = '', {}\n                serialized[task_id] = {\n                    'deps': [],\n                    'status': UNKNOWN,\n                    'workers': [],\n                    'start_time': UNKNOWN,\n                    'params': params,\n                    'name': family\n                }\n            else:\n                serialized[task_id] = self._serialize_task(task_id)\n                for dep in task.deps:\n                    self._recurse_deps(dep, serialized)\n\n    def dep_graph(self, task_id):\n        self.prune()\n        serialized = {}\n        if task_id in self._tasks:\n            self._recurse_deps(task_id, serialized)\n        return serialized\n\n    def task_list(self, status, upstream_status):\n        ''' query for a subset of tasks by status '''\n        self.prune()\n        result = {}\n        upstream_status_table = {}  # used to memoize upstream status\n        for task_id, task in self._tasks.iteritems():\n            if not status or task.status == status:\n                if (task.status != PENDING or not upstream_status or\n                    upstream_status == self._upstream_status(task_id, upstream_status_table)):\n                    serialized = self._serialize_task(task_id)\n                    result[task_id] = serialized\n        return result\n\n    def inverse_dependencies(self, task_id):\n        self.prune()\n        serialized = {}\n        if task_id in self._tasks:\n            self._traverse_inverse_deps(task_id, serialized)\n        return serialized\n\n    def _traverse_inverse_deps(self, task_id, serialized):\n        stack = [task_id]\n        serialized[task_id] = self._serialize_task(task_id)\n        while len(stack) > 0:\n            curr_id = stack.pop()\n            for id, task in self._tasks.iteritems():\n                if curr_id in task.deps:\n                    serialized[curr_id][\"deps\"].append(id)\n                    if id not in serialized:\n                        serialized[id] = self._serialize_task(id)\n                        serialized[id][\"deps\"] = []\n                        stack.append(id)\n\n    def task_search(self, task_str):\n        ''' query for a subset of tasks by task_id '''\n        self.prune()\n        result = collections.defaultdict(dict)\n        for task_id, task in self._tasks.iteritems():\n            if task_id.find(task_str) != -1:\n                serialized = self._serialize_task(task_id)\n                result[task.status][task_id] = serialized\n        return result\n\n    def fetch_error(self, task_id):\n        if self._tasks[task_id].expl is not None:\n            return {\"taskId\": task_id, \"error\": self._tasks[task_id].expl}\n        else:\n            return {\"taskId\": task_id, \"error\": \"\"}\n\n    def _update_task_history(self, task_id, status, host=None):\n        try:\n            if status == DONE or status == FAILED:\n                successful = (status == DONE)\n                self._task_history.task_finished(task_id, successful)\n            elif status == PENDING:\n                self._task_history.task_scheduled(task_id)\n            elif status == RUNNING:\n                self._task_history.task_started(task_id, host)\n        except:\n            logger.warning(\"Error saving Task history\", exc_info=1)\n\n    @property\n    def task_history(self):\n        # Used by server.py to expose the calls\n        return self._task_history\n",
          "file_patch": "@@ -321,6 +321,7 @@ class CentralPlannerScheduler(Scheduler):\n         used_resources = self._used_resources()\n         potential_resources = collections.defaultdict(int)\n         potential_workers = set([worker])\n+        n_unique_pending = 0\n \n         for task_id, task in sorted(self._tasks.iteritems(), key=self._rank(worker), reverse=True):\n             if task.status == RUNNING and worker in task.workers:\n@@ -334,6 +335,8 @@ class CentralPlannerScheduler(Scheduler):\n \n             if task.status == PENDING and worker in task.workers:\n                 locally_pending_tasks += 1\n+                if len(task.workers) == 1:\n+                    n_unique_pending += 1\n \n             if self._not_schedulable(task, potential_resources) or best_task:\n                 continue\n@@ -355,6 +358,7 @@ class CentralPlannerScheduler(Scheduler):\n             self._update_task_history(best_task, RUNNING, host=host)\n \n         return {'n_pending_tasks': locally_pending_tasks,\n+                'n_unique_pending': n_unique_pending,\n                 'task_id': best_task,\n                 'running_tasks': running_tasks}\n \n",
          "files_name_in_blame_commit": [
            "scheduler.py",
            "central_planner_test.py",
            "worker.py",
            "worker_test.py"
          ]
        }
      },
      "31c9c7ed4d01441626b57db62cf4d2e1778829d7": {
        "commit": {
          "commit_id": "31c9c7ed4d01441626b57db62cf4d2e1778829d7",
          "commit_message": "Adds resource limitations for tasks",
          "commit_author": "Dave Buchfuhrer",
          "commit_date": "2014-09-18 13:06:18",
          "commit_parent": "6f130584b2dc6bd59e9804708bb31d818e128c63"
        },
        "function": {
          "function_name": "get_work",
          "function_code_before": "def get_work(self, worker, host=None):\n    self.update(worker, {'host': host})\n    best_t = float('inf')\n    best_priority = float('-inf')\n    best_task = None\n    locally_pending_tasks = 0\n    running_tasks = []\n    for (task_id, task) in self._tasks.iteritems():\n        if worker not in task.workers:\n            continue\n        if task.status == RUNNING:\n            other_worker = self._active_workers[task.worker_running]\n            more_info = {'task_id': task_id, 'worker': str(other_worker)}\n            if other_worker is not None:\n                more_info.update(other_worker.info)\n            running_tasks.append(more_info)\n        if task.status != PENDING:\n            continue\n        locally_pending_tasks += 1\n        ok = True\n        for dep in task.deps:\n            if dep not in self._tasks:\n                ok = False\n            elif self._tasks[dep].status != DONE:\n                ok = False\n        if ok:\n            if (-task.priority, task.time) < (-best_priority, best_t):\n                best_t = task.time\n                best_priority = task.priority\n                best_task = task_id\n    if best_task:\n        t = self._tasks[best_task]\n        t.status = RUNNING\n        t.worker_running = worker\n        t.time_running = time.time()\n        self._update_task_history(best_task, RUNNING, host=host)\n    return {'n_pending_tasks': locally_pending_tasks, 'task_id': best_task, 'running_tasks': running_tasks}",
          "function_code_after": "",
          "function_before_start_line": 256,
          "function_before_end_line": 311,
          "function_after_start_line": "",
          "function_after_end_line": "",
          "function_before_token_count": 256,
          "function_after_token_count": 0,
          "functions_name_modified_file": [
            "_update_task_history",
            "prune",
            "_upstream_status",
            "ping",
            "_traverse_inverse_deps",
            "load",
            "inverse_dependencies",
            "task_search",
            "_recurse_deps",
            "add_task",
            "update_resources",
            "_update_priority",
            "task_list",
            "add_info",
            "_has_resources",
            "_used_resources",
            "_serialize_task",
            "task_history",
            "dep_graph",
            "__str__",
            "graph",
            "dump",
            "__repr__",
            "__init__",
            "update",
            "_not_schedulable",
            "add_worker",
            "_rank",
            "get_work",
            "fetch_error"
          ],
          "functions_name_all_files": [
            "_get_with_default",
            "reload",
            "output",
            "_post",
            "id_to_name_and_params",
            "get_params",
            "run_api_threaded",
            "get_global_params",
            "load",
            "inverse_dependencies",
            "test_priorities_and_dependencies",
            "test_timeout",
            "test_scheduler_overprovisioned_on_other_resource",
            "add_task",
            "add_info",
            "_has_resources",
            "_serialize_task",
            "check_task_order",
            "on_failure",
            "from_str_params",
            "_get_work",
            "_run_task",
            "__new__",
            "getint",
            "setUp",
            "__init__",
            "getintdict",
            "test_scheduler_with_priority_and_competing_resources",
            "stop",
            "test_disconnect_running",
            "test_priority_update_dependency_after_scheduling",
            "prune",
            "tearDown",
            "_upstream_status",
            "test_two_workers",
            "_email_complete_error",
            "ping",
            "test_retry",
            "inverse_dep_graph",
            "test_dep",
            "_traverse_inverse_deps",
            "_log_unexpected_error",
            "_log_remote_tasks",
            "get_config",
            "_recurse_deps",
            "test_two_worker_info",
            "getboolean",
            "_handle_next_done_task",
            "get_template_path",
            "externalize",
            "update_resources",
            "_update_priority",
            "requires",
            "run",
            "test_disallowed_state_changes",
            "_add_worker",
            "initialized",
            "namespace",
            "test_scheduler_resources_none_allow_one",
            "to_str_params",
            "_check_complete",
            "__str__",
            "_create_scheduler",
            "clear_instance_cache",
            "_request",
            "add_worker",
            "task_family",
            "test_broken_dep",
            "test_priorities_default_and_negative",
            "test_scheduler_with_insufficient_resources",
            "_log_complete_error",
            "get_param_values",
            "_get",
            "disable_instance_cache",
            "process_resources",
            "_generate_worker_info",
            "instance",
            "_requires",
            "test_scheduler_with_resources_used",
            "_email_unexpected_error",
            "input",
            "test_priority_no_decrease_with_multiple_updates",
            "initialize",
            "_validate_dependency",
            "__hash__",
            "on_success",
            "task_history",
            "get_reg",
            "graph",
            "trigger_event",
            "__eq__",
            "dump",
            "__repr__",
            "__call__",
            "test_failed_dep",
            "complete",
            "add_config_path",
            "_init_api",
            "_check_complete_value",
            "deps",
            "getpaths",
            "getfloat",
            "_rank",
            "_update_task_history",
            "test_remove_dep",
            "test_priority_update_dependency_chain",
            "_purge_children",
            "get_nonglobal_params",
            "set",
            "_wait",
            "task_search",
            "flatten",
            "event_handler",
            "task_list",
            "_used_resources",
            "add",
            "dep_graph",
            "clone",
            "test_scheduler_with_sufficient_resources",
            "get",
            "_sleeper",
            "update",
            "_not_schedulable",
            "test_scheduler_resources_none_disallow_two",
            "_validate_task",
            "test_priorities",
            "_add",
            "setTime",
            "get_work",
            "fetch_error",
            "app"
          ],
          "functions_name_co_evolved_modified_file": [
            "__init__",
            "_not_schedulable",
            "add_task",
            "update_resources",
            "_rank",
            "_has_resources",
            "_used_resources"
          ],
          "functions_name_co_evolved_all_files": [
            "process_resources",
            "test_scheduler_with_resources_used",
            "test_scheduler_overprovisioned_on_other_resource",
            "_handle_next_done_task",
            "add_task",
            "update_resources",
            "_has_resources",
            "_used_resources",
            "test_scheduler_resources_none_allow_one",
            "clone",
            "test_scheduler_with_sufficient_resources",
            "_create_scheduler",
            "__init__",
            "_not_schedulable",
            "test_scheduler_resources_none_disallow_two",
            "getintdict",
            "test_scheduler_with_priority_and_competing_resources",
            "test_scheduler_with_insufficient_resources",
            "_add",
            "_rank"
          ]
        },
        "file": {
          "file_name": "scheduler.py",
          "file_nloc": 357,
          "file_complexity": 136,
          "file_token_count": 2762,
          "file_before": "# Copyright (c) 2012 Spotify AB\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"); you may not\n# use this file except in compliance with the License. You may obtain a copy of\n# the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n# License for the specific language governing permissions and limitations under\n# the License.\n\nimport collections\nimport datetime\nimport os\nimport logging\nimport time\nimport cPickle as pickle\nimport task_history as history\nlogger = logging.getLogger(\"luigi.server\")\n\nfrom task_status import PENDING, FAILED, DONE, RUNNING, UNKNOWN\n\n\nclass Scheduler(object):\n    ''' Abstract base class\n\n    Note that the methods all take string arguments, not Task objects...\n    '''\n    add_task = NotImplemented\n    get_work = NotImplemented\n    ping = NotImplemented\n\nUPSTREAM_RUNNING = 'UPSTREAM_RUNNING'\nUPSTREAM_MISSING_INPUT = 'UPSTREAM_MISSING_INPUT'\nUPSTREAM_FAILED = 'UPSTREAM_FAILED'\n\nUPSTREAM_SEVERITY_ORDER = ('', UPSTREAM_RUNNING, UPSTREAM_MISSING_INPUT, UPSTREAM_FAILED)\nUPSTREAM_SEVERITY_KEY = lambda st: UPSTREAM_SEVERITY_ORDER.index(st)\nSTATUS_TO_UPSTREAM_MAP = {FAILED: UPSTREAM_FAILED, RUNNING: UPSTREAM_RUNNING, PENDING: UPSTREAM_MISSING_INPUT}\n\n\nclass Task(object):\n    def __init__(self, status, deps, priority=0, family='', params={}):\n        self.stakeholders = set()  # workers ids that are somehow related to this task (i.e. don't prune while any of these workers are still active)\n        self.workers = set()  # workers ids that can perform task - task is 'BROKEN' if none of these workers are active\n        if deps is None:\n            self.deps = set()\n        else:\n            self.deps = set(deps)\n        self.status = status  # PENDING, RUNNING, FAILED or DONE\n        self.time = time.time()  # Timestamp when task was first added\n        self.retry = None\n        self.remove = None\n        self.worker_running = None  # the worker id that is currently running the task or None\n        self.time_running = None  # Timestamp when picked up by worker\n        self.expl = None\n        self.priority = priority\n        self.family = family\n        self.params = params\n\n    def __repr__(self):\n        return \"Task(%r)\" % vars(self)\n\n\nclass Worker(object):\n    \"\"\" Structure for tracking worker activity and keeping their references \"\"\"\n    def __init__(self, id, last_active=None):\n        self.id = id\n        self.reference = None  # reference to the worker in the real world. (Currently a dict containing just the host)\n        self.last_active = last_active  # seconds since epoch\n        self.info = {}\n\n    def add_info(self, info):\n        self.info.update(info)\n\n    def __str__(self):\n        return self.id\n\n\nclass CentralPlannerScheduler(Scheduler):\n    ''' Async scheduler that can handle multiple workers etc\n\n    Can be run locally or on a server (using RemoteScheduler + server.Server).\n    '''\n\n    def __init__(self, retry_delay=900.0, remove_delay=600.0, worker_disconnect_delay=60.0,\n                 state_path='/var/lib/luigi-server/state.pickle', task_history=None):\n        '''\n        (all arguments are in seconds)\n        Keyword Arguments:\n        retry_delay -- How long after a Task fails to try it again, or -1 to never retry\n        remove_delay -- How long after a Task finishes to remove it from the scheduler\n        state_path -- Path to state file (tasks and active workers)\n        worker_disconnect_delay -- If a worker hasn't communicated for this long, remove it from active workers\n        '''\n        self._state_path = state_path\n        self._tasks = {}  # map from id to a Task object\n        self._retry_delay = retry_delay\n        self._remove_delay = remove_delay\n        self._worker_disconnect_delay = worker_disconnect_delay\n        self._active_workers = {}  # map from id to a Worker object\n        self._task_history = task_history or history.NopHistory()\n\n    def dump(self):\n        state = (self._tasks, self._active_workers)\n        try:\n            with open(self._state_path, 'w') as fobj:\n                pickle.dump(state, fobj)\n        except IOError:\n            logger.warning(\"Failed saving scheduler state\", exc_info=1)\n        else:\n            logger.info(\"Saved state in %s\", self._state_path)\n\n    # prone to lead to crashes when old state is unpickled with updated code. TODO some kind of version control?\n    def load(self):\n        if os.path.exists(self._state_path):\n            logger.info(\"Attempting to load state from %s\", self._state_path)\n            try:\n                with open(self._state_path) as fobj:\n                    state = pickle.load(fobj)\n            except:\n                logger.exception(\"Error when loading state. Starting from clean slate.\")\n                return\n\n            self._tasks, self._active_workers = state\n\n            # Convert from old format\n            # TODO: this is really ugly, we need something more future-proof\n            # Every time we add an attribute to the Worker class, this code needs to be updated\n            for k, v in self._active_workers.iteritems():\n                if isinstance(v, float):\n                    self._active_workers[k] = Worker(id=k, last_active=v)\n        else:\n            logger.info(\"No prior state file exists at %s. Starting with clean slate\", self._state_path)\n\n    def prune(self):\n        logger.info(\"Starting pruning of task graph\")\n        # Delete workers that haven't said anything for a while (probably killed)\n        delete_workers = []\n        for worker in self._active_workers.values():\n            if worker.last_active < time.time() - self._worker_disconnect_delay:\n                logger.info(\"Worker %s timed out (no contact for >=%ss)\", worker, self._worker_disconnect_delay)\n                delete_workers.append(worker.id)\n\n        for worker in delete_workers:\n            self._active_workers.pop(worker)\n\n        remaining_workers = set(self._active_workers.keys())\n\n        # Mark tasks with no remaining active stakeholders for deletion\n        for task_id, task in self._tasks.iteritems():\n            if not task.stakeholders.intersection(remaining_workers):\n                if task.remove is None:\n                    logger.info(\"Task %r has stakeholders %r but none remain connected -> will remove task in %s seconds\", task_id, task.stakeholders, self._remove_delay)\n                    task.remove = time.time() + self._remove_delay\n\n            if task.status == RUNNING and task.worker_running and task.worker_running not in remaining_workers:\n                # If a running worker disconnects, tag all its jobs as FAILED and subject it to the same retry logic\n                logger.info(\"Task %r is marked as running by disconnected worker %r -> marking as FAILED with retry delay of %rs\", task_id, task.worker_running, self._retry_delay)\n                task.worker_running = None\n                task.status = FAILED\n                task.retry = time.time() + self._retry_delay\n\n        # Remove tasks that have no stakeholders\n        remove_tasks = []\n        for task_id, task in self._tasks.iteritems():\n            if task.remove and time.time() > task.remove:\n                logger.info(\"Removing task %r (no connected stakeholders)\", task_id)\n                remove_tasks.append(task_id)\n\n        for task_id in remove_tasks:\n            self._tasks.pop(task_id)\n\n        # Reset FAILED tasks to PENDING if max timeout is reached, and retry delay is >= 0\n        for task in self._tasks.values():\n            if task.status == FAILED and self._retry_delay >= 0 and task.retry < time.time():\n                task.status = PENDING\n        logger.info(\"Done pruning task graph\")\n\n    def update(self, worker_id, worker_reference=None):\n        \"\"\" Keep track of whenever the worker was last active \"\"\"\n        worker = self._active_workers.setdefault(worker_id, Worker(worker_id))\n        if worker_reference:\n            worker.reference = worker_reference\n        worker.last_active = time.time()\n\n    def _update_priority(self, task, prio, worker):\n        \"\"\" Update priority of the given task\n\n        Priority can only be increased. If the task doesn't exist, a placeholder\n        task is created to preserve priority when the task is later scheduled.\n        \"\"\"\n        task.priority = prio = max(prio, task.priority)\n        for dep in task.deps or []:\n            t = self._tasks[dep] # This should always exist, see add_task\n            if prio > t.priority:\n                self._update_priority(t, prio, worker)\n\n    def add_task(self, worker, task_id, status=PENDING, runnable=True, deps=None, expl=None, priority=0, family='', params={}):\n        \"\"\"\n        * Add task identified by task_id if it doesn't exist\n        * If deps is not None, update dependency list\n        * Update status of task\n        * Add additional workers/stakeholders\n        * Update priority when needed\n        \"\"\"\n        self.update(worker)\n\n        task = self._tasks.setdefault(task_id, Task(status=PENDING, deps=deps, priority=priority, family=family, params=params))\n\n        # for setting priority, we'll sometimes create tasks with unset family and params\n        if not task.family:\n            task.family = family\n        if not task.params:\n            task.params = params\n\n        if task.remove is not None:\n            task.remove = None  # unmark task for removal so it isn't removed after being added\n\n        if not (task.status == RUNNING and status == PENDING):\n            # don't allow re-scheduling of task while it is running, it must either fail or succeed first\n            if status == PENDING or status != task.status:\n                # Update the DB only if there was a acctual change, to prevent noise.\n                # We also check for status == PENDING b/c that's the default value\n                # (so checking for status != task.status woule lie)\n                self._update_task_history(task_id, status)\n            task.status = status\n            if status == FAILED:\n                task.retry = time.time() + self._retry_delay\n\n        if deps is not None:\n            task.deps = set(deps)\n\n        task.stakeholders.add(worker)\n\n        # Task dependencies might not exist yet. Let's create dummy tasks for them for now.\n        # Otherwise the task dependencies might end up being pruned if scheduling takes a long time\n        for dep in task.deps or []:\n            t = self._tasks.setdefault(dep, Task(status=UNKNOWN, deps=None, priority=priority))\n            t.stakeholders.add(worker)\n\n        self._update_priority(task, priority, worker)\n\n        if runnable:\n            task.workers.add(worker)\n\n        if expl is not None:\n            task.expl = expl\n\n    def add_worker(self, worker, info):\n        self._active_workers[worker].add_info(info)\n\n    def get_work(self, worker, host=None):\n        # TODO: remove any expired nodes\n\n        # Algo: iterate over all nodes, find the first node with no dependencies and highest priority\n\n        # TODO: remove tasks that can't be done, figure out if the worker has absolutely\n        # nothing it can wait for\n\n        # Return remaining tasks that have no FAILED descendents\n        self.update(worker, {'host': host})\n        best_t = float('inf')\n        best_priority = float('-inf')\n        best_task = None\n        locally_pending_tasks = 0\n        running_tasks = []\n\n        for task_id, task in self._tasks.iteritems():\n            if worker not in task.workers:\n                continue\n\n            if task.status == RUNNING:\n                # Return a list of currently running tasks to the client,\n                # makes it easier to troubleshoot\n                other_worker = self._active_workers[task.worker_running]\n                more_info = {'task_id': task_id, 'worker': str(other_worker)}\n                if other_worker is not None:\n                    more_info.update(other_worker.info)\n                running_tasks.append(more_info)\n\n            if task.status != PENDING:\n                continue\n\n            locally_pending_tasks += 1\n            ok = True\n            for dep in task.deps:\n                if dep not in self._tasks:\n                    ok = False\n                elif self._tasks[dep].status != DONE:\n                    ok = False\n\n            if ok:\n                if (-task.priority, task.time) < (-best_priority, best_t):\n                    best_t = task.time\n                    best_priority = task.priority\n                    best_task = task_id\n\n        if best_task:\n            t = self._tasks[best_task]\n            t.status = RUNNING\n            t.worker_running = worker\n            t.time_running = time.time()\n            self._update_task_history(best_task, RUNNING, host=host)\n\n        return {'n_pending_tasks': locally_pending_tasks,\n                'task_id': best_task,\n                'running_tasks': running_tasks}\n\n    def ping(self, worker):\n        self.update(worker)\n\n    def _upstream_status(self, task_id, upstream_status_table):\n        if task_id in upstream_status_table:\n            return upstream_status_table[task_id]\n        elif task_id in self._tasks:\n            task_stack = [task_id]\n\n            while task_stack:\n                dep_id = task_stack.pop()\n                if dep_id in self._tasks:\n                    dep = self._tasks[dep_id]\n                    if dep_id not in upstream_status_table:\n                        if dep.status == PENDING and dep.deps:\n                            task_stack = task_stack + [dep_id] + list(dep.deps)\n                            upstream_status_table[dep_id] = ''  # will be updated postorder\n                        else:\n                            dep_status = STATUS_TO_UPSTREAM_MAP.get(dep.status, '')\n                            upstream_status_table[dep_id] = dep_status\n                    elif upstream_status_table[dep_id] == '' and dep.deps:\n                        # This is the postorder update step when we set the\n                        # status based on the previously calculated child elements\n                        upstream_status = [upstream_status_table.get(id, '') for id in dep.deps]\n                        upstream_status.append('')  # to handle empty list\n                        status = max(upstream_status, key=UPSTREAM_SEVERITY_KEY)\n                        upstream_status_table[dep_id] = status\n            return upstream_status_table[dep_id]\n\n    def _serialize_task(self, task_id):\n        task = self._tasks[task_id]\n        return {\n            'deps': list(task.deps),\n            'status': task.status,\n            'workers': list(task.workers),\n            'worker_running': task.worker_running,\n            'time_running': getattr(task, \"time_running\", None),\n            'start_time': task.time,\n            'params': task.params,\n            'name': task.family\n        }\n\n    def graph(self):\n        self.prune()\n        serialized = {}\n        for task_id, task in self._tasks.iteritems():\n            serialized[task_id] = self._serialize_task(task_id)\n        return serialized\n\n    def _recurse_deps(self, task_id, serialized):\n        if task_id not in serialized:\n            task = self._tasks.get(task_id)\n            if task is None or not task.family:\n                logger.warn('Missing task for id [%s]', task_id)\n\n                # try to infer family and params from task_id\n                try:\n                    family, _, param_str = task_id.rstrip(')').partition('(')\n                    params = dict(param.split('=') for param in param_str.split(', '))\n                except:\n                    family, params = '', {}\n                serialized[task_id] = {\n                    'deps': [],\n                    'status': UNKNOWN,\n                    'workers': [],\n                    'start_time': UNKNOWN,\n                    'params': params,\n                    'name': family\n                }\n            else:\n                serialized[task_id] = self._serialize_task(task_id)\n                for dep in task.deps:\n                    self._recurse_deps(dep, serialized)\n\n    def dep_graph(self, task_id):\n        self.prune()\n        serialized = {}\n        if task_id in self._tasks:\n            self._recurse_deps(task_id, serialized)\n        return serialized\n\n    def task_list(self, status, upstream_status):\n        ''' query for a subset of tasks by status '''\n        self.prune()\n        result = {}\n        upstream_status_table = {}  # used to memoize upstream status\n        for task_id, task in self._tasks.iteritems():\n            if not status or task.status == status:\n                if (task.status != PENDING or not upstream_status or\n                    upstream_status == self._upstream_status(task_id, upstream_status_table)):\n                    serialized = self._serialize_task(task_id)\n                    result[task_id] = serialized\n        return result\n\n    def inverse_dependencies(self, task_id):\n        self.prune()\n        serialized = {}\n        if task_id in self._tasks:\n            self._traverse_inverse_deps(task_id, serialized)\n        return serialized\n\n    def _traverse_inverse_deps(self, task_id, serialized):\n        stack = [task_id]\n        serialized[task_id] = self._serialize_task(task_id)\n        while len(stack) > 0:\n            curr_id = stack.pop()\n            for id, task in self._tasks.iteritems():\n                if curr_id in task.deps:\n                    serialized[curr_id][\"deps\"].append(id)\n                    if id not in serialized:\n                        serialized[id] = self._serialize_task(id)\n                        serialized[id][\"deps\"] = []\n                        stack.append(id)\n\n    def task_search(self, task_str):\n        ''' query for a subset of tasks by task_id '''\n        self.prune()\n        result = collections.defaultdict(dict)\n        for task_id, task in self._tasks.iteritems():\n            if task_id.find(task_str) != -1:\n                serialized = self._serialize_task(task_id)\n                result[task.status][task_id] = serialized\n        return result\n\n    def fetch_error(self, task_id):\n        if self._tasks[task_id].expl is not None:\n            return {\"taskId\": task_id, \"error\": self._tasks[task_id].expl}\n        else:\n            return {\"taskId\": task_id, \"error\": \"\"}\n\n    def _update_task_history(self, task_id, status, host=None):\n        try:\n            if status == DONE or status == FAILED:\n                successful = (status == DONE)\n                self._task_history.task_finished(task_id, successful)\n            elif status == PENDING:\n                self._task_history.task_scheduled(task_id)\n            elif status == RUNNING:\n                self._task_history.task_started(task_id, host)\n        except:\n            logger.warning(\"Error saving Task history\", exc_info=1)\n\n    @property\n    def task_history(self):\n        # Used by server.py to expose the calls\n        return self._task_history\n",
          "file_after": "# Copyright (c) 2012 Spotify AB\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"); you may not\n# use this file except in compliance with the License. You may obtain a copy of\n# the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n# License for the specific language governing permissions and limitations under\n# the License.\n\nimport collections\nimport datetime\nimport os\nimport logging\nimport time\nimport cPickle as pickle\nimport task_history as history\nlogger = logging.getLogger(\"luigi.server\")\n\nfrom task_status import PENDING, FAILED, DONE, RUNNING, UNKNOWN\n\n\nclass Scheduler(object):\n    ''' Abstract base class\n\n    Note that the methods all take string arguments, not Task objects...\n    '''\n    add_task = NotImplemented\n    get_work = NotImplemented\n    ping = NotImplemented\n\nUPSTREAM_RUNNING = 'UPSTREAM_RUNNING'\nUPSTREAM_MISSING_INPUT = 'UPSTREAM_MISSING_INPUT'\nUPSTREAM_FAILED = 'UPSTREAM_FAILED'\n\nUPSTREAM_SEVERITY_ORDER = ('', UPSTREAM_RUNNING, UPSTREAM_MISSING_INPUT, UPSTREAM_FAILED)\nUPSTREAM_SEVERITY_KEY = lambda st: UPSTREAM_SEVERITY_ORDER.index(st)\nSTATUS_TO_UPSTREAM_MAP = {FAILED: UPSTREAM_FAILED, RUNNING: UPSTREAM_RUNNING, PENDING: UPSTREAM_MISSING_INPUT}\n\n\nclass Task(object):\n    def __init__(self, status, deps, resources={}, priority=0, family='', params={}):\n        self.stakeholders = set()  # workers ids that are somehow related to this task (i.e. don't prune while any of these workers are still active)\n        self.workers = set()  # workers ids that can perform task - task is 'BROKEN' if none of these workers are active\n        if deps is None:\n            self.deps = set()\n        else:\n            self.deps = set(deps)\n        self.status = status  # PENDING, RUNNING, FAILED or DONE\n        self.time = time.time()  # Timestamp when task was first added\n        self.retry = None\n        self.remove = None\n        self.worker_running = None  # the worker id that is currently running the task or None\n        self.time_running = None  # Timestamp when picked up by worker\n        self.expl = None\n        self.priority = priority\n        self.resources = resources\n        self.family = family\n        self.params = params\n\n    def __repr__(self):\n        return \"Task(%r)\" % vars(self)\n\n\nclass Worker(object):\n    \"\"\" Structure for tracking worker activity and keeping their references \"\"\"\n    def __init__(self, id, last_active=None):\n        self.id = id\n        self.reference = None  # reference to the worker in the real world. (Currently a dict containing just the host)\n        self.last_active = last_active  # seconds since epoch\n        self.info = {}\n\n    def add_info(self, info):\n        self.info.update(info)\n\n    def __str__(self):\n        return self.id\n\n\nclass CentralPlannerScheduler(Scheduler):\n    ''' Async scheduler that can handle multiple workers etc\n\n    Can be run locally or on a server (using RemoteScheduler + server.Server).\n    '''\n\n    def __init__(self, retry_delay=900.0, remove_delay=600.0, worker_disconnect_delay=60.0,\n                 state_path='/var/lib/luigi-server/state.pickle', task_history=None,\n                 resources=None):\n        '''\n        (all arguments are in seconds)\n        Keyword Arguments:\n        retry_delay -- How long after a Task fails to try it again, or -1 to never retry\n        remove_delay -- How long after a Task finishes to remove it from the scheduler\n        state_path -- Path to state file (tasks and active workers)\n        worker_disconnect_delay -- If a worker hasn't communicated for this long, remove it from active workers\n        '''\n        self._state_path = state_path\n        self._tasks = {}  # map from id to a Task object\n        self._retry_delay = retry_delay\n        self._remove_delay = remove_delay\n        self._worker_disconnect_delay = worker_disconnect_delay\n        self._active_workers = {}  # map from id to a Worker object\n        self._task_history = task_history or history.NopHistory()\n        self._resources = resources\n\n    def dump(self):\n        state = (self._tasks, self._active_workers)\n        try:\n            with open(self._state_path, 'w') as fobj:\n                pickle.dump(state, fobj)\n        except IOError:\n            logger.warning(\"Failed saving scheduler state\", exc_info=1)\n        else:\n            logger.info(\"Saved state in %s\", self._state_path)\n\n    # prone to lead to crashes when old state is unpickled with updated code. TODO some kind of version control?\n    def load(self):\n        if os.path.exists(self._state_path):\n            logger.info(\"Attempting to load state from %s\", self._state_path)\n            try:\n                with open(self._state_path) as fobj:\n                    state = pickle.load(fobj)\n            except:\n                logger.exception(\"Error when loading state. Starting from clean slate.\")\n                return\n\n            self._tasks, self._active_workers = state\n\n            # Convert from old format\n            # TODO: this is really ugly, we need something more future-proof\n            # Every time we add an attribute to the Worker class, this code needs to be updated\n            for k, v in self._active_workers.iteritems():\n                if isinstance(v, float):\n                    self._active_workers[k] = Worker(id=k, last_active=v)\n        else:\n            logger.info(\"No prior state file exists at %s. Starting with clean slate\", self._state_path)\n\n    def prune(self):\n        logger.info(\"Starting pruning of task graph\")\n        # Delete workers that haven't said anything for a while (probably killed)\n        delete_workers = []\n        for worker in self._active_workers.values():\n            if worker.last_active < time.time() - self._worker_disconnect_delay:\n                logger.info(\"Worker %s timed out (no contact for >=%ss)\", worker, self._worker_disconnect_delay)\n                delete_workers.append(worker.id)\n\n        for worker in delete_workers:\n            self._active_workers.pop(worker)\n\n        remaining_workers = set(self._active_workers.keys())\n\n        # Mark tasks with no remaining active stakeholders for deletion\n        for task_id, task in self._tasks.iteritems():\n            if not task.stakeholders.intersection(remaining_workers):\n                if task.remove is None:\n                    logger.info(\"Task %r has stakeholders %r but none remain connected -> will remove task in %s seconds\", task_id, task.stakeholders, self._remove_delay)\n                    task.remove = time.time() + self._remove_delay\n\n            if task.status == RUNNING and task.worker_running and task.worker_running not in remaining_workers:\n                # If a running worker disconnects, tag all its jobs as FAILED and subject it to the same retry logic\n                logger.info(\"Task %r is marked as running by disconnected worker %r -> marking as FAILED with retry delay of %rs\", task_id, task.worker_running, self._retry_delay)\n                task.worker_running = None\n                task.status = FAILED\n                task.retry = time.time() + self._retry_delay\n\n        # Remove tasks that have no stakeholders\n        remove_tasks = []\n        for task_id, task in self._tasks.iteritems():\n            if task.remove and time.time() > task.remove:\n                logger.info(\"Removing task %r (no connected stakeholders)\", task_id)\n                remove_tasks.append(task_id)\n\n        for task_id in remove_tasks:\n            self._tasks.pop(task_id)\n\n        # Reset FAILED tasks to PENDING if max timeout is reached, and retry delay is >= 0\n        for task in self._tasks.values():\n            if task.status == FAILED and self._retry_delay >= 0 and task.retry < time.time():\n                task.status = PENDING\n        logger.info(\"Done pruning task graph\")\n\n    def update(self, worker_id, worker_reference=None):\n        \"\"\" Keep track of whenever the worker was last active \"\"\"\n        worker = self._active_workers.setdefault(worker_id, Worker(worker_id))\n        if worker_reference:\n            worker.reference = worker_reference\n        worker.last_active = time.time()\n\n    def _update_priority(self, task, prio, worker):\n        \"\"\" Update priority of the given task\n\n        Priority can only be increased. If the task doesn't exist, a placeholder\n        task is created to preserve priority when the task is later scheduled.\n        \"\"\"\n        task.priority = prio = max(prio, task.priority)\n        for dep in task.deps or []:\n            t = self._tasks[dep] # This should always exist, see add_task\n            if prio > t.priority:\n                self._update_priority(t, prio, worker)\n\n    def add_task(self, worker, task_id, status=PENDING, runnable=True, deps=None, expl=None,\n                 resources=None, priority=0, family='', params={}):\n        \"\"\"\n        * Add task identified by task_id if it doesn't exist\n        * If deps is not None, update dependency list\n        * Update status of task\n        * Add additional workers/stakeholders\n        * Update priority when needed\n        \"\"\"\n        self.update(worker)\n\n        task = self._tasks.setdefault(task_id, Task(\n            status=PENDING, deps=deps, resources=resources, priority=priority, family=family,\n            params=params))\n\n        # for setting priority, we'll sometimes create tasks with unset family and params\n        if not task.family:\n            task.family = family\n        if not task.params:\n            task.params = params\n\n        if task.remove is not None:\n            task.remove = None  # unmark task for removal so it isn't removed after being added\n\n        if not (task.status == RUNNING and status == PENDING):\n            # don't allow re-scheduling of task while it is running, it must either fail or succeed first\n            if status == PENDING or status != task.status:\n                # Update the DB only if there was a acctual change, to prevent noise.\n                # We also check for status == PENDING b/c that's the default value\n                # (so checking for status != task.status woule lie)\n                self._update_task_history(task_id, status)\n            task.status = status\n            if status == FAILED:\n                task.retry = time.time() + self._retry_delay\n\n        if deps is not None:\n            task.deps = set(deps)\n\n        task.stakeholders.add(worker)\n\n        # Task dependencies might not exist yet. Let's create dummy tasks for them for now.\n        # Otherwise the task dependencies might end up being pruned if scheduling takes a long time\n        for dep in task.deps or []:\n            t = self._tasks.setdefault(dep, Task(status=UNKNOWN, deps=None, priority=priority))\n            t.stakeholders.add(worker)\n\n        self._update_priority(task, priority, worker)\n\n        if runnable:\n            task.workers.add(worker)\n\n        if expl is not None:\n            task.expl = expl\n\n    def add_worker(self, worker, info):\n        self._active_workers[worker].add_info(info)\n\n    def update_resources(self, **resources):\n        if self._resources is None:\n            self._resources = {}\n        self._resources.update(resources)\n\n    def _has_resources(self, needed_resources, used_resources):\n        if needed_resources is None:\n            return True\n\n        available_resources = self._resources or {}\n        for resource, amount in needed_resources.items():\n            if amount + used_resources[resource] > available_resources.get(resource, 1):\n                return False\n        return True\n\n    def _used_resources(self):\n        used_resources = collections.defaultdict(int)\n        if self._resources is not None:\n            for task in self._tasks.itervalues():\n                if task.status == RUNNING and task.resources:\n                    for resource, amount in task.resources.items():\n                        used_resources[resource] += amount\n        return used_resources\n\n    def _rank(self, worker):\n        ''' Return worker's rank function for task scheduling '''\n        return lambda (task_id, task): (task.priority, worker in task.workers, -task.time)\n\n    def _not_schedulable(self, task, used_resources):\n        return any((\n            task.status != PENDING,\n            any(dep not in self._tasks or self._tasks[dep].status != DONE for dep in task.deps),\n            not self._has_resources(task.resources, used_resources)\n        ))\n\n    def get_work(self, worker, host=None):\n        # TODO: remove any expired nodes\n\n        # Algo: iterate over all nodes, find the highest priority node no dependencies and available\n        # resources.\n\n        # Resource checking looks both at currently available resources and at which resources would\n        # be available if all running tasks died and we rescheduled all workers greedily. We do both\n        # checks in order to prevent a worker with many low-priority tasks from starving other\n        # workers with higher priority tasks that share the same resources.\n\n        # TODO: remove tasks that can't be done, figure out if the worker has absolutely\n        # nothing it can wait for\n\n        # Return remaining tasks that have no FAILED descendents\n        self.update(worker, {'host': host})\n        best_task = None\n        locally_pending_tasks = 0\n        running_tasks = []\n        used_resources = self._used_resources()\n        potential_resources = collections.defaultdict(int)\n        potential_workers = set([worker])\n\n        for task_id, task in sorted(self._tasks.iteritems(), key=self._rank(worker), reverse=True):\n            if task.status == RUNNING and worker in task.workers:\n                # Return a list of currently running tasks to the client,\n                # makes it easier to troubleshoot\n                other_worker = self._active_workers[task.worker_running]\n                more_info = {'task_id': task_id, 'worker': str(other_worker)}\n                if other_worker is not None:\n                    more_info.update(other_worker.info)\n                running_tasks.append(more_info)\n\n            if task.status == PENDING and worker in task.workers:\n                locally_pending_tasks += 1\n\n            if self._not_schedulable(task, potential_resources) or best_task:\n                continue\n\n            if worker in task.workers and self._has_resources(task.resources, used_resources):\n                best_task = task_id\n            else:\n                # keep track of the resources used in greedy scheduling\n                for w in filter(lambda w: w not in potential_workers, task.workers):\n                    for resource, amount in (task.resources or {}).items():\n                        potential_resources[resource] += amount\n                    potential_workers.add(w)\n\n        if best_task:\n            t = self._tasks[best_task]\n            t.status = RUNNING\n            t.worker_running = worker\n            t.time_running = time.time()\n            self._update_task_history(best_task, RUNNING, host=host)\n\n        return {'n_pending_tasks': locally_pending_tasks,\n                'task_id': best_task,\n                'running_tasks': running_tasks}\n\n    def ping(self, worker):\n        self.update(worker)\n\n    def _upstream_status(self, task_id, upstream_status_table):\n        if task_id in upstream_status_table:\n            return upstream_status_table[task_id]\n        elif task_id in self._tasks:\n            task_stack = [task_id]\n\n            while task_stack:\n                dep_id = task_stack.pop()\n                if dep_id in self._tasks:\n                    dep = self._tasks[dep_id]\n                    if dep_id not in upstream_status_table:\n                        if dep.status == PENDING and dep.deps:\n                            task_stack = task_stack + [dep_id] + list(dep.deps)\n                            upstream_status_table[dep_id] = ''  # will be updated postorder\n                        else:\n                            dep_status = STATUS_TO_UPSTREAM_MAP.get(dep.status, '')\n                            upstream_status_table[dep_id] = dep_status\n                    elif upstream_status_table[dep_id] == '' and dep.deps:\n                        # This is the postorder update step when we set the\n                        # status based on the previously calculated child elements\n                        upstream_status = [upstream_status_table.get(id, '') for id in dep.deps]\n                        upstream_status.append('')  # to handle empty list\n                        status = max(upstream_status, key=UPSTREAM_SEVERITY_KEY)\n                        upstream_status_table[dep_id] = status\n            return upstream_status_table[dep_id]\n\n    def _serialize_task(self, task_id):\n        task = self._tasks[task_id]\n        return {\n            'deps': list(task.deps),\n            'status': task.status,\n            'workers': list(task.workers),\n            'worker_running': task.worker_running,\n            'time_running': getattr(task, \"time_running\", None),\n            'start_time': task.time,\n            'params': task.params,\n            'name': task.family\n        }\n\n    def graph(self):\n        self.prune()\n        serialized = {}\n        for task_id, task in self._tasks.iteritems():\n            serialized[task_id] = self._serialize_task(task_id)\n        return serialized\n\n    def _recurse_deps(self, task_id, serialized):\n        if task_id not in serialized:\n            task = self._tasks.get(task_id)\n            if task is None or not task.family:\n                logger.warn('Missing task for id [%s]', task_id)\n\n                # try to infer family and params from task_id\n                try:\n                    family, _, param_str = task_id.rstrip(')').partition('(')\n                    params = dict(param.split('=') for param in param_str.split(', '))\n                except:\n                    family, params = '', {}\n                serialized[task_id] = {\n                    'deps': [],\n                    'status': UNKNOWN,\n                    'workers': [],\n                    'start_time': UNKNOWN,\n                    'params': params,\n                    'name': family\n                }\n            else:\n                serialized[task_id] = self._serialize_task(task_id)\n                for dep in task.deps:\n                    self._recurse_deps(dep, serialized)\n\n    def dep_graph(self, task_id):\n        self.prune()\n        serialized = {}\n        if task_id in self._tasks:\n            self._recurse_deps(task_id, serialized)\n        return serialized\n\n    def task_list(self, status, upstream_status):\n        ''' query for a subset of tasks by status '''\n        self.prune()\n        result = {}\n        upstream_status_table = {}  # used to memoize upstream status\n        for task_id, task in self._tasks.iteritems():\n            if not status or task.status == status:\n                if (task.status != PENDING or not upstream_status or\n                    upstream_status == self._upstream_status(task_id, upstream_status_table)):\n                    serialized = self._serialize_task(task_id)\n                    result[task_id] = serialized\n        return result\n\n    def inverse_dependencies(self, task_id):\n        self.prune()\n        serialized = {}\n        if task_id in self._tasks:\n            self._traverse_inverse_deps(task_id, serialized)\n        return serialized\n\n    def _traverse_inverse_deps(self, task_id, serialized):\n        stack = [task_id]\n        serialized[task_id] = self._serialize_task(task_id)\n        while len(stack) > 0:\n            curr_id = stack.pop()\n            for id, task in self._tasks.iteritems():\n                if curr_id in task.deps:\n                    serialized[curr_id][\"deps\"].append(id)\n                    if id not in serialized:\n                        serialized[id] = self._serialize_task(id)\n                        serialized[id][\"deps\"] = []\n                        stack.append(id)\n\n    def task_search(self, task_str):\n        ''' query for a subset of tasks by task_id '''\n        self.prune()\n        result = collections.defaultdict(dict)\n        for task_id, task in self._tasks.iteritems():\n            if task_id.find(task_str) != -1:\n                serialized = self._serialize_task(task_id)\n                result[task.status][task_id] = serialized\n        return result\n\n    def fetch_error(self, task_id):\n        if self._tasks[task_id].expl is not None:\n            return {\"taskId\": task_id, \"error\": self._tasks[task_id].expl}\n        else:\n            return {\"taskId\": task_id, \"error\": \"\"}\n\n    def _update_task_history(self, task_id, status, host=None):\n        try:\n            if status == DONE or status == FAILED:\n                successful = (status == DONE)\n                self._task_history.task_finished(task_id, successful)\n            elif status == PENDING:\n                self._task_history.task_scheduled(task_id)\n            elif status == RUNNING:\n                self._task_history.task_started(task_id, host)\n        except:\n            logger.warning(\"Error saving Task history\", exc_info=1)\n\n    @property\n    def task_history(self):\n        # Used by server.py to expose the calls\n        return self._task_history\n",
          "file_patch": "@@ -43,7 +43,7 @@ STATUS_TO_UPSTREAM_MAP = {FAILED: UPSTREAM_FAILED, RUNNING: UPSTREAM_RUNNING, PE\n \n \n class Task(object):\n-    def __init__(self, status, deps, priority=0, family='', params={}):\n+    def __init__(self, status, deps, resources={}, priority=0, family='', params={}):\n         self.stakeholders = set()  # workers ids that are somehow related to this task (i.e. don't prune while any of these workers are still active)\n         self.workers = set()  # workers ids that can perform task - task is 'BROKEN' if none of these workers are active\n         if deps is None:\n@@ -58,6 +58,7 @@ class Task(object):\n         self.time_running = None  # Timestamp when picked up by worker\n         self.expl = None\n         self.priority = priority\n+        self.resources = resources\n         self.family = family\n         self.params = params\n \n@@ -87,7 +88,8 @@ class CentralPlannerScheduler(Scheduler):\n     '''\n \n     def __init__(self, retry_delay=900.0, remove_delay=600.0, worker_disconnect_delay=60.0,\n-                 state_path='/var/lib/luigi-server/state.pickle', task_history=None):\n+                 state_path='/var/lib/luigi-server/state.pickle', task_history=None,\n+                 resources=None):\n         '''\n         (all arguments are in seconds)\n         Keyword Arguments:\n@@ -103,6 +105,7 @@ class CentralPlannerScheduler(Scheduler):\n         self._worker_disconnect_delay = worker_disconnect_delay\n         self._active_workers = {}  # map from id to a Worker object\n         self._task_history = task_history or history.NopHistory()\n+        self._resources = resources\n \n     def dump(self):\n         state = (self._tasks, self._active_workers)\n@@ -199,7 +202,8 @@ class CentralPlannerScheduler(Scheduler):\n             if prio > t.priority:\n                 self._update_priority(t, prio, worker)\n \n-    def add_task(self, worker, task_id, status=PENDING, runnable=True, deps=None, expl=None, priority=0, family='', params={}):\n+    def add_task(self, worker, task_id, status=PENDING, runnable=True, deps=None, expl=None,\n+                 resources=None, priority=0, family='', params={}):\n         \"\"\"\n         * Add task identified by task_id if it doesn't exist\n         * If deps is not None, update dependency list\n@@ -209,7 +213,9 @@ class CentralPlannerScheduler(Scheduler):\n         \"\"\"\n         self.update(worker)\n \n-        task = self._tasks.setdefault(task_id, Task(status=PENDING, deps=deps, priority=priority, family=family, params=params))\n+        task = self._tasks.setdefault(task_id, Task(\n+            status=PENDING, deps=deps, resources=resources, priority=priority, family=family,\n+            params=params))\n \n         # for setting priority, we'll sometimes create tasks with unset family and params\n         if not task.family:\n@@ -253,27 +259,66 @@ class CentralPlannerScheduler(Scheduler):\n     def add_worker(self, worker, info):\n         self._active_workers[worker].add_info(info)\n \n+    def update_resources(self, **resources):\n+        if self._resources is None:\n+            self._resources = {}\n+        self._resources.update(resources)\n+\n+    def _has_resources(self, needed_resources, used_resources):\n+        if needed_resources is None:\n+            return True\n+\n+        available_resources = self._resources or {}\n+        for resource, amount in needed_resources.items():\n+            if amount + used_resources[resource] > available_resources.get(resource, 1):\n+                return False\n+        return True\n+\n+    def _used_resources(self):\n+        used_resources = collections.defaultdict(int)\n+        if self._resources is not None:\n+            for task in self._tasks.itervalues():\n+                if task.status == RUNNING and task.resources:\n+                    for resource, amount in task.resources.items():\n+                        used_resources[resource] += amount\n+        return used_resources\n+\n+    def _rank(self, worker):\n+        ''' Return worker's rank function for task scheduling '''\n+        return lambda (task_id, task): (task.priority, worker in task.workers, -task.time)\n+\n+    def _not_schedulable(self, task, used_resources):\n+        return any((\n+            task.status != PENDING,\n+            any(dep not in self._tasks or self._tasks[dep].status != DONE for dep in task.deps),\n+            not self._has_resources(task.resources, used_resources)\n+        ))\n+\n     def get_work(self, worker, host=None):\n         # TODO: remove any expired nodes\n \n-        # Algo: iterate over all nodes, find the first node with no dependencies and highest priority\n+        # Algo: iterate over all nodes, find the highest priority node no dependencies and available\n+        # resources.\n+\n+        # Resource checking looks both at currently available resources and at which resources would\n+        # be available if all running tasks died and we rescheduled all workers greedily. We do both\n+        # checks in order to prevent a worker with many low-priority tasks from starving other\n+        # workers with higher priority tasks that share the same resources.\n \n         # TODO: remove tasks that can't be done, figure out if the worker has absolutely\n         # nothing it can wait for\n \n         # Return remaining tasks that have no FAILED descendents\n         self.update(worker, {'host': host})\n-        best_t = float('inf')\n-        best_priority = float('-inf')\n         best_task = None\n         locally_pending_tasks = 0\n         running_tasks = []\n+        used_resources = self._used_resources()\n+        potential_resources = collections.defaultdict(int)\n+        potential_workers = set([worker])\n \n-        for task_id, task in self._tasks.iteritems():\n-            if worker not in task.workers:\n-                continue\n-\n-            if task.status == RUNNING:\n+        for task_id, task in sorted(self._tasks.iteritems(), key=self._rank(worker), reverse=True):\n+            if task.status == RUNNING and worker in task.workers:\n                 # Return a list of currently running tasks to the client,\n                 # makes it easier to troubleshoot\n                 other_worker = self._active_workers[task.worker_running]\n@@ -282,22 +327,20 @@ class CentralPlannerScheduler(Scheduler):\n                     more_info.update(other_worker.info)\n                 running_tasks.append(more_info)\n \n-            if task.status != PENDING:\n+            if task.status == PENDING and worker in task.workers:\n+                locally_pending_tasks += 1\n+\n+            if self._not_schedulable(task, potential_resources) or best_task:\n                 continue\n \n-            locally_pending_tasks += 1\n-            ok = True\n-            for dep in task.deps:\n-                if dep not in self._tasks:\n-                    ok = False\n-                elif self._tasks[dep].status != DONE:\n-                    ok = False\n-\n-            if ok:\n-                if (-task.priority, task.time) < (-best_priority, best_t):\n-                    best_t = task.time\n-                    best_priority = task.priority\n-                    best_task = task_id\n+            if worker in task.workers and self._has_resources(task.resources, used_resources):\n+                best_task = task_id\n+            else:\n+                # keep track of the resources used in greedy scheduling\n+                for w in filter(lambda w: w not in potential_workers, task.workers):\n+                    for resource, amount in (task.resources or {}).items():\n+                        potential_resources[resource] += amount\n+                    potential_workers.add(w)\n \n         if best_task:\n             t = self._tasks[best_task]\n",
          "files_name_in_blame_commit": [
            "scheduler.py",
            "task.py",
            "server.py",
            "central_planner_test.py",
            "worker.py",
            "configuration.py",
            "rpc.py"
          ]
        }
      },
      "77e027b52ea8b0645f33ff90b7ad9371a707c13b": {
        "commit": {
          "commit_id": "77e027b52ea8b0645f33ff90b7ad9371a707c13b",
          "commit_message": "More info for each worker",
          "commit_author": "Erik Bernhardsson",
          "commit_date": "2014-06-29 22:31:19",
          "commit_parent": "5d25c49cf4545a2b1ff7379149ec17b729ab6f65"
        },
        "function": {
          "function_name": "get_work",
          "function_code_before": "def get_work(self, worker, host=None):\n    self.update(worker, {'host': host})\n    best_t = float('inf')\n    best_priority = float('-inf')\n    best_task = None\n    locally_pending_tasks = 0\n    running_tasks = []\n    for (task_id, task) in self._tasks.iteritems():\n        if worker not in task.workers:\n            continue\n        if task.status == RUNNING:\n            running_tasks.append({'task_id': task_id, 'worker': str(self._active_workers.get(task.worker_running))})\n        if task.status != PENDING:\n            continue\n        locally_pending_tasks += 1\n        ok = True\n        for dep in task.deps:\n            if dep not in self._tasks:\n                ok = False\n            elif self._tasks[dep].status != DONE:\n                ok = False\n        if ok:\n            if (-task.priority, task.time) < (-best_priority, best_t):\n                best_t = task.time\n                best_priority = task.priority\n                best_task = task_id\n    if best_task:\n        t = self._tasks[best_task]\n        t.status = RUNNING\n        t.worker_running = worker\n        t.time_running = time.time()\n        self._update_task_history(best_task, RUNNING, host=host)\n    return {'n_pending_tasks': locally_pending_tasks, 'task_id': best_task, 'running_tasks': running_tasks}",
          "function_code_after": "def get_work(self, worker, host=None):\n    self.update(worker, {'host': host})\n    best_t = float('inf')\n    best_priority = float('-inf')\n    best_task = None\n    locally_pending_tasks = 0\n    running_tasks = []\n    for (task_id, task) in self._tasks.iteritems():\n        if worker not in task.workers:\n            continue\n        if task.status == RUNNING:\n            other_worker = self._active_workers[task.worker_running]\n            more_info = {'task_id': task_id, 'worker': str(other_worker)}\n            if other_worker is not None:\n                more_info.update(other_worker.info)\n            running_tasks.append(more_info)\n        if task.status != PENDING:\n            continue\n        locally_pending_tasks += 1\n        ok = True\n        for dep in task.deps:\n            if dep not in self._tasks:\n                ok = False\n            elif self._tasks[dep].status != DONE:\n                ok = False\n        if ok:\n            if (-task.priority, task.time) < (-best_priority, best_t):\n                best_t = task.time\n                best_priority = task.priority\n                best_task = task_id\n    if best_task:\n        t = self._tasks[best_task]\n        t.status = RUNNING\n        t.worker_running = worker\n        t.time_running = time.time()\n        self._update_task_history(best_task, RUNNING, host=host)\n    return {'n_pending_tasks': locally_pending_tasks, 'task_id': best_task, 'running_tasks': running_tasks}",
          "function_before_start_line": 215,
          "function_before_end_line": 264,
          "function_after_start_line": 222,
          "function_after_end_line": 277,
          "function_before_token_count": 238,
          "function_after_token_count": 256,
          "functions_name_modified_file": [
            "_update_task_history",
            "prune",
            "_upstream_status",
            "ping",
            "_traverse_inverse_deps",
            "load",
            "inverse_dependencies",
            "task_search",
            "_recurse_deps",
            "add_task",
            "task_list",
            "add_info",
            "_serialize_task",
            "task_history",
            "dep_graph",
            "__str__",
            "graph",
            "dump",
            "__repr__",
            "__init__",
            "update",
            "add_worker",
            "_get_task_params",
            "get_work",
            "_get_task_name",
            "fetch_error"
          ],
          "functions_name_all_files": [
            "_update_task_history",
            "test_remove_dep",
            "prune",
            "tearDown",
            "_upstream_status",
            "_generate_worker_info",
            "test_two_workers",
            "test_parameter_split",
            "_email_complete_error",
            "ping",
            "_fork_task",
            "test_retry",
            "inverse_dep_graph",
            "_wait",
            "_traverse_inverse_deps",
            "test_dep",
            "load",
            "inverse_dependencies",
            "_log_unexpected_error",
            "_email_unexpected_error",
            "_log_remote_tasks",
            "task_search",
            "_add_task_and_deps",
            "test_priorities_and_dependencies",
            "_recurse_deps",
            "test_timeout",
            "test_two_worker_info",
            "add_task",
            "_validate_dependency",
            "task_list",
            "add_info",
            "add",
            "run",
            "_serialize_task",
            "test_disallowed_state_changes",
            "task_history",
            "_add_worker",
            "dep_graph",
            "_run_task",
            "_get_work",
            "test_disconnect_running",
            "_check_complete",
            "__str__",
            "graph",
            "_sleeper",
            "dump",
            "__repr__",
            "setUp",
            "__init__",
            "_request",
            "update",
            "test_failed_dep",
            "add_worker",
            "_validate_task",
            "_check_complete_value",
            "test_broken_dep",
            "_get_task_params",
            "test_priorities",
            "test_priorities_default_and_negative",
            "stop",
            "_log_complete_error",
            "_add",
            "get_work",
            "_get_task_name",
            "_add_external",
            "_reap_children",
            "setTime",
            "fetch_error"
          ],
          "functions_name_co_evolved_modified_file": [
            "__init__",
            "add_worker",
            "add_info",
            "__str__"
          ],
          "functions_name_co_evolved_all_files": [
            "__init__",
            "_add_worker",
            "add_worker",
            "_generate_worker_info",
            "test_two_worker_info",
            "add_info",
            "__str__",
            "run"
          ]
        },
        "file": {
          "file_name": "scheduler.py",
          "file_nloc": 316,
          "file_complexity": 109,
          "file_token_count": 2326,
          "file_before": "# Copyright (c) 2012 Spotify AB\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"); you may not\n# use this file except in compliance with the License. You may obtain a copy of\n# the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n# License for the specific language governing permissions and limitations under\n# the License.\n\nimport collections\nimport datetime\nimport os\nimport logging\nimport time\nimport cPickle as pickle\nimport task_history as history\nlogger = logging.getLogger(\"luigi.server\")\n\nfrom task_status import PENDING, FAILED, DONE, RUNNING, UNKNOWN\n\n\nclass Scheduler(object):\n    ''' Abstract base class\n\n    Note that the methods all take string arguments, not Task objects...\n    '''\n    add_task = NotImplemented\n    get_work = NotImplemented\n    ping = NotImplemented\n\nUPSTREAM_RUNNING = 'UPSTREAM_RUNNING'\nUPSTREAM_MISSING_INPUT = 'UPSTREAM_MISSING_INPUT'\nUPSTREAM_FAILED = 'UPSTREAM_FAILED'\n\nUPSTREAM_SEVERITY_ORDER = ('', UPSTREAM_RUNNING, UPSTREAM_MISSING_INPUT, UPSTREAM_FAILED)\nUPSTREAM_SEVERITY_KEY = lambda st: UPSTREAM_SEVERITY_ORDER.index(st)\nSTATUS_TO_UPSTREAM_MAP = {FAILED: UPSTREAM_FAILED, RUNNING: UPSTREAM_RUNNING, PENDING: UPSTREAM_MISSING_INPUT}\n\n\nclass Task(object):\n    def __init__(self, status, deps, priority=0):\n        self.stakeholders = set()  # workers ids that are somehow related to this task (i.e. don't prune while any of these workers are still active)\n        self.workers = set()  # workers ids that can perform task - task is 'BROKEN' if none of these workers are active\n        if deps is None:\n            self.deps = set()\n        else:\n            self.deps = set(deps)\n        self.status = status  # PENDING, RUNNING, FAILED or DONE\n        self.time = time.time()  # Timestamp when task was first added\n        self.retry = None\n        self.remove = None\n        self.worker_running = None  # the worker id that is currently running the task or None\n        self.time_running = None  # Timestamp when picked up by worker\n        self.expl = None\n        self.priority = priority\n\n    def __repr__(self):\n        return \"Task(%r)\" % vars(self)\n\n\nclass Worker(object):\n    \"\"\" Structure for tracking worker activity and keeping their references \"\"\"\n    def __init__(self, id, last_active=None):\n        self.id = id\n        self.reference = None  # reference to the worker in the real world. (Currently a dict containing just the host)\n        self.last_active = last_active  # seconds since epoch\n\n    def __str__(self):\n        return \"%s on %s, last active %s\" % (self.id, self.reference, datetime.datetime.utcfromtimestamp(self.last_active).isoformat())\n\n\nclass CentralPlannerScheduler(Scheduler):\n    ''' Async scheduler that can handle multiple workers etc\n\n    Can be run locally or on a server (using RemoteScheduler + server.Server).\n    '''\n\n    def __init__(self, retry_delay=900.0, remove_delay=600.0, worker_disconnect_delay=60.0,\n                 state_path='/var/lib/luigi-server/state.pickle', task_history=None):\n        '''\n        (all arguments are in seconds)\n        Keyword Arguments:\n        retry_delay -- How long after a Task fails to try it again, or -1 to never retry\n        remove_delay -- How long after a Task finishes to remove it from the scheduler\n        state_path -- Path to state file (tasks and active workers)\n        worker_disconnect_delay -- If a worker hasn't communicated for this long, remove it from active workers\n        '''\n        self._state_path = state_path\n        self._tasks = {}  # map from id to a Task object\n        self._retry_delay = retry_delay\n        self._remove_delay = remove_delay\n        self._worker_disconnect_delay = worker_disconnect_delay\n        self._active_workers = {}  # map from id to a Worker object\n        self._task_history = task_history or history.NopHistory()\n\n    def dump(self):\n        state = (self._tasks, self._active_workers)\n        try:\n            with open(self._state_path, 'w') as fobj:\n                pickle.dump(state, fobj)\n        except IOError:\n            logger.warning(\"Failed saving scheduler state\", exc_info=1)\n        else:\n            logger.info(\"Saved state in %s\", self._state_path)\n\n    # prone to lead to crashes when old state is unpickled with updated code. TODO some kind of version control?\n    def load(self):\n        if os.path.exists(self._state_path):\n            logger.info(\"Attempting to load state from %s\", self._state_path)\n            with open(self._state_path) as fobj:\n                state = pickle.load(fobj)\n            self._tasks, self._active_workers = state\n\n            # Convert from old format\n            # TODO: this is really ugly, we need something more future-proof\n            # Every time we add an attribute to the Worker class, this code needs to be updated\n            for k, v in self._active_workers.iteritems():\n                if isinstance(v, float):\n                    self._active_workers[k] = Worker(id=k, last_active=v)\n        else:\n            logger.info(\"No prior state file exists at %s. Starting with clean slate\", self._state_path)\n\n    def prune(self):\n        logger.info(\"Starting pruning of task graph\")\n        # Delete workers that haven't said anything for a while (probably killed)\n        delete_workers = []\n        for worker in self._active_workers.values():\n            if worker.last_active < time.time() - self._worker_disconnect_delay:\n                logger.info(\"Worker %s timed out (no contact for >=%ss)\", worker, self._worker_disconnect_delay)\n                delete_workers.append(worker.id)\n\n        for worker in delete_workers:\n            self._active_workers.pop(worker)\n\n        remaining_workers = set(self._active_workers.keys())\n\n        # Mark tasks with no remaining active stakeholders for deletion\n        for task_id, task in self._tasks.iteritems():\n            if not task.stakeholders.intersection(remaining_workers):\n                if task.remove is None:\n                    logger.info(\"Task %r has stakeholders %r but none remain connected -> will remove task in %s seconds\", task_id, task.stakeholders, self._remove_delay)\n                    task.remove = time.time() + self._remove_delay\n\n            if task.status == RUNNING and task.worker_running and task.worker_running not in remaining_workers:\n                # If a running worker disconnects, tag all its jobs as FAILED and subject it to the same retry logic\n                logger.info(\"Task %r is marked as running by disconnected worker %r -> marking as FAILED with retry delay of %rs\", task_id, task.worker_running, self._retry_delay)\n                task.worker_running = None\n                task.status = FAILED\n                task.retry = time.time() + self._retry_delay\n\n        # Remove tasks that have no stakeholders\n        remove_tasks = []\n        for task_id, task in self._tasks.iteritems():\n            if task.remove and time.time() > task.remove:\n                logger.info(\"Removing task %r (no connected stakeholders)\", task_id)\n                remove_tasks.append(task_id)\n\n        for task_id in remove_tasks:\n            self._tasks.pop(task_id)\n\n        # Reset FAILED tasks to PENDING if max timeout is reached, and retry delay is >= 0\n        for task in self._tasks.values():\n            if task.status == FAILED and self._retry_delay >= 0 and task.retry < time.time():\n                task.status = PENDING\n        logger.info(\"Done pruning task graph\")\n\n    def update(self, worker_id, worker_reference=None):\n        \"\"\" Keep track of whenever the worker was last active \"\"\"\n        worker = self._active_workers.setdefault(worker_id, Worker(worker_id))\n        if worker_reference:\n            worker.reference = worker_reference\n        worker.last_active = time.time()\n\n    def add_task(self, worker, task_id, status=PENDING, runnable=True, deps=None, expl=None, priority=0):\n        \"\"\"\n        * Add task identified by task_id if it doesn't exist\n        * If deps is not None, update dependency list\n        * Update status of task\n        * Add additional workers/stakeholders\n        \"\"\"\n        self.update(worker)\n\n        task = self._tasks.setdefault(task_id, Task(status=PENDING, deps=deps, priority=priority))\n\n        if task.remove is not None:\n            task.remove = None  # unmark task for removal so it isn't removed after being added\n\n        if not (task.status == RUNNING and status == PENDING):\n            # don't allow re-scheduling of task while it is running, it must either fail or succeed first\n            if status == PENDING or status != task.status:\n                # Update the DB only if there was a acctual change, to prevent noise.\n                # We also check for status == PENDING b/c that's the default value\n                # (so checking for status != task.status woule lie)\n                self._update_task_history(task_id, status)\n            task.status = status\n            if status == FAILED:\n                task.retry = time.time() + self._retry_delay\n\n        if deps is not None:\n            task.deps = set(deps)\n\n        task.stakeholders.add(worker)\n\n        if runnable:\n            task.workers.add(worker)\n\n        if expl is not None:\n            task.expl = expl\n\n    def get_work(self, worker, host=None):\n        # TODO: remove any expired nodes\n\n        # Algo: iterate over all nodes, find the first node with no dependencies and highest priority\n\n        # TODO: remove tasks that can't be done, figure out if the worker has absolutely\n        # nothing it can wait for\n\n        # Return remaining tasks that have no FAILED descendents\n        self.update(worker, {'host': host})\n        best_t = float('inf')\n        best_priority = float('-inf')\n        best_task = None\n        locally_pending_tasks = 0\n        running_tasks = []\n\n        for task_id, task in self._tasks.iteritems():\n            if worker not in task.workers:\n                continue\n\n            if task.status == RUNNING:\n                running_tasks.append({'task_id': task_id, 'worker': str(self._active_workers.get(task.worker_running))})\n\n            if task.status != PENDING:\n                continue\n\n            locally_pending_tasks += 1\n            ok = True\n            for dep in task.deps:\n                if dep not in self._tasks:\n                    ok = False\n                elif self._tasks[dep].status != DONE:\n                    ok = False\n\n            if ok:\n                if (-task.priority, task.time) < (-best_priority, best_t):\n                    best_t = task.time\n                    best_priority = task.priority\n                    best_task = task_id\n\n        if best_task:\n            t = self._tasks[best_task]\n            t.status = RUNNING\n            t.worker_running = worker\n            t.time_running = time.time()\n            self._update_task_history(best_task, RUNNING, host=host)\n\n        return {'n_pending_tasks': locally_pending_tasks,\n                'task_id': best_task,\n                'running_tasks': running_tasks}\n\n    def ping(self, worker):\n        self.update(worker)\n\n    def _upstream_status(self, task_id, upstream_status_table):\n        if task_id in upstream_status_table:\n            return upstream_status_table[task_id]\n        elif task_id in self._tasks:\n            task_stack = [task_id]\n\n            while task_stack:\n                dep_id = task_stack.pop()\n                if dep_id in self._tasks:\n                    dep = self._tasks[dep_id]\n                    if dep_id not in upstream_status_table:\n                        if dep.status == PENDING and dep.deps:\n                            task_stack = task_stack + [dep_id] + list(dep.deps)\n                            upstream_status_table[dep_id] = ''  # will be updated postorder\n                        else:\n                            dep_status = STATUS_TO_UPSTREAM_MAP.get(dep.status, '')\n                            upstream_status_table[dep_id] = dep_status\n                    elif upstream_status_table[dep_id] == '' and dep.deps:\n                        # This is the postorder update step when we set the\n                        # status based on the previously calculated child elements\n                        upstream_status = [upstream_status_table.get(id, '') for id in dep.deps]\n                        upstream_status.append('')  # to handle empty list\n                        status = max(upstream_status, key=UPSTREAM_SEVERITY_KEY)\n                        upstream_status_table[dep_id] = status\n            return upstream_status_table[dep_id]\n\n    def _serialize_task(self, task_id):\n        task = self._tasks[task_id]\n        return {\n            'deps': list(task.deps),\n            'status': task.status,\n            'workers': list(task.workers),\n            'worker_running': task.worker_running,\n            'time_running': getattr(task, \"time_running\", None),\n            'start_time': task.time,\n            'params': self._get_task_params(task_id),\n            'name': self._get_task_name(task_id)\n        }\n\n    def _get_task_params(self, task_id):\n        params = {}\n        params_part = task_id.split('(')[1].strip(')')\n        params_strings = params_part.split(\", \")\n\n        for param in params_strings:\n            if not param:\n                continue\n            split_param = param.split('=')\n            if len(split_param) != 2:\n                return {'<complex parameters>': params_part}\n            params[split_param[0]] = split_param[1]\n        return params\n\n    def _get_task_name(self, task_id):\n        return task_id.split('(')[0]\n\n    def graph(self):\n        self.prune()\n        serialized = {}\n        for task_id, task in self._tasks.iteritems():\n            serialized[task_id] = self._serialize_task(task_id)\n        return serialized\n\n    def _recurse_deps(self, task_id, serialized):\n        if task_id not in serialized:\n            task = self._tasks.get(task_id)\n            if task is None:\n                logger.warn('Missing task for id [%s]', task_id)\n                serialized[task_id] = {\n                    'deps': [],\n                    'status': UNKNOWN,\n                    'workers': [],\n                    'start_time': UNKNOWN,\n                    'params': self._get_task_params(task_id),\n                    'name': self._get_task_name(task_id)\n                }\n            else:\n                serialized[task_id] = self._serialize_task(task_id)\n                for dep in task.deps:\n                    self._recurse_deps(dep, serialized)\n\n    def dep_graph(self, task_id):\n        self.prune()\n        serialized = {}\n        if task_id in self._tasks:\n            self._recurse_deps(task_id, serialized)\n        return serialized\n\n    def task_list(self, status, upstream_status):\n        ''' query for a subset of tasks by status '''\n        self.prune()\n        result = {}\n        upstream_status_table = {}  # used to memoize upstream status\n        for task_id, task in self._tasks.iteritems():\n            if not status or task.status == status:\n                if (task.status != PENDING or not upstream_status or\n                    upstream_status == self._upstream_status(task_id, upstream_status_table)):\n                    serialized = self._serialize_task(task_id)\n                    result[task_id] = serialized\n        return result\n\n    def inverse_dependencies(self, task_id):\n        self.prune()\n        serialized = {}\n        if task_id in self._tasks:\n            self._traverse_inverse_deps(task_id, serialized)\n        return serialized\n\n    def _traverse_inverse_deps(self, task_id, serialized):\n        stack = [task_id]\n        serialized[task_id] = self._serialize_task(task_id)\n        while len(stack) > 0:\n            curr_id = stack.pop()\n            for id, task in self._tasks.iteritems():\n                if curr_id in task.deps:\n                    serialized[curr_id][\"deps\"].append(id)\n                    if id not in serialized:\n                        serialized[id] = self._serialize_task(id)\n                        serialized[id][\"deps\"] = []\n                        stack.append(id)\n\n    def task_search(self, task_str):\n        ''' query for a subset of tasks by task_id '''\n        self.prune()\n        result = collections.defaultdict(dict)\n        for task_id, task in self._tasks.iteritems():\n            if task_id.find(task_str) != -1:\n                serialized = self._serialize_task(task_id)\n                result[task.status][task_id] = serialized\n        return result\n\n    def fetch_error(self, task_id):\n        if self._tasks[task_id].expl is not None:\n            return {\"taskId\": task_id, \"error\": self._tasks[task_id].expl}\n        else:\n            return {\"taskId\": task_id, \"error\": \"\"}\n\n    def _update_task_history(self, task_id, status, host=None):\n        try:\n            if status == DONE or status == FAILED:\n                successful = (status == DONE)\n                self._task_history.task_finished(task_id, successful)\n            elif status == PENDING:\n                self._task_history.task_scheduled(task_id)\n            elif status == RUNNING:\n                self._task_history.task_started(task_id, host)\n        except:\n            logger.warning(\"Error saving Task history\", exc_info=1)\n\n    @property\n    def task_history(self):\n        # Used by server.py to expose the calls\n        return self._task_history\n",
          "file_after": "# Copyright (c) 2012 Spotify AB\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"); you may not\n# use this file except in compliance with the License. You may obtain a copy of\n# the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n# License for the specific language governing permissions and limitations under\n# the License.\n\nimport collections\nimport datetime\nimport os\nimport logging\nimport time\nimport cPickle as pickle\nimport task_history as history\nlogger = logging.getLogger(\"luigi.server\")\n\nfrom task_status import PENDING, FAILED, DONE, RUNNING, UNKNOWN\n\n\nclass Scheduler(object):\n    ''' Abstract base class\n\n    Note that the methods all take string arguments, not Task objects...\n    '''\n    add_task = NotImplemented\n    get_work = NotImplemented\n    ping = NotImplemented\n\nUPSTREAM_RUNNING = 'UPSTREAM_RUNNING'\nUPSTREAM_MISSING_INPUT = 'UPSTREAM_MISSING_INPUT'\nUPSTREAM_FAILED = 'UPSTREAM_FAILED'\n\nUPSTREAM_SEVERITY_ORDER = ('', UPSTREAM_RUNNING, UPSTREAM_MISSING_INPUT, UPSTREAM_FAILED)\nUPSTREAM_SEVERITY_KEY = lambda st: UPSTREAM_SEVERITY_ORDER.index(st)\nSTATUS_TO_UPSTREAM_MAP = {FAILED: UPSTREAM_FAILED, RUNNING: UPSTREAM_RUNNING, PENDING: UPSTREAM_MISSING_INPUT}\n\n\nclass Task(object):\n    def __init__(self, status, deps, priority=0):\n        self.stakeholders = set()  # workers ids that are somehow related to this task (i.e. don't prune while any of these workers are still active)\n        self.workers = set()  # workers ids that can perform task - task is 'BROKEN' if none of these workers are active\n        if deps is None:\n            self.deps = set()\n        else:\n            self.deps = set(deps)\n        self.status = status  # PENDING, RUNNING, FAILED or DONE\n        self.time = time.time()  # Timestamp when task was first added\n        self.retry = None\n        self.remove = None\n        self.worker_running = None  # the worker id that is currently running the task or None\n        self.time_running = None  # Timestamp when picked up by worker\n        self.expl = None\n        self.priority = priority\n\n    def __repr__(self):\n        return \"Task(%r)\" % vars(self)\n\n\nclass Worker(object):\n    \"\"\" Structure for tracking worker activity and keeping their references \"\"\"\n    def __init__(self, id, last_active=None):\n        self.id = id\n        self.reference = None  # reference to the worker in the real world. (Currently a dict containing just the host)\n        self.last_active = last_active  # seconds since epoch\n        self.info = {}\n\n    def add_info(self, info):\n        self.info.update(info)\n\n    def __str__(self):\n        return self.id\n\n\nclass CentralPlannerScheduler(Scheduler):\n    ''' Async scheduler that can handle multiple workers etc\n\n    Can be run locally or on a server (using RemoteScheduler + server.Server).\n    '''\n\n    def __init__(self, retry_delay=900.0, remove_delay=600.0, worker_disconnect_delay=60.0,\n                 state_path='/var/lib/luigi-server/state.pickle', task_history=None):\n        '''\n        (all arguments are in seconds)\n        Keyword Arguments:\n        retry_delay -- How long after a Task fails to try it again, or -1 to never retry\n        remove_delay -- How long after a Task finishes to remove it from the scheduler\n        state_path -- Path to state file (tasks and active workers)\n        worker_disconnect_delay -- If a worker hasn't communicated for this long, remove it from active workers\n        '''\n        self._state_path = state_path\n        self._tasks = {}  # map from id to a Task object\n        self._retry_delay = retry_delay\n        self._remove_delay = remove_delay\n        self._worker_disconnect_delay = worker_disconnect_delay\n        self._active_workers = {}  # map from id to a Worker object\n        self._task_history = task_history or history.NopHistory()\n\n    def dump(self):\n        state = (self._tasks, self._active_workers)\n        try:\n            with open(self._state_path, 'w') as fobj:\n                pickle.dump(state, fobj)\n        except IOError:\n            logger.warning(\"Failed saving scheduler state\", exc_info=1)\n        else:\n            logger.info(\"Saved state in %s\", self._state_path)\n\n    # prone to lead to crashes when old state is unpickled with updated code. TODO some kind of version control?\n    def load(self):\n        if os.path.exists(self._state_path):\n            logger.info(\"Attempting to load state from %s\", self._state_path)\n            with open(self._state_path) as fobj:\n                state = pickle.load(fobj)\n            self._tasks, self._active_workers = state\n\n            # Convert from old format\n            # TODO: this is really ugly, we need something more future-proof\n            # Every time we add an attribute to the Worker class, this code needs to be updated\n            for k, v in self._active_workers.iteritems():\n                if isinstance(v, float):\n                    self._active_workers[k] = Worker(id=k, last_active=v)\n        else:\n            logger.info(\"No prior state file exists at %s. Starting with clean slate\", self._state_path)\n\n    def prune(self):\n        logger.info(\"Starting pruning of task graph\")\n        # Delete workers that haven't said anything for a while (probably killed)\n        delete_workers = []\n        for worker in self._active_workers.values():\n            if worker.last_active < time.time() - self._worker_disconnect_delay:\n                logger.info(\"Worker %s timed out (no contact for >=%ss)\", worker, self._worker_disconnect_delay)\n                delete_workers.append(worker.id)\n\n        for worker in delete_workers:\n            self._active_workers.pop(worker)\n\n        remaining_workers = set(self._active_workers.keys())\n\n        # Mark tasks with no remaining active stakeholders for deletion\n        for task_id, task in self._tasks.iteritems():\n            if not task.stakeholders.intersection(remaining_workers):\n                if task.remove is None:\n                    logger.info(\"Task %r has stakeholders %r but none remain connected -> will remove task in %s seconds\", task_id, task.stakeholders, self._remove_delay)\n                    task.remove = time.time() + self._remove_delay\n\n            if task.status == RUNNING and task.worker_running and task.worker_running not in remaining_workers:\n                # If a running worker disconnects, tag all its jobs as FAILED and subject it to the same retry logic\n                logger.info(\"Task %r is marked as running by disconnected worker %r -> marking as FAILED with retry delay of %rs\", task_id, task.worker_running, self._retry_delay)\n                task.worker_running = None\n                task.status = FAILED\n                task.retry = time.time() + self._retry_delay\n\n        # Remove tasks that have no stakeholders\n        remove_tasks = []\n        for task_id, task in self._tasks.iteritems():\n            if task.remove and time.time() > task.remove:\n                logger.info(\"Removing task %r (no connected stakeholders)\", task_id)\n                remove_tasks.append(task_id)\n\n        for task_id in remove_tasks:\n            self._tasks.pop(task_id)\n\n        # Reset FAILED tasks to PENDING if max timeout is reached, and retry delay is >= 0\n        for task in self._tasks.values():\n            if task.status == FAILED and self._retry_delay >= 0 and task.retry < time.time():\n                task.status = PENDING\n        logger.info(\"Done pruning task graph\")\n\n    def update(self, worker_id, worker_reference=None):\n        \"\"\" Keep track of whenever the worker was last active \"\"\"\n        worker = self._active_workers.setdefault(worker_id, Worker(worker_id))\n        if worker_reference:\n            worker.reference = worker_reference\n        worker.last_active = time.time()\n\n    def add_task(self, worker, task_id, status=PENDING, runnable=True, deps=None, expl=None, priority=0):\n        \"\"\"\n        * Add task identified by task_id if it doesn't exist\n        * If deps is not None, update dependency list\n        * Update status of task\n        * Add additional workers/stakeholders\n        \"\"\"\n        self.update(worker)\n\n        task = self._tasks.setdefault(task_id, Task(status=PENDING, deps=deps, priority=priority))\n\n        if task.remove is not None:\n            task.remove = None  # unmark task for removal so it isn't removed after being added\n\n        if not (task.status == RUNNING and status == PENDING):\n            # don't allow re-scheduling of task while it is running, it must either fail or succeed first\n            if status == PENDING or status != task.status:\n                # Update the DB only if there was a acctual change, to prevent noise.\n                # We also check for status == PENDING b/c that's the default value\n                # (so checking for status != task.status woule lie)\n                self._update_task_history(task_id, status)\n            task.status = status\n            if status == FAILED:\n                task.retry = time.time() + self._retry_delay\n\n        if deps is not None:\n            task.deps = set(deps)\n\n        task.stakeholders.add(worker)\n\n        if runnable:\n            task.workers.add(worker)\n\n        if expl is not None:\n            task.expl = expl\n\n    def add_worker(self, worker, info):\n        self._active_workers[worker].add_info(info)\n\n    def get_work(self, worker, host=None):\n        # TODO: remove any expired nodes\n\n        # Algo: iterate over all nodes, find the first node with no dependencies and highest priority\n\n        # TODO: remove tasks that can't be done, figure out if the worker has absolutely\n        # nothing it can wait for\n\n        # Return remaining tasks that have no FAILED descendents\n        self.update(worker, {'host': host})\n        best_t = float('inf')\n        best_priority = float('-inf')\n        best_task = None\n        locally_pending_tasks = 0\n        running_tasks = []\n\n        for task_id, task in self._tasks.iteritems():\n            if worker not in task.workers:\n                continue\n\n            if task.status == RUNNING:\n                # Return a list of currently running tasks to the client,\n                # makes it easier to troubleshoot\n                other_worker = self._active_workers[task.worker_running]\n                more_info = {'task_id': task_id, 'worker': str(other_worker)}\n                if other_worker is not None:\n                    more_info.update(other_worker.info)\n                running_tasks.append(more_info)\n\n            if task.status != PENDING:\n                continue\n\n            locally_pending_tasks += 1\n            ok = True\n            for dep in task.deps:\n                if dep not in self._tasks:\n                    ok = False\n                elif self._tasks[dep].status != DONE:\n                    ok = False\n\n            if ok:\n                if (-task.priority, task.time) < (-best_priority, best_t):\n                    best_t = task.time\n                    best_priority = task.priority\n                    best_task = task_id\n\n        if best_task:\n            t = self._tasks[best_task]\n            t.status = RUNNING\n            t.worker_running = worker\n            t.time_running = time.time()\n            self._update_task_history(best_task, RUNNING, host=host)\n\n        return {'n_pending_tasks': locally_pending_tasks,\n                'task_id': best_task,\n                'running_tasks': running_tasks}\n\n    def ping(self, worker):\n        self.update(worker)\n\n    def _upstream_status(self, task_id, upstream_status_table):\n        if task_id in upstream_status_table:\n            return upstream_status_table[task_id]\n        elif task_id in self._tasks:\n            task_stack = [task_id]\n\n            while task_stack:\n                dep_id = task_stack.pop()\n                if dep_id in self._tasks:\n                    dep = self._tasks[dep_id]\n                    if dep_id not in upstream_status_table:\n                        if dep.status == PENDING and dep.deps:\n                            task_stack = task_stack + [dep_id] + list(dep.deps)\n                            upstream_status_table[dep_id] = ''  # will be updated postorder\n                        else:\n                            dep_status = STATUS_TO_UPSTREAM_MAP.get(dep.status, '')\n                            upstream_status_table[dep_id] = dep_status\n                    elif upstream_status_table[dep_id] == '' and dep.deps:\n                        # This is the postorder update step when we set the\n                        # status based on the previously calculated child elements\n                        upstream_status = [upstream_status_table.get(id, '') for id in dep.deps]\n                        upstream_status.append('')  # to handle empty list\n                        status = max(upstream_status, key=UPSTREAM_SEVERITY_KEY)\n                        upstream_status_table[dep_id] = status\n            return upstream_status_table[dep_id]\n\n    def _serialize_task(self, task_id):\n        task = self._tasks[task_id]\n        return {\n            'deps': list(task.deps),\n            'status': task.status,\n            'workers': list(task.workers),\n            'worker_running': task.worker_running,\n            'time_running': getattr(task, \"time_running\", None),\n            'start_time': task.time,\n            'params': self._get_task_params(task_id),\n            'name': self._get_task_name(task_id)\n        }\n\n    def _get_task_params(self, task_id):\n        params = {}\n        params_part = task_id.split('(')[1].strip(')')\n        params_strings = params_part.split(\", \")\n\n        for param in params_strings:\n            if not param:\n                continue\n            split_param = param.split('=')\n            if len(split_param) != 2:\n                return {'<complex parameters>': params_part}\n            params[split_param[0]] = split_param[1]\n        return params\n\n    def _get_task_name(self, task_id):\n        return task_id.split('(')[0]\n\n    def graph(self):\n        self.prune()\n        serialized = {}\n        for task_id, task in self._tasks.iteritems():\n            serialized[task_id] = self._serialize_task(task_id)\n        return serialized\n\n    def _recurse_deps(self, task_id, serialized):\n        if task_id not in serialized:\n            task = self._tasks.get(task_id)\n            if task is None:\n                logger.warn('Missing task for id [%s]', task_id)\n                serialized[task_id] = {\n                    'deps': [],\n                    'status': UNKNOWN,\n                    'workers': [],\n                    'start_time': UNKNOWN,\n                    'params': self._get_task_params(task_id),\n                    'name': self._get_task_name(task_id)\n                }\n            else:\n                serialized[task_id] = self._serialize_task(task_id)\n                for dep in task.deps:\n                    self._recurse_deps(dep, serialized)\n\n    def dep_graph(self, task_id):\n        self.prune()\n        serialized = {}\n        if task_id in self._tasks:\n            self._recurse_deps(task_id, serialized)\n        return serialized\n\n    def task_list(self, status, upstream_status):\n        ''' query for a subset of tasks by status '''\n        self.prune()\n        result = {}\n        upstream_status_table = {}  # used to memoize upstream status\n        for task_id, task in self._tasks.iteritems():\n            if not status or task.status == status:\n                if (task.status != PENDING or not upstream_status or\n                    upstream_status == self._upstream_status(task_id, upstream_status_table)):\n                    serialized = self._serialize_task(task_id)\n                    result[task_id] = serialized\n        return result\n\n    def inverse_dependencies(self, task_id):\n        self.prune()\n        serialized = {}\n        if task_id in self._tasks:\n            self._traverse_inverse_deps(task_id, serialized)\n        return serialized\n\n    def _traverse_inverse_deps(self, task_id, serialized):\n        stack = [task_id]\n        serialized[task_id] = self._serialize_task(task_id)\n        while len(stack) > 0:\n            curr_id = stack.pop()\n            for id, task in self._tasks.iteritems():\n                if curr_id in task.deps:\n                    serialized[curr_id][\"deps\"].append(id)\n                    if id not in serialized:\n                        serialized[id] = self._serialize_task(id)\n                        serialized[id][\"deps\"] = []\n                        stack.append(id)\n\n    def task_search(self, task_str):\n        ''' query for a subset of tasks by task_id '''\n        self.prune()\n        result = collections.defaultdict(dict)\n        for task_id, task in self._tasks.iteritems():\n            if task_id.find(task_str) != -1:\n                serialized = self._serialize_task(task_id)\n                result[task.status][task_id] = serialized\n        return result\n\n    def fetch_error(self, task_id):\n        if self._tasks[task_id].expl is not None:\n            return {\"taskId\": task_id, \"error\": self._tasks[task_id].expl}\n        else:\n            return {\"taskId\": task_id, \"error\": \"\"}\n\n    def _update_task_history(self, task_id, status, host=None):\n        try:\n            if status == DONE or status == FAILED:\n                successful = (status == DONE)\n                self._task_history.task_finished(task_id, successful)\n            elif status == PENDING:\n                self._task_history.task_scheduled(task_id)\n            elif status == RUNNING:\n                self._task_history.task_started(task_id, host)\n        except:\n            logger.warning(\"Error saving Task history\", exc_info=1)\n\n    @property\n    def task_history(self):\n        # Used by server.py to expose the calls\n        return self._task_history\n",
          "file_patch": "@@ -69,9 +69,13 @@ class Worker(object):\n         self.id = id\n         self.reference = None  # reference to the worker in the real world. (Currently a dict containing just the host)\n         self.last_active = last_active  # seconds since epoch\n+        self.info = {}\n+\n+    def add_info(self, info):\n+        self.info.update(info)\n \n     def __str__(self):\n-        return \"%s on %s, last active %s\" % (self.id, self.reference, datetime.datetime.utcfromtimestamp(self.last_active).isoformat())\n+        return self.id\n \n \n class CentralPlannerScheduler(Scheduler):\n@@ -212,6 +216,9 @@ class CentralPlannerScheduler(Scheduler):\n         if expl is not None:\n             task.expl = expl\n \n+    def add_worker(self, worker, info):\n+        self._active_workers[worker].add_info(info)\n+\n     def get_work(self, worker, host=None):\n         # TODO: remove any expired nodes\n \n@@ -233,7 +240,13 @@ class CentralPlannerScheduler(Scheduler):\n                 continue\n \n             if task.status == RUNNING:\n-                running_tasks.append({'task_id': task_id, 'worker': str(self._active_workers.get(task.worker_running))})\n+                # Return a list of currently running tasks to the client,\n+                # makes it easier to troubleshoot\n+                other_worker = self._active_workers[task.worker_running]\n+                more_info = {'task_id': task_id, 'worker': str(other_worker)}\n+                if other_worker is not None:\n+                    more_info.update(other_worker.info)\n+                running_tasks.append(more_info)\n \n             if task.status != PENDING:\n                 continue\n",
          "files_name_in_blame_commit": [
            "rpc.py",
            "worker.py",
            "central_planner_test.py",
            "scheduler.py"
          ]
        }
      },
      "c75085b3c812c8054ef76d39676ab473e45b9600": {
        "commit": {
          "commit_id": "c75085b3c812c8054ef76d39676ab473e45b9600",
          "commit_message": "include worker id and host in log output\n\nMakes log output more usable for troubleshooting when somewhere a worker process is hung. The \"is currently run by worker-RANDOM_NUMBER\" error message will now include the host, and logs on that host will let associate it with the pid.",
          "commit_author": "Uldis Barbans",
          "commit_date": "2014-05-08 16:30:46",
          "commit_parent": "7ae59768d1d7cdfce8cf1b0a5717fa69d89d4ce1"
        },
        "function": {
          "function_name": "get_work",
          "function_code_before": "def get_work(self, worker, host=None):\n    self.update(worker)\n    best_t = float('inf')\n    best_task = None\n    locally_pending_tasks = 0\n    running_tasks = []\n    for (task_id, task) in self._tasks.iteritems():\n        if worker not in task.workers:\n            continue\n        if task.status == RUNNING:\n            running_tasks.append({'task_id': task_id, 'worker': task.worker_running})\n        if task.status != PENDING:\n            continue\n        locally_pending_tasks += 1\n        ok = True\n        for dep in task.deps:\n            if dep not in self._tasks:\n                ok = False\n            elif self._tasks[dep].status != DONE:\n                ok = False\n        if ok:\n            if task.time < best_t:\n                best_t = task.time\n                best_task = task_id\n    if best_task:\n        t = self._tasks[best_task]\n        t.status = RUNNING\n        t.worker_running = worker\n        self._update_task_history(best_task, RUNNING, host=host)\n    return {'n_pending_tasks': locally_pending_tasks, 'task_id': best_task, 'running_tasks': running_tasks}",
          "function_code_after": "def get_work(self, worker, host=None):\n    self.update(worker, {'host': host})\n    best_t = float('inf')\n    best_task = None\n    locally_pending_tasks = 0\n    running_tasks = []\n    for (task_id, task) in self._tasks.iteritems():\n        if worker not in task.workers:\n            continue\n        if task.status == RUNNING:\n            running_tasks.append({'task_id': task_id, 'worker': str(self._active_workers.get(task.worker_running))})\n        if task.status != PENDING:\n            continue\n        locally_pending_tasks += 1\n        ok = True\n        for dep in task.deps:\n            if dep not in self._tasks:\n                ok = False\n            elif self._tasks[dep].status != DONE:\n                ok = False\n        if ok:\n            if task.time < best_t:\n                best_t = task.time\n                best_task = task_id\n    if best_task:\n        t = self._tasks[best_task]\n        t.status = RUNNING\n        t.worker_running = worker\n        self._update_task_history(best_task, RUNNING, host=host)\n    return {'n_pending_tasks': locally_pending_tasks, 'task_id': best_task, 'running_tasks': running_tasks}",
          "function_before_start_line": 187,
          "function_before_end_line": 233,
          "function_after_start_line": 201,
          "function_after_end_line": 247,
          "function_before_token_count": 190,
          "function_after_token_count": 206,
          "functions_name_modified_file": [
            "_update_task_history",
            "prune",
            "_upstream_status",
            "ping",
            "_traverse_inverse_deps",
            "inverse_dependencies",
            "load",
            "_recurse_deps",
            "add_task",
            "task_list",
            "_serialize_task",
            "task_history",
            "dep_graph",
            "__str__",
            "graph",
            "dump",
            "__repr__",
            "__init__",
            "update",
            "_get_task_params",
            "get_work",
            "_get_task_name",
            "fetch_error"
          ],
          "functions_name_all_files": [
            "_update_task_history",
            "test_remove_dep",
            "prune",
            "tearDown",
            "_upstream_status",
            "test_two_workers",
            "test_parameter_split",
            "_email_complete_error",
            "ping",
            "_fork_task",
            "test_retry",
            "test_dep",
            "_traverse_inverse_deps",
            "inverse_dependencies",
            "load",
            "_log_unexpected_error",
            "_email_unexpected_error",
            "_log_remote_tasks",
            "_add_task_and_deps",
            "_recurse_deps",
            "test_timeout",
            "test_two_worker_info",
            "add_task",
            "_validate_dependency",
            "task_list",
            "add",
            "run",
            "_serialize_task",
            "test_disallowed_state_changes",
            "task_history",
            "dep_graph",
            "_run_task",
            "_get_work",
            "test_disconnect_running",
            "_check_complete",
            "__str__",
            "graph",
            "_sleeper",
            "dump",
            "__repr__",
            "setUp",
            "__init__",
            "update",
            "test_failed_dep",
            "_validate_task",
            "_check_complete_value",
            "test_broken_dep",
            "_get_task_params",
            "stop",
            "_log_complete_error",
            "_add",
            "get_work",
            "_get_task_name",
            "_add_external",
            "_reap_children",
            "setTime",
            "fetch_error"
          ],
          "functions_name_co_evolved_modified_file": [
            "__init__",
            "update",
            "prune",
            "__str__"
          ],
          "functions_name_co_evolved_all_files": [
            "__init__",
            "update",
            "prune",
            "_run_task",
            "test_two_worker_info",
            "__str__",
            "run"
          ]
        },
        "file": {
          "file_name": "scheduler.py",
          "file_nloc": 287,
          "file_complexity": 99,
          "file_token_count": 2095,
          "file_before": "# Copyright (c) 2012 Spotify AB\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"); you may not\n# use this file except in compliance with the License. You may obtain a copy of\n# the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n# License for the specific language governing permissions and limitations under\n# the License.\n\nimport os\nimport logging\nimport time\nimport cPickle as pickle\nimport task_history as history\nlogger = logging.getLogger(\"luigi.server\")\n\nfrom task_status import PENDING, FAILED, DONE, RUNNING, UNKNOWN\n\n\nclass Scheduler(object):\n    ''' Abstract base class\n\n    Note that the methods all take string arguments, not Task objects...\n    '''\n    add_task = NotImplemented\n    get_work = NotImplemented\n    ping = NotImplemented\n\nUPSTREAM_RUNNING = 'UPSTREAM_RUNNING'\nUPSTREAM_MISSING_INPUT = 'UPSTREAM_MISSING_INPUT'\nUPSTREAM_FAILED = 'UPSTREAM_FAILED'\n\nUPSTREAM_SEVERITY_ORDER = ('', UPSTREAM_RUNNING, UPSTREAM_MISSING_INPUT, UPSTREAM_FAILED)\nUPSTREAM_SEVERITY_KEY = lambda st: UPSTREAM_SEVERITY_ORDER.index(st)\nSTATUS_TO_UPSTREAM_MAP = {FAILED: UPSTREAM_FAILED, RUNNING: UPSTREAM_RUNNING, PENDING: UPSTREAM_MISSING_INPUT}\n\n\nclass Task(object):\n    def __init__(self, status, deps):\n        self.stakeholders = set()  # workers that are somehow related to this task (i.e. don't prune while any of these workers are still active)\n        self.workers = set()  # workers that can perform task - task is 'BROKEN' if none of these workers are active\n        if deps is None:\n            self.deps = set()\n        else:\n            self.deps = set(deps)\n        self.status = status  # PENDING, RUNNING, FAILED or DONE\n        self.time = time.time()  # Timestamp when task was first added\n        self.retry = None\n        self.remove = None\n        self.worker_running = None  # the worker that is currently running the task or None\n        self.expl = None\n\n    def __repr__(self):\n        return \"Task(%r)\" % vars(self)\n\n\nclass CentralPlannerScheduler(Scheduler):\n    ''' Async scheduler that can handle multiple workers etc\n\n    Can be run locally or on a server (using RemoteScheduler + server.Server).\n    '''\n\n    def __init__(self, retry_delay=900.0, remove_delay=600.0, worker_disconnect_delay=60.0,\n                 state_path='/var/lib/luigi-server/state.pickle', task_history=None):\n        '''\n        (all arguments are in seconds)\n        Keyword Arguments:\n        retry_delay -- How long after a Task fails to try it again, or -1 to never retry\n        remove_delay -- How long after a Task finishes to remove it from the scheduler\n        state_path -- Path to state file (tasks and active workers)\n        worker_disconnect_delay -- If a worker hasn't communicated for this long, remove it from active workers\n        '''\n        self._state_path = state_path\n        self._tasks = {}\n        self._retry_delay = retry_delay\n        self._remove_delay = remove_delay\n        self._worker_disconnect_delay = worker_disconnect_delay\n        self._active_workers = {}  # map from id to timestamp (last updated)\n        self._task_history = task_history or history.NopHistory()\n        # TODO: have a Worker object instead, add more data to it\n\n    def dump(self):\n        state = (self._tasks, self._active_workers)\n        try:\n            with open(self._state_path, 'w') as fobj:\n                pickle.dump(state, fobj)\n        except IOError:\n            logger.warning(\"Failed saving scheduler state\", exc_info=1)\n        else:\n            logger.info(\"Saved state in %s\", self._state_path)\n\n    def load(self):\n        if os.path.exists(self._state_path):\n            logger.info(\"Attempting to load state from %s\", self._state_path)\n            with open(self._state_path) as fobj:\n                state = pickle.load(fobj)\n            self._tasks, self._active_workers = state\n        else:\n            logger.info(\"No prior state file exists at %s. Starting with clean slate\", self._state_path)\n\n    def prune(self):\n        logger.info(\"Starting pruning of task graph\")\n        # Delete workers that haven't said anything for a while (probably killed)\n        delete_workers = []\n        for worker in self._active_workers:\n            if self._active_workers[worker] < time.time() - self._worker_disconnect_delay:\n                logger.info(\"worker %r updated at %s timed out (no contact for >=%ss)\", worker, self._active_workers[worker], self._worker_disconnect_delay)\n                delete_workers.append(worker)\n\n        for worker in delete_workers:\n            self._active_workers.pop(worker)\n\n        remaining_workers = set(self._active_workers.keys())\n\n        # Mark tasks with no remaining active stakeholders for deletion\n        for task_id, task in self._tasks.iteritems():\n            if not task.stakeholders.intersection(remaining_workers):\n                if task.remove is None:\n                    logger.info(\"Task %r has stakeholders %r but none remain connected -> will remove task in %s seconds\", task_id, task.stakeholders, self._remove_delay)\n                    task.remove = time.time() + self._remove_delay\n\n            if task.status == RUNNING and task.worker_running and task.worker_running not in remaining_workers:\n                # If a running worker disconnects, tag all its jobs as FAILED and subject it to the same retry logic\n                logger.info(\"Task %r is marked as running by disconnected worker %r -> marking as FAILED with retry delay of %rs\", task_id, task.worker_running, self._retry_delay)\n                task.worker_running = None\n                task.status = FAILED\n                task.retry = time.time() + self._retry_delay\n\n        # Remove tasks that have no stakeholders\n        remove_tasks = []\n        for task_id, task in self._tasks.iteritems():\n            if task.remove and time.time() > task.remove:\n                logger.info(\"Removing task %r (no connected stakeholders)\", task_id)\n                remove_tasks.append(task_id)\n\n        for task_id in remove_tasks:\n            self._tasks.pop(task_id)\n\n        # Reset FAILED tasks to PENDING if max timeout is reached, and retry delay is >= 0\n        for task in self._tasks.values():\n            if task.status == FAILED and self._retry_delay >= 0 and task.retry < time.time():\n                task.status = PENDING\n        logger.info(\"Done pruning task graph\")\n\n    def update(self, worker):\n        # update timestamp so that we keep track\n        # of whenever the worker was last active\n        self._active_workers[worker] = time.time()\n\n    def add_task(self, worker, task_id, status=PENDING, runnable=True, deps=None, expl=None):\n        \"\"\"\n        * Add task identified by task_id if it doesn't exist\n        * If deps is not None, update dependency list\n        * Update status of task\n        * Add additional workers/stakeholders\n        \"\"\"\n        self.update(worker)\n\n        task = self._tasks.setdefault(task_id, Task(status=PENDING, deps=deps))\n\n        if task.remove is not None:\n            task.remove = None  # unmark task for removal so it isn't removed after being added\n\n        if not (task.status == RUNNING and status == PENDING):\n            # don't allow re-scheduling of task while it is running, it must either fail or succeed first\n            task.status = status\n            if status == FAILED:\n                task.retry = time.time() + self._retry_delay\n\n        if deps is not None:\n            task.deps = set(deps)\n\n        task.stakeholders.add(worker)\n\n        if runnable:\n            task.workers.add(worker)\n\n        if expl is not None:\n            task.expl = expl\n        self._update_task_history(task_id, status)\n\n    def get_work(self, worker, host=None):\n        # TODO: remove any expired nodes\n\n        # Algo: iterate over all nodes, find first node with no dependencies\n\n        # TODO: remove tasks that can't be done, figure out if the worker has absolutely\n        # nothing it can wait for\n\n        # Return remaining tasks that have no FAILED descendents\n        self.update(worker)\n        best_t = float('inf')\n        best_task = None\n        locally_pending_tasks = 0\n        running_tasks = []\n\n        for task_id, task in self._tasks.iteritems():\n            if worker not in task.workers:\n                continue\n\n            if task.status == RUNNING:\n                running_tasks.append({'task_id': task_id, 'worker': task.worker_running})\n\n            if task.status != PENDING:\n                continue\n\n            locally_pending_tasks += 1\n            ok = True\n            for dep in task.deps:\n                if dep not in self._tasks:\n                    ok = False\n                elif self._tasks[dep].status != DONE:\n                    ok = False\n\n            if ok:\n                if task.time < best_t:\n                    best_t = task.time\n                    best_task = task_id\n\n        if best_task:\n            t = self._tasks[best_task]\n            t.status = RUNNING\n            t.worker_running = worker\n            self._update_task_history(best_task, RUNNING, host=host)\n\n        return {'n_pending_tasks': locally_pending_tasks,\n                'task_id': best_task,\n                'running_tasks': running_tasks}\n\n    def ping(self, worker):\n        self.update(worker)\n\n    def _upstream_status(self, task_id, upstream_status_table):\n        if task_id in upstream_status_table:\n            return upstream_status_table[task_id]\n        elif task_id in self._tasks:\n            task_stack = [task_id]\n\n            while task_stack:\n                dep_id = task_stack.pop()\n                if dep_id in self._tasks:\n                    dep = self._tasks[dep_id]\n                    if dep_id not in upstream_status_table:\n                        if dep.status == PENDING and dep.deps:\n                            task_stack = task_stack + [dep_id] + list(dep.deps)\n                            upstream_status_table[dep_id] = ''  # will be updated postorder\n                        else:\n                            dep_status = STATUS_TO_UPSTREAM_MAP.get(dep.status, '')\n                            upstream_status_table[dep_id] = dep_status\n                    elif upstream_status_table[dep_id] == '' and dep.deps:\n                        # This is the postorder update step when we set the\n                        # status based on the previously calculated child elements\n                        upstream_status = [upstream_status_table.get(id, '') for id in dep.deps]\n                        upstream_status.append('')  # to handle empty list\n                        status = max(upstream_status, key=UPSTREAM_SEVERITY_KEY)\n                        upstream_status_table[dep_id] = status\n            return upstream_status_table[dep_id]\n\n    def _serialize_task(self, task_id):\n        task = self._tasks[task_id]\n        return {\n            'deps': list(task.deps),\n            'status': task.status,\n            'workers': list(task.workers),\n            'start_time': task.time,\n            'params': self._get_task_params(task_id),\n            'name': self._get_task_name(task_id)\n        }\n\n    def _get_task_params(self, task_id):\n        params = {}\n        params_part = task_id.split('(')[1].strip(')')\n        params_strings = params_part.split(\", \")\n\n        for param in params_strings:\n            if not param:\n                continue\n            split_param = param.split('=')\n            if len(split_param) != 2:\n                return {'<complex parameters>': params_part}\n            params[split_param[0]] = split_param[1]\n        return params\n\n    def _get_task_name(self, task_id):\n        return task_id.split('(')[0]\n\n    def graph(self):\n        self.prune()\n        serialized = {}\n        for task_id, task in self._tasks.iteritems():\n            serialized[task_id] = self._serialize_task(task_id)\n        return serialized\n\n    def _recurse_deps(self, task_id, serialized):\n        if task_id not in serialized:\n            task = self._tasks.get(task_id)\n            if task is None:\n                logger.warn('Missing task for id [%s]', task_id)\n                serialized[task_id] = {\n                    'deps': [],\n                    'status': UNKNOWN,\n                    'workers': [],\n                    'start_time': UNKNOWN,\n                    'params': self._get_task_params(task_id),\n                    'name': self._get_task_name(task_id)\n                }\n            else:\n                serialized[task_id] = self._serialize_task(task_id)\n                for dep in task.deps:\n                    self._recurse_deps(dep, serialized)\n\n    def dep_graph(self, task_id):\n        self.prune()\n        serialized = {}\n        if task_id in self._tasks:\n            self._recurse_deps(task_id, serialized)\n        return serialized\n\n    def task_list(self, status, upstream_status):\n        ''' query for a subset of tasks by status '''\n        self.prune()\n        result = {}\n        upstream_status_table = {}  # used to memoize upstream status\n        for task_id, task in self._tasks.iteritems():\n            if not status or task.status == status:\n                if (task.status != PENDING or not upstream_status or\n                    upstream_status == self._upstream_status(task_id, upstream_status_table)):\n                    serialized = self._serialize_task(task_id)\n                    result[task_id] = serialized\n        return result\n\n    def inverse_dependencies(self, task_id):\n        self.prune()\n        serialized = {}\n        if task_id in self._tasks:\n            self._traverse_inverse_deps(task_id, serialized)\n        return serialized\n\n    def _traverse_inverse_deps(self, task_id, serialized):\n        stack = [task_id]\n        serialized[task_id] = self._serialize_task(task_id)\n        while len(stack) > 0:\n            curr_id = stack.pop()\n            for id, task in self._tasks.iteritems():\n                if curr_id in task.deps:\n                    serialized[curr_id][\"deps\"].append(id)\n                    if id not in serialized:\n                        serialized[id] = self._serialize_task(id)\n                        serialized[id][\"deps\"] = []\n                        stack.append(id)\n\n    def fetch_error(self, task_id):\n        if self._tasks[task_id].expl is not None:\n            return {\"taskId\": task_id, \"error\": self._tasks[task_id].expl}\n        else:\n            return {\"taskId\": task_id, \"error\": \"\"}\n\n    def _update_task_history(self, task_id, status, host=None):\n        try:\n            if status == DONE or status == FAILED:\n                successful = (status == DONE)\n                self._task_history.task_finished(task_id, successful)\n            elif status == PENDING:\n                self._task_history.task_scheduled(task_id)\n            elif status == RUNNING:\n                self._task_history.task_started(task_id, host)\n        except:\n            logger.warning(\"Error saving Task history\", exc_info=1)\n\n    @property\n    def task_history(self):\n        # Used by server.py to expose the calls\n        return self._task_history\n",
          "file_after": "# Copyright (c) 2012 Spotify AB\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"); you may not\n# use this file except in compliance with the License. You may obtain a copy of\n# the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n# License for the specific language governing permissions and limitations under\n# the License.\n\nimport datetime\nimport os\nimport logging\nimport time\nimport cPickle as pickle\nimport task_history as history\nlogger = logging.getLogger(\"luigi.server\")\n\nfrom task_status import PENDING, FAILED, DONE, RUNNING, UNKNOWN\n\n\nclass Scheduler(object):\n    ''' Abstract base class\n\n    Note that the methods all take string arguments, not Task objects...\n    '''\n    add_task = NotImplemented\n    get_work = NotImplemented\n    ping = NotImplemented\n\nUPSTREAM_RUNNING = 'UPSTREAM_RUNNING'\nUPSTREAM_MISSING_INPUT = 'UPSTREAM_MISSING_INPUT'\nUPSTREAM_FAILED = 'UPSTREAM_FAILED'\n\nUPSTREAM_SEVERITY_ORDER = ('', UPSTREAM_RUNNING, UPSTREAM_MISSING_INPUT, UPSTREAM_FAILED)\nUPSTREAM_SEVERITY_KEY = lambda st: UPSTREAM_SEVERITY_ORDER.index(st)\nSTATUS_TO_UPSTREAM_MAP = {FAILED: UPSTREAM_FAILED, RUNNING: UPSTREAM_RUNNING, PENDING: UPSTREAM_MISSING_INPUT}\n\n\nclass Task(object):\n    def __init__(self, status, deps):\n        self.stakeholders = set()  # workers ids that are somehow related to this task (i.e. don't prune while any of these workers are still active)\n        self.workers = set()  # workers ids that can perform task - task is 'BROKEN' if none of these workers are active\n        if deps is None:\n            self.deps = set()\n        else:\n            self.deps = set(deps)\n        self.status = status  # PENDING, RUNNING, FAILED or DONE\n        self.time = time.time()  # Timestamp when task was first added\n        self.retry = None\n        self.remove = None\n        self.worker_running = None  # the worker id that is currently running the task or None\n        self.expl = None\n\n    def __repr__(self):\n        return \"Task(%r)\" % vars(self)\n\n\nclass Worker(object):\n    \"\"\" Structure for tracking worker activity and keeping their references \"\"\"\n    def __init__(self, id):\n        self.id = id\n        self.reference = None  # reference to the worker in the real world. (Currently a dict containing just the host)\n        self.last_active = None  # seconds since epoch\n\n    def __str__(self):\n        return \"%s on %s, last active %s\" % (self.id, self.reference, datetime.datetime.utcfromtimestamp(self.last_active).isoformat())\n\n\nclass CentralPlannerScheduler(Scheduler):\n    ''' Async scheduler that can handle multiple workers etc\n\n    Can be run locally or on a server (using RemoteScheduler + server.Server).\n    '''\n\n    def __init__(self, retry_delay=900.0, remove_delay=600.0, worker_disconnect_delay=60.0,\n                 state_path='/var/lib/luigi-server/state.pickle', task_history=None):\n        '''\n        (all arguments are in seconds)\n        Keyword Arguments:\n        retry_delay -- How long after a Task fails to try it again, or -1 to never retry\n        remove_delay -- How long after a Task finishes to remove it from the scheduler\n        state_path -- Path to state file (tasks and active workers)\n        worker_disconnect_delay -- If a worker hasn't communicated for this long, remove it from active workers\n        '''\n        self._state_path = state_path\n        self._tasks = {}  # map from id to a Task object\n        self._retry_delay = retry_delay\n        self._remove_delay = remove_delay\n        self._worker_disconnect_delay = worker_disconnect_delay\n        self._active_workers = {}  # map from id to a Worker object\n        self._task_history = task_history or history.NopHistory()\n\n    def dump(self):\n        state = (self._tasks, self._active_workers)\n        try:\n            with open(self._state_path, 'w') as fobj:\n                pickle.dump(state, fobj)\n        except IOError:\n            logger.warning(\"Failed saving scheduler state\", exc_info=1)\n        else:\n            logger.info(\"Saved state in %s\", self._state_path)\n\n    # prone to lead to crashes when old state is unpickled with updated code. TODO some kind of version control?\n    def load(self):\n        if os.path.exists(self._state_path):\n            logger.info(\"Attempting to load state from %s\", self._state_path)\n            with open(self._state_path) as fobj:\n                state = pickle.load(fobj)\n            self._tasks, self._active_workers = state\n        else:\n            logger.info(\"No prior state file exists at %s. Starting with clean slate\", self._state_path)\n\n    def prune(self):\n        logger.info(\"Starting pruning of task graph\")\n        # Delete workers that haven't said anything for a while (probably killed)\n        delete_workers = []\n        for worker in self._active_workers.values():\n            if worker.last_active < time.time() - self._worker_disconnect_delay:\n                logger.info(\"Worker %s timed out (no contact for >=%ss)\", worker, self._worker_disconnect_delay)\n                delete_workers.append(worker.id)\n\n        for worker in delete_workers:\n            self._active_workers.pop(worker)\n\n        remaining_workers = set(self._active_workers.keys())\n\n        # Mark tasks with no remaining active stakeholders for deletion\n        for task_id, task in self._tasks.iteritems():\n            if not task.stakeholders.intersection(remaining_workers):\n                if task.remove is None:\n                    logger.info(\"Task %r has stakeholders %r but none remain connected -> will remove task in %s seconds\", task_id, task.stakeholders, self._remove_delay)\n                    task.remove = time.time() + self._remove_delay\n\n            if task.status == RUNNING and task.worker_running and task.worker_running not in remaining_workers:\n                # If a running worker disconnects, tag all its jobs as FAILED and subject it to the same retry logic\n                logger.info(\"Task %r is marked as running by disconnected worker %r -> marking as FAILED with retry delay of %rs\", task_id, task.worker_running, self._retry_delay)\n                task.worker_running = None\n                task.status = FAILED\n                task.retry = time.time() + self._retry_delay\n\n        # Remove tasks that have no stakeholders\n        remove_tasks = []\n        for task_id, task in self._tasks.iteritems():\n            if task.remove and time.time() > task.remove:\n                logger.info(\"Removing task %r (no connected stakeholders)\", task_id)\n                remove_tasks.append(task_id)\n\n        for task_id in remove_tasks:\n            self._tasks.pop(task_id)\n\n        # Reset FAILED tasks to PENDING if max timeout is reached, and retry delay is >= 0\n        for task in self._tasks.values():\n            if task.status == FAILED and self._retry_delay >= 0 and task.retry < time.time():\n                task.status = PENDING\n        logger.info(\"Done pruning task graph\")\n\n    def update(self, worker_id, worker_reference=None):\n        \"\"\" Keep track of whenever the worker was last active \"\"\"\n        worker = self._active_workers.setdefault(worker_id, Worker(worker_id))\n        if worker_reference:\n            worker.reference = worker_reference\n        worker.last_active = time.time()\n\n    def add_task(self, worker, task_id, status=PENDING, runnable=True, deps=None, expl=None):\n        \"\"\"\n        * Add task identified by task_id if it doesn't exist\n        * If deps is not None, update dependency list\n        * Update status of task\n        * Add additional workers/stakeholders\n        \"\"\"\n        self.update(worker)\n\n        task = self._tasks.setdefault(task_id, Task(status=PENDING, deps=deps))\n\n        if task.remove is not None:\n            task.remove = None  # unmark task for removal so it isn't removed after being added\n\n        if not (task.status == RUNNING and status == PENDING):\n            # don't allow re-scheduling of task while it is running, it must either fail or succeed first\n            task.status = status\n            if status == FAILED:\n                task.retry = time.time() + self._retry_delay\n\n        if deps is not None:\n            task.deps = set(deps)\n\n        task.stakeholders.add(worker)\n\n        if runnable:\n            task.workers.add(worker)\n\n        if expl is not None:\n            task.expl = expl\n        self._update_task_history(task_id, status)\n\n    def get_work(self, worker, host=None):\n        # TODO: remove any expired nodes\n\n        # Algo: iterate over all nodes, find first node with no dependencies\n\n        # TODO: remove tasks that can't be done, figure out if the worker has absolutely\n        # nothing it can wait for\n\n        # Return remaining tasks that have no FAILED descendents\n        self.update(worker, {'host': host})\n        best_t = float('inf')\n        best_task = None\n        locally_pending_tasks = 0\n        running_tasks = []\n\n        for task_id, task in self._tasks.iteritems():\n            if worker not in task.workers:\n                continue\n\n            if task.status == RUNNING:\n                running_tasks.append({'task_id': task_id, 'worker': str(self._active_workers.get(task.worker_running))})\n\n            if task.status != PENDING:\n                continue\n\n            locally_pending_tasks += 1\n            ok = True\n            for dep in task.deps:\n                if dep not in self._tasks:\n                    ok = False\n                elif self._tasks[dep].status != DONE:\n                    ok = False\n\n            if ok:\n                if task.time < best_t:\n                    best_t = task.time\n                    best_task = task_id\n\n        if best_task:\n            t = self._tasks[best_task]\n            t.status = RUNNING\n            t.worker_running = worker\n            self._update_task_history(best_task, RUNNING, host=host)\n\n        return {'n_pending_tasks': locally_pending_tasks,\n                'task_id': best_task,\n                'running_tasks': running_tasks}\n\n    def ping(self, worker):\n        self.update(worker)\n\n    def _upstream_status(self, task_id, upstream_status_table):\n        if task_id in upstream_status_table:\n            return upstream_status_table[task_id]\n        elif task_id in self._tasks:\n            task_stack = [task_id]\n\n            while task_stack:\n                dep_id = task_stack.pop()\n                if dep_id in self._tasks:\n                    dep = self._tasks[dep_id]\n                    if dep_id not in upstream_status_table:\n                        if dep.status == PENDING and dep.deps:\n                            task_stack = task_stack + [dep_id] + list(dep.deps)\n                            upstream_status_table[dep_id] = ''  # will be updated postorder\n                        else:\n                            dep_status = STATUS_TO_UPSTREAM_MAP.get(dep.status, '')\n                            upstream_status_table[dep_id] = dep_status\n                    elif upstream_status_table[dep_id] == '' and dep.deps:\n                        # This is the postorder update step when we set the\n                        # status based on the previously calculated child elements\n                        upstream_status = [upstream_status_table.get(id, '') for id in dep.deps]\n                        upstream_status.append('')  # to handle empty list\n                        status = max(upstream_status, key=UPSTREAM_SEVERITY_KEY)\n                        upstream_status_table[dep_id] = status\n            return upstream_status_table[dep_id]\n\n    def _serialize_task(self, task_id):\n        task = self._tasks[task_id]\n        return {\n            'deps': list(task.deps),\n            'status': task.status,\n            'workers': list(task.workers),\n            'start_time': task.time,\n            'params': self._get_task_params(task_id),\n            'name': self._get_task_name(task_id)\n        }\n\n    def _get_task_params(self, task_id):\n        params = {}\n        params_part = task_id.split('(')[1].strip(')')\n        params_strings = params_part.split(\", \")\n\n        for param in params_strings:\n            if not param:\n                continue\n            split_param = param.split('=')\n            if len(split_param) != 2:\n                return {'<complex parameters>': params_part}\n            params[split_param[0]] = split_param[1]\n        return params\n\n    def _get_task_name(self, task_id):\n        return task_id.split('(')[0]\n\n    def graph(self):\n        self.prune()\n        serialized = {}\n        for task_id, task in self._tasks.iteritems():\n            serialized[task_id] = self._serialize_task(task_id)\n        return serialized\n\n    def _recurse_deps(self, task_id, serialized):\n        if task_id not in serialized:\n            task = self._tasks.get(task_id)\n            if task is None:\n                logger.warn('Missing task for id [%s]', task_id)\n                serialized[task_id] = {\n                    'deps': [],\n                    'status': UNKNOWN,\n                    'workers': [],\n                    'start_time': UNKNOWN,\n                    'params': self._get_task_params(task_id),\n                    'name': self._get_task_name(task_id)\n                }\n            else:\n                serialized[task_id] = self._serialize_task(task_id)\n                for dep in task.deps:\n                    self._recurse_deps(dep, serialized)\n\n    def dep_graph(self, task_id):\n        self.prune()\n        serialized = {}\n        if task_id in self._tasks:\n            self._recurse_deps(task_id, serialized)\n        return serialized\n\n    def task_list(self, status, upstream_status):\n        ''' query for a subset of tasks by status '''\n        self.prune()\n        result = {}\n        upstream_status_table = {}  # used to memoize upstream status\n        for task_id, task in self._tasks.iteritems():\n            if not status or task.status == status:\n                if (task.status != PENDING or not upstream_status or\n                    upstream_status == self._upstream_status(task_id, upstream_status_table)):\n                    serialized = self._serialize_task(task_id)\n                    result[task_id] = serialized\n        return result\n\n    def inverse_dependencies(self, task_id):\n        self.prune()\n        serialized = {}\n        if task_id in self._tasks:\n            self._traverse_inverse_deps(task_id, serialized)\n        return serialized\n\n    def _traverse_inverse_deps(self, task_id, serialized):\n        stack = [task_id]\n        serialized[task_id] = self._serialize_task(task_id)\n        while len(stack) > 0:\n            curr_id = stack.pop()\n            for id, task in self._tasks.iteritems():\n                if curr_id in task.deps:\n                    serialized[curr_id][\"deps\"].append(id)\n                    if id not in serialized:\n                        serialized[id] = self._serialize_task(id)\n                        serialized[id][\"deps\"] = []\n                        stack.append(id)\n\n    def fetch_error(self, task_id):\n        if self._tasks[task_id].expl is not None:\n            return {\"taskId\": task_id, \"error\": self._tasks[task_id].expl}\n        else:\n            return {\"taskId\": task_id, \"error\": \"\"}\n\n    def _update_task_history(self, task_id, status, host=None):\n        try:\n            if status == DONE or status == FAILED:\n                successful = (status == DONE)\n                self._task_history.task_finished(task_id, successful)\n            elif status == PENDING:\n                self._task_history.task_scheduled(task_id)\n            elif status == RUNNING:\n                self._task_history.task_started(task_id, host)\n        except:\n            logger.warning(\"Error saving Task history\", exc_info=1)\n\n    @property\n    def task_history(self):\n        # Used by server.py to expose the calls\n        return self._task_history\n",
          "file_patch": "@@ -12,6 +12,7 @@\n # License for the specific language governing permissions and limitations under\n # the License.\n \n+import datetime\n import os\n import logging\n import time\n@@ -42,8 +43,8 @@ STATUS_TO_UPSTREAM_MAP = {FAILED: UPSTREAM_FAILED, RUNNING: UPSTREAM_RUNNING, PE\n \n class Task(object):\n     def __init__(self, status, deps):\n-        self.stakeholders = set()  # workers that are somehow related to this task (i.e. don't prune while any of these workers are still active)\n-        self.workers = set()  # workers that can perform task - task is 'BROKEN' if none of these workers are active\n+        self.stakeholders = set()  # workers ids that are somehow related to this task (i.e. don't prune while any of these workers are still active)\n+        self.workers = set()  # workers ids that can perform task - task is 'BROKEN' if none of these workers are active\n         if deps is None:\n             self.deps = set()\n         else:\n@@ -52,13 +53,24 @@ class Task(object):\n         self.time = time.time()  # Timestamp when task was first added\n         self.retry = None\n         self.remove = None\n-        self.worker_running = None  # the worker that is currently running the task or None\n+        self.worker_running = None  # the worker id that is currently running the task or None\n         self.expl = None\n \n     def __repr__(self):\n         return \"Task(%r)\" % vars(self)\n \n \n+class Worker(object):\n+    \"\"\" Structure for tracking worker activity and keeping their references \"\"\"\n+    def __init__(self, id):\n+        self.id = id\n+        self.reference = None  # reference to the worker in the real world. (Currently a dict containing just the host)\n+        self.last_active = None  # seconds since epoch\n+\n+    def __str__(self):\n+        return \"%s on %s, last active %s\" % (self.id, self.reference, datetime.datetime.utcfromtimestamp(self.last_active).isoformat())\n+\n+\n class CentralPlannerScheduler(Scheduler):\n     ''' Async scheduler that can handle multiple workers etc\n \n@@ -76,13 +88,12 @@ class CentralPlannerScheduler(Scheduler):\n         worker_disconnect_delay -- If a worker hasn't communicated for this long, remove it from active workers\n         '''\n         self._state_path = state_path\n-        self._tasks = {}\n+        self._tasks = {}  # map from id to a Task object\n         self._retry_delay = retry_delay\n         self._remove_delay = remove_delay\n         self._worker_disconnect_delay = worker_disconnect_delay\n-        self._active_workers = {}  # map from id to timestamp (last updated)\n+        self._active_workers = {}  # map from id to a Worker object\n         self._task_history = task_history or history.NopHistory()\n-        # TODO: have a Worker object instead, add more data to it\n \n     def dump(self):\n         state = (self._tasks, self._active_workers)\n@@ -94,6 +105,7 @@ class CentralPlannerScheduler(Scheduler):\n         else:\n             logger.info(\"Saved state in %s\", self._state_path)\n \n+    # prone to lead to crashes when old state is unpickled with updated code. TODO some kind of version control?\n     def load(self):\n         if os.path.exists(self._state_path):\n             logger.info(\"Attempting to load state from %s\", self._state_path)\n@@ -107,10 +119,10 @@ class CentralPlannerScheduler(Scheduler):\n         logger.info(\"Starting pruning of task graph\")\n         # Delete workers that haven't said anything for a while (probably killed)\n         delete_workers = []\n-        for worker in self._active_workers:\n-            if self._active_workers[worker] < time.time() - self._worker_disconnect_delay:\n-                logger.info(\"worker %r updated at %s timed out (no contact for >=%ss)\", worker, self._active_workers[worker], self._worker_disconnect_delay)\n-                delete_workers.append(worker)\n+        for worker in self._active_workers.values():\n+            if worker.last_active < time.time() - self._worker_disconnect_delay:\n+                logger.info(\"Worker %s timed out (no contact for >=%ss)\", worker, self._worker_disconnect_delay)\n+                delete_workers.append(worker.id)\n \n         for worker in delete_workers:\n             self._active_workers.pop(worker)\n@@ -147,10 +159,12 @@ class CentralPlannerScheduler(Scheduler):\n                 task.status = PENDING\n         logger.info(\"Done pruning task graph\")\n \n-    def update(self, worker):\n-        # update timestamp so that we keep track\n-        # of whenever the worker was last active\n-        self._active_workers[worker] = time.time()\n+    def update(self, worker_id, worker_reference=None):\n+        \"\"\" Keep track of whenever the worker was last active \"\"\"\n+        worker = self._active_workers.setdefault(worker_id, Worker(worker_id))\n+        if worker_reference:\n+            worker.reference = worker_reference\n+        worker.last_active = time.time()\n \n     def add_task(self, worker, task_id, status=PENDING, runnable=True, deps=None, expl=None):\n         \"\"\"\n@@ -193,7 +207,7 @@ class CentralPlannerScheduler(Scheduler):\n         # nothing it can wait for\n \n         # Return remaining tasks that have no FAILED descendents\n-        self.update(worker)\n+        self.update(worker, {'host': host})\n         best_t = float('inf')\n         best_task = None\n         locally_pending_tasks = 0\n@@ -204,7 +218,7 @@ class CentralPlannerScheduler(Scheduler):\n                 continue\n \n             if task.status == RUNNING:\n-                running_tasks.append({'task_id': task_id, 'worker': task.worker_running})\n+                running_tasks.append({'task_id': task_id, 'worker': str(self._active_workers.get(task.worker_running))})\n \n             if task.status != PENDING:\n                 continue\n",
          "files_name_in_blame_commit": [
            "scheduler.py",
            "central_planner_test.py",
            "worker.py"
          ]
        }
      },
      "725172e3d8d970a505931a2556cda9a235dd2373": {
        "commit": {
          "commit_id": "725172e3d8d970a505931a2556cda9a235dd2373",
          "commit_message": "Verbose output when multiple workers try running the same task(s)\n\nNot fully backwards compatible: if you update the scheduler before the worker, you will have problems\nThe other way is fine though (updating the worker before the scheduler)\n\nAlso removed support for fallback to an older way of calling the scheduler - this was fixed in the scheduler in July 2013 so it should not be a problem.",
          "commit_author": "Erik Bernhardsson",
          "commit_date": "2013-09-24 11:03:14",
          "commit_parent": "ebdbbecf58240ec30a572f0dbff7b262fcf57ffa"
        },
        "function": {
          "function_name": "get_work",
          "function_code_before": "def get_work(self, worker, host=None):\n    self.update(worker)\n    best_t = float('inf')\n    best_task = None\n    locally_pending_tasks = 0\n    for (task_id, task) in self._tasks.iteritems():\n        if worker not in task.workers:\n            continue\n        if task.status != PENDING:\n            continue\n        locally_pending_tasks += 1\n        ok = True\n        for dep in task.deps:\n            if dep not in self._tasks:\n                ok = False\n            elif self._tasks[dep].status != DONE:\n                ok = False\n        if ok:\n            if task.time < best_t:\n                best_t = task.time\n                best_task = task_id\n    if best_task:\n        t = self._tasks[best_task]\n        t.status = RUNNING\n        t.worker_running = worker\n        self._update_task_history(best_task, RUNNING, host=host)\n    return (locally_pending_tasks, best_task)",
          "function_code_after": "def get_work(self, worker, host=None):\n    self.update(worker)\n    best_t = float('inf')\n    best_task = None\n    locally_pending_tasks = 0\n    running_tasks = []\n    for (task_id, task) in self._tasks.iteritems():\n        if worker not in task.workers:\n            continue\n        if task.status == RUNNING:\n            running_tasks.append({'task_id': task_id, 'worker': task.worker_running})\n        if task.status != PENDING:\n            continue\n        locally_pending_tasks += 1\n        ok = True\n        for dep in task.deps:\n            if dep not in self._tasks:\n                ok = False\n            elif self._tasks[dep].status != DONE:\n                ok = False\n        if ok:\n            if task.time < best_t:\n                best_t = task.time\n                best_task = task_id\n    if best_task:\n        t = self._tasks[best_task]\n        t.status = RUNNING\n        t.worker_running = worker\n        self._update_task_history(best_task, RUNNING, host=host)\n    return {'n_pending_tasks': locally_pending_tasks, 'task_id': best_task, 'running_tasks': running_tasks}",
          "function_before_start_line": 185,
          "function_before_end_line": 225,
          "function_after_start_line": 185,
          "function_after_end_line": 231,
          "function_before_token_count": 153,
          "function_after_token_count": 190,
          "functions_name_modified_file": [
            "_update_task_history",
            "prune",
            "_upstream_status",
            "ping",
            "load",
            "_recurse_deps",
            "add_task",
            "task_list",
            "_serialize_task",
            "task_history",
            "dep_graph",
            "graph",
            "dump",
            "__repr__",
            "__init__",
            "update",
            "_get_task_params",
            "get_work",
            "_get_task_name",
            "fetch_error"
          ],
          "functions_name_all_files": [
            "_update_task_history",
            "test_remove_dep",
            "prune",
            "tearDown",
            "_upstream_status",
            "test_two_workers",
            "test_parameter_split",
            "_email_complete_error",
            "ping",
            "test_retry",
            "test_dep",
            "_wait",
            "load",
            "_log_unexpected_error",
            "_email_unexpected_error",
            "_add_task_and_deps",
            "_recurse_deps",
            "test_timeout",
            "create_worker",
            "test_two_worker_info",
            "add_task",
            "_validate_dependency",
            "task_list",
            "add",
            "run",
            "create_remote_scheduler",
            "_serialize_task",
            "test_disallowed_state_changes",
            "task_history",
            "dep_graph",
            "_run_task",
            "create_local_scheduler",
            "test_disconnect_running",
            "test_customized_worker",
            "graph",
            "dump",
            "__repr__",
            "setUp",
            "__init__",
            "_request",
            "update",
            "test_failed_dep",
            "complete",
            "_validate_task",
            "_check_complete_value",
            "test_broken_dep",
            "_get_task_params",
            "test_cmdline_custom_worker",
            "stop",
            "_log_complete_error",
            "_add",
            "get_work",
            "_get_task_name",
            "_add_external",
            "setTime",
            "fetch_error"
          ],
          "functions_name_co_evolved_modified_file": [],
          "functions_name_co_evolved_all_files": [
            "test_remove_dep",
            "test_timeout",
            "test_failed_dep",
            "test_two_worker_info",
            "test_broken_dep",
            "test_two_workers",
            "test_retry",
            "test_dep",
            "run",
            "test_disconnect_running",
            "test_disallowed_state_changes"
          ]
        },
        "file": {
          "file_name": "scheduler.py",
          "file_nloc": 256,
          "file_complexity": 89,
          "file_token_count": 1849,
          "file_before": "# Copyright (c) 2012 Spotify AB\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"); you may not\n# use this file except in compliance with the License. You may obtain a copy of\n# the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n# License for the specific language governing permissions and limitations under\n# the License.\n\nimport os\nimport logging\nimport time\nimport cPickle as pickle\nimport task_history as history\nlogger = logging.getLogger(\"luigi.server\")\n\nfrom task_status import PENDING, FAILED, DONE, RUNNING, UNKNOWN\n\n\nclass Scheduler(object):\n    ''' Abstract base class\n\n    Note that the methods all take string arguments, not Task objects...\n    '''\n    add_task = NotImplemented\n    get_work = NotImplemented\n    ping = NotImplemented\n\nUPSTREAM_RUNNING = 'UPSTREAM_RUNNING'\nUPSTREAM_MISSING_INPUT = 'UPSTREAM_MISSING_INPUT'\nUPSTREAM_FAILED = 'UPSTREAM_FAILED'\n\nUPSTREAM_SEVERITY_ORDER = ('', UPSTREAM_RUNNING, UPSTREAM_MISSING_INPUT, UPSTREAM_FAILED)\nUPSTREAM_SEVERITY_KEY = lambda st: UPSTREAM_SEVERITY_ORDER.index(st)\nSTATUS_TO_UPSTREAM_MAP = {FAILED: UPSTREAM_FAILED, RUNNING: UPSTREAM_RUNNING, PENDING: UPSTREAM_MISSING_INPUT}\n\n\nclass Task(object):\n    def __init__(self, status, deps):\n        self.stakeholders = set()  # workers that are somehow related to this task (i.e. don't prune while any of these workers are still active)\n        self.workers = set()  # workers that can perform task - task is 'BROKEN' if none of these workers are active\n        if deps is None:\n            self.deps = set()\n        else:\n            self.deps = set(deps)\n        self.status = status  # PENDING, RUNNING, FAILED or DONE\n        self.time = time.time()  # Timestamp when task was first added\n        self.retry = None\n        self.remove = None\n        self.worker_running = None  # the worker that is currently running the task or None\n        self.expl = None\n\n    def __repr__(self):\n        return \"Task(%r)\" % vars(self)\n\n\nclass CentralPlannerScheduler(Scheduler):\n    ''' Async scheduler that can handle multiple workers etc\n\n    Can be run locally or on a server (using RemoteScheduler + server.Server).\n    '''\n\n    def __init__(self, retry_delay=900.0, remove_delay=600.0, worker_disconnect_delay=60.0, task_history=None):\n        '''\n        (all arguments are in seconds)\n        Keyword Arguments:\n        retry_delay -- How long after a Task fails to try it again, or -1 to never retry\n        remove_delay -- How long after a Task finishes to remove it from the scheduler\n        worker_disconnect_delay -- If a worker hasn't communicated for this long, remove it from active workers\n        '''\n        self._state_path = '/var/lib/luigi-server/state.pickle'\n        self._tasks = {}\n        self._retry_delay = retry_delay\n        self._remove_delay = remove_delay\n        self._worker_disconnect_delay = worker_disconnect_delay\n        self._active_workers = {}  # map from id to timestamp (last updated)\n        self._task_history = task_history or history.NopHistory()\n        # TODO: have a Worker object instead, add more data to it\n\n    def dump(self):\n        state = (self._tasks, self._active_workers)\n        try:\n            with open(self._state_path, 'w') as fobj:\n                pickle.dump(state, fobj)\n        except IOError:\n            logger.warning(\"Failed saving scheduler state\", exc_info=1)\n        else:\n            logger.info(\"Saved state in %s\", self._state_path)\n\n    def load(self):\n        if os.path.exists(self._state_path):\n            logger.info(\"Attempting to load state from %s\", self._state_path)\n            with open(self._state_path) as fobj:\n                state = pickle.load(fobj)\n            self._tasks, self._active_workers = state\n        else:\n            logger.info(\"No prior state file exists at %s. Starting with clean slate\", self._state_path)\n\n    def prune(self):\n        logger.info(\"Starting pruning of task graph\")\n        # Delete workers that haven't said anything for a while (probably killed)\n        delete_workers = []\n        for worker in self._active_workers:\n            if self._active_workers[worker] < time.time() - self._worker_disconnect_delay:\n                logger.info(\"worker %r updated at %s timed out (no contact for >=%ss)\", worker, self._active_workers[worker], self._worker_disconnect_delay)\n                delete_workers.append(worker)\n\n        for worker in delete_workers:\n            self._active_workers.pop(worker)\n\n        remaining_workers = set(self._active_workers.keys())\n\n        # Mark tasks with no remaining active stakeholders for deletion\n        for task_id, task in self._tasks.iteritems():\n            if not task.stakeholders.intersection(remaining_workers):\n                if task.remove is None:\n                    logger.info(\"Task %r has stakeholders %r but none remain connected -> will remove task in %s seconds\", task_id, task.stakeholders, self._remove_delay)\n                    task.remove = time.time() + self._remove_delay\n\n            if task.status == RUNNING and task.worker_running and task.worker_running not in remaining_workers:\n                # If a running worker disconnects, tag all its jobs as FAILED and subject it to the same retry logic\n                logger.info(\"Task %r is marked as running by disconnected worker %r -> marking as FAILED with retry delay of %rs\", task_id, task.worker_running, self._retry_delay)\n                task.worker_running = None\n                task.status = FAILED\n                task.retry = time.time() + self._retry_delay\n\n        # Remove tasks that have no stakeholders\n        remove_tasks = []\n        for task_id, task in self._tasks.iteritems():\n            if task.remove and time.time() > task.remove:\n                logger.info(\"Removing task %r (no connected stakeholders)\", task_id)\n                remove_tasks.append(task_id)\n\n        for task_id in remove_tasks:\n            self._tasks.pop(task_id)\n\n        # Reset FAILED tasks to PENDING if max timeout is reached, and retry delay is >= 0\n        for task in self._tasks.values():\n            if task.status == FAILED and self._retry_delay >= 0 and task.retry < time.time():\n                task.status = PENDING\n        logger.info(\"Done pruning task graph\")\n\n    def update(self, worker):\n        # update timestamp so that we keep track\n        # of whenever the worker was last active\n        self._active_workers[worker] = time.time()\n\n    def add_task(self, worker, task_id, status=PENDING, runnable=True, deps=None, expl=None):\n        \"\"\"\n        * Add task identified by task_id if it doesn't exist\n        * If deps is not None, update dependency list\n        * Update status of task\n        * Add additional workers/stakeholders\n        \"\"\"\n        self.update(worker)\n\n        task = self._tasks.setdefault(task_id, Task(status=PENDING, deps=deps))\n\n        if task.remove is not None:\n            task.remove = None  # unmark task for removal so it isn't removed after being added\n\n        if not (task.status == RUNNING and status == PENDING):\n            # don't allow re-scheduling of task while it is running, it must either fail or succeed first\n            task.status = status\n            if status == FAILED:\n                task.retry = time.time() + self._retry_delay\n\n        if deps is not None:\n            task.deps = set(deps)\n\n        task.stakeholders.add(worker)\n\n        if runnable:\n            task.workers.add(worker)\n\n        if expl is not None:\n            task.expl = expl\n        self._update_task_history(task_id, status)\n\n    def get_work(self, worker, host=None):\n        # TODO: remove any expired nodes\n\n        # Algo: iterate over all nodes, find first node with no dependencies\n\n        # TODO: remove tasks that can't be done, figure out if the worker has absolutely\n        # nothing it can wait for\n\n        # Return remaining tasks that have no FAILED descendents\n        self.update(worker)\n        best_t = float('inf')\n        best_task = None\n        locally_pending_tasks = 0\n\n        for task_id, task in self._tasks.iteritems():\n            if worker not in task.workers:\n                continue\n\n            if task.status != PENDING:\n                continue\n\n            locally_pending_tasks += 1\n            ok = True\n            for dep in task.deps:\n                if dep not in self._tasks:\n                    ok = False\n                elif self._tasks[dep].status != DONE:\n                    ok = False\n\n            if ok:\n                if task.time < best_t:\n                    best_t = task.time\n                    best_task = task_id\n\n        if best_task:\n            t = self._tasks[best_task]\n            t.status = RUNNING\n            t.worker_running = worker\n            self._update_task_history(best_task, RUNNING, host=host)\n\n        return locally_pending_tasks, best_task\n\n    def ping(self, worker):\n        self.update(worker)\n\n    def _upstream_status(self, task_id, upstream_status_table):\n        if task_id in upstream_status_table:\n            return upstream_status_table[task_id]\n        elif task_id in self._tasks:\n            task_stack = [task_id]\n\n            while task_stack:\n                dep_id = task_stack.pop()\n                if dep_id in self._tasks:\n                    dep = self._tasks[dep_id]\n                    if dep_id not in upstream_status_table:\n                        if dep.status == PENDING and dep.deps:\n                            task_stack = task_stack + [dep_id] + list(dep.deps)\n                            upstream_status_table[dep_id] = ''  # will be updated postorder\n                        else:\n                            dep_status = STATUS_TO_UPSTREAM_MAP.get(dep.status, '')\n                            upstream_status_table[dep_id] = dep_status\n                    elif upstream_status_table[dep_id] == '' and dep.deps:\n                        # This is the postorder update step when we set the\n                        # status based on the previously calculated child elements\n                        upstream_status = [upstream_status_table.get(id, '') for id in dep.deps]\n                        upstream_status.append('')  # to handle empty list\n                        status = max(upstream_status, key=UPSTREAM_SEVERITY_KEY)\n                        upstream_status_table[dep_id] = status\n            return upstream_status_table[dep_id]\n\n    def _serialize_task(self, task_id):\n        task = self._tasks[task_id]\n        return {\n            'deps': list(task.deps),\n            'status': task.status,\n            'workers': list(task.workers),\n            'start_time': task.time,\n            'params': self._get_task_params(task_id),\n            'name': self._get_task_name(task_id)\n        }\n\n    def _get_task_params(self, task_id):\n        params = {}\n        params_part = task_id.split('(')[1].strip(')')\n        params_strings = params_part.split(\", \")\n\n        for param in params_strings:\n            if not param:\n                continue\n            split_param = param.split('=')\n            if len(split_param) != 2:\n                return {'<complex parameters>': params_part}\n            params[split_param[0]] = split_param[1]\n        return params\n\n    def _get_task_name(self, task_id):\n        return task_id.split('(')[0]\n\n    def graph(self):\n        self.prune()\n        serialized = {}\n        for task_id, task in self._tasks.iteritems():\n            serialized[task_id] = self._serialize_task(task_id)\n        return serialized\n\n    def _recurse_deps(self, task_id, serialized):\n        if task_id not in serialized:\n            task = self._tasks.get(task_id)\n            if task is None:\n                logger.warn('Missing task for id [%s]' % task_id)\n                serialized[task_id] = {\n                    'deps': [],\n                    'status': UNKNOWN,\n                    'workers': [],\n                    'start_time': UNKNOWN,\n                    'params': self._get_task_params(task_id),\n                    'name': self._get_task_name(task_id)\n                }\n            else:\n                serialized[task_id] = self._serialize_task(task_id)\n                for dep in task.deps:\n                    self._recurse_deps(dep, serialized)\n\n    def dep_graph(self, task_id):\n        self.prune()\n        serialized = {}\n        if task_id in self._tasks:\n            self._recurse_deps(task_id, serialized)\n        return serialized\n\n    def task_list(self, status, upstream_status):\n        ''' query for a subset of tasks by status '''\n        self.prune()\n        result = {}\n        upstream_status_table = {}  # used to memoize upstream status\n        for task_id, task in self._tasks.iteritems():\n            if not status or task.status == status:\n                if (task.status != PENDING or not upstream_status or\n                    upstream_status == self._upstream_status(task_id, upstream_status_table)):\n                    serialized = self._serialize_task(task_id)\n                    result[task_id] = serialized\n        return result\n\n    def fetch_error(self, task_id):\n        if self._tasks[task_id].expl is not None:\n            return {\"taskId\": task_id, \"error\": self._tasks[task_id].expl}\n        else:\n            return {\"taskId\": task_id, \"error\": \"\"}\n\n    def _update_task_history(self, task_id, status, host=None):\n        try:\n            if status == DONE or status == FAILED:\n                successful = (status == DONE)\n                self._task_history.task_finished(task_id, successful)\n            elif status == PENDING:\n                self._task_history.task_scheduled(task_id)\n            elif status == RUNNING:\n                self._task_history.task_started(task_id, host)\n        except:\n            logger.warning(\"Error saving Task history\", exc_info=1)\n\n    @property\n    def task_history(self):\n        # Used by server.py to expose the calls\n        return self._task_history\n",
          "file_after": "# Copyright (c) 2012 Spotify AB\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"); you may not\n# use this file except in compliance with the License. You may obtain a copy of\n# the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n# License for the specific language governing permissions and limitations under\n# the License.\n\nimport os\nimport logging\nimport time\nimport cPickle as pickle\nimport task_history as history\nlogger = logging.getLogger(\"luigi.server\")\n\nfrom task_status import PENDING, FAILED, DONE, RUNNING, UNKNOWN\n\n\nclass Scheduler(object):\n    ''' Abstract base class\n\n    Note that the methods all take string arguments, not Task objects...\n    '''\n    add_task = NotImplemented\n    get_work = NotImplemented\n    ping = NotImplemented\n\nUPSTREAM_RUNNING = 'UPSTREAM_RUNNING'\nUPSTREAM_MISSING_INPUT = 'UPSTREAM_MISSING_INPUT'\nUPSTREAM_FAILED = 'UPSTREAM_FAILED'\n\nUPSTREAM_SEVERITY_ORDER = ('', UPSTREAM_RUNNING, UPSTREAM_MISSING_INPUT, UPSTREAM_FAILED)\nUPSTREAM_SEVERITY_KEY = lambda st: UPSTREAM_SEVERITY_ORDER.index(st)\nSTATUS_TO_UPSTREAM_MAP = {FAILED: UPSTREAM_FAILED, RUNNING: UPSTREAM_RUNNING, PENDING: UPSTREAM_MISSING_INPUT}\n\n\nclass Task(object):\n    def __init__(self, status, deps):\n        self.stakeholders = set()  # workers that are somehow related to this task (i.e. don't prune while any of these workers are still active)\n        self.workers = set()  # workers that can perform task - task is 'BROKEN' if none of these workers are active\n        if deps is None:\n            self.deps = set()\n        else:\n            self.deps = set(deps)\n        self.status = status  # PENDING, RUNNING, FAILED or DONE\n        self.time = time.time()  # Timestamp when task was first added\n        self.retry = None\n        self.remove = None\n        self.worker_running = None  # the worker that is currently running the task or None\n        self.expl = None\n\n    def __repr__(self):\n        return \"Task(%r)\" % vars(self)\n\n\nclass CentralPlannerScheduler(Scheduler):\n    ''' Async scheduler that can handle multiple workers etc\n\n    Can be run locally or on a server (using RemoteScheduler + server.Server).\n    '''\n\n    def __init__(self, retry_delay=900.0, remove_delay=600.0, worker_disconnect_delay=60.0, task_history=None):\n        '''\n        (all arguments are in seconds)\n        Keyword Arguments:\n        retry_delay -- How long after a Task fails to try it again, or -1 to never retry\n        remove_delay -- How long after a Task finishes to remove it from the scheduler\n        worker_disconnect_delay -- If a worker hasn't communicated for this long, remove it from active workers\n        '''\n        self._state_path = '/var/lib/luigi-server/state.pickle'\n        self._tasks = {}\n        self._retry_delay = retry_delay\n        self._remove_delay = remove_delay\n        self._worker_disconnect_delay = worker_disconnect_delay\n        self._active_workers = {}  # map from id to timestamp (last updated)\n        self._task_history = task_history or history.NopHistory()\n        # TODO: have a Worker object instead, add more data to it\n\n    def dump(self):\n        state = (self._tasks, self._active_workers)\n        try:\n            with open(self._state_path, 'w') as fobj:\n                pickle.dump(state, fobj)\n        except IOError:\n            logger.warning(\"Failed saving scheduler state\", exc_info=1)\n        else:\n            logger.info(\"Saved state in %s\", self._state_path)\n\n    def load(self):\n        if os.path.exists(self._state_path):\n            logger.info(\"Attempting to load state from %s\", self._state_path)\n            with open(self._state_path) as fobj:\n                state = pickle.load(fobj)\n            self._tasks, self._active_workers = state\n        else:\n            logger.info(\"No prior state file exists at %s. Starting with clean slate\", self._state_path)\n\n    def prune(self):\n        logger.info(\"Starting pruning of task graph\")\n        # Delete workers that haven't said anything for a while (probably killed)\n        delete_workers = []\n        for worker in self._active_workers:\n            if self._active_workers[worker] < time.time() - self._worker_disconnect_delay:\n                logger.info(\"worker %r updated at %s timed out (no contact for >=%ss)\", worker, self._active_workers[worker], self._worker_disconnect_delay)\n                delete_workers.append(worker)\n\n        for worker in delete_workers:\n            self._active_workers.pop(worker)\n\n        remaining_workers = set(self._active_workers.keys())\n\n        # Mark tasks with no remaining active stakeholders for deletion\n        for task_id, task in self._tasks.iteritems():\n            if not task.stakeholders.intersection(remaining_workers):\n                if task.remove is None:\n                    logger.info(\"Task %r has stakeholders %r but none remain connected -> will remove task in %s seconds\", task_id, task.stakeholders, self._remove_delay)\n                    task.remove = time.time() + self._remove_delay\n\n            if task.status == RUNNING and task.worker_running and task.worker_running not in remaining_workers:\n                # If a running worker disconnects, tag all its jobs as FAILED and subject it to the same retry logic\n                logger.info(\"Task %r is marked as running by disconnected worker %r -> marking as FAILED with retry delay of %rs\", task_id, task.worker_running, self._retry_delay)\n                task.worker_running = None\n                task.status = FAILED\n                task.retry = time.time() + self._retry_delay\n\n        # Remove tasks that have no stakeholders\n        remove_tasks = []\n        for task_id, task in self._tasks.iteritems():\n            if task.remove and time.time() > task.remove:\n                logger.info(\"Removing task %r (no connected stakeholders)\", task_id)\n                remove_tasks.append(task_id)\n\n        for task_id in remove_tasks:\n            self._tasks.pop(task_id)\n\n        # Reset FAILED tasks to PENDING if max timeout is reached, and retry delay is >= 0\n        for task in self._tasks.values():\n            if task.status == FAILED and self._retry_delay >= 0 and task.retry < time.time():\n                task.status = PENDING\n        logger.info(\"Done pruning task graph\")\n\n    def update(self, worker):\n        # update timestamp so that we keep track\n        # of whenever the worker was last active\n        self._active_workers[worker] = time.time()\n\n    def add_task(self, worker, task_id, status=PENDING, runnable=True, deps=None, expl=None):\n        \"\"\"\n        * Add task identified by task_id if it doesn't exist\n        * If deps is not None, update dependency list\n        * Update status of task\n        * Add additional workers/stakeholders\n        \"\"\"\n        self.update(worker)\n\n        task = self._tasks.setdefault(task_id, Task(status=PENDING, deps=deps))\n\n        if task.remove is not None:\n            task.remove = None  # unmark task for removal so it isn't removed after being added\n\n        if not (task.status == RUNNING and status == PENDING):\n            # don't allow re-scheduling of task while it is running, it must either fail or succeed first\n            task.status = status\n            if status == FAILED:\n                task.retry = time.time() + self._retry_delay\n\n        if deps is not None:\n            task.deps = set(deps)\n\n        task.stakeholders.add(worker)\n\n        if runnable:\n            task.workers.add(worker)\n\n        if expl is not None:\n            task.expl = expl\n        self._update_task_history(task_id, status)\n\n    def get_work(self, worker, host=None):\n        # TODO: remove any expired nodes\n\n        # Algo: iterate over all nodes, find first node with no dependencies\n\n        # TODO: remove tasks that can't be done, figure out if the worker has absolutely\n        # nothing it can wait for\n\n        # Return remaining tasks that have no FAILED descendents\n        self.update(worker)\n        best_t = float('inf')\n        best_task = None\n        locally_pending_tasks = 0\n        running_tasks = []\n\n        for task_id, task in self._tasks.iteritems():\n            if worker not in task.workers:\n                continue\n\n            if task.status == RUNNING:\n                running_tasks.append({'task_id': task_id, 'worker': task.worker_running})\n\n            if task.status != PENDING:\n                continue\n\n            locally_pending_tasks += 1\n            ok = True\n            for dep in task.deps:\n                if dep not in self._tasks:\n                    ok = False\n                elif self._tasks[dep].status != DONE:\n                    ok = False\n\n            if ok:\n                if task.time < best_t:\n                    best_t = task.time\n                    best_task = task_id\n\n        if best_task:\n            t = self._tasks[best_task]\n            t.status = RUNNING\n            t.worker_running = worker\n            self._update_task_history(best_task, RUNNING, host=host)\n\n        return {'n_pending_tasks': locally_pending_tasks,\n                'task_id': best_task,\n                'running_tasks': running_tasks}\n\n    def ping(self, worker):\n        self.update(worker)\n\n    def _upstream_status(self, task_id, upstream_status_table):\n        if task_id in upstream_status_table:\n            return upstream_status_table[task_id]\n        elif task_id in self._tasks:\n            task_stack = [task_id]\n\n            while task_stack:\n                dep_id = task_stack.pop()\n                if dep_id in self._tasks:\n                    dep = self._tasks[dep_id]\n                    if dep_id not in upstream_status_table:\n                        if dep.status == PENDING and dep.deps:\n                            task_stack = task_stack + [dep_id] + list(dep.deps)\n                            upstream_status_table[dep_id] = ''  # will be updated postorder\n                        else:\n                            dep_status = STATUS_TO_UPSTREAM_MAP.get(dep.status, '')\n                            upstream_status_table[dep_id] = dep_status\n                    elif upstream_status_table[dep_id] == '' and dep.deps:\n                        # This is the postorder update step when we set the\n                        # status based on the previously calculated child elements\n                        upstream_status = [upstream_status_table.get(id, '') for id in dep.deps]\n                        upstream_status.append('')  # to handle empty list\n                        status = max(upstream_status, key=UPSTREAM_SEVERITY_KEY)\n                        upstream_status_table[dep_id] = status\n            return upstream_status_table[dep_id]\n\n    def _serialize_task(self, task_id):\n        task = self._tasks[task_id]\n        return {\n            'deps': list(task.deps),\n            'status': task.status,\n            'workers': list(task.workers),\n            'start_time': task.time,\n            'params': self._get_task_params(task_id),\n            'name': self._get_task_name(task_id)\n        }\n\n    def _get_task_params(self, task_id):\n        params = {}\n        params_part = task_id.split('(')[1].strip(')')\n        params_strings = params_part.split(\", \")\n\n        for param in params_strings:\n            if not param:\n                continue\n            split_param = param.split('=')\n            if len(split_param) != 2:\n                return {'<complex parameters>': params_part}\n            params[split_param[0]] = split_param[1]\n        return params\n\n    def _get_task_name(self, task_id):\n        return task_id.split('(')[0]\n\n    def graph(self):\n        self.prune()\n        serialized = {}\n        for task_id, task in self._tasks.iteritems():\n            serialized[task_id] = self._serialize_task(task_id)\n        return serialized\n\n    def _recurse_deps(self, task_id, serialized):\n        if task_id not in serialized:\n            task = self._tasks.get(task_id)\n            if task is None:\n                logger.warn('Missing task for id [%s]' % task_id)\n                serialized[task_id] = {\n                    'deps': [],\n                    'status': UNKNOWN,\n                    'workers': [],\n                    'start_time': UNKNOWN,\n                    'params': self._get_task_params(task_id),\n                    'name': self._get_task_name(task_id)\n                }\n            else:\n                serialized[task_id] = self._serialize_task(task_id)\n                for dep in task.deps:\n                    self._recurse_deps(dep, serialized)\n\n    def dep_graph(self, task_id):\n        self.prune()\n        serialized = {}\n        if task_id in self._tasks:\n            self._recurse_deps(task_id, serialized)\n        return serialized\n\n    def task_list(self, status, upstream_status):\n        ''' query for a subset of tasks by status '''\n        self.prune()\n        result = {}\n        upstream_status_table = {}  # used to memoize upstream status\n        for task_id, task in self._tasks.iteritems():\n            if not status or task.status == status:\n                if (task.status != PENDING or not upstream_status or\n                    upstream_status == self._upstream_status(task_id, upstream_status_table)):\n                    serialized = self._serialize_task(task_id)\n                    result[task_id] = serialized\n        return result\n\n    def fetch_error(self, task_id):\n        if self._tasks[task_id].expl is not None:\n            return {\"taskId\": task_id, \"error\": self._tasks[task_id].expl}\n        else:\n            return {\"taskId\": task_id, \"error\": \"\"}\n\n    def _update_task_history(self, task_id, status, host=None):\n        try:\n            if status == DONE or status == FAILED:\n                successful = (status == DONE)\n                self._task_history.task_finished(task_id, successful)\n            elif status == PENDING:\n                self._task_history.task_scheduled(task_id)\n            elif status == RUNNING:\n                self._task_history.task_started(task_id, host)\n        except:\n            logger.warning(\"Error saving Task history\", exc_info=1)\n\n    @property\n    def task_history(self):\n        # Used by server.py to expose the calls\n        return self._task_history\n",
          "file_patch": "@@ -195,11 +195,15 @@ class CentralPlannerScheduler(Scheduler):\n         best_t = float('inf')\n         best_task = None\n         locally_pending_tasks = 0\n+        running_tasks = []\n \n         for task_id, task in self._tasks.iteritems():\n             if worker not in task.workers:\n                 continue\n \n+            if task.status == RUNNING:\n+                running_tasks.append({'task_id': task_id, 'worker': task.worker_running})\n+\n             if task.status != PENDING:\n                 continue\n \n@@ -222,7 +226,9 @@ class CentralPlannerScheduler(Scheduler):\n             t.worker_running = worker\n             self._update_task_history(best_task, RUNNING, host=host)\n \n-        return locally_pending_tasks, best_task\n+        return {'n_pending_tasks': locally_pending_tasks,\n+                'task_id': best_task,\n+                'running_tasks': running_tasks}\n \n     def ping(self, worker):\n         self.update(worker)\n",
          "files_name_in_blame_commit": [
            "scheduler.py",
            "central_planner_test.py",
            "worker.py",
            "rpc.py",
            "customized_run_test.py"
          ]
        }
      },
      "aef50abd92da39826958d77ddacef3d323d106a4": {
        "commit": {
          "commit_id": "aef50abd92da39826958d77ddacef3d323d106a4",
          "commit_message": "Refactorisations and fixes to enable multiple worker processes\n\nChange-Id: I89eee63970124a5656e4a43603f6d9eff3d14dbf\nReviewed-on: https://gerrit.spotify.net/gerrit/17053\nTC-Compile-And-Test: teamcity teamcity <teamcity@spotify.com>\nReviewed-by: Anders Nyman <nyman@spotify.com>",
          "commit_author": "Elias Freider",
          "commit_date": "2012-07-12 12:06:26",
          "commit_parent": "ccf61987be48c231dc269536fe5af3654e4288ba"
        },
        "function": {
          "function_name": "get_work",
          "function_code_before": "",
          "function_code_after": "",
          "function_before_start_line": "",
          "function_before_end_line": "",
          "function_after_start_line": "",
          "function_after_end_line": "",
          "function_before_token_count": 0,
          "function_after_token_count": 0,
          "functions_name_modified_file": [
            "__init__",
            "update",
            "prune",
            "status",
            "add_task",
            "ping",
            "add_dep",
            "get_work",
            "graph"
          ],
          "functions_name_all_files": [
            "test_remove_dep",
            "prune",
            "tearDown",
            "clear",
            "output",
            "test_cmdline",
            "check_pid",
            "status",
            "test_two_workers",
            "test_fail",
            "ping",
            "get_params",
            "test_retry",
            "test_invoke",
            "expose",
            "load_config",
            "test_dep",
            "expose_main",
            "send_email",
            "work",
            "fork_linked_workers",
            "test_timeout",
            "input",
            "open",
            "flatten",
            "add_task",
            "task",
            "externalize",
            "__hash__",
            "requires",
            "add",
            "run",
            "get_main",
            "test_external_dep",
            "on_failure",
            "test_disallowed_state_changes",
            "test_simple",
            "exists",
            "_process_args",
            "_run_task",
            "get_reg",
            "dep",
            "disable",
            "graph",
            "__repr__",
            "setUp",
            "__init__",
            "setup_interface_logging",
            "_request",
            "update",
            "__call__",
            "test_failed_dep",
            "complete",
            "test_broken_dep",
            "test_unknown_dep",
            "deps",
            "getpaths",
            "add_dep",
            "setTime",
            "get_work",
            "test_disconnect_running",
            "get_param_values",
            "from_input",
            "write_pid"
          ],
          "functions_name_co_evolved_modified_file": [],
          "functions_name_co_evolved_all_files": [
            "test_remove_dep",
            "test_two_workers",
            "test_retry",
            "test_invoke",
            "test_dep",
            "send_email",
            "fork_linked_workers",
            "test_timeout",
            "open",
            "run",
            "test_disallowed_state_changes",
            "_process_args",
            "_run_task",
            "setUp",
            "__init__",
            "test_failed_dep",
            "complete",
            "test_broken_dep",
            "test_unknown_dep",
            "test_disconnect_running"
          ]
        },
        "file": {
          "file_name": "scheduler.py",
          "file_nloc": 141,
          "file_complexity": 45,
          "file_token_count": 974,
          "file_before": "import logging\nimport time\nlogger = logging.getLogger(\"luigi-interface\")\n\n\nclass Scheduler(object):\n    ''' Abstract base class\n\n    Note that the methods all take string arguments, not Task objects...\n    '''\n\n    add_task = NotImplemented\n    add_dep = NotImplemented\n    get_work = NotImplemented\n    ping = NotImplemented  # TODO: remove?\n    status = NotImplemented  # merge with add_task?\n\n\nclass DummyScheduler(Scheduler):\n    ''' DEPRECATED\n\n    For local scheduling, now we are using CentralPlanner but with no RPC in between.\n    However, this class is left as an example of a super small Schedule implementation\n    that actually works fairly well.\n    '''\n    def __init__(self):\n        self.__schedule = []\n\n    def add_task(self, task, status, worker):\n        if status == 'PENDING':\n            self.__schedule.append(task)\n\n    def add_dep(self, task, status, worker):\n        pass\n\n    def get_work(self, worker):\n        if len(self.__schedule):\n            # TODO: check for dependencies:\n            #for task_2 in task.deps():\n            #    if not task_2.complete():\n            #        print task,'has dependency', task_2, 'which is not complete',\n            #        break\n            return False, self.__schedule.pop()\n        else:\n            return True, None\n\n    def status(self, task, status, expl, worker):\n        pass\n\n\nclass Task(object):\n    def __init__(self, status):\n        self.workers = set()\n        self.deps = set()\n        self.status = status\n        self.time = time.time()\n        self.retry = None\n        self.remove = None\n        self.worker_running = None\n\n_default_worker = 'default-worker'  # for testing\n\n\nclass CentralPlannerScheduler(Scheduler):\n    ''' Async scheduler that can handle multiple workers etc\n\n    Can be run locally or on a server (using RemoteScheduler + server.Server).\n    '''\n    def __init__(self, retry_delay=900.0, remove_delay=600.0, worker_disconnect_delay=60.0):  # seconds\n        self.__tasks = {}\n        self.__retry_delay = retry_delay\n        self.__remove_delay = remove_delay\n        self.__worker_disconnect_delay = worker_disconnect_delay\n        self.__workers = {}  # map from id to timestamp (last updated)\n        # TODO: have a Worker object instead, add more data to it\n\n    def prune(self):\n        # Remove workers that disconnected, together with their corresponding tasks\n        # TODO: remove dependencies? (But they should always have the same worker right? So it's unnecessary)\n\n        delete_workers = []\n        for worker in self.__workers:\n            if self.__workers[worker] < time.time() - self.__worker_disconnect_delay:\n                print 'worker', worker, 'updated at', self.__workers[worker], 'timed out at', time.time(), '-', self.__worker_disconnect_delay\n                delete_workers.append(worker)\n\n        for worker in delete_workers:\n            self.__workers.pop(worker)\n\n        remaining_workers = set(list(self.__workers.keys()))\n\n        # Remove tasks corresponding to disconnected workers\n        for task, t in self.__tasks.iteritems():\n            if not t.workers.intersection(remaining_workers):\n                if t.remove == None:\n                    print 'task', task, 'has workers', self.__tasks[task].workers, 'but only', remaining_workers, 'remain -> will remove task in', self.__remove_delay, 'seconds'\n                    t.remove = time.time() + self.__remove_delay  # TODO: configure!!\n\n            if t.status == 'RUNNING' and t.worker_running and t.worker_running not in remaining_workers:\n                # If a running worker disconnects, tag all its jobs as FAILED and subject it to the same retry logic\n                print 'task', task, 'is running by worker', t.worker_running, 'but only', remaining_workers, 'remain -> will reset task'\n                t.worker_running = None\n                t.status = 'FAILED'\n                t.retry = time.time() + self.__retry_delay\n\n        # Remove tasks that timed out\n        remove_tasks = []\n        for task, t in self.__tasks.iteritems():\n            if t.remove and time.time() > t.remove:\n                print 'Removing task', task\n                remove_tasks.append(task)\n\n        for task in remove_tasks:\n            self.__tasks.pop(task)\n\n        # Reset FAILED tasks to PENDING if max timeout is reached\n        for task in self.__tasks.values():\n            if task.status == 'FAILED' and task.retry < time.time():\n                task.status = 'PENDING'\n\n    def update(self, worker):\n        # update timestamp so that we keep track\n        # of whenever the worker was last active\n        self.__workers[worker] = time.time()\n        self.prune()\n\n    def add_task(self, task, worker=_default_worker, status='PENDING'):\n        self.update(worker)\n        p = self.__tasks.setdefault(task, Task(status=status))\n\n        disallowed_state_changes = set([('RUNNING', 'PENDING')])\n\n        if (p.status, status) not in disallowed_state_changes:\n            p.status = status\n            p.workers.add(worker)\n            p.remove = None\n            p.deps.clear()\n\n    def add_dep(self, task, dep_task, worker=_default_worker):\n        self.update(worker)\n        # print task, '->', dep_task\n        # print self.__tasks\n        # self.__tasks.setdefault(task, Task()).deps.add(dep_task)\n        self.__tasks[task].deps.add(dep_task)\n\n    def get_work(self, worker=_default_worker):\n        # TODO: remove any expired nodes\n\n        # Algo: iterate over all nodes, find first node with no dependencies\n        # TODO: remove tasks that can't be done, figure out if the worker has absolutely\n        # nothing it can wait for\n        self.update(worker)\n        best_t = float('inf')\n        best_task = None\n        n_can_do = 0  # stupid thingie\n        for task, p in self.__tasks.iteritems():\n            if worker not in p.workers:\n                continue\n\n            if p.status != 'PENDING':\n                continue\n\n            n_can_do += 1\n\n            ok = True\n            for dep in p.deps:\n                if dep not in self.__tasks:\n                    ok = False\n                elif self.__tasks[dep].status != 'DONE':\n                    ok = False\n\n            if ok:\n                if p.time < best_t:\n                    best_t = p.time\n                    best_task = task\n\n        if best_task:\n            t = self.__tasks[best_task]\n            t.status = 'RUNNING'\n            t.worker_running = worker\n\n        return (n_can_do == 0), best_task\n\n    def status(self, task, status, worker=_default_worker, expl=None):\n        self.update(worker)\n        self.__tasks[task].status = status\n        if status == 'FAILED':\n            self.__tasks[task].retry = time.time() + self.__retry_delay\n\n    def ping(self, worker=_default_worker):\n        self.update(worker)\n        # TODO: if run locally, there is no need to ping this scheduler obviously!\n        pass  # autoupdate will take care of it\n\n    def graph(self):\n        serialized = {}\n        for taskname, task in self.__tasks.iteritems():\n            serialized[taskname] = {\n                'deps': list(task.deps),\n                'status': task.status\n            }\n        return serialized\n",
          "file_after": "import logging\nimport time\nlogger = logging.getLogger(\"luigi-interface\")\n\n\nclass Scheduler(object):\n    ''' Abstract base class\n\n    Note that the methods all take string arguments, not Task objects...\n    '''\n\n    add_task = NotImplemented\n    add_dep = NotImplemented\n    get_work = NotImplemented\n    ping = NotImplemented  # TODO: remove?\n    status = NotImplemented  # merge with add_task?\n\n\nclass DummyScheduler(Scheduler):\n    ''' DEPRECATED\n\n    For local scheduling, now we are using CentralPlanner but with no RPC in between.\n    However, this class is left as an example of a super small Schedule implementation\n    that actually works fairly well.\n    '''\n    def __init__(self):\n        self.__schedule = []\n\n    def add_task(self, task, status, worker):\n        if status == 'PENDING':\n            self.__schedule.append(task)\n\n    def add_dep(self, task, status, worker):\n        pass\n\n    def get_work(self, worker):\n        if len(self.__schedule):\n            # TODO: check for dependencies:\n            #for task_2 in task.deps():\n            #    if not task_2.complete():\n            #        print task,'has dependency', task_2, 'which is not complete',\n            #        break\n            return 1, self.__schedule.pop()\n        else:\n            return 0, None\n\n    def status(self, task, status, expl, worker):\n        pass\n\n\nclass Task(object):\n    def __init__(self, status):\n        self.workers = set()\n        self.deps = set()\n        self.status = status\n        self.time = time.time()\n        self.retry = None\n        self.remove = None\n        self.worker_running = None\n\n_default_worker = 'default-worker'  # for testing\n\n\nclass CentralPlannerScheduler(Scheduler):\n    ''' Async scheduler that can handle multiple workers etc\n\n    Can be run locally or on a server (using RemoteScheduler + server.Server).\n    '''\n    def __init__(self, retry_delay=900.0, remove_delay=600.0, worker_disconnect_delay=60.0):  # seconds\n        self.__tasks = {}\n        self.__retry_delay = retry_delay\n        self.__remove_delay = remove_delay\n        self.__worker_disconnect_delay = worker_disconnect_delay\n        self.__workers = {}  # map from id to timestamp (last updated)\n        # TODO: have a Worker object instead, add more data to it\n\n    def prune(self):\n        # Remove workers that disconnected, together with their corresponding tasks\n        # TODO: remove dependencies? (But they should always have the same worker right? So it's unnecessary)\n\n        delete_workers = []\n        for worker in self.__workers:\n            if self.__workers[worker] < time.time() - self.__worker_disconnect_delay:\n                print 'worker', worker, 'updated at', self.__workers[worker], 'timed out at', time.time(), '-', self.__worker_disconnect_delay\n                delete_workers.append(worker)\n\n        for worker in delete_workers:\n            self.__workers.pop(worker)\n\n        remaining_workers = set(list(self.__workers.keys()))\n\n        # Remove tasks corresponding to disconnected workers\n        for task, t in self.__tasks.iteritems():\n            if not t.workers.intersection(remaining_workers):\n                if t.remove == None:\n                    print 'task', task, 'has workers', self.__tasks[task].workers, 'but only', remaining_workers, 'remain -> will remove task in', self.__remove_delay, 'seconds'\n                    t.remove = time.time() + self.__remove_delay  # TODO: configure!!\n\n            if t.status == 'RUNNING' and t.worker_running and t.worker_running not in remaining_workers:\n                # If a running worker disconnects, tag all its jobs as FAILED and subject it to the same retry logic\n                print 'task', task, 'is running by worker', t.worker_running, 'but only', remaining_workers, 'remain -> will reset task'\n                t.worker_running = None\n                t.status = 'FAILED'\n                t.retry = time.time() + self.__retry_delay\n\n        # Remove tasks that timed out\n        remove_tasks = []\n        for task, t in self.__tasks.iteritems():\n            if t.remove and time.time() > t.remove:\n                print 'Removing task', task\n                remove_tasks.append(task)\n\n        for task in remove_tasks:\n            self.__tasks.pop(task)\n\n        # Reset FAILED tasks to PENDING if max timeout is reached\n        for task in self.__tasks.values():\n            if task.status == 'FAILED' and task.retry < time.time():\n                task.status = 'PENDING'\n\n    def update(self, worker):\n        # update timestamp so that we keep track\n        # of whenever the worker was last active\n        self.__workers[worker] = time.time()\n        self.prune()\n\n    def add_task(self, task, worker=_default_worker, status='PENDING'):\n        self.update(worker)\n        p = self.__tasks.setdefault(task, Task(status=status))\n\n        disallowed_state_changes = set([('RUNNING', 'PENDING')])\n\n        if (p.status, status) not in disallowed_state_changes:\n            p.status = status\n            p.workers.add(worker)\n            p.remove = None\n            p.deps.clear()\n\n    def add_dep(self, task, dep_task, worker=_default_worker):\n        self.update(worker)\n        # print task, '->', dep_task\n        # print self.__tasks\n        # self.__tasks.setdefault(task, Task()).deps.add(dep_task)\n        self.__tasks[task].deps.add(dep_task)\n\n    def get_work(self, worker=_default_worker):\n        # TODO: remove any expired nodes\n\n        # Algo: iterate over all nodes, find first node with no dependencies\n\n        # TODO: remove tasks that can't be done, figure out if the worker has absolutely\n        # nothing it can wait for\n\n        # Return remaining tasks that have no FAILED descendents\n\n        self.update(worker)\n        best_t = float('inf')\n        best_task = None\n        locally_pending_tasks = 0\n\n        for task_id, task in self.__tasks.iteritems():\n            if worker not in task.workers:\n                continue\n\n            if task.status != 'PENDING':\n                continue\n\n            locally_pending_tasks += 1\n            ok = True\n            for dep in task.deps:\n                if dep not in self.__tasks:\n                    ok = False\n                elif self.__tasks[dep].status != 'DONE':\n                    ok = False\n\n            if ok:\n                if task.time < best_t:\n                    best_t = task.time\n                    best_task = task_id\n\n        if best_task:\n            t = self.__tasks[best_task]\n            t.status = 'RUNNING'\n            t.worker_running = worker\n\n        return locally_pending_tasks, best_task\n\n    def status(self, task, status, worker=_default_worker, expl=None):\n        self.update(worker)\n        self.__tasks[task].status = status\n        if status == 'FAILED':\n            self.__tasks[task].retry = time.time() + self.__retry_delay\n\n    def ping(self, worker=_default_worker):\n        self.update(worker)\n        # TODO: if run locally, there is no need to ping this scheduler obviously!\n        pass  # autoupdate will take care of it\n\n    def graph(self):\n        serialized = {}\n        for taskname, task in self.__tasks.iteritems():\n            serialized[taskname] = {\n                'deps': list(task.deps),\n                'status': task.status\n            }\n        return serialized\n",
          "file_patch": "@@ -40,9 +40,9 @@ class DummyScheduler(Scheduler):\n             #    if not task_2.complete():\n             #        print task,'has dependency', task_2, 'which is not complete',\n             #        break\n-            return False, self.__schedule.pop()\n+            return 1, self.__schedule.pop()\n         else:\n-            return True, None\n+            return 0, None\n \n     def status(self, task, status, expl, worker):\n         pass\n@@ -147,39 +147,43 @@ class CentralPlannerScheduler(Scheduler):\n         # TODO: remove any expired nodes\n \n         # Algo: iterate over all nodes, find first node with no dependencies\n+\n         # TODO: remove tasks that can't be done, figure out if the worker has absolutely\n         # nothing it can wait for\n+\n+        # Return remaining tasks that have no FAILED descendents\n+\n         self.update(worker)\n         best_t = float('inf')\n         best_task = None\n-        n_can_do = 0  # stupid thingie\n-        for task, p in self.__tasks.iteritems():\n-            if worker not in p.workers:\n-                continue\n+        locally_pending_tasks = 0\n \n-            if p.status != 'PENDING':\n+        for task_id, task in self.__tasks.iteritems():\n+            if worker not in task.workers:\n                 continue\n \n-            n_can_do += 1\n+            if task.status != 'PENDING':\n+                continue\n \n+            locally_pending_tasks += 1\n             ok = True\n-            for dep in p.deps:\n+            for dep in task.deps:\n                 if dep not in self.__tasks:\n                     ok = False\n                 elif self.__tasks[dep].status != 'DONE':\n                     ok = False\n \n             if ok:\n-                if p.time < best_t:\n-                    best_t = p.time\n-                    best_task = task\n+                if task.time < best_t:\n+                    best_t = task.time\n+                    best_task = task_id\n \n         if best_task:\n             t = self.__tasks[best_task]\n             t.status = 'RUNNING'\n             t.worker_running = worker\n \n-        return (n_can_do == 0), best_task\n+        return locally_pending_tasks, best_task\n \n     def status(self, task, status, worker=_default_worker, expl=None):\n         self.update(worker)\n",
          "files_name_in_blame_commit": [
            "scheduler.py",
            "daemonizer.py",
            "recursion_test.py",
            "interface.py",
            "task.py",
            "worker_test.py",
            "central_planner_test.py",
            "worker.py",
            "fib_test.py",
            "instance_test.py",
            "rpc.py",
            "mock.py"
          ]
        }
      },
      "9303334c2ea815b8c52401d60a46a5e3d842c1df": {
        "commit": {
          "commit_id": "9303334c2ea815b8c52401d60a46a5e3d842c1df",
          "commit_message": "Decoupled visualizer server from graph API. Moved from cherrypy to tornado. Multiple visualizer processes. Moved CentralScheduler to scheduler.py. Fixed linting warnings.",
          "commit_author": "Elias Freider",
          "commit_date": "2012-04-03 12:59:53",
          "commit_parent": "fe35670decdaae9b5f119b47dbb8e0987c7e5d8d"
        },
        "function": {
          "function_name": "get_work",
          "function_code_before": "def get_work(self, worker):\n    if len(self.__schedule):\n        return (False, self.__schedule.pop())\n    else:\n        return (True, None)",
          "function_code_after": "",
          "function_before_start_line": 31,
          "function_before_end_line": 40,
          "function_after_start_line": "",
          "function_after_end_line": "",
          "function_before_token_count": 31,
          "function_after_token_count": 0,
          "functions_name_modified_file": [
            "__init__",
            "prune",
            "status",
            "add_task",
            "ping",
            "add_dep",
            "autoupdate",
            "get_work",
            "graph"
          ],
          "functions_name_all_files": [
            "test_remove_dep",
            "prune",
            "tearDown",
            "check_pid",
            "status",
            "test_two_workers",
            "ping",
            "test_retry",
            "test_dep",
            "work",
            "fork_linked_workers",
            "test_timeout",
            "add_task",
            "task",
            "add",
            "run",
            "on_graph",
            "test_disallowed_state_changes",
            "dep",
            "get",
            "graph",
            "setUp",
            "__init__",
            "_request",
            "apps",
            "test_failed_dep",
            "test_broken_dep",
            "test_unknown_dep",
            "add_dep",
            "autoupdate",
            "setTime",
            "get_work",
            "test_disconnect_running",
            "write_pid"
          ],
          "functions_name_co_evolved_modified_file": [
            "__init__",
            "prune",
            "status",
            "add_task",
            "ping",
            "add_dep",
            "autoupdate",
            "request",
            "graph"
          ],
          "functions_name_co_evolved_all_files": [
            "test_remove_dep",
            "prune",
            "check_pid",
            "status",
            "test_two_workers",
            "ping",
            "work",
            "fork_linked_workers",
            "test_timeout",
            "add_task",
            "task",
            "request",
            "add",
            "run",
            "draw",
            "on_graph",
            "json_wrapped",
            "dep",
            "get",
            "graph",
            "__init__",
            "_request",
            "apps",
            "test_failed_dep",
            "test_broken_dep",
            "autoupdate",
            "add_dep",
            "write_pid"
          ]
        },
        "file": {
          "file_name": "scheduler.py",
          "file_nloc": 146,
          "file_complexity": 46,
          "file_token_count": 993,
          "file_before": "class Scheduler(object):\n    ''' Abstract base class\n\n    Note that the methods all take string arguments, not Task objects...\n    '''\n\n    add_task = NotImplemented\n    add_dep = NotImplemented\n    get_work = NotImplemented\n    ping = NotImplemented # TODO: remove?\n    status = NotImplemented # merge with add_task?\n\nclass DummyScheduler(object):\n    ''' DEPRECATED\n\n    For local scheduling, now we are using CentralPlanner but with no RPC in between.\n    However, this class is left as an example of a super small Schedule implementation\n    that actually works fairly well.\n    '''\n    def __init__(self):\n        import collections\n        self.__schedule = []\n\n    def add_task(self, task, status, worker):\n        if status == 'PENDING':\n            self.__schedule.append(task)\n\n    def add_dep(self, task, status, worker):\n        pass\n\n    def get_work(self, worker):\n        if len(self.__schedule):\n            # TODO: check for dependencies:\n            #for task_2 in task.deps():\n            #    if not task_2.complete():\n            #        print task,'has dependency', task_2, 'which is not complete',\n            #        break\n            return False, self.__schedule.pop()\n        else:\n            return True, None\n\n    def status(self, task, status, expl, worker):\n        pass\n\nclass RemoteScheduler(Scheduler):\n    ''' Scheduler that just relays everything to a central planner\n\n        TODO: Move this to rpc.py?\n    '''\n\n    def __init__(self, host='localhost', port=8081):\n        self.__host = host\n        self.__port = port\n\n    def request(self, url, data):\n        import urllib, urllib2, json\n        # TODO(erikbern): do POST requests instead\n        data = {'data': json.dumps(data)}\n        url = 'http://%s:%d%s?%s' % (self.__host, self.__port, url, urllib.urlencode(data))\n        req = urllib2.Request(url)\n        response = urllib2.urlopen(req)\n        page = response.read()\n        result = json.loads(page)\n        return result\n\n    def ping(self, worker):\n        self.request('/api/ping', {'worker': worker}) # Keep-alive\n\n    def add_task(self, task, status, worker):\n        self.request('/api/task', {'worker': worker, 'task': task, 'status': status})        \n\n    def add_dep(self, task, task_2, worker):\n        self.request('/api/dep', {'worker': worker, 'task': task, 'dep_task': task_2})\n\n    def get_work(self, worker):\n        import time\n        time.sleep(1.0)\n        done, task = self.request('/api/work', {'worker': worker})\n        if done:\n            return True, None\n        else:\n            return False, task\n\n    def status(self, task, status, expl, worker):\n        self.request('/api/status', {'worker': worker, 'task': task, 'status': status, 'expl': expl})\n",
          "file_after": "import logging\nimport time\nlogger = logging.getLogger(\"luigi-interface\")\n\n\nclass Scheduler(object):\n    ''' Abstract base class\n\n    Note that the methods all take string arguments, not Task objects...\n    '''\n\n    add_task = NotImplemented\n    add_dep = NotImplemented\n    get_work = NotImplemented\n    ping = NotImplemented  # TODO: remove?\n    status = NotImplemented  # merge with add_task?\n\n\nclass DummyScheduler(Scheduler):\n    ''' DEPRECATED\n\n    For local scheduling, now we are using CentralPlanner but with no RPC in between.\n    However, this class is left as an example of a super small Schedule implementation\n    that actually works fairly well.\n    '''\n    def __init__(self):\n        self.__schedule = []\n\n    def add_task(self, task, status, worker):\n        if status == 'PENDING':\n            self.__schedule.append(task)\n\n    def add_dep(self, task, status, worker):\n        pass\n\n    def get_work(self, worker):\n        if len(self.__schedule):\n            # TODO: check for dependencies:\n            #for task_2 in task.deps():\n            #    if not task_2.complete():\n            #        print task,'has dependency', task_2, 'which is not complete',\n            #        break\n            return False, self.__schedule.pop()\n        else:\n            return True, None\n\n    def status(self, task, status, expl, worker):\n        pass\n\n\nclass Task(object):\n    def __init__(self, status):\n        self.workers = set()\n        self.deps = set()\n        self.status = status\n        self.time = time.time()\n        self.retry = None\n        self.remove = None\n        self.worker_running = None\n\n_default_worker = 'default-worker'  # for testing\n\n\nclass CentralPlannerScheduler(Scheduler):\n    ''' Async scheduler that can handle multiple workers etc\n\n    Can be run locally or on a server (using RemoteScheduler + server.Server).\n    '''\n    def __init__(self, retry_delay=60.0, remove_delay=600.0, worker_disconnect_delay=60.0):  # seconds\n        self.__tasks = {}\n        self.__retry_delay = retry_delay\n        self.__remove_delay = remove_delay\n        self.__worker_disconnect_delay = worker_disconnect_delay\n        self.__workers = {}  # map from id to timestamp (last updated)\n        # TODO: have a Worker object instead, add more data to it\n\n    def prune(self):\n        # Remove workers that disconnected, together with their corresponding tasks\n        # TODO: remove dependencies? (But they should always have the same worker right? So it's unnecessary)\n\n        delete_workers = []\n        for worker in self.__workers:\n            if self.__workers[worker] < time.time() - self.__worker_disconnect_delay:\n                print 'worker', worker, 'updated at', self.__workers[worker], 'timed out at', time.time(), '-', self.__worker_disconnect_delay\n                delete_workers.append(worker)\n\n        for worker in delete_workers:\n            self.__workers.pop(worker)\n\n        remaining_workers = set(list(self.__workers.keys()))\n\n        # Remove tasks corresponding to disconnected workers\n        for task, t in self.__tasks.iteritems():\n            if not t.workers.intersection(remaining_workers):\n                if t.remove == None:\n                    print 'task', task, 'has workers', self.__tasks[task].workers, 'but only', remaining_workers, 'remain -> will remove task in', self.__remove_delay, 'seconds'\n                    t.remove = time.time() + self.__remove_delay  # TODO: configure!!\n\n            if t.status == 'RUNNING' and t.worker_running and t.worker_running not in remaining_workers:\n                # If a running worker disconnects, tag all its jobs as FAILED and subject it to the same retry logic\n                print 'task', task, 'is running by worker', t.worker_running, 'but only', remaining_workers, 'remain -> will reset task'\n                t.worker_running = None\n                t.status = 'FAILED'\n                t.retry = time.time() + self.__retry_delay\n\n        # Remove tasks that timed out\n        remove_tasks = []\n        for task, t in self.__tasks.iteritems():\n            if t.remove and time.time() > t.remove:\n                print 'Removing task', task\n                remove_tasks.append(task)\n\n        for task in remove_tasks:\n            self.__tasks.pop(task)\n\n        # Reset FAILED tasks to PENDING if max timeout is reached\n        for task in self.__tasks.values():\n            if task.status == 'FAILED' and task.retry < time.time():\n                task.status = 'PENDING'\n\n    def autoupdate(f):\n        def g(self, *args, **kwargs):\n            # update timestamp so that we keep track\n            # of whenever the worker was last active\n            worker = kwargs.get('worker', _default_worker)\n            self.__workers[worker] = time.time()\n            self.prune()\n            return f(self, *args, **kwargs)\n        return g\n\n    @autoupdate\n    def add_task(self, task, worker=_default_worker, status='PENDING'):\n        p = self.__tasks.setdefault(task, Task(status=status))\n\n        disallowed_state_changes = set([('RUNNING', 'PENDING')])\n\n        if (p.status, status) not in disallowed_state_changes:\n            p.status = status\n            p.workers.add(worker)\n            p.remove = None\n            p.deps.clear()\n\n    @autoupdate\n    def add_dep(self, task, dep_task, worker=_default_worker):\n        # print task, '->', dep_task\n        # print self.__tasks\n        # self.__tasks.setdefault(task, Task()).deps.add(dep_task)\n        self.__tasks[task].deps.add(dep_task)\n\n    @autoupdate\n    def get_work(self, worker=_default_worker):\n        # TODO: remove any expired nodes\n\n        # Algo: iterate over all nodes, find first node with no dependencies\n        # TODO: remove tasks that can't be done, figure out if the worker has absolutely\n        # nothing it can wait for\n\n        best_t = float('inf')\n        best_task = None\n        n_can_do = 0  # stupid thingie\n        for task, p in self.__tasks.iteritems():\n            if worker not in p.workers:\n                continue\n\n            if p.status != 'PENDING':\n                continue\n\n            n_can_do += 1\n\n            ok = True\n            for dep in p.deps:\n                if dep not in self.__tasks:\n                    ok = False\n                elif self.__tasks[dep].status != 'DONE':\n                    ok = False\n\n            if ok:\n                if p.time < best_t:\n                    best_t = p.time\n                    best_task = task\n\n        if best_task:\n            t = self.__tasks[best_task]\n            t.status = 'RUNNING'\n            t.worker_running = worker\n\n        return (n_can_do == 0), best_task\n\n    @autoupdate\n    def status(self, task, status, worker=_default_worker, expl=None):\n        self.__tasks[task].status = status\n        if status == 'FAILED':\n            self.__tasks[task].retry = time.time() + self.__retry_delay\n\n    @autoupdate\n    def ping(self, worker=_default_worker):\n        # TODO: if run locally, there is no need to ping this scheduler obviously!\n        pass  # autoupdate will take care of it\n\n    @autoupdate\n    def graph(self):\n        serialized = {}\n        for taskname, task in self.__tasks.iteritems():\n            serialized[taskname] = {\n                'deps': list(task.deps),\n                'status': task.status\n            }\n        return serialized\n",
          "file_patch": "@@ -1,3 +1,8 @@\n+import logging\n+import time\n+logger = logging.getLogger(\"luigi-interface\")\n+\n+\n class Scheduler(object):\n     ''' Abstract base class\n \n@@ -7,10 +12,11 @@ class Scheduler(object):\n     add_task = NotImplemented\n     add_dep = NotImplemented\n     get_work = NotImplemented\n-    ping = NotImplemented # TODO: remove?\n-    status = NotImplemented # merge with add_task?\n+    ping = NotImplemented  # TODO: remove?\n+    status = NotImplemented  # merge with add_task?\n \n-class DummyScheduler(object):\n+\n+class DummyScheduler(Scheduler):\n     ''' DEPRECATED\n \n     For local scheduling, now we are using CentralPlanner but with no RPC in between.\n@@ -18,7 +24,6 @@ class DummyScheduler(object):\n     that actually works fairly well.\n     '''\n     def __init__(self):\n-        import collections\n         self.__schedule = []\n \n     def add_task(self, task, status, worker):\n@@ -42,44 +47,162 @@ class DummyScheduler(object):\n     def status(self, task, status, expl, worker):\n         pass\n \n-class RemoteScheduler(Scheduler):\n-    ''' Scheduler that just relays everything to a central planner\n \n-        TODO: Move this to rpc.py?\n-    '''\n+class Task(object):\n+    def __init__(self, status):\n+        self.workers = set()\n+        self.deps = set()\n+        self.status = status\n+        self.time = time.time()\n+        self.retry = None\n+        self.remove = None\n+        self.worker_running = None\n \n-    def __init__(self, host='localhost', port=8081):\n-        self.__host = host\n-        self.__port = port\n+_default_worker = 'default-worker'  # for testing\n \n-    def request(self, url, data):\n-        import urllib, urllib2, json\n-        # TODO(erikbern): do POST requests instead\n-        data = {'data': json.dumps(data)}\n-        url = 'http://%s:%d%s?%s' % (self.__host, self.__port, url, urllib.urlencode(data))\n-        req = urllib2.Request(url)\n-        response = urllib2.urlopen(req)\n-        page = response.read()\n-        result = json.loads(page)\n-        return result\n-\n-    def ping(self, worker):\n-        self.request('/api/ping', {'worker': worker}) # Keep-alive\n-\n-    def add_task(self, task, status, worker):\n-        self.request('/api/task', {'worker': worker, 'task': task, 'status': status})        \n \n-    def add_dep(self, task, task_2, worker):\n-        self.request('/api/dep', {'worker': worker, 'task': task, 'dep_task': task_2})\n+class CentralPlannerScheduler(Scheduler):\n+    ''' Async scheduler that can handle multiple workers etc\n \n-    def get_work(self, worker):\n-        import time\n-        time.sleep(1.0)\n-        done, task = self.request('/api/work', {'worker': worker})\n-        if done:\n-            return True, None\n-        else:\n-            return False, task\n-\n-    def status(self, task, status, expl, worker):\n-        self.request('/api/status', {'worker': worker, 'task': task, 'status': status, 'expl': expl})\n+    Can be run locally or on a server (using RemoteScheduler + server.Server).\n+    '''\n+    def __init__(self, retry_delay=60.0, remove_delay=600.0, worker_disconnect_delay=60.0):  # seconds\n+        self.__tasks = {}\n+        self.__retry_delay = retry_delay\n+        self.__remove_delay = remove_delay\n+        self.__worker_disconnect_delay = worker_disconnect_delay\n+        self.__workers = {}  # map from id to timestamp (last updated)\n+        # TODO: have a Worker object instead, add more data to it\n+\n+    def prune(self):\n+        # Remove workers that disconnected, together with their corresponding tasks\n+        # TODO: remove dependencies? (But they should always have the same worker right? So it's unnecessary)\n+\n+        delete_workers = []\n+        for worker in self.__workers:\n+            if self.__workers[worker] < time.time() - self.__worker_disconnect_delay:\n+                print 'worker', worker, 'updated at', self.__workers[worker], 'timed out at', time.time(), '-', self.__worker_disconnect_delay\n+                delete_workers.append(worker)\n+\n+        for worker in delete_workers:\n+            self.__workers.pop(worker)\n+\n+        remaining_workers = set(list(self.__workers.keys()))\n+\n+        # Remove tasks corresponding to disconnected workers\n+        for task, t in self.__tasks.iteritems():\n+            if not t.workers.intersection(remaining_workers):\n+                if t.remove == None:\n+                    print 'task', task, 'has workers', self.__tasks[task].workers, 'but only', remaining_workers, 'remain -> will remove task in', self.__remove_delay, 'seconds'\n+                    t.remove = time.time() + self.__remove_delay  # TODO: configure!!\n+\n+            if t.status == 'RUNNING' and t.worker_running and t.worker_running not in remaining_workers:\n+                # If a running worker disconnects, tag all its jobs as FAILED and subject it to the same retry logic\n+                print 'task', task, 'is running by worker', t.worker_running, 'but only', remaining_workers, 'remain -> will reset task'\n+                t.worker_running = None\n+                t.status = 'FAILED'\n+                t.retry = time.time() + self.__retry_delay\n+\n+        # Remove tasks that timed out\n+        remove_tasks = []\n+        for task, t in self.__tasks.iteritems():\n+            if t.remove and time.time() > t.remove:\n+                print 'Removing task', task\n+                remove_tasks.append(task)\n+\n+        for task in remove_tasks:\n+            self.__tasks.pop(task)\n+\n+        # Reset FAILED tasks to PENDING if max timeout is reached\n+        for task in self.__tasks.values():\n+            if task.status == 'FAILED' and task.retry < time.time():\n+                task.status = 'PENDING'\n+\n+    def autoupdate(f):\n+        def g(self, *args, **kwargs):\n+            # update timestamp so that we keep track\n+            # of whenever the worker was last active\n+            worker = kwargs.get('worker', _default_worker)\n+            self.__workers[worker] = time.time()\n+            self.prune()\n+            return f(self, *args, **kwargs)\n+        return g\n+\n+    @autoupdate\n+    def add_task(self, task, worker=_default_worker, status='PENDING'):\n+        p = self.__tasks.setdefault(task, Task(status=status))\n+\n+        disallowed_state_changes = set([('RUNNING', 'PENDING')])\n+\n+        if (p.status, status) not in disallowed_state_changes:\n+            p.status = status\n+            p.workers.add(worker)\n+            p.remove = None\n+            p.deps.clear()\n+\n+    @autoupdate\n+    def add_dep(self, task, dep_task, worker=_default_worker):\n+        # print task, '->', dep_task\n+        # print self.__tasks\n+        # self.__tasks.setdefault(task, Task()).deps.add(dep_task)\n+        self.__tasks[task].deps.add(dep_task)\n+\n+    @autoupdate\n+    def get_work(self, worker=_default_worker):\n+        # TODO: remove any expired nodes\n+\n+        # Algo: iterate over all nodes, find first node with no dependencies\n+        # TODO: remove tasks that can't be done, figure out if the worker has absolutely\n+        # nothing it can wait for\n+\n+        best_t = float('inf')\n+        best_task = None\n+        n_can_do = 0  # stupid thingie\n+        for task, p in self.__tasks.iteritems():\n+            if worker not in p.workers:\n+                continue\n+\n+            if p.status != 'PENDING':\n+                continue\n+\n+            n_can_do += 1\n+\n+            ok = True\n+            for dep in p.deps:\n+                if dep not in self.__tasks:\n+                    ok = False\n+                elif self.__tasks[dep].status != 'DONE':\n+                    ok = False\n+\n+            if ok:\n+                if p.time < best_t:\n+                    best_t = p.time\n+                    best_task = task\n+\n+        if best_task:\n+            t = self.__tasks[best_task]\n+            t.status = 'RUNNING'\n+            t.worker_running = worker\n+\n+        return (n_can_do == 0), best_task\n+\n+    @autoupdate\n+    def status(self, task, status, worker=_default_worker, expl=None):\n+        self.__tasks[task].status = status\n+        if status == 'FAILED':\n+            self.__tasks[task].retry = time.time() + self.__retry_delay\n+\n+    @autoupdate\n+    def ping(self, worker=_default_worker):\n+        # TODO: if run locally, there is no need to ping this scheduler obviously!\n+        pass  # autoupdate will take care of it\n+\n+    @autoupdate\n+    def graph(self):\n+        serialized = {}\n+        for taskname, task in self.__tasks.iteritems():\n+            serialized[taskname] = {\n+                'deps': list(task.deps),\n+                'status': task.status\n+            }\n+        return serialized\n",
          "files_name_in_blame_commit": [
            "scheduler.py",
            "daemonizer.py",
            "server.py",
            "worker_test.py",
            "central_planner_test.py",
            "worker.py",
            "rpc.py",
            "__init__.py",
            "central_planner.py"
          ]
        }
      }
    }
  }
}