{
  "id": "53",
  "blame_commit": {
    "commit": {
      "commit_id": "a3d6fc6defb9cd690a5f66a18130a3aa080f402d",
      "commit_message": "Use HTTP1Connection in SimpleAsyncHTTPClient.\n\nThis incidentally adds support for chunked request bodies on the server side.",
      "commit_author": "Ben Darnell",
      "commit_date": "2014-03-02 15:48:26",
      "commit_parent": "e564ddbe32bb2b250beef44eca3bdeb03fe4838d"
    },
    "function": {
      "function_name": "_read_body",
      "function_code_before": "def _read_body(self, headers):\n    content_length = headers.get('Content-Length')\n    if content_length:\n        content_length = int(content_length)\n        if content_length > self.stream.max_buffer_size:\n            raise httputil.BadRequestException('Content-Length too long')\n        if headers.get('Expect') == '100-continue':\n            self.stream.write(b'HTTP/1.1 100 (Continue)\\r\\n\\r\\n')\n        return self.stream.read_bytes(content_length)\n    return None",
      "function_code_after": "def _read_body(self, is_client, headers, delegate):\n    content_length = headers.get('Content-Length')\n    if content_length:\n        content_length = int(content_length)\n        if content_length > self.stream.max_buffer_size:\n            raise httputil.BadRequestException('Content-Length too long')\n        return self._read_fixed_body(content_length, delegate)\n    if headers.get('Transfer-Encoding') == 'chunked':\n        return self._read_chunked_body(delegate)\n    if is_client:\n        return self._read_body_until_close(delegate)\n    return None",
      "function_before_start_line": 186,
      "function_before_end_line": 195,
      "function_after_start_line": 215,
      "function_after_end_line": 226,
      "function_before_token_count": 70,
      "function_after_token_count": 82,
      "functions_name_modified_file": [
        "write",
        "_read_body",
        "start_serving",
        "_process_requests",
        "close",
        "_read_body_until_close",
        "_read_fixed_body",
        "process_response",
        "set_close_callback",
        "__init__",
        "_can_keep_alive",
        "_finish_request",
        "_process_message",
        "_parse_headers",
        "_clear_request_state",
        "_on_connection_close",
        "_read_chunked_body",
        "_on_write_complete",
        "finish"
      ],
      "functions_name_all_files": [
        "test_malformed_first_line",
        "_parseparam",
        "test_request_timeout",
        "on_connection_close",
        "test_singleton",
        "_not_supported",
        "_generate_challenge_response",
        "_handle_challenge",
        "_handle_websocket_headers",
        "test_unix_socket",
        "test_http10",
        "_on_frame_type",
        "parse_response_start_line",
        "test_ip_headers",
        "select_subprotocol",
        "_execute",
        "_on_frame_start",
        "_on_frame_length_64",
        "headers_received",
        "test_finish_while_closed",
        "fetch_json",
        "allow_draft76",
        "start_serving",
        "test_pipelined_cancel",
        "test_http10_keepalive",
        "_read_body_until_close",
        "_on_end_delimiter",
        "_on_timeout",
        "challenge_response",
        "doctests",
        "_release",
        "__getitem__",
        "setUp",
        "__init__",
        "cookies",
        "get_ssl_certificate",
        "test_options_request",
        "test_resolve_timeout",
        "parse_multipart_form_data",
        "test_types",
        "write_message",
        "_on_frame_length_16",
        "get_ssl_options",
        "_on_write_complete",
        "_on_masked_frame_data",
        "test_host_header",
        "_on_resolve",
        "tearDown",
        "test_100_continue",
        "_parse_header",
        "_run_callback",
        "ping",
        "connect",
        "test_head_request",
        "test_see_other_redirect",
        "read_message",
        "websocket_connect",
        "write",
        "_write_frame",
        "_on_connect",
        "test_request_without_xprotocol",
        "prepare",
        "_read_body",
        "test_max_redirects",
        "open",
        "get_handlers",
        "test_two_requests",
        "test_default_certificates_exist",
        "_websocket_mask_python",
        "test_header_reuse",
        "write_ping",
        "__delitem__",
        "_on_end_request",
        "__contains__",
        "_parse_request_range",
        "test_missing_key",
        "on_message",
        "_on_masking_key",
        "parse_body_arguments",
        "_process_requests",
        "test_cancel_during_download",
        "supports_http_1_1",
        "test_manual_protocol",
        "test_connection_limit",
        "test_redirect_connection_limit",
        "fetch_impl",
        "accept_connection",
        "_process_message",
        "on_close",
        "_clear_request_state",
        "compute_accept_value",
        "test_ipv6",
        "async_callback",
        "start_request",
        "_read_chunked_body",
        "raw_fetch",
        "data_received",
        "respond_100",
        "test_large_post",
        "get_ssl_version",
        "_release_fetch",
        "test_hostname_mapping",
        "finish",
        "full_url",
        "test_malformed_headers",
        "read_response",
        "_int_or_none",
        "request_time",
        "post",
        "test_malformed_body",
        "test_no_content_length",
        "respond_200",
        "test_missing_arguments",
        "initialize",
        "test_port_mapping",
        "_on_close",
        "get_list",
        "get_httpserver_options",
        "test_connection_refused",
        "test_query_string_encoding",
        "test_double_slash",
        "create_client",
        "test_gzip",
        "parse_line",
        "get_http_client",
        "_accept_connection",
        "set_nodelay",
        "__repr__",
        "set_close_callback",
        "test_request_close",
        "test_empty_request",
        "_parse_headers",
        "copy",
        "on_pong",
        "__setitem__",
        "get_app",
        "test_scheme_headers",
        "_abort",
        "test_empty_post_parameters",
        "test_multipart_form",
        "__missing__",
        "_receive_message",
        "test_max_clients",
        "_create_stream",
        "_calculate_part",
        "_on_http_response",
        "read_headers",
        "_on_frame_data",
        "test_no_content",
        "_challenge_response",
        "_on_length_indicator",
        "test_pipelined_requests",
        "_handle_message",
        "check_type",
        "get_all",
        "add",
        "_handle_exception",
        "options",
        "get_websocket_scheme",
        "close",
        "_read_fixed_body",
        "_process_queue",
        "get",
        "process_response",
        "_write_response",
        "_get_content_range",
        "_remove_timeout",
        "test_unix_socket_bad_request",
        "update",
        "_can_keep_alive",
        "parse",
        "format_timestamp",
        "test_empty_query_string",
        "_finish_request",
        "test_ssl",
        "test_multiple_content_length_accepted",
        "test_queue_timeout",
        "test_non_ssl_request",
        "_on_connection_close",
        "handle_stream",
        "head",
        "_receive_frame",
        "_handle_request",
        "url_concat"
      ],
      "functions_name_co_evolved_modified_file": [
        "__init__",
        "_process_requests",
        "_read_body_until_close",
        "_read_fixed_body",
        "_process_message",
        "process_response",
        "_read_chunked_body",
        "start_serving"
      ],
      "functions_name_co_evolved_all_files": [
        "_on_resolve",
        "_on_chunk_data",
        "_on_body",
        "parse_response_start_line",
        "_on_connect",
        "test_no_content_length",
        "set_request",
        "headers_received",
        "_handle_exception",
        "start_serving",
        "_process_requests",
        "_read_body_until_close",
        "_read_fixed_body",
        "_on_headers",
        "get",
        "process_response",
        "_on_chunk_length",
        "__init__",
        "_process_message",
        "_handle_1xx",
        "handle_stream",
        "get_app",
        "_read_chunked_body",
        "raw_fetch",
        "data_received",
        "test_multipart_form",
        "finish"
      ]
    },
    "file": {
      "file_name": "http1connection.py",
      "file_nloc": 173,
      "file_complexity": 55,
      "file_token_count": 1136,
      "file_before": "#!/usr/bin/env python\n#\n# Copyright 2014 Facebook\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"); you may\n# not use this file except in compliance with the License. You may obtain\n# a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n# License for the specific language governing permissions and limitations\n# under the License.\n\nfrom __future__ import absolute_import, division, print_function, with_statement\n\nfrom tornado.concurrent import Future\nfrom tornado.escape import native_str\nfrom tornado import gen\nfrom tornado import httputil\nfrom tornado import iostream\nfrom tornado.log import gen_log\nfrom tornado import stack_context\n\n\nclass HTTP1Connection(object):\n    \"\"\"Handles a connection to an HTTP client, executing HTTP requests.\n\n    We parse HTTP headers and bodies, and execute the request callback\n    until the HTTP conection is closed.\n    \"\"\"\n    def __init__(self, stream, address, delegate, no_keep_alive=False,\n                 protocol=None):\n        self.stream = stream\n        self.address = address\n        # Save the socket's address family now so we know how to\n        # interpret self.address even after the stream is closed\n        # and its socket attribute replaced with None.\n        self.address_family = stream.socket.family\n        self.delegate = delegate\n        self.no_keep_alive = no_keep_alive\n        if protocol:\n            self.protocol = protocol\n        elif isinstance(stream, iostream.SSLIOStream):\n            self.protocol = \"https\"\n        else:\n            self.protocol = \"http\"\n        self._disconnect_on_finish = False\n        self._clear_request_state()\n        self.stream.set_close_callback(self._on_connection_close)\n        self._finish_future = None\n        # Register the future on the IOLoop so its errors get logged.\n        stream.io_loop.add_future(self._process_requests(),\n                                  lambda f: f.result())\n\n    @gen.coroutine\n    def _process_requests(self):\n        while True:\n            try:\n                header_data = yield self.stream.read_until(b\"\\r\\n\\r\\n\")\n                request_delegate = self.delegate.start_request(self)\n                self._finish_future = Future()\n                start_line, headers = self._parse_headers(header_data)\n                self._disconnect_on_finish = not self._can_keep_alive(\n                    start_line, headers)\n                request_delegate.headers_received(start_line, headers)\n                body_future = self._read_body(headers)\n                if body_future is not None:\n                    request_delegate.data_received((yield body_future))\n                request_delegate.finish()\n                yield self._finish_future\n            except httputil.BadRequestException as e:\n                gen_log.info(\"Malformed HTTP request from %r: %s\",\n                             self.address, e)\n                self.close()\n                return\n            except iostream.StreamClosedError:\n                self.close()\n                return\n\n\n    def _clear_request_state(self):\n        \"\"\"Clears the per-request state.\n\n        This is run in between requests to allow the previous handler\n        to be garbage collected (and prevent spurious close callbacks),\n        and when the connection is closed (to break up cycles and\n        facilitate garbage collection in cpython).\n        \"\"\"\n        self._request_finished = False\n        self._write_callback = None\n        self._close_callback = None\n\n    def set_close_callback(self, callback):\n        \"\"\"Sets a callback that will be run when the connection is closed.\n\n        Use this instead of accessing\n        `HTTPConnection.stream.set_close_callback\n        <.BaseIOStream.set_close_callback>` directly (which was the\n        recommended approach prior to Tornado 3.0).\n        \"\"\"\n        self._close_callback = stack_context.wrap(callback)\n\n    def _on_connection_close(self):\n        if self._close_callback is not None:\n            callback = self._close_callback\n            self._close_callback = None\n            callback()\n        if self._finish_future is not None and not self._finish_future.done():\n            self._finish_future.set_result(None)\n        # Delete any unfinished callbacks to break up reference cycles.\n        self._clear_request_state()\n\n    def close(self):\n        self.stream.close()\n        # Remove this reference to self, which would otherwise cause a\n        # cycle and delay garbage collection of this connection.\n        self._clear_request_state()\n\n    def write(self, chunk, callback=None):\n        \"\"\"Writes a chunk of output to the stream.\"\"\"\n        if not self.stream.closed():\n            self._write_callback = stack_context.wrap(callback)\n            self.stream.write(chunk, self._on_write_complete)\n\n    def finish(self):\n        \"\"\"Finishes the request.\"\"\"\n        self._request_finished = True\n        # No more data is coming, so instruct TCP to send any remaining\n        # data immediately instead of waiting for a full packet or ack.\n        self.stream.set_nodelay(True)\n        if not self.stream.writing():\n            self._finish_request()\n\n    def _on_write_complete(self):\n        if self._write_callback is not None:\n            callback = self._write_callback\n            self._write_callback = None\n            callback()\n        # _on_write_complete is enqueued on the IOLoop whenever the\n        # IOStream's write buffer becomes empty, but it's possible for\n        # another callback that runs on the IOLoop before it to\n        # simultaneously write more data and finish the request.  If\n        # there is still data in the IOStream, a future\n        # _on_write_complete will be responsible for calling\n        # _finish_request.\n        if self._request_finished and not self.stream.writing():\n            self._finish_request()\n\n    def _can_keep_alive(self, start_line, headers):\n        if self.no_keep_alive:\n            return False\n        connection_header = headers.get(\"Connection\")\n        if connection_header is not None:\n            connection_header = connection_header.lower()\n        if start_line.endswith(\"HTTP/1.1\"):\n            return connection_header != \"close\"\n        elif (\"Content-Length\" in headers\n              or start_line.startswith((\"HEAD \", \"GET \"))):\n            return connection_header == \"keep-alive\"\n        return False\n\n    def _finish_request(self):\n        self._clear_request_state()\n        if self._disconnect_on_finish:\n            self.close()\n            return\n        # Turn Nagle's algorithm back on, leaving the stream in its\n        # default state for the next request.\n        self.stream.set_nodelay(False)\n        self._finish_future.set_result(None)\n\n    def _parse_headers(self, data):\n        data = native_str(data.decode('latin1'))\n        eol = data.find(\"\\r\\n\")\n        start_line = data[:eol]\n        try:\n            headers = httputil.HTTPHeaders.parse(data[eol:])\n        except ValueError:\n            # probably form split() if there was no ':' in the line\n            raise httputil.BadRequestException(\"Malformed HTTP headers\")\n        return start_line, headers\n\n    def _read_body(self, headers):\n        content_length = headers.get(\"Content-Length\")\n        if content_length:\n            content_length = int(content_length)\n            if content_length > self.stream.max_buffer_size:\n                raise httputil.BadRequestException(\"Content-Length too long\")\n            if headers.get(\"Expect\") == \"100-continue\":\n                self.stream.write(b\"HTTP/1.1 100 (Continue)\\r\\n\\r\\n\")\n            return self.stream.read_bytes(content_length)\n        return None\n",
      "file_after": "#!/usr/bin/env python\n#\n# Copyright 2014 Facebook\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"); you may\n# not use this file except in compliance with the License. You may obtain\n# a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n# License for the specific language governing permissions and limitations\n# under the License.\n\nfrom __future__ import absolute_import, division, print_function, with_statement\n\nfrom tornado.concurrent import Future\nfrom tornado.escape import native_str\nfrom tornado import gen\nfrom tornado import httputil\nfrom tornado import iostream\nfrom tornado.log import gen_log\nfrom tornado import stack_context\n\n\nclass HTTP1Connection(object):\n    \"\"\"Handles a connection to an HTTP client, executing HTTP requests.\n\n    We parse HTTP headers and bodies, and execute the request callback\n    until the HTTP conection is closed.\n    \"\"\"\n    def __init__(self, stream, address, no_keep_alive=False, protocol=None):\n        self.stream = stream\n        self.address = address\n        # Save the socket's address family now so we know how to\n        # interpret self.address even after the stream is closed\n        # and its socket attribute replaced with None.\n        self.address_family = stream.socket.family\n        self.no_keep_alive = no_keep_alive\n        if protocol:\n            self.protocol = protocol\n        elif isinstance(stream, iostream.SSLIOStream):\n            self.protocol = \"https\"\n        else:\n            self.protocol = \"http\"\n        self._disconnect_on_finish = False\n        self._clear_request_state()\n        self.stream.set_close_callback(self._on_connection_close)\n        self._finish_future = None\n\n    def start_serving(self, delegate):\n        assert isinstance(delegate, httputil.HTTPConnectionDelegate)\n        # Register the future on the IOLoop so its errors get logged.\n        self.stream.io_loop.add_future(self._process_requests(delegate),\n                                       lambda f: f.result())\n\n    @gen.coroutine\n    def _process_requests(self, delegate):\n        while True:\n            request_delegate = delegate.start_request(self)\n            try:\n                ret = yield self._process_message(request_delegate, False)\n            except iostream.StreamClosedError:\n                self.close()\n                return\n            if not ret:\n                return\n\n    def process_response(self, delegate, method):\n        return self._process_message(delegate, True, method=method)\n\n    @gen.coroutine\n    def _process_message(self, delegate, is_client, method=None):\n        assert isinstance(delegate, httputil.HTTPStreamDelegate)\n        try:\n            header_data = yield self.stream.read_until_regex(b\"\\r?\\n\\r?\\n\")\n            self._finish_future = Future()\n            start_line, headers = self._parse_headers(header_data)\n            self._disconnect_on_finish = not self._can_keep_alive(\n                start_line, headers)\n            ret = delegate.headers_received(start_line, headers)\n            # TODO: finalize the 'detach' interface.\n            if ret == 'detach':\n                return\n            skip_body = False\n            if is_client:\n                if method == 'HEAD':\n                    skip_body = True\n                code = httputil.parse_response_start_line(start_line).code\n                if code == 304:\n                    skip_body = True\n                if code >= 100 and code < 200:\n                    yield self._process_message(delegate, is_client, method=method)\n            else:\n                if headers.get(\"Expect\") == \"100-continue\":\n                    self.stream.write(b\"HTTP/1.1 100 (Continue)\\r\\n\\r\\n\")\n            if not skip_body:\n                body_future = self._read_body(is_client, headers, delegate)\n                if body_future is not None:\n                    yield body_future\n            delegate.finish()\n            yield self._finish_future\n        except httputil.BadRequestException as e:\n            gen_log.info(\"Malformed HTTP request from %r: %s\",\n                         self.address, e)\n            self.close()\n            raise gen.Return(False)\n        raise gen.Return(True)\n\n\n    def _clear_request_state(self):\n        \"\"\"Clears the per-request state.\n\n        This is run in between requests to allow the previous handler\n        to be garbage collected (and prevent spurious close callbacks),\n        and when the connection is closed (to break up cycles and\n        facilitate garbage collection in cpython).\n        \"\"\"\n        self._request_finished = False\n        self._write_callback = None\n        self._close_callback = None\n\n    def set_close_callback(self, callback):\n        \"\"\"Sets a callback that will be run when the connection is closed.\n\n        Use this instead of accessing\n        `HTTPConnection.stream.set_close_callback\n        <.BaseIOStream.set_close_callback>` directly (which was the\n        recommended approach prior to Tornado 3.0).\n        \"\"\"\n        self._close_callback = stack_context.wrap(callback)\n\n    def _on_connection_close(self):\n        if self._close_callback is not None:\n            callback = self._close_callback\n            self._close_callback = None\n            callback()\n        if self._finish_future is not None and not self._finish_future.done():\n            self._finish_future.set_result(None)\n        # Delete any unfinished callbacks to break up reference cycles.\n        self._clear_request_state()\n\n    def close(self):\n        self.stream.close()\n        # Remove this reference to self, which would otherwise cause a\n        # cycle and delay garbage collection of this connection.\n        self._clear_request_state()\n\n    def write(self, chunk, callback=None):\n        \"\"\"Writes a chunk of output to the stream.\"\"\"\n        if not self.stream.closed():\n            self._write_callback = stack_context.wrap(callback)\n            self.stream.write(chunk, self._on_write_complete)\n\n    def finish(self):\n        \"\"\"Finishes the request.\"\"\"\n        self._request_finished = True\n        # No more data is coming, so instruct TCP to send any remaining\n        # data immediately instead of waiting for a full packet or ack.\n        self.stream.set_nodelay(True)\n        if not self.stream.writing():\n            self._finish_request()\n\n    def _on_write_complete(self):\n        if self._write_callback is not None:\n            callback = self._write_callback\n            self._write_callback = None\n            callback()\n        # _on_write_complete is enqueued on the IOLoop whenever the\n        # IOStream's write buffer becomes empty, but it's possible for\n        # another callback that runs on the IOLoop before it to\n        # simultaneously write more data and finish the request.  If\n        # there is still data in the IOStream, a future\n        # _on_write_complete will be responsible for calling\n        # _finish_request.\n        if self._request_finished and not self.stream.writing():\n            self._finish_request()\n\n    def _can_keep_alive(self, start_line, headers):\n        if self.no_keep_alive:\n            return False\n        connection_header = headers.get(\"Connection\")\n        if connection_header is not None:\n            connection_header = connection_header.lower()\n        if start_line.endswith(\"HTTP/1.1\"):\n            return connection_header != \"close\"\n        elif (\"Content-Length\" in headers\n              or start_line.startswith((\"HEAD \", \"GET \"))):\n            return connection_header == \"keep-alive\"\n        return False\n\n    def _finish_request(self):\n        self._clear_request_state()\n        if self._disconnect_on_finish:\n            self.close()\n            return\n        # Turn Nagle's algorithm back on, leaving the stream in its\n        # default state for the next request.\n        self.stream.set_nodelay(False)\n        self._finish_future.set_result(None)\n\n    def _parse_headers(self, data):\n        data = native_str(data.decode('latin1'))\n        eol = data.find(\"\\r\\n\")\n        start_line = data[:eol]\n        try:\n            headers = httputil.HTTPHeaders.parse(data[eol:])\n        except ValueError:\n            # probably form split() if there was no ':' in the line\n            raise httputil.BadRequestException(\"Malformed HTTP headers\")\n        return start_line, headers\n\n    def _read_body(self, is_client, headers, delegate):\n        content_length = headers.get(\"Content-Length\")\n        if content_length:\n            content_length = int(content_length)\n            if content_length > self.stream.max_buffer_size:\n                raise httputil.BadRequestException(\"Content-Length too long\")\n            return self._read_fixed_body(content_length, delegate)\n        if headers.get(\"Transfer-Encoding\") == \"chunked\":\n            return self._read_chunked_body(delegate)\n        if is_client:\n            return self._read_body_until_close(delegate)\n        return None\n\n    @gen.coroutine\n    def _read_fixed_body(self, content_length, delegate):\n        body = yield self.stream.read_bytes(content_length)\n        delegate.data_received(body)\n\n    @gen.coroutine\n    def _read_chunked_body(self, delegate):\n        # TODO: \"chunk extensions\" http://tools.ietf.org/html/rfc2616#section-3.6.1\n        while True:\n            chunk_len = yield self.stream.read_until(b\"\\r\\n\")\n            chunk_len = int(chunk_len.strip(), 16)\n            if chunk_len == 0:\n                return\n            # chunk ends with \\r\\n\n            chunk = yield self.stream.read_bytes(chunk_len + 2)\n            assert chunk[-2:] == b\"\\r\\n\"\n            delegate.data_received(chunk[:-2])\n\n    @gen.coroutine\n    def _read_body_until_close(self, delegate):\n        body = yield self.stream.read_until_close()\n        delegate.data_received(body)\n",
      "file_patch": "@@ -31,15 +31,13 @@ class HTTP1Connection(object):\n     We parse HTTP headers and bodies, and execute the request callback\n     until the HTTP conection is closed.\n     \"\"\"\n-    def __init__(self, stream, address, delegate, no_keep_alive=False,\n-                 protocol=None):\n+    def __init__(self, stream, address, no_keep_alive=False, protocol=None):\n         self.stream = stream\n         self.address = address\n         # Save the socket's address family now so we know how to\n         # interpret self.address even after the stream is closed\n         # and its socket attribute replaced with None.\n         self.address_family = stream.socket.family\n-        self.delegate = delegate\n         self.no_keep_alive = no_keep_alive\n         if protocol:\n             self.protocol = protocol\n@@ -51,34 +49,65 @@ class HTTP1Connection(object):\n         self._clear_request_state()\n         self.stream.set_close_callback(self._on_connection_close)\n         self._finish_future = None\n+\n+    def start_serving(self, delegate):\n+        assert isinstance(delegate, httputil.HTTPConnectionDelegate)\n         # Register the future on the IOLoop so its errors get logged.\n-        stream.io_loop.add_future(self._process_requests(),\n-                                  lambda f: f.result())\n+        self.stream.io_loop.add_future(self._process_requests(delegate),\n+                                       lambda f: f.result())\n \n     @gen.coroutine\n-    def _process_requests(self):\n+    def _process_requests(self, delegate):\n         while True:\n+            request_delegate = delegate.start_request(self)\n             try:\n-                header_data = yield self.stream.read_until(b\"\\r\\n\\r\\n\")\n-                request_delegate = self.delegate.start_request(self)\n-                self._finish_future = Future()\n-                start_line, headers = self._parse_headers(header_data)\n-                self._disconnect_on_finish = not self._can_keep_alive(\n-                    start_line, headers)\n-                request_delegate.headers_received(start_line, headers)\n-                body_future = self._read_body(headers)\n-                if body_future is not None:\n-                    request_delegate.data_received((yield body_future))\n-                request_delegate.finish()\n-                yield self._finish_future\n-            except httputil.BadRequestException as e:\n-                gen_log.info(\"Malformed HTTP request from %r: %s\",\n-                             self.address, e)\n-                self.close()\n-                return\n+                ret = yield self._process_message(request_delegate, False)\n             except iostream.StreamClosedError:\n                 self.close()\n                 return\n+            if not ret:\n+                return\n+\n+    def process_response(self, delegate, method):\n+        return self._process_message(delegate, True, method=method)\n+\n+    @gen.coroutine\n+    def _process_message(self, delegate, is_client, method=None):\n+        assert isinstance(delegate, httputil.HTTPStreamDelegate)\n+        try:\n+            header_data = yield self.stream.read_until_regex(b\"\\r?\\n\\r?\\n\")\n+            self._finish_future = Future()\n+            start_line, headers = self._parse_headers(header_data)\n+            self._disconnect_on_finish = not self._can_keep_alive(\n+                start_line, headers)\n+            ret = delegate.headers_received(start_line, headers)\n+            # TODO: finalize the 'detach' interface.\n+            if ret == 'detach':\n+                return\n+            skip_body = False\n+            if is_client:\n+                if method == 'HEAD':\n+                    skip_body = True\n+                code = httputil.parse_response_start_line(start_line).code\n+                if code == 304:\n+                    skip_body = True\n+                if code >= 100 and code < 200:\n+                    yield self._process_message(delegate, is_client, method=method)\n+            else:\n+                if headers.get(\"Expect\") == \"100-continue\":\n+                    self.stream.write(b\"HTTP/1.1 100 (Continue)\\r\\n\\r\\n\")\n+            if not skip_body:\n+                body_future = self._read_body(is_client, headers, delegate)\n+                if body_future is not None:\n+                    yield body_future\n+            delegate.finish()\n+            yield self._finish_future\n+        except httputil.BadRequestException as e:\n+            gen_log.info(\"Malformed HTTP request from %r: %s\",\n+                         self.address, e)\n+            self.close()\n+            raise gen.Return(False)\n+        raise gen.Return(True)\n \n \n     def _clear_request_state(self):\n@@ -183,13 +212,38 @@ class HTTP1Connection(object):\n             raise httputil.BadRequestException(\"Malformed HTTP headers\")\n         return start_line, headers\n \n-    def _read_body(self, headers):\n+    def _read_body(self, is_client, headers, delegate):\n         content_length = headers.get(\"Content-Length\")\n         if content_length:\n             content_length = int(content_length)\n             if content_length > self.stream.max_buffer_size:\n                 raise httputil.BadRequestException(\"Content-Length too long\")\n-            if headers.get(\"Expect\") == \"100-continue\":\n-                self.stream.write(b\"HTTP/1.1 100 (Continue)\\r\\n\\r\\n\")\n-            return self.stream.read_bytes(content_length)\n+            return self._read_fixed_body(content_length, delegate)\n+        if headers.get(\"Transfer-Encoding\") == \"chunked\":\n+            return self._read_chunked_body(delegate)\n+        if is_client:\n+            return self._read_body_until_close(delegate)\n         return None\n+\n+    @gen.coroutine\n+    def _read_fixed_body(self, content_length, delegate):\n+        body = yield self.stream.read_bytes(content_length)\n+        delegate.data_received(body)\n+\n+    @gen.coroutine\n+    def _read_chunked_body(self, delegate):\n+        # TODO: \"chunk extensions\" http://tools.ietf.org/html/rfc2616#section-3.6.1\n+        while True:\n+            chunk_len = yield self.stream.read_until(b\"\\r\\n\")\n+            chunk_len = int(chunk_len.strip(), 16)\n+            if chunk_len == 0:\n+                return\n+            # chunk ends with \\r\\n\n+            chunk = yield self.stream.read_bytes(chunk_len + 2)\n+            assert chunk[-2:] == b\"\\r\\n\"\n+            delegate.data_received(chunk[:-2])\n+\n+    @gen.coroutine\n+    def _read_body_until_close(self, delegate):\n+        body = yield self.stream.read_until_close()\n+        delegate.data_received(body)\n",
      "files_name_in_blame_commit": [
        "httpserver.py",
        "simple_httpclient.py",
        "httpserver_test.py",
        "websocket.py",
        "http1connection.py",
        "simple_httpclient_test.py",
        "httputil.py"
      ]
    }
  },
  "commits_modify_file_before_fix": {
    "size": 69
  },
  "recursive_blame_commits": {
    "recursive_blame_function_lines": {
      "186": {
        "commit_id": "5c283f9c6bf6c91d65be65a09dbada5a0116e2ad",
        "line_code": "    def _read_body(self, headers):",
        "commit_date": "2014-02-23 16:39:29",
        "valid": 1
      },
      "187": {
        "commit_id": "656f78e18a14aa1b3a018f6922bf40b1204e0e22",
        "line_code": "        content_length = headers.get(\"Content-Length\")",
        "commit_date": "2014-02-23 12:50:10",
        "valid": 1
      },
      "188": {
        "commit_id": "656f78e18a14aa1b3a018f6922bf40b1204e0e22",
        "line_code": "        if content_length:",
        "commit_date": "2014-02-23 12:50:10",
        "valid": 1
      },
      "189": {
        "commit_id": "656f78e18a14aa1b3a018f6922bf40b1204e0e22",
        "line_code": "            content_length = int(content_length)",
        "commit_date": "2014-02-23 12:50:10",
        "valid": 1
      },
      "190": {
        "commit_id": "656f78e18a14aa1b3a018f6922bf40b1204e0e22",
        "line_code": "            if content_length > self.stream.max_buffer_size:",
        "commit_date": "2014-02-23 12:50:10",
        "valid": 1
      },
      "191": {
        "commit_id": "e564ddbe32bb2b250beef44eca3bdeb03fe4838d",
        "line_code": "                raise httputil.BadRequestException(\"Content-Length too long\")",
        "commit_date": "2014-02-24 00:53:53",
        "valid": 1
      },
      "192": {
        "commit_id": "656f78e18a14aa1b3a018f6922bf40b1204e0e22",
        "line_code": "            if headers.get(\"Expect\") == \"100-continue\":",
        "commit_date": "2014-02-23 12:50:10",
        "valid": 1
      },
      "193": {
        "commit_id": "656f78e18a14aa1b3a018f6922bf40b1204e0e22",
        "line_code": "                self.stream.write(b\"HTTP/1.1 100 (Continue)\\r\\n\\r\\n\")",
        "commit_date": "2014-02-23 12:50:10",
        "valid": 1
      },
      "194": {
        "commit_id": "5c283f9c6bf6c91d65be65a09dbada5a0116e2ad",
        "line_code": "            return self.stream.read_bytes(content_length)",
        "commit_date": "2014-02-23 16:39:29",
        "valid": 1
      },
      "195": {
        "commit_id": "5c283f9c6bf6c91d65be65a09dbada5a0116e2ad",
        "line_code": "        return None",
        "commit_date": "2014-02-23 16:39:29",
        "valid": 1
      }
    },
    "commits": {
      "e564ddbe32bb2b250beef44eca3bdeb03fe4838d": {
        "commit": {
          "commit_id": "e564ddbe32bb2b250beef44eca3bdeb03fe4838d",
          "commit_message": "Move HTTPServerRequest-specific logic from http1connection to httpserver.",
          "commit_author": "Ben Darnell",
          "commit_date": "2014-02-24 00:53:53",
          "commit_parent": "5c283f9c6bf6c91d65be65a09dbada5a0116e2ad"
        },
        "function": {
          "function_name": "_read_body",
          "function_code_before": "def _read_body(self, headers):\n    content_length = headers.get('Content-Length')\n    if content_length:\n        content_length = int(content_length)\n        if content_length > self.stream.max_buffer_size:\n            raise _BadRequestException('Content-Length too long')\n        if headers.get('Expect') == '100-continue':\n            self.stream.write(b'HTTP/1.1 100 (Continue)\\r\\n\\r\\n')\n        return self.stream.read_bytes(content_length)\n    return None",
          "function_code_after": "def _read_body(self, headers):\n    content_length = headers.get('Content-Length')\n    if content_length:\n        content_length = int(content_length)\n        if content_length > self.stream.max_buffer_size:\n            raise httputil.BadRequestException('Content-Length too long')\n        if headers.get('Expect') == '100-continue':\n            self.stream.write(b'HTTP/1.1 100 (Continue)\\r\\n\\r\\n')\n        return self.stream.read_bytes(content_length)\n    return None",
          "function_before_start_line": 228,
          "function_before_end_line": 237,
          "function_after_start_line": 186,
          "function_after_end_line": 195,
          "function_before_token_count": 68,
          "function_after_token_count": 70,
          "functions_name_modified_file": [
            "__init__",
            "_process_requests",
            "close",
            "write",
            "_can_keep_alive",
            "_on_write_complete",
            "_finish_request",
            "_read_body",
            "_parse_headers",
            "_clear_request_state",
            "_on_connection_close",
            "set_close_callback",
            "finish"
          ],
          "functions_name_all_files": [
            "full_url",
            "_parseparam",
            "_int_or_none",
            "_parse_header",
            "request_time",
            "url_concat",
            "write",
            "_read_body",
            "get_all",
            "headers_received",
            "__delitem__",
            "__contains__",
            "add",
            "_parse_request_range",
            "parse_body_arguments",
            "_process_requests",
            "close",
            "get_list",
            "supports_http_1_1",
            "doctests",
            "get",
            "parse_line",
            "_get_content_range",
            "__repr__",
            "set_close_callback",
            "__getitem__",
            "__init__",
            "update",
            "_can_keep_alive",
            "parse",
            "format_timestamp",
            "_finish_request",
            "cookies",
            "_parse_headers",
            "_clear_request_state",
            "copy",
            "_on_connection_close",
            "get_ssl_certificate",
            "handle_stream",
            "__setitem__",
            "start_request",
            "parse_multipart_form_data",
            "data_received",
            "_on_write_complete",
            "__missing__",
            "finish"
          ],
          "functions_name_co_evolved_modified_file": [
            "__init__",
            "_process_requests",
            "_can_keep_alive",
            "_make_request",
            "_finish_request",
            "_parse_headers",
            "_clear_request_state",
            "_parse_body"
          ],
          "functions_name_co_evolved_all_files": [
            "__init__",
            "_process_requests",
            "_can_keep_alive",
            "_make_request",
            "_finish_request",
            "_parse_headers",
            "_clear_request_state",
            "headers_received",
            "handle_stream",
            "start_request",
            "data_received",
            "_parse_body",
            "finish"
          ]
        },
        "file": {
          "file_name": "http1connection.py",
          "file_nloc": 128,
          "file_complexity": 37,
          "file_token_count": 808,
          "file_before": "#!/usr/bin/env python\n#\n# Copyright 2014 Facebook\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"); you may\n# not use this file except in compliance with the License. You may obtain\n# a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n# License for the specific language governing permissions and limitations\n# under the License.\n\nfrom __future__ import absolute_import, division, print_function, with_statement\n\nimport socket\n\nfrom tornado.concurrent import Future\nfrom tornado.escape import native_str\nfrom tornado import gen\nfrom tornado import httputil\nfrom tornado import iostream\nfrom tornado.log import gen_log\nfrom tornado import netutil\nfrom tornado import stack_context\n\n\nclass _BadRequestException(Exception):\n    \"\"\"Exception class for malformed HTTP requests.\"\"\"\n    pass\n\n\nclass HTTP1Connection(object):\n    \"\"\"Handles a connection to an HTTP client, executing HTTP requests.\n\n    We parse HTTP headers and bodies, and execute the request callback\n    until the HTTP conection is closed.\n    \"\"\"\n    def __init__(self, stream, address, request_callback, no_keep_alive=False,\n                 xheaders=False, protocol=None):\n        self.stream = stream\n        self.address = address\n        # Save the socket's address family now so we know how to\n        # interpret self.address even after the stream is closed\n        # and its socket attribute replaced with None.\n        self.address_family = stream.socket.family\n        self.request_callback = request_callback\n        self.no_keep_alive = no_keep_alive\n        self.xheaders = xheaders\n        if protocol:\n            self.protocol = protocol\n        elif isinstance(stream, iostream.SSLIOStream):\n            self.protocol = \"https\"\n        else:\n            self.protocol = \"http\"\n        self._clear_request_state()\n        self.stream.set_close_callback(self._on_connection_close)\n        self._finish_future = None\n        # Register the future on the IOLoop so its errors get logged.\n        stream.io_loop.add_future(self._process_requests(),\n                                  lambda f: f.result())\n\n    @gen.coroutine\n    def _process_requests(self):\n        while True:\n            try:\n                header_data = yield self.stream.read_until(b\"\\r\\n\\r\\n\")\n                self._finish_future = Future()\n                start_line, headers = self._parse_headers(header_data)\n                request = self._make_request(start_line, headers)\n                self._request = request\n                body_future = self._read_body(headers)\n                if body_future is not None:\n                    request.body = yield body_future\n                self._parse_body(request)\n                self.request_callback(request)\n                yield self._finish_future\n            except _BadRequestException as e:\n                gen_log.info(\"Malformed HTTP request from %r: %s\",\n                             self.address, e)\n                self.close()\n                return\n            except iostream.StreamClosedError:\n                self.close()\n                return\n\n\n    def _clear_request_state(self):\n        \"\"\"Clears the per-request state.\n\n        This is run in between requests to allow the previous handler\n        to be garbage collected (and prevent spurious close callbacks),\n        and when the connection is closed (to break up cycles and\n        facilitate garbage collection in cpython).\n        \"\"\"\n        self._request = None\n        self._request_finished = False\n        self._write_callback = None\n        self._close_callback = None\n\n    def set_close_callback(self, callback):\n        \"\"\"Sets a callback that will be run when the connection is closed.\n\n        Use this instead of accessing\n        `HTTPConnection.stream.set_close_callback\n        <.BaseIOStream.set_close_callback>` directly (which was the\n        recommended approach prior to Tornado 3.0).\n        \"\"\"\n        self._close_callback = stack_context.wrap(callback)\n\n    def _on_connection_close(self):\n        if self._close_callback is not None:\n            callback = self._close_callback\n            self._close_callback = None\n            callback()\n        if self._finish_future is not None and not self._finish_future.done():\n            self._finish_future.set_result(None)\n        # Delete any unfinished callbacks to break up reference cycles.\n        self._clear_request_state()\n\n    def close(self):\n        self.stream.close()\n        # Remove this reference to self, which would otherwise cause a\n        # cycle and delay garbage collection of this connection.\n        self._clear_request_state()\n\n    def write(self, chunk, callback=None):\n        \"\"\"Writes a chunk of output to the stream.\"\"\"\n        if not self.stream.closed():\n            self._write_callback = stack_context.wrap(callback)\n            self.stream.write(chunk, self._on_write_complete)\n\n    def finish(self):\n        \"\"\"Finishes the request.\"\"\"\n        self._request_finished = True\n        # No more data is coming, so instruct TCP to send any remaining\n        # data immediately instead of waiting for a full packet or ack.\n        self.stream.set_nodelay(True)\n        if not self.stream.writing():\n            self._finish_request()\n\n    def _on_write_complete(self):\n        if self._write_callback is not None:\n            callback = self._write_callback\n            self._write_callback = None\n            callback()\n        # _on_write_complete is enqueued on the IOLoop whenever the\n        # IOStream's write buffer becomes empty, but it's possible for\n        # another callback that runs on the IOLoop before it to\n        # simultaneously write more data and finish the request.  If\n        # there is still data in the IOStream, a future\n        # _on_write_complete will be responsible for calling\n        # _finish_request.\n        if self._request_finished and not self.stream.writing():\n            self._finish_request()\n\n    def _finish_request(self):\n        if self.no_keep_alive or self._request is None:\n            disconnect = True\n        else:\n            connection_header = self._request.headers.get(\"Connection\")\n            if connection_header is not None:\n                connection_header = connection_header.lower()\n            if self._request.supports_http_1_1():\n                disconnect = connection_header == \"close\"\n            elif (\"Content-Length\" in self._request.headers\n                    or self._request.method in (\"HEAD\", \"GET\")):\n                disconnect = connection_header != \"keep-alive\"\n            else:\n                disconnect = True\n        self._clear_request_state()\n        if disconnect:\n            self.close()\n            return\n        # Turn Nagle's algorithm back on, leaving the stream in its\n        # default state for the next request.\n        self.stream.set_nodelay(False)\n        self._finish_future.set_result(None)\n\n    def _parse_headers(self, data):\n        data = native_str(data.decode('latin1'))\n        eol = data.find(\"\\r\\n\")\n        start_line = data[:eol]\n        try:\n            headers = httputil.HTTPHeaders.parse(data[eol:])\n        except ValueError:\n            # probably form split() if there was no ':' in the line\n            raise _BadRequestException(\"Malformed HTTP headers\")\n        return start_line, headers\n\n    def _make_request(self, start_line, headers):\n        try:\n            method, uri, version = start_line.split(\" \")\n        except ValueError:\n            raise _BadRequestException(\"Malformed HTTP request line\")\n        if not version.startswith(\"HTTP/\"):\n            raise _BadRequestException(\"Malformed HTTP version in HTTP Request-Line\")\n        # HTTPRequest wants an IP, not a full socket address\n        if self.address_family in (socket.AF_INET, socket.AF_INET6):\n            remote_ip = self.address[0]\n        else:\n            # Unix (or other) socket; fake the remote address\n            remote_ip = '0.0.0.0'\n\n        protocol = self.protocol\n\n        # xheaders can override the defaults\n        if self.xheaders:\n            # Squid uses X-Forwarded-For, others use X-Real-Ip\n            ip = headers.get(\"X-Forwarded-For\", remote_ip)\n            ip = ip.split(',')[-1].strip()\n            ip = headers.get(\"X-Real-Ip\", ip)\n            if netutil.is_valid_ip(ip):\n                remote_ip = ip\n            # AWS uses X-Forwarded-Proto\n            proto_header = headers.get(\n                \"X-Scheme\", headers.get(\"X-Forwarded-Proto\", self.protocol))\n            if proto_header in (\"http\", \"https\"):\n                protocol = proto_header\n\n        return httputil.HTTPServerRequest(\n            connection=self, method=method, uri=uri, version=version,\n            headers=headers, remote_ip=remote_ip, protocol=protocol)\n\n    def _read_body(self, headers):\n        content_length = headers.get(\"Content-Length\")\n        if content_length:\n            content_length = int(content_length)\n            if content_length > self.stream.max_buffer_size:\n                raise _BadRequestException(\"Content-Length too long\")\n            if headers.get(\"Expect\") == \"100-continue\":\n                self.stream.write(b\"HTTP/1.1 100 (Continue)\\r\\n\\r\\n\")\n            return self.stream.read_bytes(content_length)\n        return None\n\n    def _parse_body(self, request):\n        if self._request.method in (\"POST\", \"PATCH\", \"PUT\"):\n            httputil.parse_body_arguments(\n                self._request.headers.get(\"Content-Type\", \"\"), request.body,\n                self._request.body_arguments, self._request.files)\n\n            for k, v in self._request.body_arguments.items():\n                self._request.arguments.setdefault(k, []).extend(v)\n",
          "file_after": "#!/usr/bin/env python\n#\n# Copyright 2014 Facebook\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"); you may\n# not use this file except in compliance with the License. You may obtain\n# a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n# License for the specific language governing permissions and limitations\n# under the License.\n\nfrom __future__ import absolute_import, division, print_function, with_statement\n\nfrom tornado.concurrent import Future\nfrom tornado.escape import native_str\nfrom tornado import gen\nfrom tornado import httputil\nfrom tornado import iostream\nfrom tornado.log import gen_log\nfrom tornado import stack_context\n\n\nclass HTTP1Connection(object):\n    \"\"\"Handles a connection to an HTTP client, executing HTTP requests.\n\n    We parse HTTP headers and bodies, and execute the request callback\n    until the HTTP conection is closed.\n    \"\"\"\n    def __init__(self, stream, address, delegate, no_keep_alive=False,\n                 protocol=None):\n        self.stream = stream\n        self.address = address\n        # Save the socket's address family now so we know how to\n        # interpret self.address even after the stream is closed\n        # and its socket attribute replaced with None.\n        self.address_family = stream.socket.family\n        self.delegate = delegate\n        self.no_keep_alive = no_keep_alive\n        if protocol:\n            self.protocol = protocol\n        elif isinstance(stream, iostream.SSLIOStream):\n            self.protocol = \"https\"\n        else:\n            self.protocol = \"http\"\n        self._disconnect_on_finish = False\n        self._clear_request_state()\n        self.stream.set_close_callback(self._on_connection_close)\n        self._finish_future = None\n        # Register the future on the IOLoop so its errors get logged.\n        stream.io_loop.add_future(self._process_requests(),\n                                  lambda f: f.result())\n\n    @gen.coroutine\n    def _process_requests(self):\n        while True:\n            try:\n                header_data = yield self.stream.read_until(b\"\\r\\n\\r\\n\")\n                request_delegate = self.delegate.start_request(self)\n                self._finish_future = Future()\n                start_line, headers = self._parse_headers(header_data)\n                self._disconnect_on_finish = not self._can_keep_alive(\n                    start_line, headers)\n                request_delegate.headers_received(start_line, headers)\n                body_future = self._read_body(headers)\n                if body_future is not None:\n                    request_delegate.data_received((yield body_future))\n                request_delegate.finish()\n                yield self._finish_future\n            except httputil.BadRequestException as e:\n                gen_log.info(\"Malformed HTTP request from %r: %s\",\n                             self.address, e)\n                self.close()\n                return\n            except iostream.StreamClosedError:\n                self.close()\n                return\n\n\n    def _clear_request_state(self):\n        \"\"\"Clears the per-request state.\n\n        This is run in between requests to allow the previous handler\n        to be garbage collected (and prevent spurious close callbacks),\n        and when the connection is closed (to break up cycles and\n        facilitate garbage collection in cpython).\n        \"\"\"\n        self._request_finished = False\n        self._write_callback = None\n        self._close_callback = None\n\n    def set_close_callback(self, callback):\n        \"\"\"Sets a callback that will be run when the connection is closed.\n\n        Use this instead of accessing\n        `HTTPConnection.stream.set_close_callback\n        <.BaseIOStream.set_close_callback>` directly (which was the\n        recommended approach prior to Tornado 3.0).\n        \"\"\"\n        self._close_callback = stack_context.wrap(callback)\n\n    def _on_connection_close(self):\n        if self._close_callback is not None:\n            callback = self._close_callback\n            self._close_callback = None\n            callback()\n        if self._finish_future is not None and not self._finish_future.done():\n            self._finish_future.set_result(None)\n        # Delete any unfinished callbacks to break up reference cycles.\n        self._clear_request_state()\n\n    def close(self):\n        self.stream.close()\n        # Remove this reference to self, which would otherwise cause a\n        # cycle and delay garbage collection of this connection.\n        self._clear_request_state()\n\n    def write(self, chunk, callback=None):\n        \"\"\"Writes a chunk of output to the stream.\"\"\"\n        if not self.stream.closed():\n            self._write_callback = stack_context.wrap(callback)\n            self.stream.write(chunk, self._on_write_complete)\n\n    def finish(self):\n        \"\"\"Finishes the request.\"\"\"\n        self._request_finished = True\n        # No more data is coming, so instruct TCP to send any remaining\n        # data immediately instead of waiting for a full packet or ack.\n        self.stream.set_nodelay(True)\n        if not self.stream.writing():\n            self._finish_request()\n\n    def _on_write_complete(self):\n        if self._write_callback is not None:\n            callback = self._write_callback\n            self._write_callback = None\n            callback()\n        # _on_write_complete is enqueued on the IOLoop whenever the\n        # IOStream's write buffer becomes empty, but it's possible for\n        # another callback that runs on the IOLoop before it to\n        # simultaneously write more data and finish the request.  If\n        # there is still data in the IOStream, a future\n        # _on_write_complete will be responsible for calling\n        # _finish_request.\n        if self._request_finished and not self.stream.writing():\n            self._finish_request()\n\n    def _can_keep_alive(self, start_line, headers):\n        if self.no_keep_alive:\n            return False\n        connection_header = headers.get(\"Connection\")\n        if connection_header is not None:\n            connection_header = connection_header.lower()\n        if start_line.endswith(\"HTTP/1.1\"):\n            return connection_header != \"close\"\n        elif (\"Content-Length\" in headers\n              or start_line.startswith((\"HEAD \", \"GET \"))):\n            return connection_header == \"keep-alive\"\n        return False\n\n    def _finish_request(self):\n        self._clear_request_state()\n        if self._disconnect_on_finish:\n            self.close()\n            return\n        # Turn Nagle's algorithm back on, leaving the stream in its\n        # default state for the next request.\n        self.stream.set_nodelay(False)\n        self._finish_future.set_result(None)\n\n    def _parse_headers(self, data):\n        data = native_str(data.decode('latin1'))\n        eol = data.find(\"\\r\\n\")\n        start_line = data[:eol]\n        try:\n            headers = httputil.HTTPHeaders.parse(data[eol:])\n        except ValueError:\n            # probably form split() if there was no ':' in the line\n            raise httputil.BadRequestException(\"Malformed HTTP headers\")\n        return start_line, headers\n\n    def _read_body(self, headers):\n        content_length = headers.get(\"Content-Length\")\n        if content_length:\n            content_length = int(content_length)\n            if content_length > self.stream.max_buffer_size:\n                raise httputil.BadRequestException(\"Content-Length too long\")\n            if headers.get(\"Expect\") == \"100-continue\":\n                self.stream.write(b\"HTTP/1.1 100 (Continue)\\r\\n\\r\\n\")\n            return self.stream.read_bytes(content_length)\n        return None\n",
          "file_patch": "@@ -16,46 +16,38 @@\n \n from __future__ import absolute_import, division, print_function, with_statement\n \n-import socket\n-\n from tornado.concurrent import Future\n from tornado.escape import native_str\n from tornado import gen\n from tornado import httputil\n from tornado import iostream\n from tornado.log import gen_log\n-from tornado import netutil\n from tornado import stack_context\n \n \n-class _BadRequestException(Exception):\n-    \"\"\"Exception class for malformed HTTP requests.\"\"\"\n-    pass\n-\n-\n class HTTP1Connection(object):\n     \"\"\"Handles a connection to an HTTP client, executing HTTP requests.\n \n     We parse HTTP headers and bodies, and execute the request callback\n     until the HTTP conection is closed.\n     \"\"\"\n-    def __init__(self, stream, address, request_callback, no_keep_alive=False,\n-                 xheaders=False, protocol=None):\n+    def __init__(self, stream, address, delegate, no_keep_alive=False,\n+                 protocol=None):\n         self.stream = stream\n         self.address = address\n         # Save the socket's address family now so we know how to\n         # interpret self.address even after the stream is closed\n         # and its socket attribute replaced with None.\n         self.address_family = stream.socket.family\n-        self.request_callback = request_callback\n+        self.delegate = delegate\n         self.no_keep_alive = no_keep_alive\n-        self.xheaders = xheaders\n         if protocol:\n             self.protocol = protocol\n         elif isinstance(stream, iostream.SSLIOStream):\n             self.protocol = \"https\"\n         else:\n             self.protocol = \"http\"\n+        self._disconnect_on_finish = False\n         self._clear_request_state()\n         self.stream.set_close_callback(self._on_connection_close)\n         self._finish_future = None\n@@ -68,17 +60,18 @@ class HTTP1Connection(object):\n         while True:\n             try:\n                 header_data = yield self.stream.read_until(b\"\\r\\n\\r\\n\")\n+                request_delegate = self.delegate.start_request(self)\n                 self._finish_future = Future()\n                 start_line, headers = self._parse_headers(header_data)\n-                request = self._make_request(start_line, headers)\n-                self._request = request\n+                self._disconnect_on_finish = not self._can_keep_alive(\n+                    start_line, headers)\n+                request_delegate.headers_received(start_line, headers)\n                 body_future = self._read_body(headers)\n                 if body_future is not None:\n-                    request.body = yield body_future\n-                self._parse_body(request)\n-                self.request_callback(request)\n+                    request_delegate.data_received((yield body_future))\n+                request_delegate.finish()\n                 yield self._finish_future\n-            except _BadRequestException as e:\n+            except httputil.BadRequestException as e:\n                 gen_log.info(\"Malformed HTTP request from %r: %s\",\n                              self.address, e)\n                 self.close()\n@@ -96,7 +89,6 @@ class HTTP1Connection(object):\n         and when the connection is closed (to break up cycles and\n         facilitate garbage collection in cpython).\n         \"\"\"\n-        self._request = None\n         self._request_finished = False\n         self._write_callback = None\n         self._close_callback = None\n@@ -157,22 +149,22 @@ class HTTP1Connection(object):\n         if self._request_finished and not self.stream.writing():\n             self._finish_request()\n \n+    def _can_keep_alive(self, start_line, headers):\n+        if self.no_keep_alive:\n+            return False\n+        connection_header = headers.get(\"Connection\")\n+        if connection_header is not None:\n+            connection_header = connection_header.lower()\n+        if start_line.endswith(\"HTTP/1.1\"):\n+            return connection_header != \"close\"\n+        elif (\"Content-Length\" in headers\n+              or start_line.startswith((\"HEAD \", \"GET \"))):\n+            return connection_header == \"keep-alive\"\n+        return False\n+\n     def _finish_request(self):\n-        if self.no_keep_alive or self._request is None:\n-            disconnect = True\n-        else:\n-            connection_header = self._request.headers.get(\"Connection\")\n-            if connection_header is not None:\n-                connection_header = connection_header.lower()\n-            if self._request.supports_http_1_1():\n-                disconnect = connection_header == \"close\"\n-            elif (\"Content-Length\" in self._request.headers\n-                    or self._request.method in (\"HEAD\", \"GET\")):\n-                disconnect = connection_header != \"keep-alive\"\n-            else:\n-                disconnect = True\n         self._clear_request_state()\n-        if disconnect:\n+        if self._disconnect_on_finish:\n             self.close()\n             return\n         # Turn Nagle's algorithm back on, leaving the stream in its\n@@ -188,59 +180,16 @@ class HTTP1Connection(object):\n             headers = httputil.HTTPHeaders.parse(data[eol:])\n         except ValueError:\n             # probably form split() if there was no ':' in the line\n-            raise _BadRequestException(\"Malformed HTTP headers\")\n+            raise httputil.BadRequestException(\"Malformed HTTP headers\")\n         return start_line, headers\n \n-    def _make_request(self, start_line, headers):\n-        try:\n-            method, uri, version = start_line.split(\" \")\n-        except ValueError:\n-            raise _BadRequestException(\"Malformed HTTP request line\")\n-        if not version.startswith(\"HTTP/\"):\n-            raise _BadRequestException(\"Malformed HTTP version in HTTP Request-Line\")\n-        # HTTPRequest wants an IP, not a full socket address\n-        if self.address_family in (socket.AF_INET, socket.AF_INET6):\n-            remote_ip = self.address[0]\n-        else:\n-            # Unix (or other) socket; fake the remote address\n-            remote_ip = '0.0.0.0'\n-\n-        protocol = self.protocol\n-\n-        # xheaders can override the defaults\n-        if self.xheaders:\n-            # Squid uses X-Forwarded-For, others use X-Real-Ip\n-            ip = headers.get(\"X-Forwarded-For\", remote_ip)\n-            ip = ip.split(',')[-1].strip()\n-            ip = headers.get(\"X-Real-Ip\", ip)\n-            if netutil.is_valid_ip(ip):\n-                remote_ip = ip\n-            # AWS uses X-Forwarded-Proto\n-            proto_header = headers.get(\n-                \"X-Scheme\", headers.get(\"X-Forwarded-Proto\", self.protocol))\n-            if proto_header in (\"http\", \"https\"):\n-                protocol = proto_header\n-\n-        return httputil.HTTPServerRequest(\n-            connection=self, method=method, uri=uri, version=version,\n-            headers=headers, remote_ip=remote_ip, protocol=protocol)\n-\n     def _read_body(self, headers):\n         content_length = headers.get(\"Content-Length\")\n         if content_length:\n             content_length = int(content_length)\n             if content_length > self.stream.max_buffer_size:\n-                raise _BadRequestException(\"Content-Length too long\")\n+                raise httputil.BadRequestException(\"Content-Length too long\")\n             if headers.get(\"Expect\") == \"100-continue\":\n                 self.stream.write(b\"HTTP/1.1 100 (Continue)\\r\\n\\r\\n\")\n             return self.stream.read_bytes(content_length)\n         return None\n-\n-    def _parse_body(self, request):\n-        if self._request.method in (\"POST\", \"PATCH\", \"PUT\"):\n-            httputil.parse_body_arguments(\n-                self._request.headers.get(\"Content-Type\", \"\"), request.body,\n-                self._request.body_arguments, self._request.files)\n-\n-            for k, v in self._request.body_arguments.items():\n-                self._request.arguments.setdefault(k, []).extend(v)\n",
          "files_name_in_blame_commit": [
            "http1connection.py",
            "httputil.py",
            "httpserver.py"
          ]
        }
      },
      "5c283f9c6bf6c91d65be65a09dbada5a0116e2ad": {
        "commit": {
          "commit_id": "5c283f9c6bf6c91d65be65a09dbada5a0116e2ad",
          "commit_message": "Refactor HTTP1Connection to use coroutines.",
          "commit_author": "Ben Darnell",
          "commit_date": "2014-02-23 16:39:29",
          "commit_parent": "5d7e37c0c322152805c0d033915b2d309d16a199"
        },
        "function": {
          "function_name": "_read_body",
          "function_code_before": "",
          "function_code_after": "def _read_body(self, headers):\n    content_length = headers.get('Content-Length')\n    if content_length:\n        content_length = int(content_length)\n        if content_length > self.stream.max_buffer_size:\n            raise _BadRequestException('Content-Length too long')\n        if headers.get('Expect') == '100-continue':\n            self.stream.write(b'HTTP/1.1 100 (Continue)\\r\\n\\r\\n')\n        return self.stream.read_bytes(content_length)\n    return None",
          "function_before_start_line": "",
          "function_before_end_line": "",
          "function_after_start_line": 228,
          "function_after_end_line": 237,
          "function_before_token_count": 0,
          "function_after_token_count": 68,
          "functions_name_modified_file": [
            "__init__",
            "_process_requests",
            "close",
            "write",
            "_on_write_complete",
            "_make_request",
            "_finish_request",
            "_read_body",
            "_parse_headers",
            "_clear_request_state",
            "_on_connection_close",
            "_parse_body",
            "set_close_callback",
            "finish"
          ],
          "functions_name_all_files": [
            "__init__",
            "_process_requests",
            "close",
            "write",
            "_on_write_complete",
            "_make_request",
            "_finish_request",
            "_read_body",
            "_parse_headers",
            "_clear_request_state",
            "_on_connection_close",
            "_parse_body",
            "set_close_callback",
            "finish"
          ],
          "functions_name_co_evolved_modified_file": [
            "__init__",
            "close",
            "_process_requests",
            "_make_request",
            "_finish_request",
            "_on_request_body",
            "_parse_headers",
            "_on_headers",
            "_on_connection_close",
            "_parse_body"
          ],
          "functions_name_co_evolved_all_files": [
            "__init__",
            "close",
            "_process_requests",
            "_make_request",
            "_finish_request",
            "_on_request_body",
            "_parse_headers",
            "_on_headers",
            "_on_connection_close",
            "_parse_body"
          ]
        },
        "file": {
          "file_name": "http1connection.py",
          "file_nloc": 166,
          "file_complexity": 47,
          "file_token_count": 1109,
          "file_before": "#!/usr/bin/env python\n#\n# Copyright 2014 Facebook\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"); you may\n# not use this file except in compliance with the License. You may obtain\n# a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n# License for the specific language governing permissions and limitations\n# under the License.\n\nfrom __future__ import absolute_import, division, print_function, with_statement\n\nimport socket\n\nfrom tornado.escape import native_str\nfrom tornado import httputil\nfrom tornado import iostream\nfrom tornado.log import gen_log\nfrom tornado import netutil\nfrom tornado import stack_context\n\n\nclass _BadRequestException(Exception):\n    \"\"\"Exception class for malformed HTTP requests.\"\"\"\n    pass\n\n\nclass HTTP1Connection(object):\n    \"\"\"Handles a connection to an HTTP client, executing HTTP requests.\n\n    We parse HTTP headers and bodies, and execute the request callback\n    until the HTTP conection is closed.\n    \"\"\"\n    def __init__(self, stream, address, request_callback, no_keep_alive=False,\n                 xheaders=False, protocol=None):\n        self.stream = stream\n        self.address = address\n        # Save the socket's address family now so we know how to\n        # interpret self.address even after the stream is closed\n        # and its socket attribute replaced with None.\n        self.address_family = stream.socket.family\n        self.request_callback = request_callback\n        self.no_keep_alive = no_keep_alive\n        self.xheaders = xheaders\n        if protocol:\n            self.protocol = protocol\n        elif isinstance(stream, iostream.SSLIOStream):\n            self.protocol = \"https\"\n        else:\n            self.protocol = \"http\"\n        self._clear_request_state()\n        # Save stack context here, outside of any request.  This keeps\n        # contexts from one request from leaking into the next.\n        self._header_callback = stack_context.wrap(self._on_headers)\n        self.stream.set_close_callback(self._on_connection_close)\n        self.stream.read_until(b\"\\r\\n\\r\\n\", self._header_callback)\n\n    def _clear_request_state(self):\n        \"\"\"Clears the per-request state.\n\n        This is run in between requests to allow the previous handler\n        to be garbage collected (and prevent spurious close callbacks),\n        and when the connection is closed (to break up cycles and\n        facilitate garbage collection in cpython).\n        \"\"\"\n        self._request = None\n        self._request_finished = False\n        self._write_callback = None\n        self._close_callback = None\n\n    def set_close_callback(self, callback):\n        \"\"\"Sets a callback that will be run when the connection is closed.\n\n        Use this instead of accessing\n        `HTTPConnection.stream.set_close_callback\n        <.BaseIOStream.set_close_callback>` directly (which was the\n        recommended approach prior to Tornado 3.0).\n        \"\"\"\n        self._close_callback = stack_context.wrap(callback)\n\n    def _on_connection_close(self):\n        if self._close_callback is not None:\n            callback = self._close_callback\n            self._close_callback = None\n            callback()\n        # Delete any unfinished callbacks to break up reference cycles.\n        self._header_callback = None\n        self._clear_request_state()\n\n    def close(self):\n        self.stream.close()\n        # Remove this reference to self, which would otherwise cause a\n        # cycle and delay garbage collection of this connection.\n        self._header_callback = None\n        self._clear_request_state()\n\n    def write(self, chunk, callback=None):\n        \"\"\"Writes a chunk of output to the stream.\"\"\"\n        if not self.stream.closed():\n            self._write_callback = stack_context.wrap(callback)\n            self.stream.write(chunk, self._on_write_complete)\n\n    def finish(self):\n        \"\"\"Finishes the request.\"\"\"\n        self._request_finished = True\n        # No more data is coming, so instruct TCP to send any remaining\n        # data immediately instead of waiting for a full packet or ack.\n        self.stream.set_nodelay(True)\n        if not self.stream.writing():\n            self._finish_request()\n\n    def _on_write_complete(self):\n        if self._write_callback is not None:\n            callback = self._write_callback\n            self._write_callback = None\n            callback()\n        # _on_write_complete is enqueued on the IOLoop whenever the\n        # IOStream's write buffer becomes empty, but it's possible for\n        # another callback that runs on the IOLoop before it to\n        # simultaneously write more data and finish the request.  If\n        # there is still data in the IOStream, a future\n        # _on_write_complete will be responsible for calling\n        # _finish_request.\n        if self._request_finished and not self.stream.writing():\n            self._finish_request()\n\n    def _finish_request(self):\n        if self.no_keep_alive or self._request is None:\n            disconnect = True\n        else:\n            connection_header = self._request.headers.get(\"Connection\")\n            if connection_header is not None:\n                connection_header = connection_header.lower()\n            if self._request.supports_http_1_1():\n                disconnect = connection_header == \"close\"\n            elif (\"Content-Length\" in self._request.headers\n                    or self._request.method in (\"HEAD\", \"GET\")):\n                disconnect = connection_header != \"keep-alive\"\n            else:\n                disconnect = True\n        self._clear_request_state()\n        if disconnect:\n            self.close()\n            return\n        try:\n            # Use a try/except instead of checking stream.closed()\n            # directly, because in some cases the stream doesn't discover\n            # that it's closed until you try to read from it.\n            self.stream.read_until(b\"\\r\\n\\r\\n\", self._header_callback)\n\n            # Turn Nagle's algorithm back on, leaving the stream in its\n            # default state for the next request.\n            self.stream.set_nodelay(False)\n        except iostream.StreamClosedError:\n            self.close()\n\n    def _on_headers(self, data):\n        try:\n            data = native_str(data.decode('latin1'))\n            eol = data.find(\"\\r\\n\")\n            start_line = data[:eol]\n            try:\n                method, uri, version = start_line.split(\" \")\n            except ValueError:\n                raise _BadRequestException(\"Malformed HTTP request line\")\n            if not version.startswith(\"HTTP/\"):\n                raise _BadRequestException(\"Malformed HTTP version in HTTP Request-Line\")\n            try:\n                headers = httputil.HTTPHeaders.parse(data[eol:])\n            except ValueError:\n                # Probably from split() if there was no ':' in the line\n                raise _BadRequestException(\"Malformed HTTP headers\")\n\n            # HTTPRequest wants an IP, not a full socket address\n            if self.address_family in (socket.AF_INET, socket.AF_INET6):\n                remote_ip = self.address[0]\n            else:\n                # Unix (or other) socket; fake the remote address\n                remote_ip = '0.0.0.0'\n\n            protocol = self.protocol\n\n            # xheaders can override the defaults\n            if self.xheaders:\n                # Squid uses X-Forwarded-For, others use X-Real-Ip\n                ip = headers.get(\"X-Forwarded-For\", remote_ip)\n                ip = ip.split(',')[-1].strip()\n                ip = headers.get(\"X-Real-Ip\", ip)\n                if netutil.is_valid_ip(ip):\n                    remote_ip = ip\n                # AWS uses X-Forwarded-Proto\n                proto_header = headers.get(\n                    \"X-Scheme\", headers.get(\"X-Forwarded-Proto\", self.protocol))\n                if proto_header in (\"http\", \"https\"):\n                    protocol = proto_header\n\n            self._request = httputil.HTTPServerRequest(\n                connection=self, method=method, uri=uri, version=version,\n                headers=headers, remote_ip=remote_ip, protocol=protocol)\n\n            content_length = headers.get(\"Content-Length\")\n            if content_length:\n                content_length = int(content_length)\n                if content_length > self.stream.max_buffer_size:\n                    raise _BadRequestException(\"Content-Length too long\")\n                if headers.get(\"Expect\") == \"100-continue\":\n                    self.stream.write(b\"HTTP/1.1 100 (Continue)\\r\\n\\r\\n\")\n                self.stream.read_bytes(content_length, self._on_request_body)\n                return\n\n            self.request_callback(self._request)\n        except _BadRequestException as e:\n            gen_log.info(\"Malformed HTTP request from %r: %s\",\n                         self.address, e)\n            self.close()\n            return\n\n    def _on_request_body(self, data):\n        self._request.body = data\n        if self._request.method in (\"POST\", \"PATCH\", \"PUT\"):\n            httputil.parse_body_arguments(\n                self._request.headers.get(\"Content-Type\", \"\"), data,\n                self._request.body_arguments, self._request.files)\n\n            for k, v in self._request.body_arguments.items():\n                self._request.arguments.setdefault(k, []).extend(v)\n        self.request_callback(self._request)\n",
          "file_after": "#!/usr/bin/env python\n#\n# Copyright 2014 Facebook\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"); you may\n# not use this file except in compliance with the License. You may obtain\n# a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n# License for the specific language governing permissions and limitations\n# under the License.\n\nfrom __future__ import absolute_import, division, print_function, with_statement\n\nimport socket\n\nfrom tornado.concurrent import Future\nfrom tornado.escape import native_str\nfrom tornado import gen\nfrom tornado import httputil\nfrom tornado import iostream\nfrom tornado.log import gen_log\nfrom tornado import netutil\nfrom tornado import stack_context\n\n\nclass _BadRequestException(Exception):\n    \"\"\"Exception class for malformed HTTP requests.\"\"\"\n    pass\n\n\nclass HTTP1Connection(object):\n    \"\"\"Handles a connection to an HTTP client, executing HTTP requests.\n\n    We parse HTTP headers and bodies, and execute the request callback\n    until the HTTP conection is closed.\n    \"\"\"\n    def __init__(self, stream, address, request_callback, no_keep_alive=False,\n                 xheaders=False, protocol=None):\n        self.stream = stream\n        self.address = address\n        # Save the socket's address family now so we know how to\n        # interpret self.address even after the stream is closed\n        # and its socket attribute replaced with None.\n        self.address_family = stream.socket.family\n        self.request_callback = request_callback\n        self.no_keep_alive = no_keep_alive\n        self.xheaders = xheaders\n        if protocol:\n            self.protocol = protocol\n        elif isinstance(stream, iostream.SSLIOStream):\n            self.protocol = \"https\"\n        else:\n            self.protocol = \"http\"\n        self._clear_request_state()\n        self.stream.set_close_callback(self._on_connection_close)\n        self._finish_future = None\n        # Register the future on the IOLoop so its errors get logged.\n        stream.io_loop.add_future(self._process_requests(),\n                                  lambda f: f.result())\n\n    @gen.coroutine\n    def _process_requests(self):\n        while True:\n            try:\n                header_data = yield self.stream.read_until(b\"\\r\\n\\r\\n\")\n                self._finish_future = Future()\n                start_line, headers = self._parse_headers(header_data)\n                request = self._make_request(start_line, headers)\n                self._request = request\n                body_future = self._read_body(headers)\n                if body_future is not None:\n                    request.body = yield body_future\n                self._parse_body(request)\n                self.request_callback(request)\n                yield self._finish_future\n            except _BadRequestException as e:\n                gen_log.info(\"Malformed HTTP request from %r: %s\",\n                             self.address, e)\n                self.close()\n                return\n            except iostream.StreamClosedError:\n                self.close()\n                return\n\n\n    def _clear_request_state(self):\n        \"\"\"Clears the per-request state.\n\n        This is run in between requests to allow the previous handler\n        to be garbage collected (and prevent spurious close callbacks),\n        and when the connection is closed (to break up cycles and\n        facilitate garbage collection in cpython).\n        \"\"\"\n        self._request = None\n        self._request_finished = False\n        self._write_callback = None\n        self._close_callback = None\n\n    def set_close_callback(self, callback):\n        \"\"\"Sets a callback that will be run when the connection is closed.\n\n        Use this instead of accessing\n        `HTTPConnection.stream.set_close_callback\n        <.BaseIOStream.set_close_callback>` directly (which was the\n        recommended approach prior to Tornado 3.0).\n        \"\"\"\n        self._close_callback = stack_context.wrap(callback)\n\n    def _on_connection_close(self):\n        if self._close_callback is not None:\n            callback = self._close_callback\n            self._close_callback = None\n            callback()\n        if self._finish_future is not None and not self._finish_future.done():\n            self._finish_future.set_result(None)\n        # Delete any unfinished callbacks to break up reference cycles.\n        self._clear_request_state()\n\n    def close(self):\n        self.stream.close()\n        # Remove this reference to self, which would otherwise cause a\n        # cycle and delay garbage collection of this connection.\n        self._clear_request_state()\n\n    def write(self, chunk, callback=None):\n        \"\"\"Writes a chunk of output to the stream.\"\"\"\n        if not self.stream.closed():\n            self._write_callback = stack_context.wrap(callback)\n            self.stream.write(chunk, self._on_write_complete)\n\n    def finish(self):\n        \"\"\"Finishes the request.\"\"\"\n        self._request_finished = True\n        # No more data is coming, so instruct TCP to send any remaining\n        # data immediately instead of waiting for a full packet or ack.\n        self.stream.set_nodelay(True)\n        if not self.stream.writing():\n            self._finish_request()\n\n    def _on_write_complete(self):\n        if self._write_callback is not None:\n            callback = self._write_callback\n            self._write_callback = None\n            callback()\n        # _on_write_complete is enqueued on the IOLoop whenever the\n        # IOStream's write buffer becomes empty, but it's possible for\n        # another callback that runs on the IOLoop before it to\n        # simultaneously write more data and finish the request.  If\n        # there is still data in the IOStream, a future\n        # _on_write_complete will be responsible for calling\n        # _finish_request.\n        if self._request_finished and not self.stream.writing():\n            self._finish_request()\n\n    def _finish_request(self):\n        if self.no_keep_alive or self._request is None:\n            disconnect = True\n        else:\n            connection_header = self._request.headers.get(\"Connection\")\n            if connection_header is not None:\n                connection_header = connection_header.lower()\n            if self._request.supports_http_1_1():\n                disconnect = connection_header == \"close\"\n            elif (\"Content-Length\" in self._request.headers\n                    or self._request.method in (\"HEAD\", \"GET\")):\n                disconnect = connection_header != \"keep-alive\"\n            else:\n                disconnect = True\n        self._clear_request_state()\n        if disconnect:\n            self.close()\n            return\n        # Turn Nagle's algorithm back on, leaving the stream in its\n        # default state for the next request.\n        self.stream.set_nodelay(False)\n        self._finish_future.set_result(None)\n\n    def _parse_headers(self, data):\n        data = native_str(data.decode('latin1'))\n        eol = data.find(\"\\r\\n\")\n        start_line = data[:eol]\n        try:\n            headers = httputil.HTTPHeaders.parse(data[eol:])\n        except ValueError:\n            # probably form split() if there was no ':' in the line\n            raise _BadRequestException(\"Malformed HTTP headers\")\n        return start_line, headers\n\n    def _make_request(self, start_line, headers):\n        try:\n            method, uri, version = start_line.split(\" \")\n        except ValueError:\n            raise _BadRequestException(\"Malformed HTTP request line\")\n        if not version.startswith(\"HTTP/\"):\n            raise _BadRequestException(\"Malformed HTTP version in HTTP Request-Line\")\n        # HTTPRequest wants an IP, not a full socket address\n        if self.address_family in (socket.AF_INET, socket.AF_INET6):\n            remote_ip = self.address[0]\n        else:\n            # Unix (or other) socket; fake the remote address\n            remote_ip = '0.0.0.0'\n\n        protocol = self.protocol\n\n        # xheaders can override the defaults\n        if self.xheaders:\n            # Squid uses X-Forwarded-For, others use X-Real-Ip\n            ip = headers.get(\"X-Forwarded-For\", remote_ip)\n            ip = ip.split(',')[-1].strip()\n            ip = headers.get(\"X-Real-Ip\", ip)\n            if netutil.is_valid_ip(ip):\n                remote_ip = ip\n            # AWS uses X-Forwarded-Proto\n            proto_header = headers.get(\n                \"X-Scheme\", headers.get(\"X-Forwarded-Proto\", self.protocol))\n            if proto_header in (\"http\", \"https\"):\n                protocol = proto_header\n\n        return httputil.HTTPServerRequest(\n            connection=self, method=method, uri=uri, version=version,\n            headers=headers, remote_ip=remote_ip, protocol=protocol)\n\n    def _read_body(self, headers):\n        content_length = headers.get(\"Content-Length\")\n        if content_length:\n            content_length = int(content_length)\n            if content_length > self.stream.max_buffer_size:\n                raise _BadRequestException(\"Content-Length too long\")\n            if headers.get(\"Expect\") == \"100-continue\":\n                self.stream.write(b\"HTTP/1.1 100 (Continue)\\r\\n\\r\\n\")\n            return self.stream.read_bytes(content_length)\n        return None\n\n    def _parse_body(self, request):\n        if self._request.method in (\"POST\", \"PATCH\", \"PUT\"):\n            httputil.parse_body_arguments(\n                self._request.headers.get(\"Content-Type\", \"\"), request.body,\n                self._request.body_arguments, self._request.files)\n\n            for k, v in self._request.body_arguments.items():\n                self._request.arguments.setdefault(k, []).extend(v)\n",
          "file_patch": "@@ -18,7 +18,9 @@ from __future__ import absolute_import, division, print_function, with_statement\n \n import socket\n \n+from tornado.concurrent import Future\n from tornado.escape import native_str\n+from tornado import gen\n from tornado import httputil\n from tornado import iostream\n from tornado.log import gen_log\n@@ -55,11 +57,36 @@ class HTTP1Connection(object):\n         else:\n             self.protocol = \"http\"\n         self._clear_request_state()\n-        # Save stack context here, outside of any request.  This keeps\n-        # contexts from one request from leaking into the next.\n-        self._header_callback = stack_context.wrap(self._on_headers)\n         self.stream.set_close_callback(self._on_connection_close)\n-        self.stream.read_until(b\"\\r\\n\\r\\n\", self._header_callback)\n+        self._finish_future = None\n+        # Register the future on the IOLoop so its errors get logged.\n+        stream.io_loop.add_future(self._process_requests(),\n+                                  lambda f: f.result())\n+\n+    @gen.coroutine\n+    def _process_requests(self):\n+        while True:\n+            try:\n+                header_data = yield self.stream.read_until(b\"\\r\\n\\r\\n\")\n+                self._finish_future = Future()\n+                start_line, headers = self._parse_headers(header_data)\n+                request = self._make_request(start_line, headers)\n+                self._request = request\n+                body_future = self._read_body(headers)\n+                if body_future is not None:\n+                    request.body = yield body_future\n+                self._parse_body(request)\n+                self.request_callback(request)\n+                yield self._finish_future\n+            except _BadRequestException as e:\n+                gen_log.info(\"Malformed HTTP request from %r: %s\",\n+                             self.address, e)\n+                self.close()\n+                return\n+            except iostream.StreamClosedError:\n+                self.close()\n+                return\n+\n \n     def _clear_request_state(self):\n         \"\"\"Clears the per-request state.\n@@ -89,15 +116,15 @@ class HTTP1Connection(object):\n             callback = self._close_callback\n             self._close_callback = None\n             callback()\n+        if self._finish_future is not None and not self._finish_future.done():\n+            self._finish_future.set_result(None)\n         # Delete any unfinished callbacks to break up reference cycles.\n-        self._header_callback = None\n         self._clear_request_state()\n \n     def close(self):\n         self.stream.close()\n         # Remove this reference to self, which would otherwise cause a\n         # cycle and delay garbage collection of this connection.\n-        self._header_callback = None\n         self._clear_request_state()\n \n     def write(self, chunk, callback=None):\n@@ -148,86 +175,72 @@ class HTTP1Connection(object):\n         if disconnect:\n             self.close()\n             return\n+        # Turn Nagle's algorithm back on, leaving the stream in its\n+        # default state for the next request.\n+        self.stream.set_nodelay(False)\n+        self._finish_future.set_result(None)\n+\n+    def _parse_headers(self, data):\n+        data = native_str(data.decode('latin1'))\n+        eol = data.find(\"\\r\\n\")\n+        start_line = data[:eol]\n         try:\n-            # Use a try/except instead of checking stream.closed()\n-            # directly, because in some cases the stream doesn't discover\n-            # that it's closed until you try to read from it.\n-            self.stream.read_until(b\"\\r\\n\\r\\n\", self._header_callback)\n-\n-            # Turn Nagle's algorithm back on, leaving the stream in its\n-            # default state for the next request.\n-            self.stream.set_nodelay(False)\n-        except iostream.StreamClosedError:\n-            self.close()\n+            headers = httputil.HTTPHeaders.parse(data[eol:])\n+        except ValueError:\n+            # probably form split() if there was no ':' in the line\n+            raise _BadRequestException(\"Malformed HTTP headers\")\n+        return start_line, headers\n \n-    def _on_headers(self, data):\n+    def _make_request(self, start_line, headers):\n         try:\n-            data = native_str(data.decode('latin1'))\n-            eol = data.find(\"\\r\\n\")\n-            start_line = data[:eol]\n-            try:\n-                method, uri, version = start_line.split(\" \")\n-            except ValueError:\n-                raise _BadRequestException(\"Malformed HTTP request line\")\n-            if not version.startswith(\"HTTP/\"):\n-                raise _BadRequestException(\"Malformed HTTP version in HTTP Request-Line\")\n-            try:\n-                headers = httputil.HTTPHeaders.parse(data[eol:])\n-            except ValueError:\n-                # Probably from split() if there was no ':' in the line\n-                raise _BadRequestException(\"Malformed HTTP headers\")\n-\n-            # HTTPRequest wants an IP, not a full socket address\n-            if self.address_family in (socket.AF_INET, socket.AF_INET6):\n-                remote_ip = self.address[0]\n-            else:\n-                # Unix (or other) socket; fake the remote address\n-                remote_ip = '0.0.0.0'\n-\n-            protocol = self.protocol\n-\n-            # xheaders can override the defaults\n-            if self.xheaders:\n-                # Squid uses X-Forwarded-For, others use X-Real-Ip\n-                ip = headers.get(\"X-Forwarded-For\", remote_ip)\n-                ip = ip.split(',')[-1].strip()\n-                ip = headers.get(\"X-Real-Ip\", ip)\n-                if netutil.is_valid_ip(ip):\n-                    remote_ip = ip\n-                # AWS uses X-Forwarded-Proto\n-                proto_header = headers.get(\n-                    \"X-Scheme\", headers.get(\"X-Forwarded-Proto\", self.protocol))\n-                if proto_header in (\"http\", \"https\"):\n-                    protocol = proto_header\n-\n-            self._request = httputil.HTTPServerRequest(\n-                connection=self, method=method, uri=uri, version=version,\n-                headers=headers, remote_ip=remote_ip, protocol=protocol)\n-\n-            content_length = headers.get(\"Content-Length\")\n-            if content_length:\n-                content_length = int(content_length)\n-                if content_length > self.stream.max_buffer_size:\n-                    raise _BadRequestException(\"Content-Length too long\")\n-                if headers.get(\"Expect\") == \"100-continue\":\n-                    self.stream.write(b\"HTTP/1.1 100 (Continue)\\r\\n\\r\\n\")\n-                self.stream.read_bytes(content_length, self._on_request_body)\n-                return\n-\n-            self.request_callback(self._request)\n-        except _BadRequestException as e:\n-            gen_log.info(\"Malformed HTTP request from %r: %s\",\n-                         self.address, e)\n-            self.close()\n-            return\n-\n-    def _on_request_body(self, data):\n-        self._request.body = data\n+            method, uri, version = start_line.split(\" \")\n+        except ValueError:\n+            raise _BadRequestException(\"Malformed HTTP request line\")\n+        if not version.startswith(\"HTTP/\"):\n+            raise _BadRequestException(\"Malformed HTTP version in HTTP Request-Line\")\n+        # HTTPRequest wants an IP, not a full socket address\n+        if self.address_family in (socket.AF_INET, socket.AF_INET6):\n+            remote_ip = self.address[0]\n+        else:\n+            # Unix (or other) socket; fake the remote address\n+            remote_ip = '0.0.0.0'\n+\n+        protocol = self.protocol\n+\n+        # xheaders can override the defaults\n+        if self.xheaders:\n+            # Squid uses X-Forwarded-For, others use X-Real-Ip\n+            ip = headers.get(\"X-Forwarded-For\", remote_ip)\n+            ip = ip.split(',')[-1].strip()\n+            ip = headers.get(\"X-Real-Ip\", ip)\n+            if netutil.is_valid_ip(ip):\n+                remote_ip = ip\n+            # AWS uses X-Forwarded-Proto\n+            proto_header = headers.get(\n+                \"X-Scheme\", headers.get(\"X-Forwarded-Proto\", self.protocol))\n+            if proto_header in (\"http\", \"https\"):\n+                protocol = proto_header\n+\n+        return httputil.HTTPServerRequest(\n+            connection=self, method=method, uri=uri, version=version,\n+            headers=headers, remote_ip=remote_ip, protocol=protocol)\n+\n+    def _read_body(self, headers):\n+        content_length = headers.get(\"Content-Length\")\n+        if content_length:\n+            content_length = int(content_length)\n+            if content_length > self.stream.max_buffer_size:\n+                raise _BadRequestException(\"Content-Length too long\")\n+            if headers.get(\"Expect\") == \"100-continue\":\n+                self.stream.write(b\"HTTP/1.1 100 (Continue)\\r\\n\\r\\n\")\n+            return self.stream.read_bytes(content_length)\n+        return None\n+\n+    def _parse_body(self, request):\n         if self._request.method in (\"POST\", \"PATCH\", \"PUT\"):\n             httputil.parse_body_arguments(\n-                self._request.headers.get(\"Content-Type\", \"\"), data,\n+                self._request.headers.get(\"Content-Type\", \"\"), request.body,\n                 self._request.body_arguments, self._request.files)\n \n             for k, v in self._request.body_arguments.items():\n                 self._request.arguments.setdefault(k, []).extend(v)\n-        self.request_callback(self._request)\n",
          "files_name_in_blame_commit": [
            "http1connection.py"
          ]
        }
      },
      "656f78e18a14aa1b3a018f6922bf40b1204e0e22": {
        "commit": {
          "commit_id": "656f78e18a14aa1b3a018f6922bf40b1204e0e22",
          "commit_message": "HTTP refactoring part 1: move shareable parts out of httpserver.py.\n\ntornado.httpserver.HTTPRequest -> tornado.httputil.HTTPServerRequest\ntornado.httpsrever.HTTPConnection -> tornado.http1connection.HTTP1Connection",
          "commit_author": "Ben Darnell",
          "commit_date": "2014-02-23 12:50:10",
          "commit_parent": "824226d71be4eadcffe13da2c3793b1e872549c2"
        },
        "function": {
          "function_name": "_read_body",
          "function_code_before": "",
          "function_code_after": "",
          "function_before_start_line": "",
          "function_before_end_line": "",
          "function_after_start_line": "",
          "function_after_end_line": "",
          "function_before_token_count": 0,
          "function_after_token_count": 0,
          "functions_name_modified_file": [
            "__init__",
            "close",
            "write",
            "_on_write_complete",
            "_finish_request",
            "_on_request_body",
            "_clear_request_state",
            "_on_connection_close",
            "_on_headers",
            "set_close_callback",
            "finish"
          ],
          "functions_name_all_files": [
            "full_url",
            "_parseparam",
            "_int_or_none",
            "_parse_header",
            "request_time",
            "url_concat",
            "write",
            "_on_request_body",
            "get_all",
            "__delitem__",
            "__contains__",
            "add",
            "_parse_request_range",
            "parse_body_arguments",
            "close",
            "get_list",
            "supports_http_1_1",
            "doctests",
            "_on_headers",
            "get",
            "parse_line",
            "_get_content_range",
            "__repr__",
            "set_close_callback",
            "__getitem__",
            "__init__",
            "update",
            "parse",
            "format_timestamp",
            "_finish_request",
            "cookies",
            "_clear_request_state",
            "copy",
            "_on_connection_close",
            "get_ssl_certificate",
            "handle_stream",
            "__setitem__",
            "parse_multipart_form_data",
            "_on_write_complete",
            "__missing__",
            "finish"
          ],
          "functions_name_co_evolved_modified_file": [
            "__init__",
            "close",
            "write",
            "_on_write_complete",
            "_finish_request",
            "_on_request_body",
            "_clear_request_state",
            "_on_headers",
            "_on_connection_close",
            "set_close_callback",
            "finish"
          ],
          "functions_name_co_evolved_all_files": [
            "__init__",
            "close",
            "write",
            "full_url",
            "_on_write_complete",
            "supports_http_1_1",
            "_finish_request",
            "cookies",
            "_on_request_body",
            "_clear_request_state",
            "_on_headers",
            "_on_connection_close",
            "get_ssl_certificate",
            "request_time",
            "__repr__",
            "set_close_callback",
            "finish"
          ]
        },
        "file": {
          "file_name": "http1connection.py",
          "file_nloc": 132,
          "file_complexity": 35,
          "file_token_count": 885,
          "file_before": null,
          "file_after": "#!/usr/bin/env python\n#\n# Copyright 2014 Facebook\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"); you may\n# not use this file except in compliance with the License. You may obtain\n# a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n# License for the specific language governing permissions and limitations\n# under the License.\n\nfrom __future__ import absolute_import, division, print_function, with_statement\n\nimport socket\n\nfrom tornado.escape import native_str\nfrom tornado import httputil\nfrom tornado import iostream\nfrom tornado.log import gen_log\nfrom tornado import stack_context\n\n\nclass _BadRequestException(Exception):\n    \"\"\"Exception class for malformed HTTP requests.\"\"\"\n    pass\n\n\nclass HTTP1Connection(object):\n    \"\"\"Handles a connection to an HTTP client, executing HTTP requests.\n\n    We parse HTTP headers and bodies, and execute the request callback\n    until the HTTP conection is closed.\n    \"\"\"\n    def __init__(self, stream, address, request_callback, no_keep_alive=False,\n                 xheaders=False, protocol=None):\n        self.stream = stream\n        self.address = address\n        # Save the socket's address family now so we know how to\n        # interpret self.address even after the stream is closed\n        # and its socket attribute replaced with None.\n        self.address_family = stream.socket.family\n        self.request_callback = request_callback\n        self.no_keep_alive = no_keep_alive\n        self.xheaders = xheaders\n        self.protocol = protocol\n        self._clear_request_state()\n        # Save stack context here, outside of any request.  This keeps\n        # contexts from one request from leaking into the next.\n        self._header_callback = stack_context.wrap(self._on_headers)\n        self.stream.set_close_callback(self._on_connection_close)\n        self.stream.read_until(b\"\\r\\n\\r\\n\", self._header_callback)\n\n    def _clear_request_state(self):\n        \"\"\"Clears the per-request state.\n\n        This is run in between requests to allow the previous handler\n        to be garbage collected (and prevent spurious close callbacks),\n        and when the connection is closed (to break up cycles and\n        facilitate garbage collection in cpython).\n        \"\"\"\n        self._request = None\n        self._request_finished = False\n        self._write_callback = None\n        self._close_callback = None\n\n    def set_close_callback(self, callback):\n        \"\"\"Sets a callback that will be run when the connection is closed.\n\n        Use this instead of accessing\n        `HTTPConnection.stream.set_close_callback\n        <.BaseIOStream.set_close_callback>` directly (which was the\n        recommended approach prior to Tornado 3.0).\n        \"\"\"\n        self._close_callback = stack_context.wrap(callback)\n\n    def _on_connection_close(self):\n        if self._close_callback is not None:\n            callback = self._close_callback\n            self._close_callback = None\n            callback()\n        # Delete any unfinished callbacks to break up reference cycles.\n        self._header_callback = None\n        self._clear_request_state()\n\n    def close(self):\n        self.stream.close()\n        # Remove this reference to self, which would otherwise cause a\n        # cycle and delay garbage collection of this connection.\n        self._header_callback = None\n        self._clear_request_state()\n\n    def write(self, chunk, callback=None):\n        \"\"\"Writes a chunk of output to the stream.\"\"\"\n        if not self.stream.closed():\n            self._write_callback = stack_context.wrap(callback)\n            self.stream.write(chunk, self._on_write_complete)\n\n    def finish(self):\n        \"\"\"Finishes the request.\"\"\"\n        self._request_finished = True\n        # No more data is coming, so instruct TCP to send any remaining\n        # data immediately instead of waiting for a full packet or ack.\n        self.stream.set_nodelay(True)\n        if not self.stream.writing():\n            self._finish_request()\n\n    def _on_write_complete(self):\n        if self._write_callback is not None:\n            callback = self._write_callback\n            self._write_callback = None\n            callback()\n        # _on_write_complete is enqueued on the IOLoop whenever the\n        # IOStream's write buffer becomes empty, but it's possible for\n        # another callback that runs on the IOLoop before it to\n        # simultaneously write more data and finish the request.  If\n        # there is still data in the IOStream, a future\n        # _on_write_complete will be responsible for calling\n        # _finish_request.\n        if self._request_finished and not self.stream.writing():\n            self._finish_request()\n\n    def _finish_request(self):\n        if self.no_keep_alive or self._request is None:\n            disconnect = True\n        else:\n            connection_header = self._request.headers.get(\"Connection\")\n            if connection_header is not None:\n                connection_header = connection_header.lower()\n            if self._request.supports_http_1_1():\n                disconnect = connection_header == \"close\"\n            elif (\"Content-Length\" in self._request.headers\n                    or self._request.method in (\"HEAD\", \"GET\")):\n                disconnect = connection_header != \"keep-alive\"\n            else:\n                disconnect = True\n        self._clear_request_state()\n        if disconnect:\n            self.close()\n            return\n        try:\n            # Use a try/except instead of checking stream.closed()\n            # directly, because in some cases the stream doesn't discover\n            # that it's closed until you try to read from it.\n            self.stream.read_until(b\"\\r\\n\\r\\n\", self._header_callback)\n\n            # Turn Nagle's algorithm back on, leaving the stream in its\n            # default state for the next request.\n            self.stream.set_nodelay(False)\n        except iostream.StreamClosedError:\n            self.close()\n\n    def _on_headers(self, data):\n        try:\n            data = native_str(data.decode('latin1'))\n            eol = data.find(\"\\r\\n\")\n            start_line = data[:eol]\n            try:\n                method, uri, version = start_line.split(\" \")\n            except ValueError:\n                raise _BadRequestException(\"Malformed HTTP request line\")\n            if not version.startswith(\"HTTP/\"):\n                raise _BadRequestException(\"Malformed HTTP version in HTTP Request-Line\")\n            try:\n                headers = httputil.HTTPHeaders.parse(data[eol:])\n            except ValueError:\n                # Probably from split() if there was no ':' in the line\n                raise _BadRequestException(\"Malformed HTTP headers\")\n\n            # HTTPRequest wants an IP, not a full socket address\n            if self.address_family in (socket.AF_INET, socket.AF_INET6):\n                remote_ip = self.address[0]\n            else:\n                # Unix (or other) socket; fake the remote address\n                remote_ip = '0.0.0.0'\n\n            self._request = httputil.HTTPServerRequest(\n                connection=self, method=method, uri=uri, version=version,\n                headers=headers, remote_ip=remote_ip, protocol=self.protocol)\n\n            content_length = headers.get(\"Content-Length\")\n            if content_length:\n                content_length = int(content_length)\n                if content_length > self.stream.max_buffer_size:\n                    raise _BadRequestException(\"Content-Length too long\")\n                if headers.get(\"Expect\") == \"100-continue\":\n                    self.stream.write(b\"HTTP/1.1 100 (Continue)\\r\\n\\r\\n\")\n                self.stream.read_bytes(content_length, self._on_request_body)\n                return\n\n            self.request_callback(self._request)\n        except _BadRequestException as e:\n            gen_log.info(\"Malformed HTTP request from %r: %s\",\n                         self.address, e)\n            self.close()\n            return\n\n    def _on_request_body(self, data):\n        self._request.body = data\n        if self._request.method in (\"POST\", \"PATCH\", \"PUT\"):\n            httputil.parse_body_arguments(\n                self._request.headers.get(\"Content-Type\", \"\"), data,\n                self._request.body_arguments, self._request.files)\n\n            for k, v in self._request.body_arguments.items():\n                self._request.arguments.setdefault(k, []).extend(v)\n        self.request_callback(self._request)\n",
          "file_patch": "@@ -0,0 +1,211 @@\n+#!/usr/bin/env python\n+#\n+# Copyright 2014 Facebook\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\"); you may\n+# not use this file except in compliance with the License. You may obtain\n+# a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n+# License for the specific language governing permissions and limitations\n+# under the License.\n+\n+from __future__ import absolute_import, division, print_function, with_statement\n+\n+import socket\n+\n+from tornado.escape import native_str\n+from tornado import httputil\n+from tornado import iostream\n+from tornado.log import gen_log\n+from tornado import stack_context\n+\n+\n+class _BadRequestException(Exception):\n+    \"\"\"Exception class for malformed HTTP requests.\"\"\"\n+    pass\n+\n+\n+class HTTP1Connection(object):\n+    \"\"\"Handles a connection to an HTTP client, executing HTTP requests.\n+\n+    We parse HTTP headers and bodies, and execute the request callback\n+    until the HTTP conection is closed.\n+    \"\"\"\n+    def __init__(self, stream, address, request_callback, no_keep_alive=False,\n+                 xheaders=False, protocol=None):\n+        self.stream = stream\n+        self.address = address\n+        # Save the socket's address family now so we know how to\n+        # interpret self.address even after the stream is closed\n+        # and its socket attribute replaced with None.\n+        self.address_family = stream.socket.family\n+        self.request_callback = request_callback\n+        self.no_keep_alive = no_keep_alive\n+        self.xheaders = xheaders\n+        self.protocol = protocol\n+        self._clear_request_state()\n+        # Save stack context here, outside of any request.  This keeps\n+        # contexts from one request from leaking into the next.\n+        self._header_callback = stack_context.wrap(self._on_headers)\n+        self.stream.set_close_callback(self._on_connection_close)\n+        self.stream.read_until(b\"\\r\\n\\r\\n\", self._header_callback)\n+\n+    def _clear_request_state(self):\n+        \"\"\"Clears the per-request state.\n+\n+        This is run in between requests to allow the previous handler\n+        to be garbage collected (and prevent spurious close callbacks),\n+        and when the connection is closed (to break up cycles and\n+        facilitate garbage collection in cpython).\n+        \"\"\"\n+        self._request = None\n+        self._request_finished = False\n+        self._write_callback = None\n+        self._close_callback = None\n+\n+    def set_close_callback(self, callback):\n+        \"\"\"Sets a callback that will be run when the connection is closed.\n+\n+        Use this instead of accessing\n+        `HTTPConnection.stream.set_close_callback\n+        <.BaseIOStream.set_close_callback>` directly (which was the\n+        recommended approach prior to Tornado 3.0).\n+        \"\"\"\n+        self._close_callback = stack_context.wrap(callback)\n+\n+    def _on_connection_close(self):\n+        if self._close_callback is not None:\n+            callback = self._close_callback\n+            self._close_callback = None\n+            callback()\n+        # Delete any unfinished callbacks to break up reference cycles.\n+        self._header_callback = None\n+        self._clear_request_state()\n+\n+    def close(self):\n+        self.stream.close()\n+        # Remove this reference to self, which would otherwise cause a\n+        # cycle and delay garbage collection of this connection.\n+        self._header_callback = None\n+        self._clear_request_state()\n+\n+    def write(self, chunk, callback=None):\n+        \"\"\"Writes a chunk of output to the stream.\"\"\"\n+        if not self.stream.closed():\n+            self._write_callback = stack_context.wrap(callback)\n+            self.stream.write(chunk, self._on_write_complete)\n+\n+    def finish(self):\n+        \"\"\"Finishes the request.\"\"\"\n+        self._request_finished = True\n+        # No more data is coming, so instruct TCP to send any remaining\n+        # data immediately instead of waiting for a full packet or ack.\n+        self.stream.set_nodelay(True)\n+        if not self.stream.writing():\n+            self._finish_request()\n+\n+    def _on_write_complete(self):\n+        if self._write_callback is not None:\n+            callback = self._write_callback\n+            self._write_callback = None\n+            callback()\n+        # _on_write_complete is enqueued on the IOLoop whenever the\n+        # IOStream's write buffer becomes empty, but it's possible for\n+        # another callback that runs on the IOLoop before it to\n+        # simultaneously write more data and finish the request.  If\n+        # there is still data in the IOStream, a future\n+        # _on_write_complete will be responsible for calling\n+        # _finish_request.\n+        if self._request_finished and not self.stream.writing():\n+            self._finish_request()\n+\n+    def _finish_request(self):\n+        if self.no_keep_alive or self._request is None:\n+            disconnect = True\n+        else:\n+            connection_header = self._request.headers.get(\"Connection\")\n+            if connection_header is not None:\n+                connection_header = connection_header.lower()\n+            if self._request.supports_http_1_1():\n+                disconnect = connection_header == \"close\"\n+            elif (\"Content-Length\" in self._request.headers\n+                    or self._request.method in (\"HEAD\", \"GET\")):\n+                disconnect = connection_header != \"keep-alive\"\n+            else:\n+                disconnect = True\n+        self._clear_request_state()\n+        if disconnect:\n+            self.close()\n+            return\n+        try:\n+            # Use a try/except instead of checking stream.closed()\n+            # directly, because in some cases the stream doesn't discover\n+            # that it's closed until you try to read from it.\n+            self.stream.read_until(b\"\\r\\n\\r\\n\", self._header_callback)\n+\n+            # Turn Nagle's algorithm back on, leaving the stream in its\n+            # default state for the next request.\n+            self.stream.set_nodelay(False)\n+        except iostream.StreamClosedError:\n+            self.close()\n+\n+    def _on_headers(self, data):\n+        try:\n+            data = native_str(data.decode('latin1'))\n+            eol = data.find(\"\\r\\n\")\n+            start_line = data[:eol]\n+            try:\n+                method, uri, version = start_line.split(\" \")\n+            except ValueError:\n+                raise _BadRequestException(\"Malformed HTTP request line\")\n+            if not version.startswith(\"HTTP/\"):\n+                raise _BadRequestException(\"Malformed HTTP version in HTTP Request-Line\")\n+            try:\n+                headers = httputil.HTTPHeaders.parse(data[eol:])\n+            except ValueError:\n+                # Probably from split() if there was no ':' in the line\n+                raise _BadRequestException(\"Malformed HTTP headers\")\n+\n+            # HTTPRequest wants an IP, not a full socket address\n+            if self.address_family in (socket.AF_INET, socket.AF_INET6):\n+                remote_ip = self.address[0]\n+            else:\n+                # Unix (or other) socket; fake the remote address\n+                remote_ip = '0.0.0.0'\n+\n+            self._request = httputil.HTTPServerRequest(\n+                connection=self, method=method, uri=uri, version=version,\n+                headers=headers, remote_ip=remote_ip, protocol=self.protocol)\n+\n+            content_length = headers.get(\"Content-Length\")\n+            if content_length:\n+                content_length = int(content_length)\n+                if content_length > self.stream.max_buffer_size:\n+                    raise _BadRequestException(\"Content-Length too long\")\n+                if headers.get(\"Expect\") == \"100-continue\":\n+                    self.stream.write(b\"HTTP/1.1 100 (Continue)\\r\\n\\r\\n\")\n+                self.stream.read_bytes(content_length, self._on_request_body)\n+                return\n+\n+            self.request_callback(self._request)\n+        except _BadRequestException as e:\n+            gen_log.info(\"Malformed HTTP request from %r: %s\",\n+                         self.address, e)\n+            self.close()\n+            return\n+\n+    def _on_request_body(self, data):\n+        self._request.body = data\n+        if self._request.method in (\"POST\", \"PATCH\", \"PUT\"):\n+            httputil.parse_body_arguments(\n+                self._request.headers.get(\"Content-Type\", \"\"), data,\n+                self._request.body_arguments, self._request.files)\n+\n+            for k, v in self._request.body_arguments.items():\n+                self._request.arguments.setdefault(k, []).extend(v)\n+        self.request_callback(self._request)\n",
          "files_name_in_blame_commit": [
            "http1connection.py",
            "httputil.py",
            "httpserver.py"
          ]
        }
      }
    }
  }
}