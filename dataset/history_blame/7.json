{
  "id": "7",
  "blame_commit": {
    "commit": {
      "commit_id": "a5bc6c937c8f02ae0d39c3aac235c9bf7c55d33e",
      "commit_message": "Keeps track of tasks in scheduler grouped by status\n\nI've been seeing half-second get_work runtimes recently on my scheduler. The\ncause of this has been a large backfill that scheduled a lot of done tasks. With\nabout 100k done tasks and about 350 pending tasks, a lot of time is wasted\nlooping over done tasks just to ignore them. This essentially caches these\ncomputations by keeping a dictionary to group tasks by status and updating it\nwhenever task statuses are changed. This has sped my scheduling up by an order\nof magnitude, now taking about 20ms per get_work.\n\nIn order to ensure updates happen every time, the task state has to take over\nany functions that modify status. This is a necessary for remote task storage\nanyway, so it had to be done eventually. There are no new tests here because\nthere are no functionality changes, only improvements in efficiency and\ndifferent internal representations.",
      "commit_author": "Dave Buchfuhrer",
      "commit_date": "2015-01-08 21:26:10",
      "commit_parent": "5f21334ae80549b1677b01540c1404cd06e1e1fa"
    },
    "function": {
      "function_name": "set_status",
      "function_code_before": "def set_status(self, new_status, config):\n    if new_status == SUSPENDED:\n        new_status = PENDING\n    if new_status == DISABLED and self.status == RUNNING:\n        return\n    if self.status == DISABLED:\n        if new_status == DONE:\n            self.re_enable()\n        elif self.scheduler_disable_time is not None:\n            return\n    if new_status == FAILED and self.can_disable():\n        self.add_failure()\n        if self.has_excessive_failures():\n            self.scheduler_disable_time = time.time()\n            new_status = DISABLED\n            notifications.send_error_email('Luigi Scheduler: DISABLED {task} due to excessive failures'.format(task=self.id), '{task} failed {failures} times in the last {window} seconds, so it is being disabled for {persist} seconds'.format(failures=config.disable_failures, task=self.id, window=config.disable_window, persist=config.disable_persist))\n    elif new_status == DISABLED:\n        self.scheduler_disable_time = None\n    self.status = new_status",
      "function_code_after": "def set_status(self, task, new_status, config=None):\n    if new_status == FAILED:\n        assert config is not None\n    if new_status == SUSPENDED:\n        new_status = PENDING\n    if new_status == DISABLED and task.status == RUNNING:\n        return\n    if task.status == DISABLED:\n        if new_status == DONE:\n            self.re_enable(task)\n        elif task.scheduler_disable_time is not None:\n            return\n    if new_status == FAILED and task.can_disable():\n        task.add_failure()\n        if task.has_excessive_failures():\n            task.scheduler_disable_time = time.time()\n            new_status = DISABLED\n            notifications.send_error_email('Luigi Scheduler: DISABLED {task} due to excessive failures'.format(task=task.id), '{task} failed {failures} times in the last {window} seconds, so it is being disabled for {persist} seconds'.format(failures=config.disable_failures, task=task.id, window=config.disable_window, persist=config.disable_persist))\n    elif new_status == DISABLED:\n        task.scheduler_disable_time = None\n    self._status_tasks[task.status].pop(task.id)\n    self._status_tasks[new_status][task.id] = task\n    task.status = new_status",
      "function_before_start_line": 152,
      "function_before_end_line": 185,
      "function_after_start_line": 257,
      "function_after_end_line": 295,
      "function_before_token_count": 151,
      "function_after_token_count": 194,
      "functions_name_modified_file": [
        "add_failure",
        "get_worker_ids",
        "prune",
        "_schedulable",
        "clear",
        "has_excessive_failures",
        "_upstream_status",
        "_update_task_history",
        "can_disable",
        "ping",
        "get_pending_tasks",
        "_traverse_inverse_deps",
        "load",
        "inverse_dependencies",
        "num_failures",
        "task_search",
        "_recurse_deps",
        "get_worker",
        "get_active_workers",
        "worker_list",
        "add_task",
        "update_resources",
        "_update_priority",
        "task_list",
        "add_info",
        "get_active_tasks",
        "_has_resources",
        "_used_resources",
        "has_task",
        "fix_time",
        "_serialize_task",
        "task_history",
        "dep_graph",
        "__str__",
        "graph",
        "re_enable",
        "dump",
        "__repr__",
        "inactivate_tasks",
        "__init__",
        "update",
        "set_status",
        "add_worker",
        "get_task",
        "get_running_tasks",
        "_rank",
        "get_work",
        "re_enable_task",
        "fetch_error",
        "inactivate_workers"
      ],
      "functions_name_all_files": [
        "add_failure",
        "get_worker_ids",
        "prune",
        "_schedulable",
        "clear",
        "has_excessive_failures",
        "_upstream_status",
        "_update_task_history",
        "can_disable",
        "ping",
        "get_pending_tasks",
        "_traverse_inverse_deps",
        "load",
        "inverse_dependencies",
        "num_failures",
        "task_search",
        "_recurse_deps",
        "get_worker",
        "get_active_workers",
        "worker_list",
        "add_task",
        "update_resources",
        "_update_priority",
        "task_list",
        "add_info",
        "get_active_tasks",
        "_has_resources",
        "_used_resources",
        "has_task",
        "fix_time",
        "_serialize_task",
        "task_history",
        "dep_graph",
        "__str__",
        "graph",
        "re_enable",
        "dump",
        "__repr__",
        "inactivate_tasks",
        "__init__",
        "update",
        "set_status",
        "add_worker",
        "get_task",
        "get_running_tasks",
        "_rank",
        "get_work",
        "re_enable_task",
        "fetch_error",
        "inactivate_workers"
      ],
      "functions_name_co_evolved_modified_file": [
        "__init__",
        "prune",
        "get_task",
        "add_task",
        "get_running_tasks",
        "task_list",
        "_rank",
        "get_active_tasks",
        "re_enable_task",
        "get_pending_tasks",
        "get_work",
        "load",
        "re_enable",
        "inactivate_tasks"
      ],
      "functions_name_co_evolved_all_files": [
        "__init__",
        "prune",
        "get_task",
        "add_task",
        "get_running_tasks",
        "task_list",
        "_rank",
        "get_active_tasks",
        "re_enable_task",
        "get_pending_tasks",
        "get_work",
        "load",
        "re_enable",
        "inactivate_tasks"
      ]
    },
    "file": {
      "file_name": "scheduler.py",
      "file_nloc": 602,
      "file_complexity": 224,
      "file_token_count": 4496,
      "file_before": "# Copyright (c) 2012 Spotify AB\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"); you may not\n# use this file except in compliance with the License. You may obtain a copy of\n# the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n# License for the specific language governing permissions and limitations under\n# the License.\n\nimport collections\nimport datetime\nimport functools\nimport notifications\nimport os\nimport logging\nimport time\nimport cPickle as pickle\nimport task_history as history\nlogger = logging.getLogger(\"luigi.server\")\n\nfrom task_status import PENDING, FAILED, DONE, RUNNING, SUSPENDED, UNKNOWN, DISABLED\n\n\nclass Scheduler(object):\n    ''' Abstract base class\n\n    Note that the methods all take string arguments, not Task objects...\n    '''\n    add_task = NotImplemented\n    get_work = NotImplemented\n    ping = NotImplemented\n\nUPSTREAM_RUNNING = 'UPSTREAM_RUNNING'\nUPSTREAM_MISSING_INPUT = 'UPSTREAM_MISSING_INPUT'\nUPSTREAM_FAILED = 'UPSTREAM_FAILED'\nUPSTREAM_DISABLED = 'UPSTREAM_DISABLED'\n\nUPSTREAM_SEVERITY_ORDER = (\n    '',\n    UPSTREAM_RUNNING,\n    UPSTREAM_MISSING_INPUT,\n    UPSTREAM_FAILED,\n    UPSTREAM_DISABLED,\n)\nUPSTREAM_SEVERITY_KEY = lambda st: UPSTREAM_SEVERITY_ORDER.index(st)\nSTATUS_TO_UPSTREAM_MAP = {\n    FAILED: UPSTREAM_FAILED,\n    RUNNING: UPSTREAM_RUNNING,\n    PENDING: UPSTREAM_MISSING_INPUT,\n    DISABLED: UPSTREAM_DISABLED,\n}\n\n\n# We're passing around this config a lot, so let's put it on an object\nSchedulerConfig = collections.namedtuple('SchedulerConfig', [\n        'retry_delay', 'remove_delay', 'worker_disconnect_delay',\n        'disable_failures', 'disable_window', 'disable_persist', 'disable_time',\n        'max_shown_tasks',\n])\n\n\ndef fix_time(x):\n    # Backwards compatibility for a fix in Dec 2014. Prior to the fix, pickled state might store datetime objects\n    # Let's remove this function soon\n    if isinstance(x, datetime.datetime):\n        return time.mktime(x.timetuple())\n    else:\n        return x\n\n\nclass Failures(object):\n    \"\"\" This class tracks the number of failures in a given time window\n\n    Failures added are marked with the current timestamp, and this class counts\n    the number of failures in a sliding time window ending at the present.\n\n    \"\"\"\n\n    def __init__(self, window):\n        \"\"\" Initialize with the given window\n\n        :param window: how long to track failures for, as a float (number of seconds)\n        \"\"\"\n        self.window = window\n        self.failures = collections.deque()\n\n    def add_failure(self):\n        \"\"\" Add a failure event with the current timestamp \"\"\"\n        self.failures.append(time.time())\n\n    def num_failures(self):\n        \"\"\" Return the number of failures in the window \"\"\"\n        min_time = time.time() - self.window\n\n        while self.failures and fix_time(self.failures[0]) < min_time:\n            self.failures.popleft()\n\n        return len(self.failures)\n\n    def clear(self):\n        \"\"\" Clear the failure queue \"\"\"\n        self.failures.clear()\n\n\nclass Task(object):\n    def __init__(self, id, status, deps, resources={}, priority=0, family='', params={},\n                 disable_failures=None, disable_window=None):\n        self.id = id\n        self.stakeholders = set()  # workers ids that are somehow related to this task (i.e. don't prune while any of these workers are still active)\n        self.workers = set()  # workers ids that can perform task - task is 'BROKEN' if none of these workers are active\n        if deps is None:\n            self.deps = set()\n        else:\n            self.deps = set(deps)\n        self.status = status  # PENDING, RUNNING, FAILED or DONE\n        self.time = time.time()  # Timestamp when task was first added\n        self.retry = None\n        self.remove = None\n        self.worker_running = None  # the worker id that is currently running the task or None\n        self.time_running = None  # Timestamp when picked up by worker\n        self.expl = None\n        self.priority = priority\n        self.resources = resources\n        self.family = family\n        self.params = params\n        self.disable_failures = disable_failures\n        self.failures = Failures(disable_window)\n        self.scheduler_disable_time = None\n\n    def __repr__(self):\n        return \"Task(%r)\" % vars(self)\n\n    def add_failure(self):\n        self.failures.add_failure()\n\n    def has_excessive_failures(self):\n        return self.failures.num_failures() >= self.disable_failures\n\n    def can_disable(self):\n        return self.disable_failures is not None\n\n    def re_enable(self):\n        self.scheduler_disable_time = None\n        self.status = FAILED\n        self.failures.clear()\n\n    def set_status(self, new_status, config):\n        # not sure why we have SUSPENDED, as it can never be set\n        if new_status == SUSPENDED:\n            new_status = PENDING\n\n        if new_status == DISABLED and self.status == RUNNING:\n            return\n\n        if self.status == DISABLED:\n            if new_status == DONE:\n                self.re_enable()\n\n            # don't allow workers to override a scheduler disable\n            elif self.scheduler_disable_time is not None:\n                return\n\n        if new_status == FAILED and self.can_disable():\n            self.add_failure()\n            if self.has_excessive_failures():\n                self.scheduler_disable_time = time.time()\n                new_status = DISABLED\n                notifications.send_error_email(\n                    'Luigi Scheduler: DISABLED {task} due to excessive failures'.format(task=self.id),\n                    '{task} failed {failures} times in the last {window} seconds, so it is being '\n                    'disabled for {persist} seconds'.format(\n                        failures=config.disable_failures,\n                        task=self.id,\n                        window=config.disable_window,\n                        persist=config.disable_persist,\n                        ))\n        elif new_status == DISABLED:\n            self.scheduler_disable_time = None\n\n        self.status = new_status\n\n    def prune(self, config):\n        remove = False\n\n        # Mark tasks with no remaining active stakeholders for deletion\n        if not self.stakeholders:\n            if self.remove is None:\n                logger.info(\"Task %r has stakeholders %r but none remain connected -> will remove task in %s seconds\", self.id, self.stakeholders, config.remove_delay)\n                self.remove = time.time() + config.remove_delay\n\n        # If a running worker disconnects, tag all its jobs as FAILED and subject it to the same retry logic\n        if self.status == RUNNING and self.worker_running and self.worker_running not in self.stakeholders:\n            logger.info(\"Task %r is marked as running by disconnected worker %r -> marking as FAILED with retry delay of %rs\", self.id, self.worker_running, config.retry_delay)\n            self.worker_running = None\n            self.set_status(FAILED, config)\n            self.retry = time.time() + config.retry_delay\n\n        # Re-enable task after the disable time expires\n        if self.status == DISABLED and self.scheduler_disable_time:\n            if time.time() - fix_time(self.scheduler_disable_time) > config.disable_time:\n                self.re_enable()\n\n        # Remove tasks that have no stakeholders\n        if self.remove and time.time() > self.remove:\n            logger.info(\"Removing task %r (no connected stakeholders)\", self.id)\n            remove = True\n\n        # Reset FAILED tasks to PENDING if max timeout is reached, and retry delay is >= 0\n        if self.status == FAILED and config.retry_delay >= 0 and self.retry < time.time():\n            self.set_status(PENDING, config)\n\n        return remove\n\n\nclass Worker(object):\n    \"\"\" Structure for tracking worker activity and keeping their references \"\"\"\n    def __init__(self, id, last_active=None):\n        self.id = id\n        self.reference = None  # reference to the worker in the real world. (Currently a dict containing just the host)\n        self.last_active = last_active  # seconds since epoch\n        self.started = time.time()  # seconds since epoch\n        self.info = {}\n\n    def add_info(self, info):\n        self.info.update(info)\n\n    def update(self, worker_reference):\n        if worker_reference:\n            self.reference = worker_reference\n        self.last_active = time.time()\n\n    def prune(self, config):\n        # Delete workers that haven't said anything for a while (probably killed)\n        if self.last_active + config.worker_disconnect_delay < time.time():\n            return True\n\n    def __str__(self):\n        return self.id\n\n\nclass SimpleTaskState(object):\n    ''' Keep track of the current state and handle persistance\n\n    The point of this class is to enable other ways to keep state, eg. by using a database\n    These will be implemented by creating an abstract base class that this and other classes\n    inherit from.\n    '''\n\n    def __init__(self, state_path):\n        self._state_path = state_path\n        self._tasks = {}  # map from id to a Task object\n        self._active_workers = {}  # map from id to a Worker object\n\n    def dump(self):\n        state = (self._tasks, self._active_workers)\n        try:\n            with open(self._state_path, 'w') as fobj:\n                pickle.dump(state, fobj)\n        except IOError:\n            logger.warning(\"Failed saving scheduler state\", exc_info=1)\n        else:\n            logger.info(\"Saved state in %s\", self._state_path)\n\n    # prone to lead to crashes when old state is unpickled with updated code. TODO some kind of version control?\n    def load(self):\n        if os.path.exists(self._state_path):\n            logger.info(\"Attempting to load state from %s\", self._state_path)\n            try:\n                with open(self._state_path) as fobj:\n                    state = pickle.load(fobj)\n            except:\n                logger.exception(\"Error when loading state. Starting from clean slate.\")\n                return\n\n            self._tasks, self._active_workers = state\n\n            # Convert from old format\n            # TODO: this is really ugly, we need something more future-proof\n            # Every time we add an attribute to the Worker class, this code needs to be updated\n            for k, v in self._active_workers.iteritems():\n                if isinstance(v, float):\n                    self._active_workers[k] = Worker(id=k, last_active=v)\n        else:\n            logger.info(\"No prior state file exists at %s. Starting with clean slate\", self._state_path)\n\n    def get_active_tasks(self):\n        for task in self._tasks.itervalues():\n            yield task\n\n    def get_pending_tasks(self):\n        for task in self._tasks.itervalues():\n            if task.status in [PENDING, RUNNING]:\n                yield task\n\n    def get_task(self, task_id, default=None, setdefault=None):\n        if setdefault:\n            return self._tasks.setdefault(task_id, setdefault)\n        else:\n            return self._tasks.get(task_id, default)\n\n    def has_task(self, task_id):\n        return task_id in self._tasks\n\n    def inactivate_tasks(self, delete_tasks):\n        # The terminology is a bit confusing: we used to \"delete\" tasks when they became inactive,\n        # but with a pluggable state storage, you might very well want to keep some history of\n        # older tasks as well. That's why we call it \"inactivate\" (as in the verb)\n        for task in delete_tasks:\n            self._tasks.pop(task)\n\n    def get_active_workers(self, last_active_lt=None):\n        for worker in self._active_workers.itervalues():\n            if last_active_lt is not None and worker.last_active >= last_active_lt:\n                continue\n            yield worker\n\n    def get_worker_ids(self):\n        return self._active_workers.keys() # only used for unit tests\n\n    def get_worker(self, worker_id):\n        return self._active_workers.setdefault(worker_id, Worker(worker_id))\n\n    def inactivate_workers(self, delete_workers):\n        # Mark workers as inactive\n        for worker in delete_workers:\n            self._active_workers.pop(worker)\n\n        # remove workers from tasks\n        for task in self.get_active_tasks():\n            task.stakeholders.difference_update(delete_workers)\n            task.workers.difference_update(delete_workers)\n\n\nclass CentralPlannerScheduler(Scheduler):\n    ''' Async scheduler that can handle multiple workers etc\n\n    Can be run locally or on a server (using RemoteScheduler + server.Server).\n    '''\n\n    def __init__(self, retry_delay=900.0, remove_delay=600.0, worker_disconnect_delay=60.0,\n                 state_path='/var/lib/luigi-server/state.pickle', task_history=None,\n                 resources=None, disable_persist=0, disable_window=0, disable_failures=None,\n                 max_shown_tasks=100000):\n        '''\n        (all arguments are in seconds)\n        Keyword Arguments:\n        retry_delay -- How long after a Task fails to try it again, or -1 to never retry\n        remove_delay -- How long after a Task finishes to remove it from the scheduler\n        state_path -- Path to state file (tasks and active workers)\n        worker_disconnect_delay -- If a worker hasn't communicated for this long, remove it from active workers\n        '''\n        self._config = SchedulerConfig(\n            retry_delay=retry_delay,\n            remove_delay=remove_delay,\n            worker_disconnect_delay=worker_disconnect_delay,\n            disable_failures=disable_failures,\n            disable_window=disable_window,\n            disable_persist=disable_persist,\n            disable_time=disable_persist,\n            max_shown_tasks=max_shown_tasks,\n        )\n\n        self._task_history = task_history or history.NopHistory()\n        self._state = SimpleTaskState(state_path)\n\n        self._task_history = task_history or history.NopHistory()\n        self._resources = resources\n        self._make_task = functools.partial(\n            Task, disable_failures=disable_failures,\n            disable_window=disable_window)\n\n    def load(self):\n        self._state.load()\n\n    def dump(self):\n        self._state.dump()\n\n    def prune(self):\n        logger.info(\"Starting pruning of task graph\")\n        remove_workers = []\n        for worker in self._state.get_active_workers():\n            if worker.prune(self._config):\n                logger.info(\"Worker %s timed out (no contact for >=%ss)\", worker, self._config.worker_disconnect_delay)\n                remove_workers.append(worker.id)\n\n        self._state.inactivate_workers(remove_workers)\n\n        remove_tasks = []\n        for task in self._state.get_active_tasks():\n            if task.prune(self._config):\n                remove_tasks.append(task.id)\n\n        self._state.inactivate_tasks(remove_tasks)\n\n        logger.info(\"Done pruning task graph\")\n\n    def update(self, worker_id, worker_reference=None):\n        \"\"\" Keep track of whenever the worker was last active \"\"\"\n        worker = self._state.get_worker(worker_id)\n        worker.update(worker_reference)\n\n    def _update_priority(self, task, prio, worker):\n        \"\"\" Update priority of the given task\n\n        Priority can only be increased. If the task doesn't exist, a placeholder\n        task is created to preserve priority when the task is later scheduled.\n        \"\"\"\n        task.priority = prio = max(prio, task.priority)\n        for dep in task.deps or []:\n            t = self._state.get_task(dep)\n            if t is not None and prio > t.priority:\n                self._update_priority(t, prio, worker)\n\n    def add_task(self, worker, task_id, status=PENDING, runnable=True,\n                 deps=None, new_deps=None, expl=None, resources=None,\n                 priority=0, family='', params={}):\n        \"\"\"\n        * Add task identified by task_id if it doesn't exist\n        * If deps is not None, update dependency list\n        * Update status of task\n        * Add additional workers/stakeholders\n        * Update priority when needed\n        \"\"\"\n        self.update(worker)\n\n        task = self._state.get_task(task_id, setdefault=self._make_task(\n                id=task_id, status=PENDING, deps=deps, resources=resources,\n                priority=priority, family=family, params=params))\n\n        # for setting priority, we'll sometimes create tasks with unset family and params\n        if not task.family:\n            task.family = family\n        if not task.params:\n            task.params = params\n\n        if task.remove is not None:\n            task.remove = None  # unmark task for removal so it isn't removed after being added\n\n        if not (task.status == RUNNING and status == PENDING):\n            # don't allow re-scheduling of task while it is running, it must either fail or succeed first\n            if status == PENDING or status != task.status:\n                # Update the DB only if there was a acctual change, to prevent noise.\n                # We also check for status == PENDING b/c that's the default value\n                # (so checking for status != task.status woule lie)\n                self._update_task_history(task_id, status)\n            task.set_status(PENDING if status == SUSPENDED else status, self._config)\n            if status == FAILED:\n                task.retry = time.time() + self._config.retry_delay\n\n        if deps is not None:\n            task.deps = set(deps)\n\n        if new_deps is not None:\n            task.deps.update(new_deps)\n\n        task.stakeholders.add(worker)\n        task.resources = resources\n\n        # Task dependencies might not exist yet. Let's create dummy tasks for them for now.\n        # Otherwise the task dependencies might end up being pruned if scheduling takes a long time\n        for dep in task.deps or []:\n            t = self._state.get_task(dep, setdefault=self._make_task(id=dep, status=UNKNOWN, deps=None, priority=priority))\n            t.stakeholders.add(worker)\n\n        self._update_priority(task, priority, worker)\n\n        if runnable:\n            task.workers.add(worker)\n\n        if expl is not None:\n            task.expl = expl\n\n    def add_worker(self, worker, info):\n        self._state.get_worker(worker).add_info(info)\n\n    def update_resources(self, **resources):\n        if self._resources is None:\n            self._resources = {}\n        self._resources.update(resources)\n\n    def _has_resources(self, needed_resources, used_resources):\n        if needed_resources is None:\n            return True\n\n        available_resources = self._resources or {}\n        for resource, amount in needed_resources.items():\n            if amount + used_resources[resource] > available_resources.get(resource, 1):\n                return False\n        return True\n\n    def _used_resources(self):\n        used_resources = collections.defaultdict(int)\n        if self._resources is not None:\n            for task in self._state.get_active_tasks():\n                if task.status == RUNNING and task.resources:\n                    for resource, amount in task.resources.items():\n                        used_resources[resource] += amount\n        return used_resources\n\n    def _rank(self):\n        ''' Return worker's rank function for task scheduling '''\n        dependents = collections.defaultdict(int)\n        def not_done(t):\n            task = self._state.get_task(t, default=None)\n            return task is None or task.status != DONE\n        for task in self._state.get_active_tasks():\n            if task.status != DONE:\n                deps = filter(not_done, task.deps)\n                inverse_num_deps = 1.0 / max(len(deps), 1)\n                for dep in deps:\n                    dependents[dep] += inverse_num_deps\n\n        return lambda task: (task.priority, dependents[task.id], -task.time)\n\n    def _schedulable(self, task):\n        if task.status != PENDING:\n            return False\n        for dep in task.deps:\n            dep_task = self._state.get_task(dep, default=None)\n            if dep_task is None or dep_task.status != DONE:\n                return False\n        return True\n\n    def get_work(self, worker, host=None):\n        # TODO: remove any expired nodes\n\n        # Algo: iterate over all nodes, find the highest priority node no dependencies and available\n        # resources.\n\n        # Resource checking looks both at currently available resources and at which resources would\n        # be available if all running tasks died and we rescheduled all workers greedily. We do both\n        # checks in order to prevent a worker with many low-priority tasks from starving other\n        # workers with higher priority tasks that share the same resources.\n\n        # TODO: remove tasks that can't be done, figure out if the worker has absolutely\n        # nothing it can wait for\n\n        # Return remaining tasks that have no FAILED descendents\n        self.update(worker, {'host': host})\n        best_task = None\n        best_task_id = None\n        locally_pending_tasks = 0\n        running_tasks = []\n\n        used_resources = self._used_resources()\n        greedy_resources = collections.defaultdict(int)\n        n_unique_pending = 0\n        greedy_workers = dict((worker.id, worker.info.get('workers', 1))\n                              for worker in self._state.get_active_workers())\n\n        tasks = list(self._state.get_pending_tasks())\n        tasks.sort(key=self._rank(), reverse=True)\n\n        for task in tasks:\n            if task.status == 'RUNNING' and worker in task.workers:\n                # Return a list of currently running tasks to the client,\n                # makes it easier to troubleshoot\n                other_worker = self._state.get_worker(task.worker_running)\n                more_info = {'task_id': task.id, 'worker': str(other_worker)}\n                if other_worker is not None:\n                    more_info.update(other_worker.info)\n                    running_tasks.append(more_info)\n\n            if task.status == PENDING and worker in task.workers:\n                locally_pending_tasks += 1\n                if len(task.workers) == 1:\n                    n_unique_pending += 1\n\n            if task.status == RUNNING and task.worker_running in greedy_workers:\n                greedy_workers[task.worker_running] -= 1\n                for resource, amount in (task.resources or {}).items():\n                    greedy_resources[resource] += amount\n\n            if not best_task and self._schedulable(task) and self._has_resources(task.resources, greedy_resources):\n                if worker in task.workers and self._has_resources(task.resources, used_resources):\n                    best_task = task\n                    best_task_id = task.id\n                else:\n                    for task_worker in task.workers:\n                        if greedy_workers.get(task_worker, 0) > 0:\n                            # use up a worker\n                            greedy_workers[task_worker] -= 1\n\n                            # keep track of the resources used in greedy scheduling\n                            for resource, amount in (task.resources or {}).items():\n                                greedy_resources[resource] += amount\n\n                            break\n\n        if best_task:\n            best_task.status = RUNNING\n            best_task.worker_running = worker\n            best_task.time_running = time.time()\n            self._update_task_history(best_task.id, RUNNING, host=host)\n\n        return {'n_pending_tasks': locally_pending_tasks,\n                'n_unique_pending': n_unique_pending,\n                'task_id': best_task_id,\n                'running_tasks': running_tasks}\n\n    def ping(self, worker):\n        self.update(worker)\n\n    def _upstream_status(self, task_id, upstream_status_table):\n        if task_id in upstream_status_table:\n            return upstream_status_table[task_id]\n        elif self._state.has_task(task_id):\n            task_stack = [task_id]\n\n            while task_stack:\n                dep_id = task_stack.pop()\n                if self._state.has_task(dep_id):\n                    dep = self._state.get_task(dep_id)\n                    if dep_id not in upstream_status_table:\n                        if dep.status == PENDING and dep.deps:\n                            task_stack = task_stack + [dep_id] + list(dep.deps)\n                            upstream_status_table[dep_id] = ''  # will be updated postorder\n                        else:\n                            dep_status = STATUS_TO_UPSTREAM_MAP.get(dep.status, '')\n                            upstream_status_table[dep_id] = dep_status\n                    elif upstream_status_table[dep_id] == '' and dep.deps:\n                        # This is the postorder update step when we set the\n                        # status based on the previously calculated child elements\n                        upstream_status = [upstream_status_table.get(id, '') for id in dep.deps]\n                        upstream_status.append('')  # to handle empty list\n                        status = max(upstream_status, key=UPSTREAM_SEVERITY_KEY)\n                        upstream_status_table[dep_id] = status\n            return upstream_status_table[dep_id]\n\n    def _serialize_task(self, task_id, include_deps=True):\n        task = self._state.get_task(task_id)\n        ret = {\n            'deps': list(task.deps),\n            'status': task.status,\n            'workers': list(task.workers),\n            'worker_running': task.worker_running,\n            'time_running': getattr(task, \"time_running\", None),\n            'start_time': task.time,\n            'params': task.params,\n            'name': task.family,\n            'priority': task.priority,\n            'resources': task.resources,\n        }\n        if include_deps:\n            ret['deps'] = list(task.deps)\n        return ret\n\n    def graph(self):\n        self.prune()\n        serialized = {}\n        for task in self._state.get_active_tasks():\n            serialized[task.id] = self._serialize_task(task.id)\n        return serialized\n\n    def _recurse_deps(self, task_id, serialized):\n        if task_id not in serialized:\n            task = self._state.get_task(task_id)\n            if task is None or not task.family:\n                logger.warn('Missing task for id [%s]', task_id)\n\n                # try to infer family and params from task_id\n                try:\n                    family, _, param_str = task_id.rstrip(')').partition('(')\n                    params = dict(param.split('=') for param in param_str.split(', '))\n                except:\n                    family, params = '', {}\n                serialized[task_id] = {\n                    'deps': [],\n                    'status': UNKNOWN,\n                    'workers': [],\n                    'start_time': UNKNOWN,\n                    'params': params,\n                    'name': family,\n                    'priority': 0,\n                }\n            else:\n                serialized[task_id] = self._serialize_task(task_id)\n                for dep in task.deps:\n                    self._recurse_deps(dep, serialized)\n\n    def dep_graph(self, task_id):\n        self.prune()\n        serialized = {}\n        if self._state.has_task(task_id):\n            self._recurse_deps(task_id, serialized)\n        return serialized\n\n    def task_list(self, status, upstream_status, limit=True):\n        ''' query for a subset of tasks by status '''\n        self.prune()\n        result = {}\n        upstream_status_table = {}  # used to memoize upstream status\n        for task in self._state.get_active_tasks():\n            if not status or task.status == status:\n                if (task.status != PENDING or not upstream_status or\n                    upstream_status == self._upstream_status(task.id, upstream_status_table)):\n                    serialized = self._serialize_task(task.id, False)\n                    result[task.id] = serialized\n        if limit and len(result) > self._config.max_shown_tasks:\n            return {'num_tasks': len(result)}\n        return result\n\n    def worker_list(self, include_running=True):\n        self.prune()\n        workers = [\n            dict(\n                name=worker.id,\n                last_active=worker.last_active,\n                started=getattr(worker, 'started', None),\n                **worker.info\n            ) for worker in self._state.get_active_workers()]\n        workers.sort(key=lambda worker: worker['started'], reverse=True)\n        if include_running:\n            running = collections.defaultdict(dict)\n            num_pending = collections.defaultdict(int)\n            num_uniques = collections.defaultdict(int)\n            for task in self._state.get_pending_tasks():\n                if task.status == RUNNING and task.worker_running:\n                    running[task.worker_running][task.id] = self._serialize_task(task.id, False)\n                elif task.status == PENDING:\n                    for worker in task.workers:\n                        num_pending[worker] += 1\n                    if len(task.workers) == 1:\n                        num_uniques[list(task.workers)[0]] += 1\n            for worker in workers:\n                tasks = running[worker['name']]\n                worker['num_running'] = len(tasks)\n                worker['num_pending'] = num_pending[worker['name']]\n                worker['num_uniques'] = num_uniques[worker['name']]\n                worker['running'] = tasks\n        return workers\n\n    def inverse_dependencies(self, task_id):\n        self.prune()\n        serialized = {}\n        if self._state.has_task(task_id):\n            self._traverse_inverse_deps(task_id, serialized)\n        return serialized\n\n    def _traverse_inverse_deps(self, task_id, serialized):\n        stack = [task_id]\n        serialized[task_id] = self._serialize_task(task_id)\n        while len(stack) > 0:\n            curr_id = stack.pop()\n            for task in self._state.get_active_tasks():\n                if curr_id in task.deps:\n                    serialized[curr_id][\"deps\"].append(task.id)\n                    if task.id not in serialized:\n                        serialized[task.id] = self._serialize_task(task.id)\n                        serialized[task.id][\"deps\"] = []\n                        stack.append(task.id)\n\n    def task_search(self, task_str):\n        ''' query for a subset of tasks by task_id '''\n        self.prune()\n        result = collections.defaultdict(dict)\n        for task in self._state.get_active_tasks():\n            if task.id.find(task_str) != -1:\n                serialized = self._serialize_task(task.id, False)\n                result[task.status][task.id] = serialized\n        return result\n\n    def re_enable_task(self, task_id):\n        serialized = {}\n        task = self._state.get_task(task_id)\n        if task and task.status == DISABLED and task.scheduler_disable_time:\n            task.re_enable()\n            serialized = self._serialize_task(task_id)\n        return serialized\n\n    def fetch_error(self, task_id):\n        if self._state.has_task(task_id):\n            return {\"taskId\": task_id, \"error\": self._state.get_task(task_id).expl}\n        else:\n            return {\"taskId\": task_id, \"error\": \"\"}\n\n    def _update_task_history(self, task_id, status, host=None):\n        try:\n            if status == DONE or status == FAILED:\n                successful = (status == DONE)\n                self._task_history.task_finished(task_id, successful)\n            elif status == PENDING:\n                self._task_history.task_scheduled(task_id)\n            elif status == RUNNING:\n                self._task_history.task_started(task_id, host)\n        except:\n            logger.warning(\"Error saving Task history\", exc_info=1)\n\n    @property\n    def task_history(self):\n        # Used by server.py to expose the calls\n        return self._task_history\n",
      "file_after": "# Copyright (c) 2012 Spotify AB\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"); you may not\n# use this file except in compliance with the License. You may obtain a copy of\n# the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n# License for the specific language governing permissions and limitations under\n# the License.\n\nimport collections\nimport datetime\nimport functools\nimport itertools\nimport notifications\nimport os\nimport logging\nimport time\nimport cPickle as pickle\nimport task_history as history\nlogger = logging.getLogger(\"luigi.server\")\n\nfrom task_status import PENDING, FAILED, DONE, RUNNING, SUSPENDED, UNKNOWN, DISABLED\n\n\nclass Scheduler(object):\n    ''' Abstract base class\n\n    Note that the methods all take string arguments, not Task objects...\n    '''\n    add_task = NotImplemented\n    get_work = NotImplemented\n    ping = NotImplemented\n\nUPSTREAM_RUNNING = 'UPSTREAM_RUNNING'\nUPSTREAM_MISSING_INPUT = 'UPSTREAM_MISSING_INPUT'\nUPSTREAM_FAILED = 'UPSTREAM_FAILED'\nUPSTREAM_DISABLED = 'UPSTREAM_DISABLED'\n\nUPSTREAM_SEVERITY_ORDER = (\n    '',\n    UPSTREAM_RUNNING,\n    UPSTREAM_MISSING_INPUT,\n    UPSTREAM_FAILED,\n    UPSTREAM_DISABLED,\n)\nUPSTREAM_SEVERITY_KEY = lambda st: UPSTREAM_SEVERITY_ORDER.index(st)\nSTATUS_TO_UPSTREAM_MAP = {\n    FAILED: UPSTREAM_FAILED,\n    RUNNING: UPSTREAM_RUNNING,\n    PENDING: UPSTREAM_MISSING_INPUT,\n    DISABLED: UPSTREAM_DISABLED,\n}\n\n\n# We're passing around this config a lot, so let's put it on an object\nSchedulerConfig = collections.namedtuple('SchedulerConfig', [\n        'retry_delay', 'remove_delay', 'worker_disconnect_delay',\n        'disable_failures', 'disable_window', 'disable_persist', 'disable_time',\n        'max_shown_tasks',\n])\n\n\ndef fix_time(x):\n    # Backwards compatibility for a fix in Dec 2014. Prior to the fix, pickled state might store datetime objects\n    # Let's remove this function soon\n    if isinstance(x, datetime.datetime):\n        return time.mktime(x.timetuple())\n    else:\n        return x\n\n\nclass Failures(object):\n    \"\"\" This class tracks the number of failures in a given time window\n\n    Failures added are marked with the current timestamp, and this class counts\n    the number of failures in a sliding time window ending at the present.\n\n    \"\"\"\n\n    def __init__(self, window):\n        \"\"\" Initialize with the given window\n\n        :param window: how long to track failures for, as a float (number of seconds)\n        \"\"\"\n        self.window = window\n        self.failures = collections.deque()\n\n    def add_failure(self):\n        \"\"\" Add a failure event with the current timestamp \"\"\"\n        self.failures.append(time.time())\n\n    def num_failures(self):\n        \"\"\" Return the number of failures in the window \"\"\"\n        min_time = time.time() - self.window\n\n        while self.failures and fix_time(self.failures[0]) < min_time:\n            self.failures.popleft()\n\n        return len(self.failures)\n\n    def clear(self):\n        \"\"\" Clear the failure queue \"\"\"\n        self.failures.clear()\n\n\nclass Task(object):\n    def __init__(self, id, status, deps, resources={}, priority=0, family='', params={},\n                 disable_failures=None, disable_window=None):\n        self.id = id\n        self.stakeholders = set()  # workers ids that are somehow related to this task (i.e. don't prune while any of these workers are still active)\n        self.workers = set()  # workers ids that can perform task - task is 'BROKEN' if none of these workers are active\n        if deps is None:\n            self.deps = set()\n        else:\n            self.deps = set(deps)\n        self.status = status  # PENDING, RUNNING, FAILED or DONE\n        self.time = time.time()  # Timestamp when task was first added\n        self.retry = None\n        self.remove = None\n        self.worker_running = None  # the worker id that is currently running the task or None\n        self.time_running = None  # Timestamp when picked up by worker\n        self.expl = None\n        self.priority = priority\n        self.resources = resources\n        self.family = family\n        self.params = params\n        self.disable_failures = disable_failures\n        self.failures = Failures(disable_window)\n        self.scheduler_disable_time = None\n\n    def __repr__(self):\n        return \"Task(%r)\" % vars(self)\n\n    def add_failure(self):\n        self.failures.add_failure()\n\n    def has_excessive_failures(self):\n        return self.failures.num_failures() >= self.disable_failures\n\n    def can_disable(self):\n        return self.disable_failures is not None\n\n\nclass Worker(object):\n    \"\"\" Structure for tracking worker activity and keeping their references \"\"\"\n    def __init__(self, id, last_active=None):\n        self.id = id\n        self.reference = None  # reference to the worker in the real world. (Currently a dict containing just the host)\n        self.last_active = last_active  # seconds since epoch\n        self.started = time.time()  # seconds since epoch\n        self.info = {}\n\n    def add_info(self, info):\n        self.info.update(info)\n\n    def update(self, worker_reference):\n        if worker_reference:\n            self.reference = worker_reference\n        self.last_active = time.time()\n\n    def prune(self, config):\n        # Delete workers that haven't said anything for a while (probably killed)\n        if self.last_active + config.worker_disconnect_delay < time.time():\n            return True\n\n    def __str__(self):\n        return self.id\n\n\nclass SimpleTaskState(object):\n    ''' Keep track of the current state and handle persistance\n\n    The point of this class is to enable other ways to keep state, eg. by using a database\n    These will be implemented by creating an abstract base class that this and other classes\n    inherit from.\n    '''\n\n    def __init__(self, state_path):\n        self._state_path = state_path\n        self._tasks = {}  # map from id to a Task object\n        self._status_tasks = collections.defaultdict(dict)\n        self._active_workers = {}  # map from id to a Worker object\n\n    def dump(self):\n        state = (self._tasks, self._active_workers)\n        try:\n            with open(self._state_path, 'w') as fobj:\n                pickle.dump(state, fobj)\n        except IOError:\n            logger.warning(\"Failed saving scheduler state\", exc_info=1)\n        else:\n            logger.info(\"Saved state in %s\", self._state_path)\n\n    # prone to lead to crashes when old state is unpickled with updated code. TODO some kind of version control?\n    def load(self):\n        if os.path.exists(self._state_path):\n            logger.info(\"Attempting to load state from %s\", self._state_path)\n            try:\n                with open(self._state_path) as fobj:\n                    state = pickle.load(fobj)\n            except:\n                logger.exception(\"Error when loading state. Starting from clean slate.\")\n                return\n\n            self._tasks, self._active_workers = state\n            self._status_tasks = collections.defaultdict(dict)\n            for task in self._tasks.itervalues():\n                self._status_tasks[task.status][task.id] = task\n\n            # Convert from old format\n            # TODO: this is really ugly, we need something more future-proof\n            # Every time we add an attribute to the Worker class, this code needs to be updated\n            for k, v in self._active_workers.iteritems():\n                if isinstance(v, float):\n                    self._active_workers[k] = Worker(id=k, last_active=v)\n        else:\n            logger.info(\"No prior state file exists at %s. Starting with clean slate\", self._state_path)\n\n    def get_active_tasks(self, status=None):\n        if status:\n            for task in self._status_tasks[status].itervalues():\n                yield task\n        else:\n            for task in self._tasks.itervalues():\n                yield task\n\n    def get_running_tasks(self):\n        return self._status_tasks[RUNNING].itervalues()\n\n    def get_pending_tasks(self):\n        return itertools.chain.from_iterable(self._status_tasks[status].itervalues()\n                                             for status in [PENDING, RUNNING])\n\n    def get_task(self, task_id, default=None, setdefault=None):\n        if setdefault:\n            task = self._tasks.setdefault(task_id, setdefault)\n            self._status_tasks[task.status][task.id] = task\n            return task\n        else:\n            return self._tasks.get(task_id, default)\n\n    def has_task(self, task_id):\n        return task_id in self._tasks\n\n    def re_enable(self, task, config=None):\n        task.scheduler_disable_time = None\n        task.failures.clear()\n        if config:\n            self.set_status(task, FAILED, config)\n            task.failures.clear()\n\n    def set_status(self, task, new_status, config=None):\n        if new_status == FAILED:\n            assert config is not None\n\n        # not sure why we have SUSPENDED, as it can never be set\n        if new_status == SUSPENDED:\n            new_status = PENDING\n\n        if new_status == DISABLED and task.status == RUNNING:\n            return\n\n        if task.status == DISABLED:\n            if new_status == DONE:\n                self.re_enable(task)\n\n            # don't allow workers to override a scheduler disable\n            elif task.scheduler_disable_time is not None:\n                return\n\n        if new_status == FAILED and task.can_disable():\n            task.add_failure()\n            if task.has_excessive_failures():\n                task.scheduler_disable_time = time.time()\n                new_status = DISABLED\n                notifications.send_error_email(\n                    'Luigi Scheduler: DISABLED {task} due to excessive failures'.format(task=task.id),\n                    '{task} failed {failures} times in the last {window} seconds, so it is being '\n                    'disabled for {persist} seconds'.format(\n                        failures=config.disable_failures,\n                        task=task.id,\n                        window=config.disable_window,\n                        persist=config.disable_persist,\n                    ))\n        elif new_status == DISABLED:\n            task.scheduler_disable_time = None\n\n        self._status_tasks[task.status].pop(task.id)\n        self._status_tasks[new_status][task.id] = task\n        task.status = new_status\n\n    def prune(self, task, config):\n        remove = False\n\n        # Mark tasks with no remaining active stakeholders for deletion\n        if not task.stakeholders:\n            if task.remove is None:\n                logger.info(\"Task %r has stakeholders %r but none remain connected -> will remove \"\n                            \"task in %s seconds\", task.id, task.stakeholders, config.remove_delay)\n                task.remove = time.time() + config.remove_delay\n\n        # If a running worker disconnects, tag all its jobs as FAILED and subject it to the same retry logic\n        if task.status == RUNNING and task.worker_running and task.worker_running not in task.stakeholders:\n            logger.info(\"Task %r is marked as running by disconnected worker %r -> marking as \"\n                        \"FAILED with retry delay of %rs\", task.id, task.worker_running,\n                        config.retry_delay)\n            task.worker_running = None\n            self.set_status(task, FAILED, config)\n            task.retry = time.time() + config.retry_delay\n\n        # Re-enable task after the disable time expires\n        if task.status == DISABLED and task.scheduler_disable_time:\n            if time.time() - fix_time(task.scheduler_disable_time) > config.disable_time:\n                self.re_enable(task, config)\n\n        # Remove tasks that have no stakeholders\n        if task.remove and time.time() > task.remove:\n            logger.info(\"Removing task %r (no connected stakeholders)\", task.id)\n            remove = True\n\n        # Reset FAILED tasks to PENDING if max timeout is reached, and retry delay is >= 0\n        if task.status == FAILED and config.retry_delay >= 0 and task.retry < time.time():\n            self.set_status(task, PENDING, config)\n\n        return remove\n\n    def inactivate_tasks(self, delete_tasks):\n        # The terminology is a bit confusing: we used to \"delete\" tasks when they became inactive,\n        # but with a pluggable state storage, you might very well want to keep some history of\n        # older tasks as well. That's why we call it \"inactivate\" (as in the verb)\n        for task in delete_tasks:\n            task_obj = self._tasks.pop(task)\n            self._status_tasks[task_obj.status].pop(task)\n\n    def get_active_workers(self, last_active_lt=None):\n        for worker in self._active_workers.itervalues():\n            if last_active_lt is not None and worker.last_active >= last_active_lt:\n                continue\n            yield worker\n\n    def get_worker_ids(self):\n        return self._active_workers.keys() # only used for unit tests\n\n    def get_worker(self, worker_id):\n        return self._active_workers.setdefault(worker_id, Worker(worker_id))\n\n    def inactivate_workers(self, delete_workers):\n        # Mark workers as inactive\n        for worker in delete_workers:\n            self._active_workers.pop(worker)\n\n        # remove workers from tasks\n        for task in self.get_active_tasks():\n            task.stakeholders.difference_update(delete_workers)\n            task.workers.difference_update(delete_workers)\n\n\nclass CentralPlannerScheduler(Scheduler):\n    ''' Async scheduler that can handle multiple workers etc\n\n    Can be run locally or on a server (using RemoteScheduler + server.Server).\n    '''\n\n    def __init__(self, retry_delay=900.0, remove_delay=600.0, worker_disconnect_delay=60.0,\n                 state_path='/var/lib/luigi-server/state.pickle', task_history=None,\n                 resources=None, disable_persist=0, disable_window=0, disable_failures=None,\n                 max_shown_tasks=100000):\n        '''\n        (all arguments are in seconds)\n        Keyword Arguments:\n        retry_delay -- How long after a Task fails to try it again, or -1 to never retry\n        remove_delay -- How long after a Task finishes to remove it from the scheduler\n        state_path -- Path to state file (tasks and active workers)\n        worker_disconnect_delay -- If a worker hasn't communicated for this long, remove it from active workers\n        '''\n        self._config = SchedulerConfig(\n            retry_delay=retry_delay,\n            remove_delay=remove_delay,\n            worker_disconnect_delay=worker_disconnect_delay,\n            disable_failures=disable_failures,\n            disable_window=disable_window,\n            disable_persist=disable_persist,\n            disable_time=disable_persist,\n            max_shown_tasks=max_shown_tasks,\n        )\n\n        self._task_history = task_history or history.NopHistory()\n        self._state = SimpleTaskState(state_path)\n\n        self._task_history = task_history or history.NopHistory()\n        self._resources = resources\n        self._make_task = functools.partial(\n            Task, disable_failures=disable_failures,\n            disable_window=disable_window)\n\n    def load(self):\n        self._state.load()\n\n    def dump(self):\n        self._state.dump()\n\n    def prune(self):\n        logger.info(\"Starting pruning of task graph\")\n        remove_workers = []\n        for worker in self._state.get_active_workers():\n            if worker.prune(self._config):\n                logger.info(\"Worker %s timed out (no contact for >=%ss)\", worker, self._config.worker_disconnect_delay)\n                remove_workers.append(worker.id)\n\n        self._state.inactivate_workers(remove_workers)\n\n        remove_tasks = []\n        for task in self._state.get_active_tasks():\n            if self._state.prune(task, self._config):\n                remove_tasks.append(task.id)\n\n        self._state.inactivate_tasks(remove_tasks)\n\n        logger.info(\"Done pruning task graph\")\n\n    def update(self, worker_id, worker_reference=None):\n        \"\"\" Keep track of whenever the worker was last active \"\"\"\n        worker = self._state.get_worker(worker_id)\n        worker.update(worker_reference)\n\n    def _update_priority(self, task, prio, worker):\n        \"\"\" Update priority of the given task\n\n        Priority can only be increased. If the task doesn't exist, a placeholder\n        task is created to preserve priority when the task is later scheduled.\n        \"\"\"\n        task.priority = prio = max(prio, task.priority)\n        for dep in task.deps or []:\n            t = self._state.get_task(dep)\n            if t is not None and prio > t.priority:\n                self._update_priority(t, prio, worker)\n\n    def add_task(self, worker, task_id, status=PENDING, runnable=True,\n                 deps=None, new_deps=None, expl=None, resources=None,\n                 priority=0, family='', params={}):\n        \"\"\"\n        * Add task identified by task_id if it doesn't exist\n        * If deps is not None, update dependency list\n        * Update status of task\n        * Add additional workers/stakeholders\n        * Update priority when needed\n        \"\"\"\n        self.update(worker)\n\n        task = self._state.get_task(task_id, setdefault=self._make_task(\n                id=task_id, status=PENDING, deps=deps, resources=resources,\n                priority=priority, family=family, params=params))\n\n        # for setting priority, we'll sometimes create tasks with unset family and params\n        if not task.family:\n            task.family = family\n        if not task.params:\n            task.params = params\n\n        if task.remove is not None:\n            task.remove = None  # unmark task for removal so it isn't removed after being added\n\n        if not (task.status == RUNNING and status == PENDING):\n            # don't allow re-scheduling of task while it is running, it must either fail or succeed first\n            if status == PENDING or status != task.status:\n                # Update the DB only if there was a acctual change, to prevent noise.\n                # We also check for status == PENDING b/c that's the default value\n                # (so checking for status != task.status woule lie)\n                self._update_task_history(task_id, status)\n            self._state.set_status(task, PENDING if status == SUSPENDED else status, self._config)\n            if status == FAILED:\n                task.retry = time.time() + self._config.retry_delay\n\n        if deps is not None:\n            task.deps = set(deps)\n\n        if new_deps is not None:\n            task.deps.update(new_deps)\n\n        task.stakeholders.add(worker)\n        task.resources = resources\n\n        # Task dependencies might not exist yet. Let's create dummy tasks for them for now.\n        # Otherwise the task dependencies might end up being pruned if scheduling takes a long time\n        for dep in task.deps or []:\n            t = self._state.get_task(dep, setdefault=self._make_task(id=dep, status=UNKNOWN, deps=None, priority=priority))\n            t.stakeholders.add(worker)\n\n        self._update_priority(task, priority, worker)\n\n        if runnable:\n            task.workers.add(worker)\n\n        if expl is not None:\n            task.expl = expl\n\n    def add_worker(self, worker, info):\n        self._state.get_worker(worker).add_info(info)\n\n    def update_resources(self, **resources):\n        if self._resources is None:\n            self._resources = {}\n        self._resources.update(resources)\n\n    def _has_resources(self, needed_resources, used_resources):\n        if needed_resources is None:\n            return True\n\n        available_resources = self._resources or {}\n        for resource, amount in needed_resources.items():\n            if amount + used_resources[resource] > available_resources.get(resource, 1):\n                return False\n        return True\n\n    def _used_resources(self):\n        used_resources = collections.defaultdict(int)\n        if self._resources is not None:\n            for task in self._state.get_active_tasks():\n                if task.status == RUNNING and task.resources:\n                    for resource, amount in task.resources.items():\n                        used_resources[resource] += amount\n        return used_resources\n\n    def _rank(self):\n        ''' Return worker's rank function for task scheduling '''\n        dependents = collections.defaultdict(int)\n        def not_done(t):\n            task = self._state.get_task(t, default=None)\n            return task is None or task.status != DONE\n        for task in self._state.get_pending_tasks():\n            if task.status != DONE:\n                deps = filter(not_done, task.deps)\n                inverse_num_deps = 1.0 / max(len(deps), 1)\n                for dep in deps:\n                    dependents[dep] += inverse_num_deps\n\n        return lambda task: (task.priority, dependents[task.id], -task.time)\n\n    def _schedulable(self, task):\n        if task.status != PENDING:\n            return False\n        for dep in task.deps:\n            dep_task = self._state.get_task(dep, default=None)\n            if dep_task is None or dep_task.status != DONE:\n                return False\n        return True\n\n    def get_work(self, worker, host=None):\n        # TODO: remove any expired nodes\n\n        # Algo: iterate over all nodes, find the highest priority node no dependencies and available\n        # resources.\n\n        # Resource checking looks both at currently available resources and at which resources would\n        # be available if all running tasks died and we rescheduled all workers greedily. We do both\n        # checks in order to prevent a worker with many low-priority tasks from starving other\n        # workers with higher priority tasks that share the same resources.\n\n        # TODO: remove tasks that can't be done, figure out if the worker has absolutely\n        # nothing it can wait for\n\n        # Return remaining tasks that have no FAILED descendents\n        self.update(worker, {'host': host})\n        best_task = None\n        best_task_id = None\n        locally_pending_tasks = 0\n        running_tasks = []\n\n        used_resources = self._used_resources()\n        greedy_resources = collections.defaultdict(int)\n        n_unique_pending = 0\n        greedy_workers = dict((worker.id, worker.info.get('workers', 1))\n                              for worker in self._state.get_active_workers())\n\n        tasks = list(self._state.get_pending_tasks())\n        tasks.sort(key=self._rank(), reverse=True)\n\n        for task in tasks:\n            if task.status == 'RUNNING' and worker in task.workers:\n                # Return a list of currently running tasks to the client,\n                # makes it easier to troubleshoot\n                other_worker = self._state.get_worker(task.worker_running)\n                more_info = {'task_id': task.id, 'worker': str(other_worker)}\n                if other_worker is not None:\n                    more_info.update(other_worker.info)\n                    running_tasks.append(more_info)\n\n            if task.status == PENDING and worker in task.workers:\n                locally_pending_tasks += 1\n                if len(task.workers) == 1:\n                    n_unique_pending += 1\n\n            if task.status == RUNNING and task.worker_running in greedy_workers:\n                greedy_workers[task.worker_running] -= 1\n                for resource, amount in (task.resources or {}).items():\n                    greedy_resources[resource] += amount\n\n            if not best_task and self._schedulable(task) and self._has_resources(task.resources, greedy_resources):\n                if worker in task.workers and self._has_resources(task.resources, used_resources):\n                    best_task = task\n                    best_task_id = task.id\n                else:\n                    for task_worker in task.workers:\n                        if greedy_workers.get(task_worker, 0) > 0:\n                            # use up a worker\n                            greedy_workers[task_worker] -= 1\n\n                            # keep track of the resources used in greedy scheduling\n                            for resource, amount in (task.resources or {}).items():\n                                greedy_resources[resource] += amount\n\n                            break\n\n        if best_task:\n            self._state.set_status(best_task, RUNNING, self._config)\n            best_task.worker_running = worker\n            best_task.time_running = time.time()\n            self._update_task_history(best_task.id, RUNNING, host=host)\n\n        return {'n_pending_tasks': locally_pending_tasks,\n                'n_unique_pending': n_unique_pending,\n                'task_id': best_task_id,\n                'running_tasks': running_tasks}\n\n    def ping(self, worker):\n        self.update(worker)\n\n    def _upstream_status(self, task_id, upstream_status_table):\n        if task_id in upstream_status_table:\n            return upstream_status_table[task_id]\n        elif self._state.has_task(task_id):\n            task_stack = [task_id]\n\n            while task_stack:\n                dep_id = task_stack.pop()\n                if self._state.has_task(dep_id):\n                    dep = self._state.get_task(dep_id)\n                    if dep_id not in upstream_status_table:\n                        if dep.status == PENDING and dep.deps:\n                            task_stack = task_stack + [dep_id] + list(dep.deps)\n                            upstream_status_table[dep_id] = ''  # will be updated postorder\n                        else:\n                            dep_status = STATUS_TO_UPSTREAM_MAP.get(dep.status, '')\n                            upstream_status_table[dep_id] = dep_status\n                    elif upstream_status_table[dep_id] == '' and dep.deps:\n                        # This is the postorder update step when we set the\n                        # status based on the previously calculated child elements\n                        upstream_status = [upstream_status_table.get(id, '') for id in dep.deps]\n                        upstream_status.append('')  # to handle empty list\n                        status = max(upstream_status, key=UPSTREAM_SEVERITY_KEY)\n                        upstream_status_table[dep_id] = status\n            return upstream_status_table[dep_id]\n\n    def _serialize_task(self, task_id, include_deps=True):\n        task = self._state.get_task(task_id)\n        ret = {\n            'deps': list(task.deps),\n            'status': task.status,\n            'workers': list(task.workers),\n            'worker_running': task.worker_running,\n            'time_running': getattr(task, \"time_running\", None),\n            'start_time': task.time,\n            'params': task.params,\n            'name': task.family,\n            'priority': task.priority,\n            'resources': task.resources,\n        }\n        if include_deps:\n            ret['deps'] = list(task.deps)\n        return ret\n\n    def graph(self):\n        self.prune()\n        serialized = {}\n        for task in self._state.get_active_tasks():\n            serialized[task.id] = self._serialize_task(task.id)\n        return serialized\n\n    def _recurse_deps(self, task_id, serialized):\n        if task_id not in serialized:\n            task = self._state.get_task(task_id)\n            if task is None or not task.family:\n                logger.warn('Missing task for id [%s]', task_id)\n\n                # try to infer family and params from task_id\n                try:\n                    family, _, param_str = task_id.rstrip(')').partition('(')\n                    params = dict(param.split('=') for param in param_str.split(', '))\n                except:\n                    family, params = '', {}\n                serialized[task_id] = {\n                    'deps': [],\n                    'status': UNKNOWN,\n                    'workers': [],\n                    'start_time': UNKNOWN,\n                    'params': params,\n                    'name': family,\n                    'priority': 0,\n                }\n            else:\n                serialized[task_id] = self._serialize_task(task_id)\n                for dep in task.deps:\n                    self._recurse_deps(dep, serialized)\n\n    def dep_graph(self, task_id):\n        self.prune()\n        serialized = {}\n        if self._state.has_task(task_id):\n            self._recurse_deps(task_id, serialized)\n        return serialized\n\n    def task_list(self, status, upstream_status, limit=True):\n        ''' query for a subset of tasks by status '''\n        self.prune()\n        result = {}\n        upstream_status_table = {}  # used to memoize upstream status\n        for task in self._state.get_active_tasks(status):\n            if (task.status != PENDING or not upstream_status or\n                upstream_status == self._upstream_status(task.id, upstream_status_table)):\n                serialized = self._serialize_task(task.id, False)\n                result[task.id] = serialized\n        if limit and len(result) > self._config.max_shown_tasks:\n            return {'num_tasks': len(result)}\n        return result\n\n    def worker_list(self, include_running=True):\n        self.prune()\n        workers = [\n            dict(\n                name=worker.id,\n                last_active=worker.last_active,\n                started=getattr(worker, 'started', None),\n                **worker.info\n            ) for worker in self._state.get_active_workers()]\n        workers.sort(key=lambda worker: worker['started'], reverse=True)\n        if include_running:\n            running = collections.defaultdict(dict)\n            num_pending = collections.defaultdict(int)\n            num_uniques = collections.defaultdict(int)\n            for task in self._state.get_pending_tasks():\n                if task.status == RUNNING and task.worker_running:\n                    running[task.worker_running][task.id] = self._serialize_task(task.id, False)\n                elif task.status == PENDING:\n                    for worker in task.workers:\n                        num_pending[worker] += 1\n                    if len(task.workers) == 1:\n                        num_uniques[list(task.workers)[0]] += 1\n            for worker in workers:\n                tasks = running[worker['name']]\n                worker['num_running'] = len(tasks)\n                worker['num_pending'] = num_pending[worker['name']]\n                worker['num_uniques'] = num_uniques[worker['name']]\n                worker['running'] = tasks\n        return workers\n\n    def inverse_dependencies(self, task_id):\n        self.prune()\n        serialized = {}\n        if self._state.has_task(task_id):\n            self._traverse_inverse_deps(task_id, serialized)\n        return serialized\n\n    def _traverse_inverse_deps(self, task_id, serialized):\n        stack = [task_id]\n        serialized[task_id] = self._serialize_task(task_id)\n        while len(stack) > 0:\n            curr_id = stack.pop()\n            for task in self._state.get_active_tasks():\n                if curr_id in task.deps:\n                    serialized[curr_id][\"deps\"].append(task.id)\n                    if task.id not in serialized:\n                        serialized[task.id] = self._serialize_task(task.id)\n                        serialized[task.id][\"deps\"] = []\n                        stack.append(task.id)\n\n    def task_search(self, task_str):\n        ''' query for a subset of tasks by task_id '''\n        self.prune()\n        result = collections.defaultdict(dict)\n        for task in self._state.get_active_tasks():\n            if task.id.find(task_str) != -1:\n                serialized = self._serialize_task(task.id, False)\n                result[task.status][task.id] = serialized\n        return result\n\n    def re_enable_task(self, task_id):\n        serialized = {}\n        task = self._state.get_task(task_id)\n        if task and task.status == DISABLED and task.scheduler_disable_time:\n            self._state.re_enable(task, self._config)\n            serialized = self._serialize_task(task_id)\n        return serialized\n\n    def fetch_error(self, task_id):\n        if self._state.has_task(task_id):\n            return {\"taskId\": task_id, \"error\": self._state.get_task(task_id).expl}\n        else:\n            return {\"taskId\": task_id, \"error\": \"\"}\n\n    def _update_task_history(self, task_id, status, host=None):\n        try:\n            if status == DONE or status == FAILED:\n                successful = (status == DONE)\n                self._task_history.task_finished(task_id, successful)\n            elif status == PENDING:\n                self._task_history.task_scheduled(task_id)\n            elif status == RUNNING:\n                self._task_history.task_started(task_id, host)\n        except:\n            logger.warning(\"Error saving Task history\", exc_info=1)\n\n    @property\n    def task_history(self):\n        # Used by server.py to expose the calls\n        return self._task_history\n",
      "file_patch": "@@ -15,6 +15,7 @@\n import collections\n import datetime\n import functools\n+import itertools\n import notifications\n import os\n import logging\n@@ -144,78 +145,6 @@ class Task(object):\n     def can_disable(self):\n         return self.disable_failures is not None\n \n-    def re_enable(self):\n-        self.scheduler_disable_time = None\n-        self.status = FAILED\n-        self.failures.clear()\n-\n-    def set_status(self, new_status, config):\n-        # not sure why we have SUSPENDED, as it can never be set\n-        if new_status == SUSPENDED:\n-            new_status = PENDING\n-\n-        if new_status == DISABLED and self.status == RUNNING:\n-            return\n-\n-        if self.status == DISABLED:\n-            if new_status == DONE:\n-                self.re_enable()\n-\n-            # don't allow workers to override a scheduler disable\n-            elif self.scheduler_disable_time is not None:\n-                return\n-\n-        if new_status == FAILED and self.can_disable():\n-            self.add_failure()\n-            if self.has_excessive_failures():\n-                self.scheduler_disable_time = time.time()\n-                new_status = DISABLED\n-                notifications.send_error_email(\n-                    'Luigi Scheduler: DISABLED {task} due to excessive failures'.format(task=self.id),\n-                    '{task} failed {failures} times in the last {window} seconds, so it is being '\n-                    'disabled for {persist} seconds'.format(\n-                        failures=config.disable_failures,\n-                        task=self.id,\n-                        window=config.disable_window,\n-                        persist=config.disable_persist,\n-                        ))\n-        elif new_status == DISABLED:\n-            self.scheduler_disable_time = None\n-\n-        self.status = new_status\n-\n-    def prune(self, config):\n-        remove = False\n-\n-        # Mark tasks with no remaining active stakeholders for deletion\n-        if not self.stakeholders:\n-            if self.remove is None:\n-                logger.info(\"Task %r has stakeholders %r but none remain connected -> will remove task in %s seconds\", self.id, self.stakeholders, config.remove_delay)\n-                self.remove = time.time() + config.remove_delay\n-\n-        # If a running worker disconnects, tag all its jobs as FAILED and subject it to the same retry logic\n-        if self.status == RUNNING and self.worker_running and self.worker_running not in self.stakeholders:\n-            logger.info(\"Task %r is marked as running by disconnected worker %r -> marking as FAILED with retry delay of %rs\", self.id, self.worker_running, config.retry_delay)\n-            self.worker_running = None\n-            self.set_status(FAILED, config)\n-            self.retry = time.time() + config.retry_delay\n-\n-        # Re-enable task after the disable time expires\n-        if self.status == DISABLED and self.scheduler_disable_time:\n-            if time.time() - fix_time(self.scheduler_disable_time) > config.disable_time:\n-                self.re_enable()\n-\n-        # Remove tasks that have no stakeholders\n-        if self.remove and time.time() > self.remove:\n-            logger.info(\"Removing task %r (no connected stakeholders)\", self.id)\n-            remove = True\n-\n-        # Reset FAILED tasks to PENDING if max timeout is reached, and retry delay is >= 0\n-        if self.status == FAILED and config.retry_delay >= 0 and self.retry < time.time():\n-            self.set_status(PENDING, config)\n-\n-        return remove\n-\n \n class Worker(object):\n     \"\"\" Structure for tracking worker activity and keeping their references \"\"\"\n@@ -254,6 +183,7 @@ class SimpleTaskState(object):\n     def __init__(self, state_path):\n         self._state_path = state_path\n         self._tasks = {}  # map from id to a Task object\n+        self._status_tasks = collections.defaultdict(dict)\n         self._active_workers = {}  # map from id to a Worker object\n \n     def dump(self):\n@@ -278,6 +208,9 @@ class SimpleTaskState(object):\n                 return\n \n             self._tasks, self._active_workers = state\n+            self._status_tasks = collections.defaultdict(dict)\n+            for task in self._tasks.itervalues():\n+                self._status_tasks[task.status][task.id] = task\n \n             # Convert from old format\n             # TODO: this is really ugly, we need something more future-proof\n@@ -288,30 +221,121 @@ class SimpleTaskState(object):\n         else:\n             logger.info(\"No prior state file exists at %s. Starting with clean slate\", self._state_path)\n \n-    def get_active_tasks(self):\n-        for task in self._tasks.itervalues():\n-            yield task\n+    def get_active_tasks(self, status=None):\n+        if status:\n+            for task in self._status_tasks[status].itervalues():\n+                yield task\n+        else:\n+            for task in self._tasks.itervalues():\n+                yield task\n+\n+    def get_running_tasks(self):\n+        return self._status_tasks[RUNNING].itervalues()\n \n     def get_pending_tasks(self):\n-        for task in self._tasks.itervalues():\n-            if task.status in [PENDING, RUNNING]:\n-                yield task\n+        return itertools.chain.from_iterable(self._status_tasks[status].itervalues()\n+                                             for status in [PENDING, RUNNING])\n \n     def get_task(self, task_id, default=None, setdefault=None):\n         if setdefault:\n-            return self._tasks.setdefault(task_id, setdefault)\n+            task = self._tasks.setdefault(task_id, setdefault)\n+            self._status_tasks[task.status][task.id] = task\n+            return task\n         else:\n             return self._tasks.get(task_id, default)\n \n     def has_task(self, task_id):\n         return task_id in self._tasks\n \n+    def re_enable(self, task, config=None):\n+        task.scheduler_disable_time = None\n+        task.failures.clear()\n+        if config:\n+            self.set_status(task, FAILED, config)\n+            task.failures.clear()\n+\n+    def set_status(self, task, new_status, config=None):\n+        if new_status == FAILED:\n+            assert config is not None\n+\n+        # not sure why we have SUSPENDED, as it can never be set\n+        if new_status == SUSPENDED:\n+            new_status = PENDING\n+\n+        if new_status == DISABLED and task.status == RUNNING:\n+            return\n+\n+        if task.status == DISABLED:\n+            if new_status == DONE:\n+                self.re_enable(task)\n+\n+            # don't allow workers to override a scheduler disable\n+            elif task.scheduler_disable_time is not None:\n+                return\n+\n+        if new_status == FAILED and task.can_disable():\n+            task.add_failure()\n+            if task.has_excessive_failures():\n+                task.scheduler_disable_time = time.time()\n+                new_status = DISABLED\n+                notifications.send_error_email(\n+                    'Luigi Scheduler: DISABLED {task} due to excessive failures'.format(task=task.id),\n+                    '{task} failed {failures} times in the last {window} seconds, so it is being '\n+                    'disabled for {persist} seconds'.format(\n+                        failures=config.disable_failures,\n+                        task=task.id,\n+                        window=config.disable_window,\n+                        persist=config.disable_persist,\n+                    ))\n+        elif new_status == DISABLED:\n+            task.scheduler_disable_time = None\n+\n+        self._status_tasks[task.status].pop(task.id)\n+        self._status_tasks[new_status][task.id] = task\n+        task.status = new_status\n+\n+    def prune(self, task, config):\n+        remove = False\n+\n+        # Mark tasks with no remaining active stakeholders for deletion\n+        if not task.stakeholders:\n+            if task.remove is None:\n+                logger.info(\"Task %r has stakeholders %r but none remain connected -> will remove \"\n+                            \"task in %s seconds\", task.id, task.stakeholders, config.remove_delay)\n+                task.remove = time.time() + config.remove_delay\n+\n+        # If a running worker disconnects, tag all its jobs as FAILED and subject it to the same retry logic\n+        if task.status == RUNNING and task.worker_running and task.worker_running not in task.stakeholders:\n+            logger.info(\"Task %r is marked as running by disconnected worker %r -> marking as \"\n+                        \"FAILED with retry delay of %rs\", task.id, task.worker_running,\n+                        config.retry_delay)\n+            task.worker_running = None\n+            self.set_status(task, FAILED, config)\n+            task.retry = time.time() + config.retry_delay\n+\n+        # Re-enable task after the disable time expires\n+        if task.status == DISABLED and task.scheduler_disable_time:\n+            if time.time() - fix_time(task.scheduler_disable_time) > config.disable_time:\n+                self.re_enable(task, config)\n+\n+        # Remove tasks that have no stakeholders\n+        if task.remove and time.time() > task.remove:\n+            logger.info(\"Removing task %r (no connected stakeholders)\", task.id)\n+            remove = True\n+\n+        # Reset FAILED tasks to PENDING if max timeout is reached, and retry delay is >= 0\n+        if task.status == FAILED and config.retry_delay >= 0 and task.retry < time.time():\n+            self.set_status(task, PENDING, config)\n+\n+        return remove\n+\n     def inactivate_tasks(self, delete_tasks):\n         # The terminology is a bit confusing: we used to \"delete\" tasks when they became inactive,\n         # but with a pluggable state storage, you might very well want to keep some history of\n         # older tasks as well. That's why we call it \"inactivate\" (as in the verb)\n         for task in delete_tasks:\n-            self._tasks.pop(task)\n+            task_obj = self._tasks.pop(task)\n+            self._status_tasks[task_obj.status].pop(task)\n \n     def get_active_workers(self, last_active_lt=None):\n         for worker in self._active_workers.itervalues():\n@@ -392,7 +416,7 @@ class CentralPlannerScheduler(Scheduler):\n \n         remove_tasks = []\n         for task in self._state.get_active_tasks():\n-            if task.prune(self._config):\n+            if self._state.prune(task, self._config):\n                 remove_tasks.append(task.id)\n \n         self._state.inactivate_tasks(remove_tasks)\n@@ -448,7 +472,7 @@ class CentralPlannerScheduler(Scheduler):\n                 # We also check for status == PENDING b/c that's the default value\n                 # (so checking for status != task.status woule lie)\n                 self._update_task_history(task_id, status)\n-            task.set_status(PENDING if status == SUSPENDED else status, self._config)\n+            self._state.set_status(task, PENDING if status == SUSPENDED else status, self._config)\n             if status == FAILED:\n                 task.retry = time.time() + self._config.retry_delay\n \n@@ -508,7 +532,7 @@ class CentralPlannerScheduler(Scheduler):\n         def not_done(t):\n             task = self._state.get_task(t, default=None)\n             return task is None or task.status != DONE\n-        for task in self._state.get_active_tasks():\n+        for task in self._state.get_pending_tasks():\n             if task.status != DONE:\n                 deps = filter(not_done, task.deps)\n                 inverse_num_deps = 1.0 / max(len(deps), 1)\n@@ -593,7 +617,7 @@ class CentralPlannerScheduler(Scheduler):\n                             break\n \n         if best_task:\n-            best_task.status = RUNNING\n+            self._state.set_status(best_task, RUNNING, self._config)\n             best_task.worker_running = worker\n             best_task.time_running = time.time()\n             self._update_task_history(best_task.id, RUNNING, host=host)\n@@ -695,12 +719,11 @@ class CentralPlannerScheduler(Scheduler):\n         self.prune()\n         result = {}\n         upstream_status_table = {}  # used to memoize upstream status\n-        for task in self._state.get_active_tasks():\n-            if not status or task.status == status:\n-                if (task.status != PENDING or not upstream_status or\n-                    upstream_status == self._upstream_status(task.id, upstream_status_table)):\n-                    serialized = self._serialize_task(task.id, False)\n-                    result[task.id] = serialized\n+        for task in self._state.get_active_tasks(status):\n+            if (task.status != PENDING or not upstream_status or\n+                upstream_status == self._upstream_status(task.id, upstream_status_table)):\n+                serialized = self._serialize_task(task.id, False)\n+                result[task.id] = serialized\n         if limit and len(result) > self._config.max_shown_tasks:\n             return {'num_tasks': len(result)}\n         return result\n@@ -769,7 +792,7 @@ class CentralPlannerScheduler(Scheduler):\n         serialized = {}\n         task = self._state.get_task(task_id)\n         if task and task.status == DISABLED and task.scheduler_disable_time:\n-            task.re_enable()\n+            self._state.re_enable(task, self._config)\n             serialized = self._serialize_task(task_id)\n         return serialized\n \n",
      "files_name_in_blame_commit": [
        "scheduler.py"
      ]
    }
  },
  "commits_modify_file_before_fix": {
    "size": 149
  },
  "recursive_blame_commits": {
    "recursive_blame_function_lines": {
      "152": {
        "commit_id": "ea056a277a623c935769ecbf4d50df0af93b79a6",
        "line_code": "    def set_status(self, new_status, config):",
        "commit_date": "2014-12-10 18:52:08",
        "valid": 1
      },
      "153": {
        "commit_id": "3819379036c24388bcc0be602285de85d318f5c5",
        "line_code": "        # not sure why we have SUSPENDED, as it can never be set",
        "commit_date": "2014-12-10 18:52:07",
        "valid": 1
      },
      "154": {
        "commit_id": "3819379036c24388bcc0be602285de85d318f5c5",
        "line_code": "        if new_status == SUSPENDED:",
        "commit_date": "2014-12-10 18:52:07",
        "valid": 1
      },
      "155": {
        "commit_id": "3819379036c24388bcc0be602285de85d318f5c5",
        "line_code": "            new_status = PENDING",
        "commit_date": "2014-12-10 18:52:07",
        "valid": 1
      },
      "156": {
        "commit_id": "3819379036c24388bcc0be602285de85d318f5c5",
        "line_code": "",
        "commit_date": "2014-12-10 18:52:07",
        "valid": 0
      },
      "157": {
        "commit_id": "3819379036c24388bcc0be602285de85d318f5c5",
        "line_code": "        if new_status == DISABLED and self.status == RUNNING:",
        "commit_date": "2014-12-10 18:52:07",
        "valid": 1
      },
      "158": {
        "commit_id": "3819379036c24388bcc0be602285de85d318f5c5",
        "line_code": "            return",
        "commit_date": "2014-12-10 18:52:07",
        "valid": 1
      },
      "159": {
        "commit_id": "3819379036c24388bcc0be602285de85d318f5c5",
        "line_code": "",
        "commit_date": "2014-12-10 18:52:07",
        "valid": 0
      },
      "160": {
        "commit_id": "3819379036c24388bcc0be602285de85d318f5c5",
        "line_code": "        if self.status == DISABLED:",
        "commit_date": "2014-12-10 18:52:07",
        "valid": 1
      },
      "161": {
        "commit_id": "3819379036c24388bcc0be602285de85d318f5c5",
        "line_code": "            if new_status == DONE:",
        "commit_date": "2014-12-10 18:52:07",
        "valid": 1
      },
      "162": {
        "commit_id": "3819379036c24388bcc0be602285de85d318f5c5",
        "line_code": "                self.re_enable()",
        "commit_date": "2014-12-10 18:52:07",
        "valid": 1
      },
      "163": {
        "commit_id": "3819379036c24388bcc0be602285de85d318f5c5",
        "line_code": "",
        "commit_date": "2014-12-10 18:52:07",
        "valid": 0
      },
      "164": {
        "commit_id": "3819379036c24388bcc0be602285de85d318f5c5",
        "line_code": "            # don't allow workers to override a scheduler disable",
        "commit_date": "2014-12-10 18:52:07",
        "valid": 1
      },
      "165": {
        "commit_id": "3819379036c24388bcc0be602285de85d318f5c5",
        "line_code": "            elif self.scheduler_disable_time is not None:",
        "commit_date": "2014-12-10 18:52:07",
        "valid": 1
      },
      "166": {
        "commit_id": "3819379036c24388bcc0be602285de85d318f5c5",
        "line_code": "                return",
        "commit_date": "2014-12-10 18:52:07",
        "valid": 1
      },
      "167": {
        "commit_id": "3819379036c24388bcc0be602285de85d318f5c5",
        "line_code": "",
        "commit_date": "2014-12-10 18:52:07",
        "valid": 0
      },
      "168": {
        "commit_id": "3819379036c24388bcc0be602285de85d318f5c5",
        "line_code": "        if new_status == FAILED and self.can_disable():",
        "commit_date": "2014-12-10 18:52:07",
        "valid": 1
      },
      "169": {
        "commit_id": "3819379036c24388bcc0be602285de85d318f5c5",
        "line_code": "            self.add_failure()",
        "commit_date": "2014-12-10 18:52:07",
        "valid": 1
      },
      "170": {
        "commit_id": "3819379036c24388bcc0be602285de85d318f5c5",
        "line_code": "            if self.has_excessive_failures():",
        "commit_date": "2014-12-10 18:52:07",
        "valid": 1
      },
      "171": {
        "commit_id": "3d6f72cdf7e05471f5e8f209b1286f27af06ad41",
        "line_code": "                self.scheduler_disable_time = time.time()",
        "commit_date": "2014-12-10 18:52:08",
        "valid": 1
      },
      "172": {
        "commit_id": "3819379036c24388bcc0be602285de85d318f5c5",
        "line_code": "                new_status = DISABLED",
        "commit_date": "2014-12-10 18:52:07",
        "valid": 1
      },
      "173": {
        "commit_id": "3819379036c24388bcc0be602285de85d318f5c5",
        "line_code": "                notifications.send_error_email(",
        "commit_date": "2014-12-10 18:52:07",
        "valid": 1
      },
      "174": {
        "commit_id": "3819379036c24388bcc0be602285de85d318f5c5",
        "line_code": "                    'Luigi Scheduler: DISABLED {task} due to excessive failures'.format(task=self.id),",
        "commit_date": "2014-12-10 18:52:07",
        "valid": 1
      },
      "175": {
        "commit_id": "3819379036c24388bcc0be602285de85d318f5c5",
        "line_code": "                    '{task} failed {failures} times in the last {window} seconds, so it is being '",
        "commit_date": "2014-12-10 18:52:07",
        "valid": 1
      },
      "176": {
        "commit_id": "3819379036c24388bcc0be602285de85d318f5c5",
        "line_code": "                    'disabled for {persist} seconds'.format(",
        "commit_date": "2014-12-10 18:52:07",
        "valid": 1
      },
      "177": {
        "commit_id": "ea056a277a623c935769ecbf4d50df0af93b79a6",
        "line_code": "                        failures=config.disable_failures,",
        "commit_date": "2014-12-10 18:52:08",
        "valid": 1
      },
      "178": {
        "commit_id": "3819379036c24388bcc0be602285de85d318f5c5",
        "line_code": "                        task=self.id,",
        "commit_date": "2014-12-10 18:52:07",
        "valid": 1
      },
      "179": {
        "commit_id": "ea056a277a623c935769ecbf4d50df0af93b79a6",
        "line_code": "                        window=config.disable_window,",
        "commit_date": "2014-12-10 18:52:08",
        "valid": 1
      },
      "180": {
        "commit_id": "ea056a277a623c935769ecbf4d50df0af93b79a6",
        "line_code": "                        persist=config.disable_persist,",
        "commit_date": "2014-12-10 18:52:08",
        "valid": 1
      },
      "181": {
        "commit_id": "3819379036c24388bcc0be602285de85d318f5c5",
        "line_code": "                        ))",
        "commit_date": "2014-12-10 18:52:07",
        "valid": 1
      },
      "182": {
        "commit_id": "3819379036c24388bcc0be602285de85d318f5c5",
        "line_code": "        elif new_status == DISABLED:",
        "commit_date": "2014-12-10 18:52:07",
        "valid": 1
      },
      "183": {
        "commit_id": "3819379036c24388bcc0be602285de85d318f5c5",
        "line_code": "            self.scheduler_disable_time = None",
        "commit_date": "2014-12-10 18:52:07",
        "valid": 1
      },
      "184": {
        "commit_id": "3819379036c24388bcc0be602285de85d318f5c5",
        "line_code": "",
        "commit_date": "2014-12-10 18:52:07",
        "valid": 0
      },
      "185": {
        "commit_id": "3819379036c24388bcc0be602285de85d318f5c5",
        "line_code": "        self.status = new_status",
        "commit_date": "2014-12-10 18:52:07",
        "valid": 1
      }
    },
    "commits": {
      "3d6f72cdf7e05471f5e8f209b1286f27af06ad41": {
        "commit": {
          "commit_id": "3d6f72cdf7e05471f5e8f209b1286f27af06ad41",
          "commit_message": "Bonux fix: Consistency fix for all internal time paramters. Now everything uses unix timestamps\n\nI'm not sure if this could lead to issues if restoring a pickled state. Otoh I don't know if anyone uses the disabling feature, and worst case the issues will be small (i.e. things are never re-enabled automatically for those tasks, until they are re-added).",
          "commit_author": "Erik Bernhardsson",
          "commit_date": "2014-12-10 18:52:08",
          "commit_parent": "ea056a277a623c935769ecbf4d50df0af93b79a6"
        },
        "function": {
          "function_name": "set_status",
          "function_code_before": "def set_status(self, new_status, config):\n    if new_status == SUSPENDED:\n        new_status = PENDING\n    if new_status == DISABLED and self.status == RUNNING:\n        return\n    if self.status == DISABLED:\n        if new_status == DONE:\n            self.re_enable()\n        elif self.scheduler_disable_time is not None:\n            return\n    if new_status == FAILED and self.can_disable():\n        self.add_failure()\n        if self.has_excessive_failures():\n            self.scheduler_disable_time = datetime.datetime.now()\n            new_status = DISABLED\n            notifications.send_error_email('Luigi Scheduler: DISABLED {task} due to excessive failures'.format(task=self.id), '{task} failed {failures} times in the last {window} seconds, so it is being disabled for {persist} seconds'.format(failures=config.disable_failures, task=self.id, window=config.disable_window, persist=config.disable_persist))\n    elif new_status == DISABLED:\n        self.scheduler_disable_time = None\n    self.status = new_status",
          "function_code_after": "def set_status(self, new_status, config):\n    if new_status == SUSPENDED:\n        new_status = PENDING\n    if new_status == DISABLED and self.status == RUNNING:\n        return\n    if self.status == DISABLED:\n        if new_status == DONE:\n            self.re_enable()\n        elif self.scheduler_disable_time is not None:\n            return\n    if new_status == FAILED and self.can_disable():\n        self.add_failure()\n        if self.has_excessive_failures():\n            self.scheduler_disable_time = time.time()\n            new_status = DISABLED\n            notifications.send_error_email('Luigi Scheduler: DISABLED {task} due to excessive failures'.format(task=self.id), '{task} failed {failures} times in the last {window} seconds, so it is being disabled for {persist} seconds'.format(failures=config.disable_failures, task=self.id, window=config.disable_window, persist=config.disable_persist))\n    elif new_status == DISABLED:\n        self.scheduler_disable_time = None\n    self.status = new_status",
          "function_before_start_line": 139,
          "function_before_end_line": 172,
          "function_after_start_line": 150,
          "function_after_end_line": 183,
          "function_before_token_count": 153,
          "function_after_token_count": 151,
          "functions_name_modified_file": [
            "add_failure",
            "get_worker_ids",
            "prune",
            "_schedulable",
            "clear",
            "has_excessive_failures",
            "_upstream_status",
            "_update_task_history",
            "can_disable",
            "ping",
            "get_pending_tasks",
            "_traverse_inverse_deps",
            "load",
            "inverse_dependencies",
            "num_failures",
            "task_search",
            "_recurse_deps",
            "get_worker",
            "get_active_workers",
            "worker_list",
            "add_task",
            "update_resources",
            "_update_priority",
            "task_list",
            "add_info",
            "get_active_tasks",
            "_has_resources",
            "_used_resources",
            "has_task",
            "fix_time",
            "_serialize_task",
            "task_history",
            "dep_graph",
            "__str__",
            "graph",
            "re_enable",
            "dump",
            "__repr__",
            "inactivate_tasks",
            "__init__",
            "update",
            "set_status",
            "add_worker",
            "get_task",
            "_rank",
            "get_work",
            "re_enable_task",
            "fetch_error",
            "inactivate_workers"
          ],
          "functions_name_all_files": [
            "add_failure",
            "get_worker_ids",
            "prune",
            "_schedulable",
            "clear",
            "has_excessive_failures",
            "_upstream_status",
            "_update_task_history",
            "can_disable",
            "ping",
            "get_pending_tasks",
            "_traverse_inverse_deps",
            "load",
            "inverse_dependencies",
            "num_failures",
            "task_search",
            "_recurse_deps",
            "get_worker",
            "get_active_workers",
            "worker_list",
            "add_task",
            "update_resources",
            "_update_priority",
            "task_list",
            "add_info",
            "get_active_tasks",
            "_has_resources",
            "_used_resources",
            "has_task",
            "fix_time",
            "_serialize_task",
            "task_history",
            "dep_graph",
            "__str__",
            "graph",
            "re_enable",
            "dump",
            "__repr__",
            "inactivate_tasks",
            "__init__",
            "update",
            "set_status",
            "add_worker",
            "get_task",
            "_rank",
            "get_work",
            "re_enable_task",
            "fetch_error",
            "inactivate_workers"
          ],
          "functions_name_co_evolved_modified_file": [
            "__init__",
            "add_failure",
            "prune",
            "fix_time",
            "num_failures"
          ],
          "functions_name_co_evolved_all_files": [
            "__init__",
            "add_failure",
            "prune",
            "fix_time",
            "num_failures"
          ]
        },
        "file": {
          "file_name": "scheduler.py",
          "file_nloc": 574,
          "file_complexity": 219,
          "file_token_count": 4242,
          "file_before": "# Copyright (c) 2012 Spotify AB\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"); you may not\n# use this file except in compliance with the License. You may obtain a copy of\n# the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n# License for the specific language governing permissions and limitations under\n# the License.\n\nimport collections\nimport datetime\nimport functools\nimport notifications\nimport os\nimport logging\nimport time\nimport cPickle as pickle\nimport task_history as history\nlogger = logging.getLogger(\"luigi.server\")\n\nfrom task_status import PENDING, FAILED, DONE, RUNNING, SUSPENDED, UNKNOWN, DISABLED\n\n\nclass Scheduler(object):\n    ''' Abstract base class\n\n    Note that the methods all take string arguments, not Task objects...\n    '''\n    add_task = NotImplemented\n    get_work = NotImplemented\n    ping = NotImplemented\n\nUPSTREAM_RUNNING = 'UPSTREAM_RUNNING'\nUPSTREAM_MISSING_INPUT = 'UPSTREAM_MISSING_INPUT'\nUPSTREAM_FAILED = 'UPSTREAM_FAILED'\nUPSTREAM_DISABLED = 'UPSTREAM_DISABLED'\n\nUPSTREAM_SEVERITY_ORDER = (\n    '',\n    UPSTREAM_RUNNING,\n    UPSTREAM_MISSING_INPUT,\n    UPSTREAM_FAILED,\n    UPSTREAM_DISABLED,\n)\nUPSTREAM_SEVERITY_KEY = lambda st: UPSTREAM_SEVERITY_ORDER.index(st)\nSTATUS_TO_UPSTREAM_MAP = {\n    FAILED: UPSTREAM_FAILED,\n    RUNNING: UPSTREAM_RUNNING,\n    PENDING: UPSTREAM_MISSING_INPUT,\n    DISABLED: UPSTREAM_DISABLED,\n}\n\n\n# We're passing around this config a lot, so let's put it on an object\nSchedulerConfig = collections.namedtuple('SchedulerConfig', [\n        'retry_delay', 'remove_delay', 'worker_disconnect_delay',\n        'disable_failures', 'disable_window', 'disable_persist', 'disable_time'])\n\n\nclass Failures(object):\n    \"\"\" This class tracks the number of failures in a given time window\n\n    Failures added are marked with the current timestamp, and this class counts\n    the number of failures in a sliding time window ending at the present.\n\n    \"\"\"\n\n    def __init__(self, window):\n        \"\"\" Initialize with the given window\n\n        :param window: how long to track failures for, as a datetime.timedelta\n        \"\"\"\n        self.window = window\n        self.failures = collections.deque()\n\n    def add_failure(self):\n        \"\"\" Add a failure event with the current timestamp \"\"\"\n        self.failures.append(datetime.datetime.now())\n\n    def num_failures(self):\n        \"\"\" Return the number of failures in the window \"\"\"\n        min_time = datetime.datetime.now() - self.window\n        while self.failures and self.failures[0] < min_time:\n            self.failures.popleft()\n        return len(self.failures)\n\n    def clear(self):\n        \"\"\" Clear the failure queue \"\"\"\n        self.failures.clear()\n\n\nclass Task(object):\n    def __init__(self, id, status, deps, resources={}, priority=0, family='', params={},\n                 disable_failures=None, disable_window=None):\n        self.id = id\n        self.stakeholders = set()  # workers ids that are somehow related to this task (i.e. don't prune while any of these workers are still active)\n        self.workers = set()  # workers ids that can perform task - task is 'BROKEN' if none of these workers are active\n        if deps is None:\n            self.deps = set()\n        else:\n            self.deps = set(deps)\n        self.status = status  # PENDING, RUNNING, FAILED or DONE\n        self.time = time.time()  # Timestamp when task was first added\n        self.retry = None\n        self.remove = None\n        self.worker_running = None  # the worker id that is currently running the task or None\n        self.time_running = None  # Timestamp when picked up by worker\n        self.expl = None\n        self.priority = priority\n        self.resources = resources\n        self.family = family\n        self.params = params\n        self.disable_failures = disable_failures\n        self.failures = Failures(disable_window)\n        self.scheduler_disable_time = None\n\n    def __repr__(self):\n        return \"Task(%r)\" % vars(self)\n\n    def add_failure(self):\n        self.failures.add_failure()\n\n    def has_excessive_failures(self):\n        return self.failures.num_failures() >= self.disable_failures\n\n    def can_disable(self):\n        return self.disable_failures is not None\n\n    def re_enable(self):\n        self.scheduler_disable_time = None\n        self.status = FAILED\n        self.failures.clear()\n\n    def set_status(self, new_status, config):\n        # not sure why we have SUSPENDED, as it can never be set\n        if new_status == SUSPENDED:\n            new_status = PENDING\n\n        if new_status == DISABLED and self.status == RUNNING:\n            return\n\n        if self.status == DISABLED:\n            if new_status == DONE:\n                self.re_enable()\n\n            # don't allow workers to override a scheduler disable\n            elif self.scheduler_disable_time is not None:\n                return\n\n        if new_status == FAILED and self.can_disable():\n            self.add_failure()\n            if self.has_excessive_failures():\n                self.scheduler_disable_time = datetime.datetime.now()\n                new_status = DISABLED\n                notifications.send_error_email(\n                    'Luigi Scheduler: DISABLED {task} due to excessive failures'.format(task=self.id),\n                    '{task} failed {failures} times in the last {window} seconds, so it is being '\n                    'disabled for {persist} seconds'.format(\n                        failures=config.disable_failures,\n                        task=self.id,\n                        window=config.disable_window,\n                        persist=config.disable_persist,\n                        ))\n        elif new_status == DISABLED:\n            self.scheduler_disable_time = None\n\n        self.status = new_status\n\n    def prune(self, config):\n        remove = False\n\n        # Mark tasks with no remaining active stakeholders for deletion\n        if not self.stakeholders:\n            if self.remove is None:\n                logger.info(\"Task %r has stakeholders %r but none remain connected -> will remove task in %s seconds\", self.id, self.stakeholders, config.remove_delay)\n                self.remove = time.time() + config.remove_delay\n\n        # If a running worker disconnects, tag all its jobs as FAILED and subject it to the same retry logic\n        if self.status == RUNNING and self.worker_running and self.worker_running not in self.stakeholders:\n            logger.info(\"Task %r is marked as running by disconnected worker %r -> marking as FAILED with retry delay of %rs\", self.id, self.worker_running, config.retry_delay)\n            self.worker_running = None\n            self.set_status(FAILED, config)\n            self.retry = time.time() + config.retry_delay\n\n        # Re-enable task after the disable time expires\n        if self.status == DISABLED and self.scheduler_disable_time:\n            if datetime.datetime.now() - self.scheduler_disable_time > config.disable_time:\n                self.re_enable()\n\n        # Remove tasks that have no stakeholders\n        if self.remove and time.time() > self.remove:\n            logger.info(\"Removing task %r (no connected stakeholders)\", self.id)\n            remove = True\n\n        # Reset FAILED tasks to PENDING if max timeout is reached, and retry delay is >= 0\n        if self.status == FAILED and config.retry_delay >= 0 and self.retry < time.time():\n            self.set_status(PENDING, config)\n\n        return remove\n\n\nclass Worker(object):\n    \"\"\" Structure for tracking worker activity and keeping their references \"\"\"\n    def __init__(self, id, last_active=None):\n        self.id = id\n        self.reference = None  # reference to the worker in the real world. (Currently a dict containing just the host)\n        self.last_active = last_active  # seconds since epoch\n        self.started = time.time()  # seconds since epoch\n        self.info = {}\n\n    def add_info(self, info):\n        self.info.update(info)\n\n    def update(self, worker_reference):\n        if worker_reference:\n            self.reference = worker_reference\n        self.last_active = time.time()\n\n    def prune(self, config):\n        # Delete workers that haven't said anything for a while (probably killed)\n        if self.last_active + config.worker_disconnect_delay < time.time():\n            return True\n\n    def __str__(self):\n        return self.id\n\n\nclass SimpleTaskState(object):\n    ''' Keep track of the current state and handle persistance\n\n    The point of this class is to enable other ways to keep state, eg. by using a database\n    These will be implemented by creating an abstract base class that this and other classes\n    inherit from.\n    '''\n\n    def __init__(self, state_path):\n        self._state_path = state_path\n        self._tasks = {}  # map from id to a Task object\n        self._active_workers = {}  # map from id to a Worker object\n\n    def dump(self):\n        state = (self._tasks, self._active_workers)\n        try:\n            with open(self._state_path, 'w') as fobj:\n                pickle.dump(state, fobj)\n        except IOError:\n            logger.warning(\"Failed saving scheduler state\", exc_info=1)\n        else:\n            logger.info(\"Saved state in %s\", self._state_path)\n\n    # prone to lead to crashes when old state is unpickled with updated code. TODO some kind of version control?\n    def load(self):\n        if os.path.exists(self._state_path):\n            logger.info(\"Attempting to load state from %s\", self._state_path)\n            try:\n                with open(self._state_path) as fobj:\n                    state = pickle.load(fobj)\n            except:\n                logger.exception(\"Error when loading state. Starting from clean slate.\")\n                return\n\n            self._tasks, self._active_workers = state\n\n            # Convert from old format\n            # TODO: this is really ugly, we need something more future-proof\n            # Every time we add an attribute to the Worker class, this code needs to be updated\n            for k, v in self._active_workers.iteritems():\n                if isinstance(v, float):\n                    self._active_workers[k] = Worker(id=k, last_active=v)\n        else:\n            logger.info(\"No prior state file exists at %s. Starting with clean slate\", self._state_path)\n\n    def get_active_tasks(self):\n        for task in self._tasks.itervalues():\n            yield task\n\n    def get_pending_tasks(self):\n        for task in self._tasks.itervalues():\n            if task.status in [PENDING, RUNNING]:\n                yield task\n\n    def get_task(self, task_id, default=None, setdefault=None):\n        if setdefault:\n            return self._tasks.setdefault(task_id, setdefault)\n        else:\n            return self._tasks.get(task_id, default)\n\n    def has_task(self, task_id):\n        return task_id in self._tasks\n\n    def inactivate_tasks(self, delete_tasks):\n        # The terminology is a bit confusing: we used to \"delete\" tasks when they became inactive,\n        # but with a pluggable state storage, you might very well want to keep some history of\n        # older tasks as well. That's why we call it \"inactivate\" (as in the verb)\n        for task in delete_tasks:\n            self._tasks.pop(task)\n\n    def get_active_workers(self, last_active_lt=None):\n        for worker in self._active_workers.itervalues():\n            if last_active_lt is not None and worker.last_active >= last_active_lt:\n                continue\n            yield worker\n\n    def get_worker_ids(self):\n        return self._active_workers.keys() # only used for unit tests\n\n    def get_worker(self, worker_id):\n        return self._active_workers.setdefault(worker_id, Worker(worker_id))\n\n    def inactivate_workers(self, delete_workers):\n        # Mark workers as inactive\n        for worker in delete_workers:\n            self._active_workers.pop(worker)\n\n        # remove workers from tasks\n        for task in self.get_active_tasks():\n            task.stakeholders.difference_update(delete_workers)\n            task.workers.difference_update(delete_workers)\n\n\nclass CentralPlannerScheduler(Scheduler):\n    ''' Async scheduler that can handle multiple workers etc\n\n    Can be run locally or on a server (using RemoteScheduler + server.Server).\n    '''\n\n    def __init__(self, retry_delay=900.0, remove_delay=600.0, worker_disconnect_delay=60.0,\n                 state_path='/var/lib/luigi-server/state.pickle', task_history=None,\n                 resources=None, disable_persist=0, disable_window=0, disable_failures=None):\n        '''\n        (all arguments are in seconds)\n        Keyword Arguments:\n        retry_delay -- How long after a Task fails to try it again, or -1 to never retry\n        remove_delay -- How long after a Task finishes to remove it from the scheduler\n        state_path -- Path to state file (tasks and active workers)\n        worker_disconnect_delay -- If a worker hasn't communicated for this long, remove it from active workers\n        '''\n        self._config = SchedulerConfig(\n            retry_delay=retry_delay,\n            remove_delay=remove_delay,\n            worker_disconnect_delay=worker_disconnect_delay,\n            disable_failures=disable_failures,\n            disable_window=disable_window,\n            disable_persist=disable_persist,\n            disable_time=datetime.timedelta(seconds=disable_persist))\n\n        self._task_history = task_history or history.NopHistory()\n        self._state = SimpleTaskState(state_path)\n\n        self._task_history = task_history or history.NopHistory()\n        self._resources = resources\n        self._make_task = functools.partial(\n            Task, disable_failures=disable_failures,\n            disable_window=datetime.timedelta(seconds=disable_window))\n\n    def load(self):\n        self._state.load()\n\n    def dump(self):\n        self._state.dump()\n\n    def prune(self):\n        logger.info(\"Starting pruning of task graph\")\n        remove_workers = []\n        for worker in self._state.get_active_workers():\n            if worker.prune(self._config):\n                logger.info(\"Worker %s timed out (no contact for >=%ss)\", worker, self._config.worker_disconnect_delay)\n                remove_workers.append(worker.id)\n\n        self._state.inactivate_workers(remove_workers)\n\n        remove_tasks = []\n        for task in self._state.get_active_tasks():\n            if task.prune(self._config):\n                remove_tasks.append(task.id)\n\n        self._state.inactivate_tasks(remove_tasks)\n\n        logger.info(\"Done pruning task graph\")\n\n    def update(self, worker_id, worker_reference=None):\n        \"\"\" Keep track of whenever the worker was last active \"\"\"\n        worker = self._state.get_worker(worker_id)\n        worker.update(worker_reference)\n\n    def _update_priority(self, task, prio, worker):\n        \"\"\" Update priority of the given task\n\n        Priority can only be increased. If the task doesn't exist, a placeholder\n        task is created to preserve priority when the task is later scheduled.\n        \"\"\"\n        task.priority = prio = max(prio, task.priority)\n        for dep in task.deps or []:\n            t = self._state.get_task(dep)\n            if t is not None and prio > t.priority:\n                self._update_priority(t, prio, worker)\n\n    def add_task(self, worker, task_id, status=PENDING, runnable=True,\n                 deps=None, new_deps=None, expl=None, resources=None,\n                 priority=0, family='', params={}):\n        \"\"\"\n        * Add task identified by task_id if it doesn't exist\n        * If deps is not None, update dependency list\n        * Update status of task\n        * Add additional workers/stakeholders\n        * Update priority when needed\n        \"\"\"\n        self.update(worker)\n\n        task = self._state.get_task(task_id, setdefault=self._make_task(\n                id=task_id, status=PENDING, deps=deps, resources=resources,\n                priority=priority, family=family, params=params))\n\n        # for setting priority, we'll sometimes create tasks with unset family and params\n        if not task.family:\n            task.family = family\n        if not task.params:\n            task.params = params\n\n        if task.remove is not None:\n            task.remove = None  # unmark task for removal so it isn't removed after being added\n\n        if not (task.status == RUNNING and status == PENDING):\n            # don't allow re-scheduling of task while it is running, it must either fail or succeed first\n            if status == PENDING or status != task.status:\n                # Update the DB only if there was a acctual change, to prevent noise.\n                # We also check for status == PENDING b/c that's the default value\n                # (so checking for status != task.status woule lie)\n                self._update_task_history(task_id, status)\n            task.set_status(PENDING if status == SUSPENDED else status, self._config)\n            if status == FAILED:\n                task.retry = time.time() + self._config.retry_delay\n\n        if deps is not None:\n            task.deps = set(deps)\n\n        if new_deps is not None:\n            task.deps.update(new_deps)\n\n        task.stakeholders.add(worker)\n        task.resources = resources\n\n        # Task dependencies might not exist yet. Let's create dummy tasks for them for now.\n        # Otherwise the task dependencies might end up being pruned if scheduling takes a long time\n        for dep in task.deps or []:\n            t = self._state.get_task(dep, setdefault=self._make_task(id=dep, status=UNKNOWN, deps=None, priority=priority))\n            t.stakeholders.add(worker)\n\n        self._update_priority(task, priority, worker)\n\n        if runnable:\n            task.workers.add(worker)\n\n        if expl is not None:\n            task.expl = expl\n\n    def add_worker(self, worker, info):\n        self._state.get_worker(worker).add_info(info)\n\n    def update_resources(self, **resources):\n        if self._resources is None:\n            self._resources = {}\n        self._resources.update(resources)\n\n    def _has_resources(self, needed_resources, used_resources):\n        if needed_resources is None:\n            return True\n\n        available_resources = self._resources or {}\n        for resource, amount in needed_resources.items():\n            if amount + used_resources[resource] > available_resources.get(resource, 1):\n                return False\n        return True\n\n    def _used_resources(self):\n        used_resources = collections.defaultdict(int)\n        if self._resources is not None:\n            for task in self._state.get_active_tasks():\n                if task.status == RUNNING and task.resources:\n                    for resource, amount in task.resources.items():\n                        used_resources[resource] += amount\n        return used_resources\n\n    def _rank(self):\n        ''' Return worker's rank function for task scheduling '''\n        dependents = collections.defaultdict(int)\n        def not_done(t):\n            task = self._state.get_task(t, default=None)\n            return task is None or task.status != DONE\n        for task in self._state.get_active_tasks():\n            if task.status != DONE:\n                deps = filter(not_done, task.deps)\n                inverse_num_deps = 1.0 / max(len(deps), 1)\n                for dep in deps:\n                    dependents[dep] += inverse_num_deps\n\n        return lambda task: (task.priority, dependents[task.id], -task.time)\n\n    def _schedulable(self, task):\n        if task.status != PENDING:\n            return False\n        for dep in task.deps:\n            dep_task = self._state.get_task(dep, default=None)\n            if dep_task is None or dep_task.status != DONE:\n                return False\n        return True\n\n    def get_work(self, worker, host=None):\n        # TODO: remove any expired nodes\n\n        # Algo: iterate over all nodes, find the highest priority node no dependencies and available\n        # resources.\n\n        # Resource checking looks both at currently available resources and at which resources would\n        # be available if all running tasks died and we rescheduled all workers greedily. We do both\n        # checks in order to prevent a worker with many low-priority tasks from starving other\n        # workers with higher priority tasks that share the same resources.\n\n        # TODO: remove tasks that can't be done, figure out if the worker has absolutely\n        # nothing it can wait for\n\n        # Return remaining tasks that have no FAILED descendents\n        self.update(worker, {'host': host})\n        best_task = None\n        best_task_id = None\n        locally_pending_tasks = 0\n        running_tasks = []\n\n        used_resources = self._used_resources()\n        greedy_resources = collections.defaultdict(int)\n        n_unique_pending = 0\n        greedy_workers = dict((worker.id, worker.info.get('workers', 1))\n                              for worker in self._state.get_active_workers())\n\n        tasks = list(self._state.get_pending_tasks())\n        tasks.sort(key=self._rank(), reverse=True)\n\n        for task in tasks:\n            if task.status == 'RUNNING' and worker in task.workers:\n                # Return a list of currently running tasks to the client,\n                # makes it easier to troubleshoot\n                other_worker = self._state.get_worker(task.worker_running)\n                more_info = {'task_id': task.id, 'worker': str(other_worker)}\n                if other_worker is not None:\n                    more_info.update(other_worker.info)\n                    running_tasks.append(more_info)\n\n            if task.status == PENDING and worker in task.workers:\n                locally_pending_tasks += 1\n                if len(task.workers) == 1:\n                    n_unique_pending += 1\n\n            if task.status == RUNNING and task.worker_running in greedy_workers:\n                greedy_workers[task.worker_running] -= 1\n                for resource, amount in (task.resources or {}).items():\n                    greedy_resources[resource] += amount\n\n            if not best_task and self._schedulable(task) and self._has_resources(task.resources, greedy_resources):\n                if worker in task.workers and self._has_resources(task.resources, used_resources):\n                    best_task = task\n                    best_task_id = task.id\n                else:\n                    for task_worker in task.workers:\n                        if greedy_workers.get(task_worker, 0) > 0:\n                            # use up a worker\n                            greedy_workers[task_worker] -= 1\n\n                            # keep track of the resources used in greedy scheduling\n                            for resource, amount in (task.resources or {}).items():\n                                greedy_resources[resource] += amount\n\n                            break\n\n        if best_task:\n            best_task.status = RUNNING\n            best_task.worker_running = worker\n            best_task.time_running = time.time()\n            self._update_task_history(best_task.id, RUNNING, host=host)\n\n        return {'n_pending_tasks': locally_pending_tasks,\n                'n_unique_pending': n_unique_pending,\n                'task_id': best_task_id,\n                'running_tasks': running_tasks}\n\n    def ping(self, worker):\n        self.update(worker)\n\n    def _upstream_status(self, task_id, upstream_status_table):\n        if task_id in upstream_status_table:\n            return upstream_status_table[task_id]\n        elif self._state.has_task(task_id):\n            task_stack = [task_id]\n\n            while task_stack:\n                dep_id = task_stack.pop()\n                if self._state.has_task(dep_id):\n                    dep = self._state.get_task(dep_id)\n                    if dep_id not in upstream_status_table:\n                        if dep.status == PENDING and dep.deps:\n                            task_stack = task_stack + [dep_id] + list(dep.deps)\n                            upstream_status_table[dep_id] = ''  # will be updated postorder\n                        else:\n                            dep_status = STATUS_TO_UPSTREAM_MAP.get(dep.status, '')\n                            upstream_status_table[dep_id] = dep_status\n                    elif upstream_status_table[dep_id] == '' and dep.deps:\n                        # This is the postorder update step when we set the\n                        # status based on the previously calculated child elements\n                        upstream_status = [upstream_status_table.get(id, '') for id in dep.deps]\n                        upstream_status.append('')  # to handle empty list\n                        status = max(upstream_status, key=UPSTREAM_SEVERITY_KEY)\n                        upstream_status_table[dep_id] = status\n            return upstream_status_table[dep_id]\n\n    def _serialize_task(self, task_id, include_deps=True):\n        task = self._state.get_task(task_id)\n        ret = {\n            'deps': list(task.deps),\n            'status': task.status,\n            'workers': list(task.workers),\n            'worker_running': task.worker_running,\n            'time_running': getattr(task, \"time_running\", None),\n            'start_time': task.time,\n            'params': task.params,\n            'name': task.family,\n            'priority': task.priority,\n            'resources': task.resources,\n        }\n        if include_deps:\n            ret['deps'] = list(task.deps)\n        return ret\n\n    def graph(self):\n        self.prune()\n        serialized = {}\n        for task in self._state.get_active_tasks():\n            serialized[task.id] = self._serialize_task(task.id)\n        return serialized\n\n    def _recurse_deps(self, task_id, serialized):\n        if task_id not in serialized:\n            task = self._state.get_task(task_id)\n            if task is None or not task.family:\n                logger.warn('Missing task for id [%s]', task_id)\n\n                # try to infer family and params from task_id\n                try:\n                    family, _, param_str = task_id.rstrip(')').partition('(')\n                    params = dict(param.split('=') for param in param_str.split(', '))\n                except:\n                    family, params = '', {}\n                serialized[task_id] = {\n                    'deps': [],\n                    'status': UNKNOWN,\n                    'workers': [],\n                    'start_time': UNKNOWN,\n                    'params': params,\n                    'name': family,\n                    'priority': 0,\n                }\n            else:\n                serialized[task_id] = self._serialize_task(task_id)\n                for dep in task.deps:\n                    self._recurse_deps(dep, serialized)\n\n    def dep_graph(self, task_id):\n        self.prune()\n        serialized = {}\n        if self._state.has_task(task_id):\n            self._recurse_deps(task_id, serialized)\n        return serialized\n\n    def task_list(self, status, upstream_status):\n        ''' query for a subset of tasks by status '''\n        self.prune()\n        result = {}\n        upstream_status_table = {}  # used to memoize upstream status\n        for task in self._state.get_active_tasks():\n            if not status or task.status == status:\n                if (task.status != PENDING or not upstream_status or\n                    upstream_status == self._upstream_status(task.id, upstream_status_table)):\n                    serialized = self._serialize_task(task.id, False)\n                    result[task.id] = serialized\n        return result\n\n    def worker_list(self, include_running=True):\n        self.prune()\n        workers = [\n            dict(\n                name=worker.id,\n                last_active=worker.last_active,\n                started=getattr(worker, 'started', None),\n                **worker.info\n            ) for worker in self._state.get_active_workers()]\n        workers.sort(key=lambda worker: worker['started'], reverse=True)\n        if include_running:\n            running = collections.defaultdict(dict)\n            num_pending = collections.defaultdict(int)\n            num_uniques = collections.defaultdict(int)\n            for task in self._state.get_pending_tasks():\n                if task.status == RUNNING and task.worker_running:\n                    running[task.worker_running][task.id] = self._serialize_task(task.id, False)\n                elif task.status == PENDING:\n                    for worker in task.workers:\n                        num_pending[worker] += 1\n                    if len(task.workers) == 1:\n                        num_uniques[list(task.workers)[0]] += 1\n            for worker in workers:\n                tasks = running[worker['name']]\n                worker['num_running'] = len(tasks)\n                worker['num_pending'] = num_pending[worker['name']]\n                worker['num_uniques'] = num_uniques[worker['name']]\n                worker['running'] = tasks\n        return workers\n\n    def inverse_dependencies(self, task_id):\n        self.prune()\n        serialized = {}\n        if self._state.has_task(task_id):\n            self._traverse_inverse_deps(task_id, serialized)\n        return serialized\n\n    def _traverse_inverse_deps(self, task_id, serialized):\n        stack = [task_id]\n        serialized[task_id] = self._serialize_task(task_id)\n        while len(stack) > 0:\n            curr_id = stack.pop()\n            for task in self._state.get_active_tasks():\n                if curr_id in task.deps:\n                    serialized[curr_id][\"deps\"].append(task.id)\n                    if task.id not in serialized:\n                        serialized[task.id] = self._serialize_task(task.id)\n                        serialized[task.id][\"deps\"] = []\n                        stack.append(task.id)\n\n    def task_search(self, task_str):\n        ''' query for a subset of tasks by task_id '''\n        self.prune()\n        result = collections.defaultdict(dict)\n        for task in self._state.get_active_tasks():\n            if task.id.find(task_str) != -1:\n                serialized = self._serialize_task(task.id, False)\n                result[task.status][task.id] = serialized\n        return result\n\n    def re_enable_task(self, task_id):\n        serialized = {}\n        task = self._state.get_task(task_id)\n        if task and task.status == DISABLED and task.scheduler_disable_time:\n            task.re_enable()\n            serialized = self._serialize_task(task_id)\n        return serialized\n\n    def fetch_error(self, task_id):\n        if self._state.has_task(task_id):\n            return {\"taskId\": task_id, \"error\": self._state.get_task(task_id).expl}\n        else:\n            return {\"taskId\": task_id, \"error\": \"\"}\n\n    def _update_task_history(self, task_id, status, host=None):\n        try:\n            if status == DONE or status == FAILED:\n                successful = (status == DONE)\n                self._task_history.task_finished(task_id, successful)\n            elif status == PENDING:\n                self._task_history.task_scheduled(task_id)\n            elif status == RUNNING:\n                self._task_history.task_started(task_id, host)\n        except:\n            logger.warning(\"Error saving Task history\", exc_info=1)\n\n    @property\n    def task_history(self):\n        # Used by server.py to expose the calls\n        return self._task_history\n",
          "file_after": "# Copyright (c) 2012 Spotify AB\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"); you may not\n# use this file except in compliance with the License. You may obtain a copy of\n# the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n# License for the specific language governing permissions and limitations under\n# the License.\n\nimport collections\nimport datetime\nimport functools\nimport notifications\nimport os\nimport logging\nimport time\nimport cPickle as pickle\nimport task_history as history\nlogger = logging.getLogger(\"luigi.server\")\n\nfrom task_status import PENDING, FAILED, DONE, RUNNING, SUSPENDED, UNKNOWN, DISABLED\n\n\nclass Scheduler(object):\n    ''' Abstract base class\n\n    Note that the methods all take string arguments, not Task objects...\n    '''\n    add_task = NotImplemented\n    get_work = NotImplemented\n    ping = NotImplemented\n\nUPSTREAM_RUNNING = 'UPSTREAM_RUNNING'\nUPSTREAM_MISSING_INPUT = 'UPSTREAM_MISSING_INPUT'\nUPSTREAM_FAILED = 'UPSTREAM_FAILED'\nUPSTREAM_DISABLED = 'UPSTREAM_DISABLED'\n\nUPSTREAM_SEVERITY_ORDER = (\n    '',\n    UPSTREAM_RUNNING,\n    UPSTREAM_MISSING_INPUT,\n    UPSTREAM_FAILED,\n    UPSTREAM_DISABLED,\n)\nUPSTREAM_SEVERITY_KEY = lambda st: UPSTREAM_SEVERITY_ORDER.index(st)\nSTATUS_TO_UPSTREAM_MAP = {\n    FAILED: UPSTREAM_FAILED,\n    RUNNING: UPSTREAM_RUNNING,\n    PENDING: UPSTREAM_MISSING_INPUT,\n    DISABLED: UPSTREAM_DISABLED,\n}\n\n\n# We're passing around this config a lot, so let's put it on an object\nSchedulerConfig = collections.namedtuple('SchedulerConfig', [\n        'retry_delay', 'remove_delay', 'worker_disconnect_delay',\n        'disable_failures', 'disable_window', 'disable_persist', 'disable_time'])\n\n\ndef fix_time(x):\n    # Backwards compatibility for a fix in Dec 2014. Prior to the fix, pickled state might store datetime objects\n    # Let's remove this function soon\n    if isinstance(x, datetime.datetime):\n        return time.mktime(d.timetuple())\n    else:\n        return x\n\n\nclass Failures(object):\n    \"\"\" This class tracks the number of failures in a given time window\n\n    Failures added are marked with the current timestamp, and this class counts\n    the number of failures in a sliding time window ending at the present.\n\n    \"\"\"\n\n    def __init__(self, window):\n        \"\"\" Initialize with the given window\n\n        :param window: how long to track failures for, as a float (number of seconds)\n        \"\"\"\n        self.window = window\n        self.failures = collections.deque()\n\n    def add_failure(self):\n        \"\"\" Add a failure event with the current timestamp \"\"\"\n        self.failures.append(time.time())\n\n    def num_failures(self):\n        \"\"\" Return the number of failures in the window \"\"\"\n        min_time = time.time() - self.window\n\n        while self.failures and fix_time(self.failures[0]) < min_time:\n            self.failures.popleft()\n\n        return len(self.failures)\n\n    def clear(self):\n        \"\"\" Clear the failure queue \"\"\"\n        self.failures.clear()\n\n\nclass Task(object):\n    def __init__(self, id, status, deps, resources={}, priority=0, family='', params={},\n                 disable_failures=None, disable_window=None):\n        self.id = id\n        self.stakeholders = set()  # workers ids that are somehow related to this task (i.e. don't prune while any of these workers are still active)\n        self.workers = set()  # workers ids that can perform task - task is 'BROKEN' if none of these workers are active\n        if deps is None:\n            self.deps = set()\n        else:\n            self.deps = set(deps)\n        self.status = status  # PENDING, RUNNING, FAILED or DONE\n        self.time = time.time()  # Timestamp when task was first added\n        self.retry = None\n        self.remove = None\n        self.worker_running = None  # the worker id that is currently running the task or None\n        self.time_running = None  # Timestamp when picked up by worker\n        self.expl = None\n        self.priority = priority\n        self.resources = resources\n        self.family = family\n        self.params = params\n        self.disable_failures = disable_failures\n        self.failures = Failures(disable_window)\n        self.scheduler_disable_time = None\n\n    def __repr__(self):\n        return \"Task(%r)\" % vars(self)\n\n    def add_failure(self):\n        self.failures.add_failure()\n\n    def has_excessive_failures(self):\n        return self.failures.num_failures() >= self.disable_failures\n\n    def can_disable(self):\n        return self.disable_failures is not None\n\n    def re_enable(self):\n        self.scheduler_disable_time = None\n        self.status = FAILED\n        self.failures.clear()\n\n    def set_status(self, new_status, config):\n        # not sure why we have SUSPENDED, as it can never be set\n        if new_status == SUSPENDED:\n            new_status = PENDING\n\n        if new_status == DISABLED and self.status == RUNNING:\n            return\n\n        if self.status == DISABLED:\n            if new_status == DONE:\n                self.re_enable()\n\n            # don't allow workers to override a scheduler disable\n            elif self.scheduler_disable_time is not None:\n                return\n\n        if new_status == FAILED and self.can_disable():\n            self.add_failure()\n            if self.has_excessive_failures():\n                self.scheduler_disable_time = time.time()\n                new_status = DISABLED\n                notifications.send_error_email(\n                    'Luigi Scheduler: DISABLED {task} due to excessive failures'.format(task=self.id),\n                    '{task} failed {failures} times in the last {window} seconds, so it is being '\n                    'disabled for {persist} seconds'.format(\n                        failures=config.disable_failures,\n                        task=self.id,\n                        window=config.disable_window,\n                        persist=config.disable_persist,\n                        ))\n        elif new_status == DISABLED:\n            self.scheduler_disable_time = None\n\n        self.status = new_status\n\n    def prune(self, config):\n        remove = False\n\n        # Mark tasks with no remaining active stakeholders for deletion\n        if not self.stakeholders:\n            if self.remove is None:\n                logger.info(\"Task %r has stakeholders %r but none remain connected -> will remove task in %s seconds\", self.id, self.stakeholders, config.remove_delay)\n                self.remove = time.time() + config.remove_delay\n\n        # If a running worker disconnects, tag all its jobs as FAILED and subject it to the same retry logic\n        if self.status == RUNNING and self.worker_running and self.worker_running not in self.stakeholders:\n            logger.info(\"Task %r is marked as running by disconnected worker %r -> marking as FAILED with retry delay of %rs\", self.id, self.worker_running, config.retry_delay)\n            self.worker_running = None\n            self.set_status(FAILED, config)\n            self.retry = time.time() + config.retry_delay\n\n        # Re-enable task after the disable time expires\n        if self.status == DISABLED and self.scheduler_disable_time:\n            if time.time() - fix_time(self.scheduler_disable_time) > config.disable_time:\n                self.re_enable()\n\n        # Remove tasks that have no stakeholders\n        if self.remove and time.time() > self.remove:\n            logger.info(\"Removing task %r (no connected stakeholders)\", self.id)\n            remove = True\n\n        # Reset FAILED tasks to PENDING if max timeout is reached, and retry delay is >= 0\n        if self.status == FAILED and config.retry_delay >= 0 and self.retry < time.time():\n            self.set_status(PENDING, config)\n\n        return remove\n\n\nclass Worker(object):\n    \"\"\" Structure for tracking worker activity and keeping their references \"\"\"\n    def __init__(self, id, last_active=None):\n        self.id = id\n        self.reference = None  # reference to the worker in the real world. (Currently a dict containing just the host)\n        self.last_active = last_active  # seconds since epoch\n        self.started = time.time()  # seconds since epoch\n        self.info = {}\n\n    def add_info(self, info):\n        self.info.update(info)\n\n    def update(self, worker_reference):\n        if worker_reference:\n            self.reference = worker_reference\n        self.last_active = time.time()\n\n    def prune(self, config):\n        # Delete workers that haven't said anything for a while (probably killed)\n        if self.last_active + config.worker_disconnect_delay < time.time():\n            return True\n\n    def __str__(self):\n        return self.id\n\n\nclass SimpleTaskState(object):\n    ''' Keep track of the current state and handle persistance\n\n    The point of this class is to enable other ways to keep state, eg. by using a database\n    These will be implemented by creating an abstract base class that this and other classes\n    inherit from.\n    '''\n\n    def __init__(self, state_path):\n        self._state_path = state_path\n        self._tasks = {}  # map from id to a Task object\n        self._active_workers = {}  # map from id to a Worker object\n\n    def dump(self):\n        state = (self._tasks, self._active_workers)\n        try:\n            with open(self._state_path, 'w') as fobj:\n                pickle.dump(state, fobj)\n        except IOError:\n            logger.warning(\"Failed saving scheduler state\", exc_info=1)\n        else:\n            logger.info(\"Saved state in %s\", self._state_path)\n\n    # prone to lead to crashes when old state is unpickled with updated code. TODO some kind of version control?\n    def load(self):\n        if os.path.exists(self._state_path):\n            logger.info(\"Attempting to load state from %s\", self._state_path)\n            try:\n                with open(self._state_path) as fobj:\n                    state = pickle.load(fobj)\n            except:\n                logger.exception(\"Error when loading state. Starting from clean slate.\")\n                return\n\n            self._tasks, self._active_workers = state\n\n            # Convert from old format\n            # TODO: this is really ugly, we need something more future-proof\n            # Every time we add an attribute to the Worker class, this code needs to be updated\n            for k, v in self._active_workers.iteritems():\n                if isinstance(v, float):\n                    self._active_workers[k] = Worker(id=k, last_active=v)\n        else:\n            logger.info(\"No prior state file exists at %s. Starting with clean slate\", self._state_path)\n\n    def get_active_tasks(self):\n        for task in self._tasks.itervalues():\n            yield task\n\n    def get_pending_tasks(self):\n        for task in self._tasks.itervalues():\n            if task.status in [PENDING, RUNNING]:\n                yield task\n\n    def get_task(self, task_id, default=None, setdefault=None):\n        if setdefault:\n            return self._tasks.setdefault(task_id, setdefault)\n        else:\n            return self._tasks.get(task_id, default)\n\n    def has_task(self, task_id):\n        return task_id in self._tasks\n\n    def inactivate_tasks(self, delete_tasks):\n        # The terminology is a bit confusing: we used to \"delete\" tasks when they became inactive,\n        # but with a pluggable state storage, you might very well want to keep some history of\n        # older tasks as well. That's why we call it \"inactivate\" (as in the verb)\n        for task in delete_tasks:\n            self._tasks.pop(task)\n\n    def get_active_workers(self, last_active_lt=None):\n        for worker in self._active_workers.itervalues():\n            if last_active_lt is not None and worker.last_active >= last_active_lt:\n                continue\n            yield worker\n\n    def get_worker_ids(self):\n        return self._active_workers.keys() # only used for unit tests\n\n    def get_worker(self, worker_id):\n        return self._active_workers.setdefault(worker_id, Worker(worker_id))\n\n    def inactivate_workers(self, delete_workers):\n        # Mark workers as inactive\n        for worker in delete_workers:\n            self._active_workers.pop(worker)\n\n        # remove workers from tasks\n        for task in self.get_active_tasks():\n            task.stakeholders.difference_update(delete_workers)\n            task.workers.difference_update(delete_workers)\n\n\nclass CentralPlannerScheduler(Scheduler):\n    ''' Async scheduler that can handle multiple workers etc\n\n    Can be run locally or on a server (using RemoteScheduler + server.Server).\n    '''\n\n    def __init__(self, retry_delay=900.0, remove_delay=600.0, worker_disconnect_delay=60.0,\n                 state_path='/var/lib/luigi-server/state.pickle', task_history=None,\n                 resources=None, disable_persist=0, disable_window=0, disable_failures=None):\n        '''\n        (all arguments are in seconds)\n        Keyword Arguments:\n        retry_delay -- How long after a Task fails to try it again, or -1 to never retry\n        remove_delay -- How long after a Task finishes to remove it from the scheduler\n        state_path -- Path to state file (tasks and active workers)\n        worker_disconnect_delay -- If a worker hasn't communicated for this long, remove it from active workers\n        '''\n        self._config = SchedulerConfig(\n            retry_delay=retry_delay,\n            remove_delay=remove_delay,\n            worker_disconnect_delay=worker_disconnect_delay,\n            disable_failures=disable_failures,\n            disable_window=disable_window,\n            disable_persist=disable_persist,\n            disable_time=disable_persist)\n\n        self._task_history = task_history or history.NopHistory()\n        self._state = SimpleTaskState(state_path)\n\n        self._task_history = task_history or history.NopHistory()\n        self._resources = resources\n        self._make_task = functools.partial(\n            Task, disable_failures=disable_failures,\n            disable_window=disable_window)\n\n    def load(self):\n        self._state.load()\n\n    def dump(self):\n        self._state.dump()\n\n    def prune(self):\n        logger.info(\"Starting pruning of task graph\")\n        remove_workers = []\n        for worker in self._state.get_active_workers():\n            if worker.prune(self._config):\n                logger.info(\"Worker %s timed out (no contact for >=%ss)\", worker, self._config.worker_disconnect_delay)\n                remove_workers.append(worker.id)\n\n        self._state.inactivate_workers(remove_workers)\n\n        remove_tasks = []\n        for task in self._state.get_active_tasks():\n            if task.prune(self._config):\n                remove_tasks.append(task.id)\n\n        self._state.inactivate_tasks(remove_tasks)\n\n        logger.info(\"Done pruning task graph\")\n\n    def update(self, worker_id, worker_reference=None):\n        \"\"\" Keep track of whenever the worker was last active \"\"\"\n        worker = self._state.get_worker(worker_id)\n        worker.update(worker_reference)\n\n    def _update_priority(self, task, prio, worker):\n        \"\"\" Update priority of the given task\n\n        Priority can only be increased. If the task doesn't exist, a placeholder\n        task is created to preserve priority when the task is later scheduled.\n        \"\"\"\n        task.priority = prio = max(prio, task.priority)\n        for dep in task.deps or []:\n            t = self._state.get_task(dep)\n            if t is not None and prio > t.priority:\n                self._update_priority(t, prio, worker)\n\n    def add_task(self, worker, task_id, status=PENDING, runnable=True,\n                 deps=None, new_deps=None, expl=None, resources=None,\n                 priority=0, family='', params={}):\n        \"\"\"\n        * Add task identified by task_id if it doesn't exist\n        * If deps is not None, update dependency list\n        * Update status of task\n        * Add additional workers/stakeholders\n        * Update priority when needed\n        \"\"\"\n        self.update(worker)\n\n        task = self._state.get_task(task_id, setdefault=self._make_task(\n                id=task_id, status=PENDING, deps=deps, resources=resources,\n                priority=priority, family=family, params=params))\n\n        # for setting priority, we'll sometimes create tasks with unset family and params\n        if not task.family:\n            task.family = family\n        if not task.params:\n            task.params = params\n\n        if task.remove is not None:\n            task.remove = None  # unmark task for removal so it isn't removed after being added\n\n        if not (task.status == RUNNING and status == PENDING):\n            # don't allow re-scheduling of task while it is running, it must either fail or succeed first\n            if status == PENDING or status != task.status:\n                # Update the DB only if there was a acctual change, to prevent noise.\n                # We also check for status == PENDING b/c that's the default value\n                # (so checking for status != task.status woule lie)\n                self._update_task_history(task_id, status)\n            task.set_status(PENDING if status == SUSPENDED else status, self._config)\n            if status == FAILED:\n                task.retry = time.time() + self._config.retry_delay\n\n        if deps is not None:\n            task.deps = set(deps)\n\n        if new_deps is not None:\n            task.deps.update(new_deps)\n\n        task.stakeholders.add(worker)\n        task.resources = resources\n\n        # Task dependencies might not exist yet. Let's create dummy tasks for them for now.\n        # Otherwise the task dependencies might end up being pruned if scheduling takes a long time\n        for dep in task.deps or []:\n            t = self._state.get_task(dep, setdefault=self._make_task(id=dep, status=UNKNOWN, deps=None, priority=priority))\n            t.stakeholders.add(worker)\n\n        self._update_priority(task, priority, worker)\n\n        if runnable:\n            task.workers.add(worker)\n\n        if expl is not None:\n            task.expl = expl\n\n    def add_worker(self, worker, info):\n        self._state.get_worker(worker).add_info(info)\n\n    def update_resources(self, **resources):\n        if self._resources is None:\n            self._resources = {}\n        self._resources.update(resources)\n\n    def _has_resources(self, needed_resources, used_resources):\n        if needed_resources is None:\n            return True\n\n        available_resources = self._resources or {}\n        for resource, amount in needed_resources.items():\n            if amount + used_resources[resource] > available_resources.get(resource, 1):\n                return False\n        return True\n\n    def _used_resources(self):\n        used_resources = collections.defaultdict(int)\n        if self._resources is not None:\n            for task in self._state.get_active_tasks():\n                if task.status == RUNNING and task.resources:\n                    for resource, amount in task.resources.items():\n                        used_resources[resource] += amount\n        return used_resources\n\n    def _rank(self):\n        ''' Return worker's rank function for task scheduling '''\n        dependents = collections.defaultdict(int)\n        def not_done(t):\n            task = self._state.get_task(t, default=None)\n            return task is None or task.status != DONE\n        for task in self._state.get_active_tasks():\n            if task.status != DONE:\n                deps = filter(not_done, task.deps)\n                inverse_num_deps = 1.0 / max(len(deps), 1)\n                for dep in deps:\n                    dependents[dep] += inverse_num_deps\n\n        return lambda task: (task.priority, dependents[task.id], -task.time)\n\n    def _schedulable(self, task):\n        if task.status != PENDING:\n            return False\n        for dep in task.deps:\n            dep_task = self._state.get_task(dep, default=None)\n            if dep_task is None or dep_task.status != DONE:\n                return False\n        return True\n\n    def get_work(self, worker, host=None):\n        # TODO: remove any expired nodes\n\n        # Algo: iterate over all nodes, find the highest priority node no dependencies and available\n        # resources.\n\n        # Resource checking looks both at currently available resources and at which resources would\n        # be available if all running tasks died and we rescheduled all workers greedily. We do both\n        # checks in order to prevent a worker with many low-priority tasks from starving other\n        # workers with higher priority tasks that share the same resources.\n\n        # TODO: remove tasks that can't be done, figure out if the worker has absolutely\n        # nothing it can wait for\n\n        # Return remaining tasks that have no FAILED descendents\n        self.update(worker, {'host': host})\n        best_task = None\n        best_task_id = None\n        locally_pending_tasks = 0\n        running_tasks = []\n\n        used_resources = self._used_resources()\n        greedy_resources = collections.defaultdict(int)\n        n_unique_pending = 0\n        greedy_workers = dict((worker.id, worker.info.get('workers', 1))\n                              for worker in self._state.get_active_workers())\n\n        tasks = list(self._state.get_pending_tasks())\n        tasks.sort(key=self._rank(), reverse=True)\n\n        for task in tasks:\n            if task.status == 'RUNNING' and worker in task.workers:\n                # Return a list of currently running tasks to the client,\n                # makes it easier to troubleshoot\n                other_worker = self._state.get_worker(task.worker_running)\n                more_info = {'task_id': task.id, 'worker': str(other_worker)}\n                if other_worker is not None:\n                    more_info.update(other_worker.info)\n                    running_tasks.append(more_info)\n\n            if task.status == PENDING and worker in task.workers:\n                locally_pending_tasks += 1\n                if len(task.workers) == 1:\n                    n_unique_pending += 1\n\n            if task.status == RUNNING and task.worker_running in greedy_workers:\n                greedy_workers[task.worker_running] -= 1\n                for resource, amount in (task.resources or {}).items():\n                    greedy_resources[resource] += amount\n\n            if not best_task and self._schedulable(task) and self._has_resources(task.resources, greedy_resources):\n                if worker in task.workers and self._has_resources(task.resources, used_resources):\n                    best_task = task\n                    best_task_id = task.id\n                else:\n                    for task_worker in task.workers:\n                        if greedy_workers.get(task_worker, 0) > 0:\n                            # use up a worker\n                            greedy_workers[task_worker] -= 1\n\n                            # keep track of the resources used in greedy scheduling\n                            for resource, amount in (task.resources or {}).items():\n                                greedy_resources[resource] += amount\n\n                            break\n\n        if best_task:\n            best_task.status = RUNNING\n            best_task.worker_running = worker\n            best_task.time_running = time.time()\n            self._update_task_history(best_task.id, RUNNING, host=host)\n\n        return {'n_pending_tasks': locally_pending_tasks,\n                'n_unique_pending': n_unique_pending,\n                'task_id': best_task_id,\n                'running_tasks': running_tasks}\n\n    def ping(self, worker):\n        self.update(worker)\n\n    def _upstream_status(self, task_id, upstream_status_table):\n        if task_id in upstream_status_table:\n            return upstream_status_table[task_id]\n        elif self._state.has_task(task_id):\n            task_stack = [task_id]\n\n            while task_stack:\n                dep_id = task_stack.pop()\n                if self._state.has_task(dep_id):\n                    dep = self._state.get_task(dep_id)\n                    if dep_id not in upstream_status_table:\n                        if dep.status == PENDING and dep.deps:\n                            task_stack = task_stack + [dep_id] + list(dep.deps)\n                            upstream_status_table[dep_id] = ''  # will be updated postorder\n                        else:\n                            dep_status = STATUS_TO_UPSTREAM_MAP.get(dep.status, '')\n                            upstream_status_table[dep_id] = dep_status\n                    elif upstream_status_table[dep_id] == '' and dep.deps:\n                        # This is the postorder update step when we set the\n                        # status based on the previously calculated child elements\n                        upstream_status = [upstream_status_table.get(id, '') for id in dep.deps]\n                        upstream_status.append('')  # to handle empty list\n                        status = max(upstream_status, key=UPSTREAM_SEVERITY_KEY)\n                        upstream_status_table[dep_id] = status\n            return upstream_status_table[dep_id]\n\n    def _serialize_task(self, task_id, include_deps=True):\n        task = self._state.get_task(task_id)\n        ret = {\n            'deps': list(task.deps),\n            'status': task.status,\n            'workers': list(task.workers),\n            'worker_running': task.worker_running,\n            'time_running': getattr(task, \"time_running\", None),\n            'start_time': task.time,\n            'params': task.params,\n            'name': task.family,\n            'priority': task.priority,\n            'resources': task.resources,\n        }\n        if include_deps:\n            ret['deps'] = list(task.deps)\n        return ret\n\n    def graph(self):\n        self.prune()\n        serialized = {}\n        for task in self._state.get_active_tasks():\n            serialized[task.id] = self._serialize_task(task.id)\n        return serialized\n\n    def _recurse_deps(self, task_id, serialized):\n        if task_id not in serialized:\n            task = self._state.get_task(task_id)\n            if task is None or not task.family:\n                logger.warn('Missing task for id [%s]', task_id)\n\n                # try to infer family and params from task_id\n                try:\n                    family, _, param_str = task_id.rstrip(')').partition('(')\n                    params = dict(param.split('=') for param in param_str.split(', '))\n                except:\n                    family, params = '', {}\n                serialized[task_id] = {\n                    'deps': [],\n                    'status': UNKNOWN,\n                    'workers': [],\n                    'start_time': UNKNOWN,\n                    'params': params,\n                    'name': family,\n                    'priority': 0,\n                }\n            else:\n                serialized[task_id] = self._serialize_task(task_id)\n                for dep in task.deps:\n                    self._recurse_deps(dep, serialized)\n\n    def dep_graph(self, task_id):\n        self.prune()\n        serialized = {}\n        if self._state.has_task(task_id):\n            self._recurse_deps(task_id, serialized)\n        return serialized\n\n    def task_list(self, status, upstream_status):\n        ''' query for a subset of tasks by status '''\n        self.prune()\n        result = {}\n        upstream_status_table = {}  # used to memoize upstream status\n        for task in self._state.get_active_tasks():\n            if not status or task.status == status:\n                if (task.status != PENDING or not upstream_status or\n                    upstream_status == self._upstream_status(task.id, upstream_status_table)):\n                    serialized = self._serialize_task(task.id, False)\n                    result[task.id] = serialized\n        return result\n\n    def worker_list(self, include_running=True):\n        self.prune()\n        workers = [\n            dict(\n                name=worker.id,\n                last_active=worker.last_active,\n                started=getattr(worker, 'started', None),\n                **worker.info\n            ) for worker in self._state.get_active_workers()]\n        workers.sort(key=lambda worker: worker['started'], reverse=True)\n        if include_running:\n            running = collections.defaultdict(dict)\n            num_pending = collections.defaultdict(int)\n            num_uniques = collections.defaultdict(int)\n            for task in self._state.get_pending_tasks():\n                if task.status == RUNNING and task.worker_running:\n                    running[task.worker_running][task.id] = self._serialize_task(task.id, False)\n                elif task.status == PENDING:\n                    for worker in task.workers:\n                        num_pending[worker] += 1\n                    if len(task.workers) == 1:\n                        num_uniques[list(task.workers)[0]] += 1\n            for worker in workers:\n                tasks = running[worker['name']]\n                worker['num_running'] = len(tasks)\n                worker['num_pending'] = num_pending[worker['name']]\n                worker['num_uniques'] = num_uniques[worker['name']]\n                worker['running'] = tasks\n        return workers\n\n    def inverse_dependencies(self, task_id):\n        self.prune()\n        serialized = {}\n        if self._state.has_task(task_id):\n            self._traverse_inverse_deps(task_id, serialized)\n        return serialized\n\n    def _traverse_inverse_deps(self, task_id, serialized):\n        stack = [task_id]\n        serialized[task_id] = self._serialize_task(task_id)\n        while len(stack) > 0:\n            curr_id = stack.pop()\n            for task in self._state.get_active_tasks():\n                if curr_id in task.deps:\n                    serialized[curr_id][\"deps\"].append(task.id)\n                    if task.id not in serialized:\n                        serialized[task.id] = self._serialize_task(task.id)\n                        serialized[task.id][\"deps\"] = []\n                        stack.append(task.id)\n\n    def task_search(self, task_str):\n        ''' query for a subset of tasks by task_id '''\n        self.prune()\n        result = collections.defaultdict(dict)\n        for task in self._state.get_active_tasks():\n            if task.id.find(task_str) != -1:\n                serialized = self._serialize_task(task.id, False)\n                result[task.status][task.id] = serialized\n        return result\n\n    def re_enable_task(self, task_id):\n        serialized = {}\n        task = self._state.get_task(task_id)\n        if task and task.status == DISABLED and task.scheduler_disable_time:\n            task.re_enable()\n            serialized = self._serialize_task(task_id)\n        return serialized\n\n    def fetch_error(self, task_id):\n        if self._state.has_task(task_id):\n            return {\"taskId\": task_id, \"error\": self._state.get_task(task_id).expl}\n        else:\n            return {\"taskId\": task_id, \"error\": \"\"}\n\n    def _update_task_history(self, task_id, status, host=None):\n        try:\n            if status == DONE or status == FAILED:\n                successful = (status == DONE)\n                self._task_history.task_finished(task_id, successful)\n            elif status == PENDING:\n                self._task_history.task_scheduled(task_id)\n            elif status == RUNNING:\n                self._task_history.task_started(task_id, host)\n        except:\n            logger.warning(\"Error saving Task history\", exc_info=1)\n\n    @property\n    def task_history(self):\n        # Used by server.py to expose the calls\n        return self._task_history\n",
          "file_patch": "@@ -62,6 +62,15 @@ SchedulerConfig = collections.namedtuple('SchedulerConfig', [\n         'disable_failures', 'disable_window', 'disable_persist', 'disable_time'])\n \n \n+def fix_time(x):\n+    # Backwards compatibility for a fix in Dec 2014. Prior to the fix, pickled state might store datetime objects\n+    # Let's remove this function soon\n+    if isinstance(x, datetime.datetime):\n+        return time.mktime(d.timetuple())\n+    else:\n+        return x\n+\n+\n class Failures(object):\n     \"\"\" This class tracks the number of failures in a given time window\n \n@@ -73,20 +82,22 @@ class Failures(object):\n     def __init__(self, window):\n         \"\"\" Initialize with the given window\n \n-        :param window: how long to track failures for, as a datetime.timedelta\n+        :param window: how long to track failures for, as a float (number of seconds)\n         \"\"\"\n         self.window = window\n         self.failures = collections.deque()\n \n     def add_failure(self):\n         \"\"\" Add a failure event with the current timestamp \"\"\"\n-        self.failures.append(datetime.datetime.now())\n+        self.failures.append(time.time())\n \n     def num_failures(self):\n         \"\"\" Return the number of failures in the window \"\"\"\n-        min_time = datetime.datetime.now() - self.window\n-        while self.failures and self.failures[0] < min_time:\n+        min_time = time.time() - self.window\n+\n+        while self.failures and fix_time(self.failures[0]) < min_time:\n             self.failures.popleft()\n+\n         return len(self.failures)\n \n     def clear(self):\n@@ -155,7 +166,7 @@ class Task(object):\n         if new_status == FAILED and self.can_disable():\n             self.add_failure()\n             if self.has_excessive_failures():\n-                self.scheduler_disable_time = datetime.datetime.now()\n+                self.scheduler_disable_time = time.time()\n                 new_status = DISABLED\n                 notifications.send_error_email(\n                     'Luigi Scheduler: DISABLED {task} due to excessive failures'.format(task=self.id),\n@@ -189,7 +200,7 @@ class Task(object):\n \n         # Re-enable task after the disable time expires\n         if self.status == DISABLED and self.scheduler_disable_time:\n-            if datetime.datetime.now() - self.scheduler_disable_time > config.disable_time:\n+            if time.time() - fix_time(self.scheduler_disable_time) > config.disable_time:\n                 self.re_enable()\n \n         # Remove tasks that have no stakeholders\n@@ -347,7 +358,7 @@ class CentralPlannerScheduler(Scheduler):\n             disable_failures=disable_failures,\n             disable_window=disable_window,\n             disable_persist=disable_persist,\n-            disable_time=datetime.timedelta(seconds=disable_persist))\n+            disable_time=disable_persist)\n \n         self._task_history = task_history or history.NopHistory()\n         self._state = SimpleTaskState(state_path)\n@@ -356,7 +367,7 @@ class CentralPlannerScheduler(Scheduler):\n         self._resources = resources\n         self._make_task = functools.partial(\n             Task, disable_failures=disable_failures,\n-            disable_window=datetime.timedelta(seconds=disable_window))\n+            disable_window=disable_window)\n \n     def load(self):\n         self._state.load()\n",
          "files_name_in_blame_commit": [
            "scheduler.py"
          ]
        }
      },
      "ea056a277a623c935769ecbf4d50df0af93b79a6": {
        "commit": {
          "commit_id": "ea056a277a623c935769ecbf4d50df0af93b79a6",
          "commit_message": "pass scheduler config object instead of a lot of arguments",
          "commit_author": "Erik Bernhardsson",
          "commit_date": "2014-12-10 18:52:08",
          "commit_parent": "3947255a24ff0a1e483a8736241dc274a5507519"
        },
        "function": {
          "function_name": "set_status",
          "function_code_before": "def set_status(self, new_status, disable_failures=None, disable_window=None, disable_persist=None):\n    if new_status == SUSPENDED:\n        new_status = PENDING\n    if new_status == DISABLED and self.status == RUNNING:\n        return\n    if self.status == DISABLED:\n        if new_status == DONE:\n            self.re_enable()\n        elif self.scheduler_disable_time is not None:\n            return\n    if new_status == FAILED and self.can_disable():\n        self.add_failure()\n        if self.has_excessive_failures():\n            self.scheduler_disable_time = datetime.datetime.now()\n            new_status = DISABLED\n            notifications.send_error_email('Luigi Scheduler: DISABLED {task} due to excessive failures'.format(task=self.id), '{task} failed {failures} times in the last {window} seconds, so it is being disabled for {persist} seconds'.format(failures=disable_failures, task=self.id, window=disable_window, persist=disable_persist))\n    elif new_status == DISABLED:\n        self.scheduler_disable_time = None\n    self.status = new_status",
          "function_code_after": "def set_status(self, new_status, config):\n    if new_status == SUSPENDED:\n        new_status = PENDING\n    if new_status == DISABLED and self.status == RUNNING:\n        return\n    if self.status == DISABLED:\n        if new_status == DONE:\n            self.re_enable()\n        elif self.scheduler_disable_time is not None:\n            return\n    if new_status == FAILED and self.can_disable():\n        self.add_failure()\n        if self.has_excessive_failures():\n            self.scheduler_disable_time = datetime.datetime.now()\n            new_status = DISABLED\n            notifications.send_error_email('Luigi Scheduler: DISABLED {task} due to excessive failures'.format(task=self.id), '{task} failed {failures} times in the last {window} seconds, so it is being disabled for {persist} seconds'.format(failures=config.disable_failures, task=self.id, window=config.disable_window, persist=config.disable_persist))\n    elif new_status == DISABLED:\n        self.scheduler_disable_time = None\n    self.status = new_status",
          "function_before_start_line": 140,
          "function_before_end_line": 173,
          "function_after_start_line": 139,
          "function_after_end_line": 172,
          "function_before_token_count": 157,
          "function_after_token_count": 153,
          "functions_name_modified_file": [
            "add_failure",
            "get_worker_ids",
            "prune",
            "_schedulable",
            "clear",
            "has_excessive_failures",
            "_upstream_status",
            "_update_task_history",
            "can_disable",
            "ping",
            "get_pending_tasks",
            "_traverse_inverse_deps",
            "load",
            "inverse_dependencies",
            "num_failures",
            "task_search",
            "_recurse_deps",
            "get_worker",
            "get_active_workers",
            "worker_list",
            "add_task",
            "update_resources",
            "_update_priority",
            "task_list",
            "add_info",
            "get_active_tasks",
            "_has_resources",
            "_used_resources",
            "has_task",
            "_serialize_task",
            "task_history",
            "dep_graph",
            "__str__",
            "graph",
            "re_enable",
            "dump",
            "__repr__",
            "inactivate_tasks",
            "__init__",
            "update",
            "set_status",
            "add_worker",
            "get_task",
            "_rank",
            "get_work",
            "re_enable_task",
            "fetch_error",
            "inactivate_workers"
          ],
          "functions_name_all_files": [
            "add_failure",
            "get_worker_ids",
            "prune",
            "_schedulable",
            "clear",
            "has_excessive_failures",
            "_upstream_status",
            "_update_task_history",
            "can_disable",
            "ping",
            "get_pending_tasks",
            "_traverse_inverse_deps",
            "load",
            "inverse_dependencies",
            "num_failures",
            "task_search",
            "_recurse_deps",
            "get_worker",
            "get_active_workers",
            "worker_list",
            "add_task",
            "update_resources",
            "_update_priority",
            "task_list",
            "add_info",
            "get_active_tasks",
            "_has_resources",
            "_used_resources",
            "has_task",
            "_serialize_task",
            "task_history",
            "dep_graph",
            "__str__",
            "graph",
            "re_enable",
            "dump",
            "__repr__",
            "inactivate_tasks",
            "__init__",
            "update",
            "set_status",
            "add_worker",
            "get_task",
            "_rank",
            "get_work",
            "re_enable_task",
            "fetch_error",
            "inactivate_workers"
          ],
          "functions_name_co_evolved_modified_file": [
            "add_task",
            "prune"
          ],
          "functions_name_co_evolved_all_files": [
            "add_task",
            "prune"
          ]
        },
        "file": {
          "file_name": "scheduler.py",
          "file_nloc": 569,
          "file_complexity": 217,
          "file_token_count": 4227,
          "file_before": "# Copyright (c) 2012 Spotify AB\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"); you may not\n# use this file except in compliance with the License. You may obtain a copy of\n# the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n# License for the specific language governing permissions and limitations under\n# the License.\n\nimport collections\nimport datetime\nimport functools\nimport notifications\nimport os\nimport logging\nimport time\nimport cPickle as pickle\nimport task_history as history\nlogger = logging.getLogger(\"luigi.server\")\n\nfrom task_status import PENDING, FAILED, DONE, RUNNING, SUSPENDED, UNKNOWN, DISABLED\n\n\nclass Scheduler(object):\n    ''' Abstract base class\n\n    Note that the methods all take string arguments, not Task objects...\n    '''\n    add_task = NotImplemented\n    get_work = NotImplemented\n    ping = NotImplemented\n\nUPSTREAM_RUNNING = 'UPSTREAM_RUNNING'\nUPSTREAM_MISSING_INPUT = 'UPSTREAM_MISSING_INPUT'\nUPSTREAM_FAILED = 'UPSTREAM_FAILED'\nUPSTREAM_DISABLED = 'UPSTREAM_DISABLED'\n\nUPSTREAM_SEVERITY_ORDER = (\n    '',\n    UPSTREAM_RUNNING,\n    UPSTREAM_MISSING_INPUT,\n    UPSTREAM_FAILED,\n    UPSTREAM_DISABLED,\n)\nUPSTREAM_SEVERITY_KEY = lambda st: UPSTREAM_SEVERITY_ORDER.index(st)\nSTATUS_TO_UPSTREAM_MAP = {\n    FAILED: UPSTREAM_FAILED,\n    RUNNING: UPSTREAM_RUNNING,\n    PENDING: UPSTREAM_MISSING_INPUT,\n    DISABLED: UPSTREAM_DISABLED,\n}\n\n\n# We're passing around this config a lot, so let's put it on an object\nSchedulerConfig = collections.namedtuple('SchedulerConfig', [\n        'retry_delay', 'remove_delay', 'worker_disconnect_delay',\n        'disable_failures', 'disable_window', 'disable_persist', 'disable_time'],\n                                         verbose=True)\n\n\nclass Failures(object):\n    \"\"\" This class tracks the number of failures in a given time window\n\n    Failures added are marked with the current timestamp, and this class counts\n    the number of failures in a sliding time window ending at the present.\n\n    \"\"\"\n\n    def __init__(self, window):\n        \"\"\" Initialize with the given window\n\n        :param window: how long to track failures for, as a datetime.timedelta\n        \"\"\"\n        self.window = window\n        self.failures = collections.deque()\n\n    def add_failure(self):\n        \"\"\" Add a failure event with the current timestamp \"\"\"\n        self.failures.append(datetime.datetime.now())\n\n    def num_failures(self):\n        \"\"\" Return the number of failures in the window \"\"\"\n        min_time = datetime.datetime.now() - self.window\n        while self.failures and self.failures[0] < min_time:\n            self.failures.popleft()\n        return len(self.failures)\n\n    def clear(self):\n        \"\"\" Clear the failure queue \"\"\"\n        self.failures.clear()\n\n\nclass Task(object):\n    def __init__(self, id, status, deps, resources={}, priority=0, family='', params={},\n                 disable_failures=None, disable_window=None):\n        self.id = id\n        self.stakeholders = set()  # workers ids that are somehow related to this task (i.e. don't prune while any of these workers are still active)\n        self.workers = set()  # workers ids that can perform task - task is 'BROKEN' if none of these workers are active\n        if deps is None:\n            self.deps = set()\n        else:\n            self.deps = set(deps)\n        self.status = status  # PENDING, RUNNING, FAILED or DONE\n        self.time = time.time()  # Timestamp when task was first added\n        self.retry = None\n        self.remove = None\n        self.worker_running = None  # the worker id that is currently running the task or None\n        self.time_running = None  # Timestamp when picked up by worker\n        self.expl = None\n        self.priority = priority\n        self.resources = resources\n        self.family = family\n        self.params = params\n        self.disable_failures = disable_failures\n        self.failures = Failures(disable_window)\n        self.scheduler_disable_time = None\n\n    def __repr__(self):\n        return \"Task(%r)\" % vars(self)\n\n    def add_failure(self):\n        self.failures.add_failure()\n\n    def has_excessive_failures(self):\n        return self.failures.num_failures() >= self.disable_failures\n\n    def can_disable(self):\n        return self.disable_failures is not None\n\n    def re_enable(self):\n        self.scheduler_disable_time = None\n        self.status = FAILED\n        self.failures.clear()\n\n    def set_status(self, new_status, disable_failures=None, disable_window=None, disable_persist=None):\n        # not sure why we have SUSPENDED, as it can never be set\n        if new_status == SUSPENDED:\n            new_status = PENDING\n\n        if new_status == DISABLED and self.status == RUNNING:\n            return\n\n        if self.status == DISABLED:\n            if new_status == DONE:\n                self.re_enable()\n\n            # don't allow workers to override a scheduler disable\n            elif self.scheduler_disable_time is not None:\n                return\n\n        if new_status == FAILED and self.can_disable():\n            self.add_failure()\n            if self.has_excessive_failures():\n                self.scheduler_disable_time = datetime.datetime.now()\n                new_status = DISABLED\n                notifications.send_error_email(\n                    'Luigi Scheduler: DISABLED {task} due to excessive failures'.format(task=self.id),\n                    '{task} failed {failures} times in the last {window} seconds, so it is being '\n                    'disabled for {persist} seconds'.format(\n                        failures=disable_failures,\n                        task=self.id,\n                        window=disable_window,\n                        persist=disable_persist,\n                        ))\n        elif new_status == DISABLED:\n            self.scheduler_disable_time = None\n\n        self.status = new_status\n\n    def prune(self, remove_delay, retry_delay, disable_time,\n              disable_failures, disable_window, disable_persist):\n        remove = False\n\n        # Mark tasks with no remaining active stakeholders for deletion\n        if not self.stakeholders:\n            if self.remove is None:\n                logger.info(\"Task %r has stakeholders %r but none remain connected -> will remove task in %s seconds\", self.id, self.stakeholders, remove_delay)\n                self.remove = time.time() + remove_delay\n\n        # If a running worker disconnects, tag all its jobs as FAILED and subject it to the same retry logic\n        if self.status == RUNNING and self.worker_running and self.worker_running not in self.stakeholders:\n            logger.info(\"Task %r is marked as running by disconnected worker %r -> marking as FAILED with retry delay of %rs\", self.id, self.worker_running, retry_delay)\n            self.worker_running = None\n            self.set_status(FAILED, disable_failures, disable_window, disable_persist)\n            self.retry = time.time() + retry_delay\n\n        # Re-enable task after the disable time expires\n        if self.status == DISABLED and self.scheduler_disable_time:\n            if datetime.datetime.now() - self.scheduler_disable_time > disable_time:\n                self.re_enable()\n\n        # Remove tasks that have no stakeholders\n        if self.remove and time.time() > self.remove:\n            logger.info(\"Removing task %r (no connected stakeholders)\", self.id)\n            remove = True\n\n        # Reset FAILED tasks to PENDING if max timeout is reached, and retry delay is >= 0\n        if self.status == FAILED and retry_delay >= 0 and self.retry < time.time():\n            self.set_status(PENDING)\n\n        return remove\n\n\nclass Worker(object):\n    \"\"\" Structure for tracking worker activity and keeping their references \"\"\"\n    def __init__(self, id, last_active=None):\n        self.id = id\n        self.reference = None  # reference to the worker in the real world. (Currently a dict containing just the host)\n        self.last_active = last_active  # seconds since epoch\n        self.started = time.time()  # seconds since epoch\n        self.info = {}\n\n    def add_info(self, info):\n        self.info.update(info)\n\n    def update(self, worker_reference):\n        if worker_reference:\n            self.reference = worker_reference\n        self.last_active = time.time()\n\n    def prune(self, worker_disconnect_delay):\n        # Delete workers that haven't said anything for a while (probably killed)\n        if self.last_active + worker_disconnect_delay < time.time():\n            return True\n\n    def __str__(self):\n        return self.id\n\n\nclass SimpleTaskState(object):\n    ''' Keep track of the current state and handle persistance\n\n    The point of this class is to enable other ways to keep state, eg. by using a database\n    These will be implemented by creating an abstract base class that this and other classes\n    inherit from.\n    '''\n\n    def __init__(self, state_path):\n        self._state_path = state_path\n        self._tasks = {}  # map from id to a Task object\n        self._active_workers = {}  # map from id to a Worker object\n\n    def dump(self):\n        state = (self._tasks, self._active_workers)\n        try:\n            with open(self._state_path, 'w') as fobj:\n                pickle.dump(state, fobj)\n        except IOError:\n            logger.warning(\"Failed saving scheduler state\", exc_info=1)\n        else:\n            logger.info(\"Saved state in %s\", self._state_path)\n\n    # prone to lead to crashes when old state is unpickled with updated code. TODO some kind of version control?\n    def load(self):\n        if os.path.exists(self._state_path):\n            logger.info(\"Attempting to load state from %s\", self._state_path)\n            try:\n                with open(self._state_path) as fobj:\n                    state = pickle.load(fobj)\n            except:\n                logger.exception(\"Error when loading state. Starting from clean slate.\")\n                return\n\n            self._tasks, self._active_workers = state\n\n            # Convert from old format\n            # TODO: this is really ugly, we need something more future-proof\n            # Every time we add an attribute to the Worker class, this code needs to be updated\n            for k, v in self._active_workers.iteritems():\n                if isinstance(v, float):\n                    self._active_workers[k] = Worker(id=k, last_active=v)\n        else:\n            logger.info(\"No prior state file exists at %s. Starting with clean slate\", self._state_path)\n\n    def get_active_tasks(self):\n        for task in self._tasks.itervalues():\n            yield task\n\n    def get_pending_tasks(self):\n        for task in self._tasks.itervalues():\n            if task.status in [PENDING, RUNNING]:\n                yield task\n\n    def get_task(self, task_id, default=None, setdefault=None):\n        if setdefault:\n            return self._tasks.setdefault(task_id, setdefault)\n        else:\n            return self._tasks.get(task_id, default)\n\n    def has_task(self, task_id):\n        return task_id in self._tasks\n\n    def inactivate_tasks(self, delete_tasks):\n        # The terminology is a bit confusing: we used to \"delete\" tasks when they became inactive,\n        # but with a pluggable state storage, you might very well want to keep some history of\n        # older tasks as well. That's why we call it \"inactivate\" (as in the verb)\n        for task in delete_tasks:\n            self._tasks.pop(task)\n\n    def get_active_workers(self, last_active_lt=None):\n        for worker in self._active_workers.itervalues():\n            if last_active_lt is not None and worker.last_active >= last_active_lt:\n                continue\n            yield worker\n\n    def get_worker_ids(self):\n        return self._active_workers.keys() # only used for unit tests\n\n    def get_worker(self, worker_id):\n        return self._active_workers.setdefault(worker_id, Worker(worker_id))\n\n    def inactivate_workers(self, delete_workers):\n        # Mark workers as inactive\n        for worker in delete_workers:\n            self._active_workers.pop(worker)\n\n        # remove workers from tasks\n        for task in self.get_active_tasks():\n            task.stakeholders.difference_update(delete_workers)\n            task.workers.difference_update(delete_workers)\n\n\nclass CentralPlannerScheduler(Scheduler):\n    ''' Async scheduler that can handle multiple workers etc\n\n    Can be run locally or on a server (using RemoteScheduler + server.Server).\n    '''\n\n    def __init__(self, retry_delay=900.0, remove_delay=600.0, worker_disconnect_delay=60.0,\n                 state_path='/var/lib/luigi-server/state.pickle', task_history=None,\n                 resources=None, disable_persist=0, disable_window=0, disable_failures=None):\n        '''\n        (all arguments are in seconds)\n        Keyword Arguments:\n        retry_delay -- How long after a Task fails to try it again, or -1 to never retry\n        remove_delay -- How long after a Task finishes to remove it from the scheduler\n        state_path -- Path to state file (tasks and active workers)\n        worker_disconnect_delay -- If a worker hasn't communicated for this long, remove it from active workers\n        '''\n        self._config = SchedulerConfig(\n            retry_delay=retry_delay,\n            remove_delay=remove_delay,\n            worker_disconnect_delay=worker_disconnect_delay,\n            disable_failures=disable_failures,\n            disable_window=disable_window,\n            disable_persist=disable_persist,\n            disable_time=datetime.timedelta(seconds=disable_persist))\n\n        self._task_history = task_history or history.NopHistory()\n        self._state = SimpleTaskState(state_path)\n\n        self._task_history = task_history or history.NopHistory()\n        self._resources = resources\n        self._make_task = functools.partial(\n            Task, disable_failures=disable_failures,\n            disable_window=datetime.timedelta(seconds=disable_window))\n\n    def load(self):\n        self._state.load()\n\n    def dump(self):\n        self._state.dump()\n\n    def prune(self):\n        logger.info(\"Starting pruning of task graph\")\n        remove_workers = []\n        for worker in self._state.get_active_workers():\n            if worker.prune(self._config.worker_disconnect_delay):\n                logger.info(\"Worker %s timed out (no contact for >=%ss)\", worker, self._config.worker_disconnect_delay)\n                remove_workers.append(worker.id)\n\n        self._state.inactivate_workers(remove_workers)\n\n        remove_tasks = []\n        for task in self._state.get_active_tasks():\n            if task.prune(self._config.remove_delay, self._config.retry_delay, self._config.disable_time,\n                          self._config.disable_failures, self._config.disable_window, self._config.disable_persist):\n                remove_tasks.append(task.id)\n\n        self._state.inactivate_tasks(remove_tasks)\n\n        logger.info(\"Done pruning task graph\")\n\n    def update(self, worker_id, worker_reference=None):\n        \"\"\" Keep track of whenever the worker was last active \"\"\"\n        worker = self._state.get_worker(worker_id)\n        worker.update(worker_reference)\n\n    def _update_priority(self, task, prio, worker):\n        \"\"\" Update priority of the given task\n\n        Priority can only be increased. If the task doesn't exist, a placeholder\n        task is created to preserve priority when the task is later scheduled.\n        \"\"\"\n        task.priority = prio = max(prio, task.priority)\n        for dep in task.deps or []:\n            t = self._state.get_task(dep)\n            if t is not None and prio > t.priority:\n                self._update_priority(t, prio, worker)\n\n    def add_task(self, worker, task_id, status=PENDING, runnable=True,\n                 deps=None, new_deps=None, expl=None, resources=None,\n                 priority=0, family='', params={}):\n        \"\"\"\n        * Add task identified by task_id if it doesn't exist\n        * If deps is not None, update dependency list\n        * Update status of task\n        * Add additional workers/stakeholders\n        * Update priority when needed\n        \"\"\"\n        self.update(worker)\n\n        task = self._state.get_task(task_id, setdefault=self._make_task(\n                id=task_id, status=PENDING, deps=deps, resources=resources,\n                priority=priority, family=family, params=params))\n\n        # for setting priority, we'll sometimes create tasks with unset family and params\n        if not task.family:\n            task.family = family\n        if not task.params:\n            task.params = params\n\n        if task.remove is not None:\n            task.remove = None  # unmark task for removal so it isn't removed after being added\n\n        if not (task.status == RUNNING and status == PENDING):\n            # don't allow re-scheduling of task while it is running, it must either fail or succeed first\n            if status == PENDING or status != task.status:\n                # Update the DB only if there was a acctual change, to prevent noise.\n                # We also check for status == PENDING b/c that's the default value\n                # (so checking for status != task.status woule lie)\n                self._update_task_history(task_id, status)\n            task.set_status(PENDING if status == SUSPENDED else status,\n                            self._config.disable_failures, self._config.disable_window, self._config.disable_persist)\n            if status == FAILED:\n                task.retry = time.time() + self._config.retry_delay\n\n        if deps is not None:\n            task.deps = set(deps)\n\n        if new_deps is not None:\n            task.deps.update(new_deps)\n\n        task.stakeholders.add(worker)\n        task.resources = resources\n\n        # Task dependencies might not exist yet. Let's create dummy tasks for them for now.\n        # Otherwise the task dependencies might end up being pruned if scheduling takes a long time\n        for dep in task.deps or []:\n            t = self._state.get_task(dep, setdefault=self._make_task(id=dep, status=UNKNOWN, deps=None, priority=priority))\n            t.stakeholders.add(worker)\n\n        self._update_priority(task, priority, worker)\n\n        if runnable:\n            task.workers.add(worker)\n\n        if expl is not None:\n            task.expl = expl\n\n    def add_worker(self, worker, info):\n        self._state.get_worker(worker).add_info(info)\n\n    def update_resources(self, **resources):\n        if self._resources is None:\n            self._resources = {}\n        self._resources.update(resources)\n\n    def _has_resources(self, needed_resources, used_resources):\n        if needed_resources is None:\n            return True\n\n        available_resources = self._resources or {}\n        for resource, amount in needed_resources.items():\n            if amount + used_resources[resource] > available_resources.get(resource, 1):\n                return False\n        return True\n\n    def _used_resources(self):\n        used_resources = collections.defaultdict(int)\n        if self._resources is not None:\n            for task in self._state.get_active_tasks():\n                if task.status == RUNNING and task.resources:\n                    for resource, amount in task.resources.items():\n                        used_resources[resource] += amount\n        return used_resources\n\n    def _rank(self):\n        ''' Return worker's rank function for task scheduling '''\n        dependents = collections.defaultdict(int)\n        def not_done(t):\n            task = self._state.get_task(t, default=None)\n            return task is None or task.status != DONE\n        for task in self._state.get_active_tasks():\n            if task.status != DONE:\n                deps = filter(not_done, task.deps)\n                inverse_num_deps = 1.0 / max(len(deps), 1)\n                for dep in deps:\n                    dependents[dep] += inverse_num_deps\n\n        return lambda task: (task.priority, dependents[task.id], -task.time)\n\n    def _schedulable(self, task):\n        if task.status != PENDING:\n            return False\n        for dep in task.deps:\n            dep_task = self._state.get_task(dep, default=None)\n            if dep_task is None or dep_task.status != DONE:\n                return False\n        return True\n\n    def get_work(self, worker, host=None):\n        # TODO: remove any expired nodes\n\n        # Algo: iterate over all nodes, find the highest priority node no dependencies and available\n        # resources.\n\n        # Resource checking looks both at currently available resources and at which resources would\n        # be available if all running tasks died and we rescheduled all workers greedily. We do both\n        # checks in order to prevent a worker with many low-priority tasks from starving other\n        # workers with higher priority tasks that share the same resources.\n\n        # TODO: remove tasks that can't be done, figure out if the worker has absolutely\n        # nothing it can wait for\n\n        # Return remaining tasks that have no FAILED descendents\n        self.update(worker, {'host': host})\n        best_task = None\n        best_task_id = None\n        locally_pending_tasks = 0\n        running_tasks = []\n\n        used_resources = self._used_resources()\n        greedy_resources = collections.defaultdict(int)\n        n_unique_pending = 0\n        greedy_workers = dict((worker.id, worker.info.get('workers', 1))\n                              for worker in self._state.get_active_workers())\n\n        tasks = list(self._state.get_pending_tasks())\n        tasks.sort(key=self._rank(), reverse=True)\n\n        for task in tasks:\n            if task.status == 'RUNNING' and worker in task.workers:\n                # Return a list of currently running tasks to the client,\n                # makes it easier to troubleshoot\n                other_worker = self._state.get_worker(task.worker_running)\n                more_info = {'task_id': task.id, 'worker': str(other_worker)}\n                if other_worker is not None:\n                    more_info.update(other_worker.info)\n                    running_tasks.append(more_info)\n\n            if task.status == PENDING and worker in task.workers:\n                locally_pending_tasks += 1\n                if len(task.workers) == 1:\n                    n_unique_pending += 1\n\n            if task.status == RUNNING and task.worker_running in greedy_workers:\n                greedy_workers[task.worker_running] -= 1\n                for resource, amount in (task.resources or {}).items():\n                    greedy_resources[resource] += amount\n\n            if not best_task and self._schedulable(task) and self._has_resources(task.resources, greedy_resources):\n                if worker in task.workers and self._has_resources(task.resources, used_resources):\n                    best_task = task\n                    best_task_id = task.id\n                else:\n                    for task_worker in task.workers:\n                        if greedy_workers.get(task_worker, 0) > 0:\n                            # use up a worker\n                            greedy_workers[task_worker] -= 1\n\n                            # keep track of the resources used in greedy scheduling\n                            for resource, amount in (task.resources or {}).items():\n                                greedy_resources[resource] += amount\n\n                            break\n\n        if best_task:\n            best_task.status = RUNNING\n            best_task.worker_running = worker\n            best_task.time_running = time.time()\n            self._update_task_history(best_task.id, RUNNING, host=host)\n\n        return {'n_pending_tasks': locally_pending_tasks,\n                'n_unique_pending': n_unique_pending,\n                'task_id': best_task_id,\n                'running_tasks': running_tasks}\n\n    def ping(self, worker):\n        self.update(worker)\n\n    def _upstream_status(self, task_id, upstream_status_table):\n        if task_id in upstream_status_table:\n            return upstream_status_table[task_id]\n        elif self._state.has_task(task_id):\n            task_stack = [task_id]\n\n            while task_stack:\n                dep_id = task_stack.pop()\n                if self._state.has_task(dep_id):\n                    dep = self._state.get_task(dep_id)\n                    if dep_id not in upstream_status_table:\n                        if dep.status == PENDING and dep.deps:\n                            task_stack = task_stack + [dep_id] + list(dep.deps)\n                            upstream_status_table[dep_id] = ''  # will be updated postorder\n                        else:\n                            dep_status = STATUS_TO_UPSTREAM_MAP.get(dep.status, '')\n                            upstream_status_table[dep_id] = dep_status\n                    elif upstream_status_table[dep_id] == '' and dep.deps:\n                        # This is the postorder update step when we set the\n                        # status based on the previously calculated child elements\n                        upstream_status = [upstream_status_table.get(id, '') for id in dep.deps]\n                        upstream_status.append('')  # to handle empty list\n                        status = max(upstream_status, key=UPSTREAM_SEVERITY_KEY)\n                        upstream_status_table[dep_id] = status\n            return upstream_status_table[dep_id]\n\n    def _serialize_task(self, task_id, include_deps=True):\n        task = self._state.get_task(task_id)\n        ret = {\n            'deps': list(task.deps),\n            'status': task.status,\n            'workers': list(task.workers),\n            'worker_running': task.worker_running,\n            'time_running': getattr(task, \"time_running\", None),\n            'start_time': task.time,\n            'params': task.params,\n            'name': task.family,\n            'priority': task.priority,\n            'resources': task.resources,\n        }\n        if include_deps:\n            ret['deps'] = list(task.deps)\n        return ret\n\n    def graph(self):\n        self.prune()\n        serialized = {}\n        for task in self._state.get_active_tasks():\n            serialized[task.id] = self._serialize_task(task.id)\n        return serialized\n\n    def _recurse_deps(self, task_id, serialized):\n        if task_id not in serialized:\n            task = self._state.get_task(task_id)\n            if task is None or not task.family:\n                logger.warn('Missing task for id [%s]', task_id)\n\n                # try to infer family and params from task_id\n                try:\n                    family, _, param_str = task_id.rstrip(')').partition('(')\n                    params = dict(param.split('=') for param in param_str.split(', '))\n                except:\n                    family, params = '', {}\n                serialized[task_id] = {\n                    'deps': [],\n                    'status': UNKNOWN,\n                    'workers': [],\n                    'start_time': UNKNOWN,\n                    'params': params,\n                    'name': family,\n                    'priority': 0,\n                }\n            else:\n                serialized[task_id] = self._serialize_task(task_id)\n                for dep in task.deps:\n                    self._recurse_deps(dep, serialized)\n\n    def dep_graph(self, task_id):\n        self.prune()\n        serialized = {}\n        if self._state.has_task(task_id):\n            self._recurse_deps(task_id, serialized)\n        return serialized\n\n    def task_list(self, status, upstream_status):\n        ''' query for a subset of tasks by status '''\n        self.prune()\n        result = {}\n        upstream_status_table = {}  # used to memoize upstream status\n        for task in self._state.get_active_tasks():\n            if not status or task.status == status:\n                if (task.status != PENDING or not upstream_status or\n                    upstream_status == self._upstream_status(task.id, upstream_status_table)):\n                    serialized = self._serialize_task(task.id, False)\n                    result[task.id] = serialized\n        return result\n\n    def worker_list(self, include_running=True):\n        self.prune()\n        workers = [\n            dict(\n                name=worker.id,\n                last_active=worker.last_active,\n                started=getattr(worker, 'started', None),\n                **worker.info\n            ) for worker in self._state.get_active_workers()]\n        workers.sort(key=lambda worker: worker['started'], reverse=True)\n        if include_running:\n            running = collections.defaultdict(dict)\n            num_pending = collections.defaultdict(int)\n            num_uniques = collections.defaultdict(int)\n            for task in self._state.get_pending_tasks():\n                if task.status == RUNNING and task.worker_running:\n                    running[task.worker_running][task.id] = self._serialize_task(task.id, False)\n                elif task.status == PENDING:\n                    for worker in task.workers:\n                        num_pending[worker] += 1\n                    if len(task.workers) == 1:\n                        num_uniques[list(task.workers)[0]] += 1\n            for worker in workers:\n                tasks = running[worker['name']]\n                worker['num_running'] = len(tasks)\n                worker['num_pending'] = num_pending[worker['name']]\n                worker['num_uniques'] = num_uniques[worker['name']]\n                worker['running'] = tasks\n        return workers\n\n    def inverse_dependencies(self, task_id):\n        self.prune()\n        serialized = {}\n        if self._state.has_task(task_id):\n            self._traverse_inverse_deps(task_id, serialized)\n        return serialized\n\n    def _traverse_inverse_deps(self, task_id, serialized):\n        stack = [task_id]\n        serialized[task_id] = self._serialize_task(task_id)\n        while len(stack) > 0:\n            curr_id = stack.pop()\n            for task in self._state.get_active_tasks():\n                if curr_id in task.deps:\n                    serialized[curr_id][\"deps\"].append(task.id)\n                    if task.id not in serialized:\n                        serialized[task.id] = self._serialize_task(task.id)\n                        serialized[task.id][\"deps\"] = []\n                        stack.append(task.id)\n\n    def task_search(self, task_str):\n        ''' query for a subset of tasks by task_id '''\n        self.prune()\n        result = collections.defaultdict(dict)\n        for task in self._state.get_active_tasks():\n            if task.id.find(task_str) != -1:\n                serialized = self._serialize_task(task.id, False)\n                result[task.status][task.id] = serialized\n        return result\n\n    def re_enable_task(self, task_id):\n        serialized = {}\n        task = self._state.get_task(task_id)\n        if task and task.status == DISABLED and task.scheduler_disable_time:\n            task.re_enable()\n            serialized = self._serialize_task(task_id)\n        return serialized\n\n    def fetch_error(self, task_id):\n        if self._state.has_task(task_id):\n            return {\"taskId\": task_id, \"error\": self._state.get_task(task_id).expl}\n        else:\n            return {\"taskId\": task_id, \"error\": \"\"}\n\n    def _update_task_history(self, task_id, status, host=None):\n        try:\n            if status == DONE or status == FAILED:\n                successful = (status == DONE)\n                self._task_history.task_finished(task_id, successful)\n            elif status == PENDING:\n                self._task_history.task_scheduled(task_id)\n            elif status == RUNNING:\n                self._task_history.task_started(task_id, host)\n        except:\n            logger.warning(\"Error saving Task history\", exc_info=1)\n\n    @property\n    def task_history(self):\n        # Used by server.py to expose the calls\n        return self._task_history\n",
          "file_after": "# Copyright (c) 2012 Spotify AB\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"); you may not\n# use this file except in compliance with the License. You may obtain a copy of\n# the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n# License for the specific language governing permissions and limitations under\n# the License.\n\nimport collections\nimport datetime\nimport functools\nimport notifications\nimport os\nimport logging\nimport time\nimport cPickle as pickle\nimport task_history as history\nlogger = logging.getLogger(\"luigi.server\")\n\nfrom task_status import PENDING, FAILED, DONE, RUNNING, SUSPENDED, UNKNOWN, DISABLED\n\n\nclass Scheduler(object):\n    ''' Abstract base class\n\n    Note that the methods all take string arguments, not Task objects...\n    '''\n    add_task = NotImplemented\n    get_work = NotImplemented\n    ping = NotImplemented\n\nUPSTREAM_RUNNING = 'UPSTREAM_RUNNING'\nUPSTREAM_MISSING_INPUT = 'UPSTREAM_MISSING_INPUT'\nUPSTREAM_FAILED = 'UPSTREAM_FAILED'\nUPSTREAM_DISABLED = 'UPSTREAM_DISABLED'\n\nUPSTREAM_SEVERITY_ORDER = (\n    '',\n    UPSTREAM_RUNNING,\n    UPSTREAM_MISSING_INPUT,\n    UPSTREAM_FAILED,\n    UPSTREAM_DISABLED,\n)\nUPSTREAM_SEVERITY_KEY = lambda st: UPSTREAM_SEVERITY_ORDER.index(st)\nSTATUS_TO_UPSTREAM_MAP = {\n    FAILED: UPSTREAM_FAILED,\n    RUNNING: UPSTREAM_RUNNING,\n    PENDING: UPSTREAM_MISSING_INPUT,\n    DISABLED: UPSTREAM_DISABLED,\n}\n\n\n# We're passing around this config a lot, so let's put it on an object\nSchedulerConfig = collections.namedtuple('SchedulerConfig', [\n        'retry_delay', 'remove_delay', 'worker_disconnect_delay',\n        'disable_failures', 'disable_window', 'disable_persist', 'disable_time'])\n\n\nclass Failures(object):\n    \"\"\" This class tracks the number of failures in a given time window\n\n    Failures added are marked with the current timestamp, and this class counts\n    the number of failures in a sliding time window ending at the present.\n\n    \"\"\"\n\n    def __init__(self, window):\n        \"\"\" Initialize with the given window\n\n        :param window: how long to track failures for, as a datetime.timedelta\n        \"\"\"\n        self.window = window\n        self.failures = collections.deque()\n\n    def add_failure(self):\n        \"\"\" Add a failure event with the current timestamp \"\"\"\n        self.failures.append(datetime.datetime.now())\n\n    def num_failures(self):\n        \"\"\" Return the number of failures in the window \"\"\"\n        min_time = datetime.datetime.now() - self.window\n        while self.failures and self.failures[0] < min_time:\n            self.failures.popleft()\n        return len(self.failures)\n\n    def clear(self):\n        \"\"\" Clear the failure queue \"\"\"\n        self.failures.clear()\n\n\nclass Task(object):\n    def __init__(self, id, status, deps, resources={}, priority=0, family='', params={},\n                 disable_failures=None, disable_window=None):\n        self.id = id\n        self.stakeholders = set()  # workers ids that are somehow related to this task (i.e. don't prune while any of these workers are still active)\n        self.workers = set()  # workers ids that can perform task - task is 'BROKEN' if none of these workers are active\n        if deps is None:\n            self.deps = set()\n        else:\n            self.deps = set(deps)\n        self.status = status  # PENDING, RUNNING, FAILED or DONE\n        self.time = time.time()  # Timestamp when task was first added\n        self.retry = None\n        self.remove = None\n        self.worker_running = None  # the worker id that is currently running the task or None\n        self.time_running = None  # Timestamp when picked up by worker\n        self.expl = None\n        self.priority = priority\n        self.resources = resources\n        self.family = family\n        self.params = params\n        self.disable_failures = disable_failures\n        self.failures = Failures(disable_window)\n        self.scheduler_disable_time = None\n\n    def __repr__(self):\n        return \"Task(%r)\" % vars(self)\n\n    def add_failure(self):\n        self.failures.add_failure()\n\n    def has_excessive_failures(self):\n        return self.failures.num_failures() >= self.disable_failures\n\n    def can_disable(self):\n        return self.disable_failures is not None\n\n    def re_enable(self):\n        self.scheduler_disable_time = None\n        self.status = FAILED\n        self.failures.clear()\n\n    def set_status(self, new_status, config):\n        # not sure why we have SUSPENDED, as it can never be set\n        if new_status == SUSPENDED:\n            new_status = PENDING\n\n        if new_status == DISABLED and self.status == RUNNING:\n            return\n\n        if self.status == DISABLED:\n            if new_status == DONE:\n                self.re_enable()\n\n            # don't allow workers to override a scheduler disable\n            elif self.scheduler_disable_time is not None:\n                return\n\n        if new_status == FAILED and self.can_disable():\n            self.add_failure()\n            if self.has_excessive_failures():\n                self.scheduler_disable_time = datetime.datetime.now()\n                new_status = DISABLED\n                notifications.send_error_email(\n                    'Luigi Scheduler: DISABLED {task} due to excessive failures'.format(task=self.id),\n                    '{task} failed {failures} times in the last {window} seconds, so it is being '\n                    'disabled for {persist} seconds'.format(\n                        failures=config.disable_failures,\n                        task=self.id,\n                        window=config.disable_window,\n                        persist=config.disable_persist,\n                        ))\n        elif new_status == DISABLED:\n            self.scheduler_disable_time = None\n\n        self.status = new_status\n\n    def prune(self, config):\n        remove = False\n\n        # Mark tasks with no remaining active stakeholders for deletion\n        if not self.stakeholders:\n            if self.remove is None:\n                logger.info(\"Task %r has stakeholders %r but none remain connected -> will remove task in %s seconds\", self.id, self.stakeholders, config.remove_delay)\n                self.remove = time.time() + config.remove_delay\n\n        # If a running worker disconnects, tag all its jobs as FAILED and subject it to the same retry logic\n        if self.status == RUNNING and self.worker_running and self.worker_running not in self.stakeholders:\n            logger.info(\"Task %r is marked as running by disconnected worker %r -> marking as FAILED with retry delay of %rs\", self.id, self.worker_running, config.retry_delay)\n            self.worker_running = None\n            self.set_status(FAILED, config)\n            self.retry = time.time() + config.retry_delay\n\n        # Re-enable task after the disable time expires\n        if self.status == DISABLED and self.scheduler_disable_time:\n            if datetime.datetime.now() - self.scheduler_disable_time > config.disable_time:\n                self.re_enable()\n\n        # Remove tasks that have no stakeholders\n        if self.remove and time.time() > self.remove:\n            logger.info(\"Removing task %r (no connected stakeholders)\", self.id)\n            remove = True\n\n        # Reset FAILED tasks to PENDING if max timeout is reached, and retry delay is >= 0\n        if self.status == FAILED and config.retry_delay >= 0 and self.retry < time.time():\n            self.set_status(PENDING, config)\n\n        return remove\n\n\nclass Worker(object):\n    \"\"\" Structure for tracking worker activity and keeping their references \"\"\"\n    def __init__(self, id, last_active=None):\n        self.id = id\n        self.reference = None  # reference to the worker in the real world. (Currently a dict containing just the host)\n        self.last_active = last_active  # seconds since epoch\n        self.started = time.time()  # seconds since epoch\n        self.info = {}\n\n    def add_info(self, info):\n        self.info.update(info)\n\n    def update(self, worker_reference):\n        if worker_reference:\n            self.reference = worker_reference\n        self.last_active = time.time()\n\n    def prune(self, config):\n        # Delete workers that haven't said anything for a while (probably killed)\n        if self.last_active + config.worker_disconnect_delay < time.time():\n            return True\n\n    def __str__(self):\n        return self.id\n\n\nclass SimpleTaskState(object):\n    ''' Keep track of the current state and handle persistance\n\n    The point of this class is to enable other ways to keep state, eg. by using a database\n    These will be implemented by creating an abstract base class that this and other classes\n    inherit from.\n    '''\n\n    def __init__(self, state_path):\n        self._state_path = state_path\n        self._tasks = {}  # map from id to a Task object\n        self._active_workers = {}  # map from id to a Worker object\n\n    def dump(self):\n        state = (self._tasks, self._active_workers)\n        try:\n            with open(self._state_path, 'w') as fobj:\n                pickle.dump(state, fobj)\n        except IOError:\n            logger.warning(\"Failed saving scheduler state\", exc_info=1)\n        else:\n            logger.info(\"Saved state in %s\", self._state_path)\n\n    # prone to lead to crashes when old state is unpickled with updated code. TODO some kind of version control?\n    def load(self):\n        if os.path.exists(self._state_path):\n            logger.info(\"Attempting to load state from %s\", self._state_path)\n            try:\n                with open(self._state_path) as fobj:\n                    state = pickle.load(fobj)\n            except:\n                logger.exception(\"Error when loading state. Starting from clean slate.\")\n                return\n\n            self._tasks, self._active_workers = state\n\n            # Convert from old format\n            # TODO: this is really ugly, we need something more future-proof\n            # Every time we add an attribute to the Worker class, this code needs to be updated\n            for k, v in self._active_workers.iteritems():\n                if isinstance(v, float):\n                    self._active_workers[k] = Worker(id=k, last_active=v)\n        else:\n            logger.info(\"No prior state file exists at %s. Starting with clean slate\", self._state_path)\n\n    def get_active_tasks(self):\n        for task in self._tasks.itervalues():\n            yield task\n\n    def get_pending_tasks(self):\n        for task in self._tasks.itervalues():\n            if task.status in [PENDING, RUNNING]:\n                yield task\n\n    def get_task(self, task_id, default=None, setdefault=None):\n        if setdefault:\n            return self._tasks.setdefault(task_id, setdefault)\n        else:\n            return self._tasks.get(task_id, default)\n\n    def has_task(self, task_id):\n        return task_id in self._tasks\n\n    def inactivate_tasks(self, delete_tasks):\n        # The terminology is a bit confusing: we used to \"delete\" tasks when they became inactive,\n        # but with a pluggable state storage, you might very well want to keep some history of\n        # older tasks as well. That's why we call it \"inactivate\" (as in the verb)\n        for task in delete_tasks:\n            self._tasks.pop(task)\n\n    def get_active_workers(self, last_active_lt=None):\n        for worker in self._active_workers.itervalues():\n            if last_active_lt is not None and worker.last_active >= last_active_lt:\n                continue\n            yield worker\n\n    def get_worker_ids(self):\n        return self._active_workers.keys() # only used for unit tests\n\n    def get_worker(self, worker_id):\n        return self._active_workers.setdefault(worker_id, Worker(worker_id))\n\n    def inactivate_workers(self, delete_workers):\n        # Mark workers as inactive\n        for worker in delete_workers:\n            self._active_workers.pop(worker)\n\n        # remove workers from tasks\n        for task in self.get_active_tasks():\n            task.stakeholders.difference_update(delete_workers)\n            task.workers.difference_update(delete_workers)\n\n\nclass CentralPlannerScheduler(Scheduler):\n    ''' Async scheduler that can handle multiple workers etc\n\n    Can be run locally or on a server (using RemoteScheduler + server.Server).\n    '''\n\n    def __init__(self, retry_delay=900.0, remove_delay=600.0, worker_disconnect_delay=60.0,\n                 state_path='/var/lib/luigi-server/state.pickle', task_history=None,\n                 resources=None, disable_persist=0, disable_window=0, disable_failures=None):\n        '''\n        (all arguments are in seconds)\n        Keyword Arguments:\n        retry_delay -- How long after a Task fails to try it again, or -1 to never retry\n        remove_delay -- How long after a Task finishes to remove it from the scheduler\n        state_path -- Path to state file (tasks and active workers)\n        worker_disconnect_delay -- If a worker hasn't communicated for this long, remove it from active workers\n        '''\n        self._config = SchedulerConfig(\n            retry_delay=retry_delay,\n            remove_delay=remove_delay,\n            worker_disconnect_delay=worker_disconnect_delay,\n            disable_failures=disable_failures,\n            disable_window=disable_window,\n            disable_persist=disable_persist,\n            disable_time=datetime.timedelta(seconds=disable_persist))\n\n        self._task_history = task_history or history.NopHistory()\n        self._state = SimpleTaskState(state_path)\n\n        self._task_history = task_history or history.NopHistory()\n        self._resources = resources\n        self._make_task = functools.partial(\n            Task, disable_failures=disable_failures,\n            disable_window=datetime.timedelta(seconds=disable_window))\n\n    def load(self):\n        self._state.load()\n\n    def dump(self):\n        self._state.dump()\n\n    def prune(self):\n        logger.info(\"Starting pruning of task graph\")\n        remove_workers = []\n        for worker in self._state.get_active_workers():\n            if worker.prune(self._config):\n                logger.info(\"Worker %s timed out (no contact for >=%ss)\", worker, self._config.worker_disconnect_delay)\n                remove_workers.append(worker.id)\n\n        self._state.inactivate_workers(remove_workers)\n\n        remove_tasks = []\n        for task in self._state.get_active_tasks():\n            if task.prune(self._config):\n                remove_tasks.append(task.id)\n\n        self._state.inactivate_tasks(remove_tasks)\n\n        logger.info(\"Done pruning task graph\")\n\n    def update(self, worker_id, worker_reference=None):\n        \"\"\" Keep track of whenever the worker was last active \"\"\"\n        worker = self._state.get_worker(worker_id)\n        worker.update(worker_reference)\n\n    def _update_priority(self, task, prio, worker):\n        \"\"\" Update priority of the given task\n\n        Priority can only be increased. If the task doesn't exist, a placeholder\n        task is created to preserve priority when the task is later scheduled.\n        \"\"\"\n        task.priority = prio = max(prio, task.priority)\n        for dep in task.deps or []:\n            t = self._state.get_task(dep)\n            if t is not None and prio > t.priority:\n                self._update_priority(t, prio, worker)\n\n    def add_task(self, worker, task_id, status=PENDING, runnable=True,\n                 deps=None, new_deps=None, expl=None, resources=None,\n                 priority=0, family='', params={}):\n        \"\"\"\n        * Add task identified by task_id if it doesn't exist\n        * If deps is not None, update dependency list\n        * Update status of task\n        * Add additional workers/stakeholders\n        * Update priority when needed\n        \"\"\"\n        self.update(worker)\n\n        task = self._state.get_task(task_id, setdefault=self._make_task(\n                id=task_id, status=PENDING, deps=deps, resources=resources,\n                priority=priority, family=family, params=params))\n\n        # for setting priority, we'll sometimes create tasks with unset family and params\n        if not task.family:\n            task.family = family\n        if not task.params:\n            task.params = params\n\n        if task.remove is not None:\n            task.remove = None  # unmark task for removal so it isn't removed after being added\n\n        if not (task.status == RUNNING and status == PENDING):\n            # don't allow re-scheduling of task while it is running, it must either fail or succeed first\n            if status == PENDING or status != task.status:\n                # Update the DB only if there was a acctual change, to prevent noise.\n                # We also check for status == PENDING b/c that's the default value\n                # (so checking for status != task.status woule lie)\n                self._update_task_history(task_id, status)\n            task.set_status(PENDING if status == SUSPENDED else status, self._config)\n            if status == FAILED:\n                task.retry = time.time() + self._config.retry_delay\n\n        if deps is not None:\n            task.deps = set(deps)\n\n        if new_deps is not None:\n            task.deps.update(new_deps)\n\n        task.stakeholders.add(worker)\n        task.resources = resources\n\n        # Task dependencies might not exist yet. Let's create dummy tasks for them for now.\n        # Otherwise the task dependencies might end up being pruned if scheduling takes a long time\n        for dep in task.deps or []:\n            t = self._state.get_task(dep, setdefault=self._make_task(id=dep, status=UNKNOWN, deps=None, priority=priority))\n            t.stakeholders.add(worker)\n\n        self._update_priority(task, priority, worker)\n\n        if runnable:\n            task.workers.add(worker)\n\n        if expl is not None:\n            task.expl = expl\n\n    def add_worker(self, worker, info):\n        self._state.get_worker(worker).add_info(info)\n\n    def update_resources(self, **resources):\n        if self._resources is None:\n            self._resources = {}\n        self._resources.update(resources)\n\n    def _has_resources(self, needed_resources, used_resources):\n        if needed_resources is None:\n            return True\n\n        available_resources = self._resources or {}\n        for resource, amount in needed_resources.items():\n            if amount + used_resources[resource] > available_resources.get(resource, 1):\n                return False\n        return True\n\n    def _used_resources(self):\n        used_resources = collections.defaultdict(int)\n        if self._resources is not None:\n            for task in self._state.get_active_tasks():\n                if task.status == RUNNING and task.resources:\n                    for resource, amount in task.resources.items():\n                        used_resources[resource] += amount\n        return used_resources\n\n    def _rank(self):\n        ''' Return worker's rank function for task scheduling '''\n        dependents = collections.defaultdict(int)\n        def not_done(t):\n            task = self._state.get_task(t, default=None)\n            return task is None or task.status != DONE\n        for task in self._state.get_active_tasks():\n            if task.status != DONE:\n                deps = filter(not_done, task.deps)\n                inverse_num_deps = 1.0 / max(len(deps), 1)\n                for dep in deps:\n                    dependents[dep] += inverse_num_deps\n\n        return lambda task: (task.priority, dependents[task.id], -task.time)\n\n    def _schedulable(self, task):\n        if task.status != PENDING:\n            return False\n        for dep in task.deps:\n            dep_task = self._state.get_task(dep, default=None)\n            if dep_task is None or dep_task.status != DONE:\n                return False\n        return True\n\n    def get_work(self, worker, host=None):\n        # TODO: remove any expired nodes\n\n        # Algo: iterate over all nodes, find the highest priority node no dependencies and available\n        # resources.\n\n        # Resource checking looks both at currently available resources and at which resources would\n        # be available if all running tasks died and we rescheduled all workers greedily. We do both\n        # checks in order to prevent a worker with many low-priority tasks from starving other\n        # workers with higher priority tasks that share the same resources.\n\n        # TODO: remove tasks that can't be done, figure out if the worker has absolutely\n        # nothing it can wait for\n\n        # Return remaining tasks that have no FAILED descendents\n        self.update(worker, {'host': host})\n        best_task = None\n        best_task_id = None\n        locally_pending_tasks = 0\n        running_tasks = []\n\n        used_resources = self._used_resources()\n        greedy_resources = collections.defaultdict(int)\n        n_unique_pending = 0\n        greedy_workers = dict((worker.id, worker.info.get('workers', 1))\n                              for worker in self._state.get_active_workers())\n\n        tasks = list(self._state.get_pending_tasks())\n        tasks.sort(key=self._rank(), reverse=True)\n\n        for task in tasks:\n            if task.status == 'RUNNING' and worker in task.workers:\n                # Return a list of currently running tasks to the client,\n                # makes it easier to troubleshoot\n                other_worker = self._state.get_worker(task.worker_running)\n                more_info = {'task_id': task.id, 'worker': str(other_worker)}\n                if other_worker is not None:\n                    more_info.update(other_worker.info)\n                    running_tasks.append(more_info)\n\n            if task.status == PENDING and worker in task.workers:\n                locally_pending_tasks += 1\n                if len(task.workers) == 1:\n                    n_unique_pending += 1\n\n            if task.status == RUNNING and task.worker_running in greedy_workers:\n                greedy_workers[task.worker_running] -= 1\n                for resource, amount in (task.resources or {}).items():\n                    greedy_resources[resource] += amount\n\n            if not best_task and self._schedulable(task) and self._has_resources(task.resources, greedy_resources):\n                if worker in task.workers and self._has_resources(task.resources, used_resources):\n                    best_task = task\n                    best_task_id = task.id\n                else:\n                    for task_worker in task.workers:\n                        if greedy_workers.get(task_worker, 0) > 0:\n                            # use up a worker\n                            greedy_workers[task_worker] -= 1\n\n                            # keep track of the resources used in greedy scheduling\n                            for resource, amount in (task.resources or {}).items():\n                                greedy_resources[resource] += amount\n\n                            break\n\n        if best_task:\n            best_task.status = RUNNING\n            best_task.worker_running = worker\n            best_task.time_running = time.time()\n            self._update_task_history(best_task.id, RUNNING, host=host)\n\n        return {'n_pending_tasks': locally_pending_tasks,\n                'n_unique_pending': n_unique_pending,\n                'task_id': best_task_id,\n                'running_tasks': running_tasks}\n\n    def ping(self, worker):\n        self.update(worker)\n\n    def _upstream_status(self, task_id, upstream_status_table):\n        if task_id in upstream_status_table:\n            return upstream_status_table[task_id]\n        elif self._state.has_task(task_id):\n            task_stack = [task_id]\n\n            while task_stack:\n                dep_id = task_stack.pop()\n                if self._state.has_task(dep_id):\n                    dep = self._state.get_task(dep_id)\n                    if dep_id not in upstream_status_table:\n                        if dep.status == PENDING and dep.deps:\n                            task_stack = task_stack + [dep_id] + list(dep.deps)\n                            upstream_status_table[dep_id] = ''  # will be updated postorder\n                        else:\n                            dep_status = STATUS_TO_UPSTREAM_MAP.get(dep.status, '')\n                            upstream_status_table[dep_id] = dep_status\n                    elif upstream_status_table[dep_id] == '' and dep.deps:\n                        # This is the postorder update step when we set the\n                        # status based on the previously calculated child elements\n                        upstream_status = [upstream_status_table.get(id, '') for id in dep.deps]\n                        upstream_status.append('')  # to handle empty list\n                        status = max(upstream_status, key=UPSTREAM_SEVERITY_KEY)\n                        upstream_status_table[dep_id] = status\n            return upstream_status_table[dep_id]\n\n    def _serialize_task(self, task_id, include_deps=True):\n        task = self._state.get_task(task_id)\n        ret = {\n            'deps': list(task.deps),\n            'status': task.status,\n            'workers': list(task.workers),\n            'worker_running': task.worker_running,\n            'time_running': getattr(task, \"time_running\", None),\n            'start_time': task.time,\n            'params': task.params,\n            'name': task.family,\n            'priority': task.priority,\n            'resources': task.resources,\n        }\n        if include_deps:\n            ret['deps'] = list(task.deps)\n        return ret\n\n    def graph(self):\n        self.prune()\n        serialized = {}\n        for task in self._state.get_active_tasks():\n            serialized[task.id] = self._serialize_task(task.id)\n        return serialized\n\n    def _recurse_deps(self, task_id, serialized):\n        if task_id not in serialized:\n            task = self._state.get_task(task_id)\n            if task is None or not task.family:\n                logger.warn('Missing task for id [%s]', task_id)\n\n                # try to infer family and params from task_id\n                try:\n                    family, _, param_str = task_id.rstrip(')').partition('(')\n                    params = dict(param.split('=') for param in param_str.split(', '))\n                except:\n                    family, params = '', {}\n                serialized[task_id] = {\n                    'deps': [],\n                    'status': UNKNOWN,\n                    'workers': [],\n                    'start_time': UNKNOWN,\n                    'params': params,\n                    'name': family,\n                    'priority': 0,\n                }\n            else:\n                serialized[task_id] = self._serialize_task(task_id)\n                for dep in task.deps:\n                    self._recurse_deps(dep, serialized)\n\n    def dep_graph(self, task_id):\n        self.prune()\n        serialized = {}\n        if self._state.has_task(task_id):\n            self._recurse_deps(task_id, serialized)\n        return serialized\n\n    def task_list(self, status, upstream_status):\n        ''' query for a subset of tasks by status '''\n        self.prune()\n        result = {}\n        upstream_status_table = {}  # used to memoize upstream status\n        for task in self._state.get_active_tasks():\n            if not status or task.status == status:\n                if (task.status != PENDING or not upstream_status or\n                    upstream_status == self._upstream_status(task.id, upstream_status_table)):\n                    serialized = self._serialize_task(task.id, False)\n                    result[task.id] = serialized\n        return result\n\n    def worker_list(self, include_running=True):\n        self.prune()\n        workers = [\n            dict(\n                name=worker.id,\n                last_active=worker.last_active,\n                started=getattr(worker, 'started', None),\n                **worker.info\n            ) for worker in self._state.get_active_workers()]\n        workers.sort(key=lambda worker: worker['started'], reverse=True)\n        if include_running:\n            running = collections.defaultdict(dict)\n            num_pending = collections.defaultdict(int)\n            num_uniques = collections.defaultdict(int)\n            for task in self._state.get_pending_tasks():\n                if task.status == RUNNING and task.worker_running:\n                    running[task.worker_running][task.id] = self._serialize_task(task.id, False)\n                elif task.status == PENDING:\n                    for worker in task.workers:\n                        num_pending[worker] += 1\n                    if len(task.workers) == 1:\n                        num_uniques[list(task.workers)[0]] += 1\n            for worker in workers:\n                tasks = running[worker['name']]\n                worker['num_running'] = len(tasks)\n                worker['num_pending'] = num_pending[worker['name']]\n                worker['num_uniques'] = num_uniques[worker['name']]\n                worker['running'] = tasks\n        return workers\n\n    def inverse_dependencies(self, task_id):\n        self.prune()\n        serialized = {}\n        if self._state.has_task(task_id):\n            self._traverse_inverse_deps(task_id, serialized)\n        return serialized\n\n    def _traverse_inverse_deps(self, task_id, serialized):\n        stack = [task_id]\n        serialized[task_id] = self._serialize_task(task_id)\n        while len(stack) > 0:\n            curr_id = stack.pop()\n            for task in self._state.get_active_tasks():\n                if curr_id in task.deps:\n                    serialized[curr_id][\"deps\"].append(task.id)\n                    if task.id not in serialized:\n                        serialized[task.id] = self._serialize_task(task.id)\n                        serialized[task.id][\"deps\"] = []\n                        stack.append(task.id)\n\n    def task_search(self, task_str):\n        ''' query for a subset of tasks by task_id '''\n        self.prune()\n        result = collections.defaultdict(dict)\n        for task in self._state.get_active_tasks():\n            if task.id.find(task_str) != -1:\n                serialized = self._serialize_task(task.id, False)\n                result[task.status][task.id] = serialized\n        return result\n\n    def re_enable_task(self, task_id):\n        serialized = {}\n        task = self._state.get_task(task_id)\n        if task and task.status == DISABLED and task.scheduler_disable_time:\n            task.re_enable()\n            serialized = self._serialize_task(task_id)\n        return serialized\n\n    def fetch_error(self, task_id):\n        if self._state.has_task(task_id):\n            return {\"taskId\": task_id, \"error\": self._state.get_task(task_id).expl}\n        else:\n            return {\"taskId\": task_id, \"error\": \"\"}\n\n    def _update_task_history(self, task_id, status, host=None):\n        try:\n            if status == DONE or status == FAILED:\n                successful = (status == DONE)\n                self._task_history.task_finished(task_id, successful)\n            elif status == PENDING:\n                self._task_history.task_scheduled(task_id)\n            elif status == RUNNING:\n                self._task_history.task_started(task_id, host)\n        except:\n            logger.warning(\"Error saving Task history\", exc_info=1)\n\n    @property\n    def task_history(self):\n        # Used by server.py to expose the calls\n        return self._task_history\n",
          "file_patch": "@@ -59,8 +59,7 @@ STATUS_TO_UPSTREAM_MAP = {\n # We're passing around this config a lot, so let's put it on an object\n SchedulerConfig = collections.namedtuple('SchedulerConfig', [\n         'retry_delay', 'remove_delay', 'worker_disconnect_delay',\n-        'disable_failures', 'disable_window', 'disable_persist', 'disable_time'],\n-                                         verbose=True)\n+        'disable_failures', 'disable_window', 'disable_persist', 'disable_time'])\n \n \n class Failures(object):\n@@ -137,7 +136,7 @@ class Task(object):\n         self.status = FAILED\n         self.failures.clear()\n \n-    def set_status(self, new_status, disable_failures=None, disable_window=None, disable_persist=None):\n+    def set_status(self, new_status, config):\n         # not sure why we have SUSPENDED, as it can never be set\n         if new_status == SUSPENDED:\n             new_status = PENDING\n@@ -162,36 +161,35 @@ class Task(object):\n                     'Luigi Scheduler: DISABLED {task} due to excessive failures'.format(task=self.id),\n                     '{task} failed {failures} times in the last {window} seconds, so it is being '\n                     'disabled for {persist} seconds'.format(\n-                        failures=disable_failures,\n+                        failures=config.disable_failures,\n                         task=self.id,\n-                        window=disable_window,\n-                        persist=disable_persist,\n+                        window=config.disable_window,\n+                        persist=config.disable_persist,\n                         ))\n         elif new_status == DISABLED:\n             self.scheduler_disable_time = None\n \n         self.status = new_status\n \n-    def prune(self, remove_delay, retry_delay, disable_time,\n-              disable_failures, disable_window, disable_persist):\n+    def prune(self, config):\n         remove = False\n \n         # Mark tasks with no remaining active stakeholders for deletion\n         if not self.stakeholders:\n             if self.remove is None:\n-                logger.info(\"Task %r has stakeholders %r but none remain connected -> will remove task in %s seconds\", self.id, self.stakeholders, remove_delay)\n-                self.remove = time.time() + remove_delay\n+                logger.info(\"Task %r has stakeholders %r but none remain connected -> will remove task in %s seconds\", self.id, self.stakeholders, config.remove_delay)\n+                self.remove = time.time() + config.remove_delay\n \n         # If a running worker disconnects, tag all its jobs as FAILED and subject it to the same retry logic\n         if self.status == RUNNING and self.worker_running and self.worker_running not in self.stakeholders:\n-            logger.info(\"Task %r is marked as running by disconnected worker %r -> marking as FAILED with retry delay of %rs\", self.id, self.worker_running, retry_delay)\n+            logger.info(\"Task %r is marked as running by disconnected worker %r -> marking as FAILED with retry delay of %rs\", self.id, self.worker_running, config.retry_delay)\n             self.worker_running = None\n-            self.set_status(FAILED, disable_failures, disable_window, disable_persist)\n-            self.retry = time.time() + retry_delay\n+            self.set_status(FAILED, config)\n+            self.retry = time.time() + config.retry_delay\n \n         # Re-enable task after the disable time expires\n         if self.status == DISABLED and self.scheduler_disable_time:\n-            if datetime.datetime.now() - self.scheduler_disable_time > disable_time:\n+            if datetime.datetime.now() - self.scheduler_disable_time > config.disable_time:\n                 self.re_enable()\n \n         # Remove tasks that have no stakeholders\n@@ -200,8 +198,8 @@ class Task(object):\n             remove = True\n \n         # Reset FAILED tasks to PENDING if max timeout is reached, and retry delay is >= 0\n-        if self.status == FAILED and retry_delay >= 0 and self.retry < time.time():\n-            self.set_status(PENDING)\n+        if self.status == FAILED and config.retry_delay >= 0 and self.retry < time.time():\n+            self.set_status(PENDING, config)\n \n         return remove\n \n@@ -223,9 +221,9 @@ class Worker(object):\n             self.reference = worker_reference\n         self.last_active = time.time()\n \n-    def prune(self, worker_disconnect_delay):\n+    def prune(self, config):\n         # Delete workers that haven't said anything for a while (probably killed)\n-        if self.last_active + worker_disconnect_delay < time.time():\n+        if self.last_active + config.worker_disconnect_delay < time.time():\n             return True\n \n     def __str__(self):\n@@ -370,7 +368,7 @@ class CentralPlannerScheduler(Scheduler):\n         logger.info(\"Starting pruning of task graph\")\n         remove_workers = []\n         for worker in self._state.get_active_workers():\n-            if worker.prune(self._config.worker_disconnect_delay):\n+            if worker.prune(self._config):\n                 logger.info(\"Worker %s timed out (no contact for >=%ss)\", worker, self._config.worker_disconnect_delay)\n                 remove_workers.append(worker.id)\n \n@@ -378,8 +376,7 @@ class CentralPlannerScheduler(Scheduler):\n \n         remove_tasks = []\n         for task in self._state.get_active_tasks():\n-            if task.prune(self._config.remove_delay, self._config.retry_delay, self._config.disable_time,\n-                          self._config.disable_failures, self._config.disable_window, self._config.disable_persist):\n+            if task.prune(self._config):\n                 remove_tasks.append(task.id)\n \n         self._state.inactivate_tasks(remove_tasks)\n@@ -435,8 +432,7 @@ class CentralPlannerScheduler(Scheduler):\n                 # We also check for status == PENDING b/c that's the default value\n                 # (so checking for status != task.status woule lie)\n                 self._update_task_history(task_id, status)\n-            task.set_status(PENDING if status == SUSPENDED else status,\n-                            self._config.disable_failures, self._config.disable_window, self._config.disable_persist)\n+            task.set_status(PENDING if status == SUSPENDED else status, self._config)\n             if status == FAILED:\n                 task.retry = time.time() + self._config.retry_delay\n \n",
          "files_name_in_blame_commit": [
            "scheduler.py"
          ]
        }
      },
      "3819379036c24388bcc0be602285de85d318f5c5": {
        "commit": {
          "commit_id": "3819379036c24388bcc0be602285de85d318f5c5",
          "commit_message": "moved set_status to the Task class",
          "commit_author": "Erik Bernhardsson",
          "commit_date": "2014-12-10 18:52:07",
          "commit_parent": "f4f9be62c70234498a68273260558e736bb9e808"
        },
        "function": {
          "function_name": "set_status",
          "function_code_before": "def set_status(self, task, new_status):\n    if new_status == SUSPENDED:\n        new_status = PENDING\n    if new_status == DISABLED and task.status == RUNNING:\n        return\n    if task.status == DISABLED:\n        if new_status == DONE:\n            task.re_enable()\n        elif task.scheduler_disable_time is not None:\n            return\n    if new_status == FAILED and task.can_disable():\n        task.add_failure()\n        if task.has_excessive_failures():\n            task.scheduler_disable_time = datetime.datetime.now()\n            new_status = DISABLED\n            notifications.send_error_email('Luigi Scheduler: DISABLED {task} due to excessive failures'.format(task=task.id), '{task} failed {failures} times in the last {window} seconds, so it is being disabled for {persist} seconds'.format(failures=self._disable_failures, task=task.id, window=self._disable_window, persist=self._disable_persist))\n    elif new_status == DISABLED:\n        task.scheduler_disable_time = None\n    task.status = new_status",
          "function_code_after": "def set_status(self, new_status, disable_failures=None, disable_window=None, disable_persist=None):\n    if new_status == SUSPENDED:\n        new_status = PENDING\n    if new_status == DISABLED and self.status == RUNNING:\n        return\n    if self.status == DISABLED:\n        if new_status == DONE:\n            self.re_enable()\n        elif self.scheduler_disable_time is not None:\n            return\n    if new_status == FAILED and self.can_disable():\n        self.add_failure()\n        if self.has_excessive_failures():\n            self.scheduler_disable_time = datetime.datetime.now()\n            new_status = DISABLED\n            notifications.send_error_email('Luigi Scheduler: DISABLED {task} due to excessive failures'.format(task=self.id), '{task} failed {failures} times in the last {window} seconds, so it is being disabled for {persist} seconds'.format(failures=disable_failures, task=self.id, window=disable_window, persist=disable_persist))\n    elif new_status == DISABLED:\n        self.scheduler_disable_time = None\n    self.status = new_status",
          "function_before_start_line": 327,
          "function_before_end_line": 360,
          "function_after_start_line": 133,
          "function_after_end_line": 166,
          "function_before_token_count": 153,
          "function_after_token_count": 157,
          "functions_name_modified_file": [
            "add_failure",
            "get_worker_ids",
            "prune",
            "_schedulable",
            "clear",
            "has_excessive_failures",
            "_upstream_status",
            "_update_task_history",
            "can_disable",
            "ping",
            "get_pending_tasks",
            "_traverse_inverse_deps",
            "load",
            "inverse_dependencies",
            "num_failures",
            "task_search",
            "_recurse_deps",
            "get_worker",
            "get_active_workers",
            "worker_list",
            "add_task",
            "update_resources",
            "_update_priority",
            "task_list",
            "add_info",
            "get_active_tasks",
            "_has_resources",
            "_used_resources",
            "has_task",
            "_serialize_task",
            "task_history",
            "dep_graph",
            "__str__",
            "graph",
            "re_enable",
            "dump",
            "__repr__",
            "inactivate_tasks",
            "__init__",
            "update",
            "set_status",
            "add_worker",
            "get_task",
            "_rank",
            "get_work",
            "re_enable_task",
            "fetch_error",
            "inactivate_workers"
          ],
          "functions_name_all_files": [
            "add_failure",
            "get_worker_ids",
            "prune",
            "_schedulable",
            "clear",
            "has_excessive_failures",
            "_upstream_status",
            "_update_task_history",
            "can_disable",
            "ping",
            "get_pending_tasks",
            "_traverse_inverse_deps",
            "load",
            "inverse_dependencies",
            "num_failures",
            "task_search",
            "_recurse_deps",
            "get_worker",
            "get_active_workers",
            "worker_list",
            "add_task",
            "update_resources",
            "_update_priority",
            "task_list",
            "add_info",
            "get_active_tasks",
            "_has_resources",
            "_used_resources",
            "has_task",
            "_serialize_task",
            "task_history",
            "dep_graph",
            "__str__",
            "graph",
            "re_enable",
            "dump",
            "__repr__",
            "inactivate_tasks",
            "__init__",
            "update",
            "set_status",
            "add_worker",
            "get_task",
            "_rank",
            "get_work",
            "re_enable_task",
            "fetch_error",
            "inactivate_workers"
          ],
          "functions_name_co_evolved_modified_file": [
            "add_task",
            "prune"
          ],
          "functions_name_co_evolved_all_files": [
            "add_task",
            "prune"
          ]
        },
        "file": {
          "file_name": "scheduler.py",
          "file_nloc": 556,
          "file_complexity": 211,
          "file_token_count": 4162,
          "file_before": "# Copyright (c) 2012 Spotify AB\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"); you may not\n# use this file except in compliance with the License. You may obtain a copy of\n# the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n# License for the specific language governing permissions and limitations under\n# the License.\n\nimport collections\nimport datetime\nimport functools\nimport notifications\nimport os\nimport logging\nimport time\nimport cPickle as pickle\nimport task_history as history\nlogger = logging.getLogger(\"luigi.server\")\n\nfrom task_status import PENDING, FAILED, DONE, RUNNING, SUSPENDED, UNKNOWN, DISABLED\n\n\nclass Scheduler(object):\n    ''' Abstract base class\n\n    Note that the methods all take string arguments, not Task objects...\n    '''\n    add_task = NotImplemented\n    get_work = NotImplemented\n    ping = NotImplemented\n\nUPSTREAM_RUNNING = 'UPSTREAM_RUNNING'\nUPSTREAM_MISSING_INPUT = 'UPSTREAM_MISSING_INPUT'\nUPSTREAM_FAILED = 'UPSTREAM_FAILED'\nUPSTREAM_DISABLED = 'UPSTREAM_DISABLED'\n\nUPSTREAM_SEVERITY_ORDER = (\n    '',\n    UPSTREAM_RUNNING,\n    UPSTREAM_MISSING_INPUT,\n    UPSTREAM_FAILED,\n    UPSTREAM_DISABLED,\n)\nUPSTREAM_SEVERITY_KEY = lambda st: UPSTREAM_SEVERITY_ORDER.index(st)\nSTATUS_TO_UPSTREAM_MAP = {\n    FAILED: UPSTREAM_FAILED,\n    RUNNING: UPSTREAM_RUNNING,\n    PENDING: UPSTREAM_MISSING_INPUT,\n    DISABLED: UPSTREAM_DISABLED,\n}\n\n\nclass Failures(object):\n    \"\"\" This class tracks the number of failures in a given time window\n\n    Failures added are marked with the current timestamp, and this class counts\n    the number of failures in a sliding time window ending at the present.\n\n    \"\"\"\n\n    def __init__(self, window):\n        \"\"\" Initialize with the given window\n\n        :param window: how long to track failures for, as a datetime.timedelta\n        \"\"\"\n        self.window = window\n        self.failures = collections.deque()\n\n    def add_failure(self):\n        \"\"\" Add a failure event with the current timestamp \"\"\"\n        self.failures.append(datetime.datetime.now())\n\n    def num_failures(self):\n        \"\"\" Return the number of failures in the window \"\"\"\n        min_time = datetime.datetime.now() - self.window\n        while self.failures and self.failures[0] < min_time:\n            self.failures.popleft()\n        return len(self.failures)\n\n    def clear(self):\n        \"\"\" Clear the failure queue \"\"\"\n        self.failures.clear()\n\n\nclass Task(object):\n    def __init__(self, id, status, deps, resources={}, priority=0, family='', params={},\n                 disable_failures=None, disable_window=None):\n        self.id = id\n        self.stakeholders = set()  # workers ids that are somehow related to this task (i.e. don't prune while any of these workers are still active)\n        self.workers = set()  # workers ids that can perform task - task is 'BROKEN' if none of these workers are active\n        if deps is None:\n            self.deps = set()\n        else:\n            self.deps = set(deps)\n        self.status = status  # PENDING, RUNNING, FAILED or DONE\n        self.time = time.time()  # Timestamp when task was first added\n        self.retry = None\n        self.remove = None\n        self.worker_running = None  # the worker id that is currently running the task or None\n        self.time_running = None  # Timestamp when picked up by worker\n        self.expl = None\n        self.priority = priority\n        self.resources = resources\n        self.family = family\n        self.params = params\n        self.disable_failures = disable_failures\n        self.failures = Failures(disable_window)\n        self.scheduler_disable_time = None\n\n    def __repr__(self):\n        return \"Task(%r)\" % vars(self)\n\n    def add_failure(self):\n        self.failures.add_failure()\n\n    def has_excessive_failures(self):\n        return self.failures.num_failures() >= self.disable_failures\n\n    def can_disable(self):\n        return self.disable_failures is not None\n\n    def re_enable(self):\n        self.scheduler_disable_time = None\n        self.status = FAILED\n        self.failures.clear()\n\n\nclass Worker(object):\n    \"\"\" Structure for tracking worker activity and keeping their references \"\"\"\n    def __init__(self, id, last_active=None):\n        self.id = id\n        self.reference = None  # reference to the worker in the real world. (Currently a dict containing just the host)\n        self.last_active = last_active  # seconds since epoch\n        self.started = time.time()  # seconds since epoch\n        self.info = {}\n\n    def add_info(self, info):\n        self.info.update(info)\n\n    def __str__(self):\n        return self.id\n\n\nclass SimpleTaskState(object):\n    ''' Keep track of the current state and handle persistance\n\n    The point of this class is to enable other ways to keep state, eg. by using a database\n    These will be implemented by creating an abstract base class that this and other classes\n    inherit from.\n    '''\n\n    def __init__(self, state_path):\n        self._state_path = state_path\n        self._tasks = {}  # map from id to a Task object\n        self._active_workers = {}  # map from id to a Worker object\n\n    def dump(self):\n        state = (self._tasks, self._active_workers)\n        try:\n            with open(self._state_path, 'w') as fobj:\n                pickle.dump(state, fobj)\n        except IOError:\n            logger.warning(\"Failed saving scheduler state\", exc_info=1)\n        else:\n            logger.info(\"Saved state in %s\", self._state_path)\n\n    # prone to lead to crashes when old state is unpickled with updated code. TODO some kind of version control?\n    def load(self):\n        if os.path.exists(self._state_path):\n            logger.info(\"Attempting to load state from %s\", self._state_path)\n            try:\n                with open(self._state_path) as fobj:\n                    state = pickle.load(fobj)\n            except:\n                logger.exception(\"Error when loading state. Starting from clean slate.\")\n                return\n\n            self._tasks, self._active_workers = state\n\n            # Convert from old format\n            # TODO: this is really ugly, we need something more future-proof\n            # Every time we add an attribute to the Worker class, this code needs to be updated\n            for k, v in self._active_workers.iteritems():\n                if isinstance(v, float):\n                    self._active_workers[k] = Worker(id=k, last_active=v)\n        else:\n            logger.info(\"No prior state file exists at %s. Starting with clean slate\", self._state_path)\n\n    def get_active_tasks(self):\n        for task in self._tasks.itervalues():\n            yield task\n\n    def get_pending_tasks(self):\n        for task in self._tasks.itervalues():\n            if task.status in [PENDING, RUNNING]:\n                yield task\n\n    def get_task(self, task_id, default=None, setdefault=None):\n        if setdefault:\n            return self._tasks.setdefault(task_id, setdefault)\n        else:\n            return self._tasks.get(task_id, default)\n\n    def has_task(self, task_id):\n        return task_id in self._tasks\n\n    def inactivate_tasks(self, delete_tasks):\n        # The terminology is a bit confusing: we used to \"delete\" tasks when they became inactive,\n        # but with a pluggable state storage, you might very well want to keep some history of\n        # older tasks as well. That's why we call it \"inactivate\" (as in the verb)\n        for task in delete_tasks:\n            self._tasks.pop(task)\n\n    def get_active_workers(self, last_active_lt=None):\n        for worker in self._active_workers.itervalues():\n            if last_active_lt is not None and worker.last_active >= last_active_lt:\n                continue\n            yield worker\n\n    def get_worker_ids(self):\n        return self._active_workers.keys() # only used for unit tests\n\n    def get_worker(self, worker_id):\n        return self._active_workers.setdefault(worker_id, Worker(worker_id))\n\n    def inactivate_workers(self, delete_workers):\n        # Mark workers as inactive\n        for worker in delete_workers:\n            self._active_workers.pop(worker)\n\n        # remove workers from tasks\n        for task in self.get_active_tasks():\n            task.stakeholders.difference_update(delete_workers)\n            task.workers.difference_update(delete_workers)\n\n\nclass CentralPlannerScheduler(Scheduler):\n    ''' Async scheduler that can handle multiple workers etc\n\n    Can be run locally or on a server (using RemoteScheduler + server.Server).\n    '''\n\n    def __init__(self, retry_delay=900.0, remove_delay=600.0, worker_disconnect_delay=60.0,\n                 state_path='/var/lib/luigi-server/state.pickle', task_history=None,\n                 resources=None, disable_persist=0, disable_window=0, disable_failures=None):\n        '''\n        (all arguments are in seconds)\n        Keyword Arguments:\n        retry_delay -- How long after a Task fails to try it again, or -1 to never retry\n        remove_delay -- How long after a Task finishes to remove it from the scheduler\n        state_path -- Path to state file (tasks and active workers)\n        worker_disconnect_delay -- If a worker hasn't communicated for this long, remove it from active workers\n        '''\n        self._retry_delay = retry_delay\n        self._remove_delay = remove_delay\n        self._worker_disconnect_delay = worker_disconnect_delay\n        self._task_history = task_history or history.NopHistory()\n        self._state = SimpleTaskState(state_path)\n\n        self._task_history = task_history or history.NopHistory()\n        self._resources = resources\n        self._disable_failures = disable_failures\n        self._disable_window = disable_window\n        self._make_task = functools.partial(\n            Task, disable_failures=disable_failures,\n            disable_window=datetime.timedelta(seconds=disable_window))\n        self._disable_persist = disable_persist\n        self._disable_time = datetime.timedelta(seconds=disable_persist)\n\n    def load(self):\n        self._state.load()\n\n    def dump(self):\n        self._state.dump()\n\n    def prune(self):\n        logger.info(\"Starting pruning of task graph\")\n        # Delete workers that haven't said anything for a while (probably killed)\n        delete_workers = []\n        for worker in self._state.get_active_workers(last_active_lt=time.time() - self._worker_disconnect_delay):\n            logger.info(\"Worker %s timed out (no contact for >=%ss)\", worker, self._worker_disconnect_delay)\n            delete_workers.append(worker.id)\n\n        self._state.inactivate_workers(delete_workers)\n\n        delete_workers = set(delete_workers)\n\n        remove_tasks = []\n        for task in self._state.get_active_tasks():\n            # Mark tasks with no remaining active stakeholders for deletion\n            if not task.stakeholders:\n                if task.remove is None:\n                    logger.info(\"Task %r has stakeholders %r but none remain connected -> will remove task in %s seconds\", task.id, task.stakeholders, self._remove_delay)\n                    task.remove = time.time() + self._remove_delay\n\n            # If a running worker disconnects, tag all its jobs as FAILED and subject it to the same retry logic\n            if task.status == RUNNING and task.worker_running and task.worker_running not in task.stakeholders:\n                logger.info(\"Task %r is marked as running by disconnected worker %r -> marking as FAILED with retry delay of %rs\", task.id, task.worker_running, self._retry_delay)\n                task.worker_running = None\n                self.set_status(task, FAILED)\n                task.retry = time.time() + self._retry_delay\n\n            if task.status == DISABLED and task.scheduler_disable_time:\n                # re-enable task after the disable time expires\n                if datetime.datetime.now() - task.scheduler_disable_time > self._disable_time:\n                    task.re_enable()\n\n            # Remove tasks that have no stakeholders\n            if task.remove and time.time() > task.remove:\n                logger.info(\"Removing task %r (no connected stakeholders)\", task.id)\n                remove_tasks.append(task.id)\n\n            # Reset FAILED tasks to PENDING if max timeout is reached, and retry delay is >= 0\n            if task.status == FAILED and self._retry_delay >= 0 and task.retry < time.time():\n                self.set_status(task, PENDING)\n\n        self._state.inactivate_tasks(remove_tasks)\n\n        logger.info(\"Done pruning task graph\")\n\n    def set_status(self, task, new_status):\n        # not sure why we have SUSPENDED, as it can never be set\n        if new_status == SUSPENDED:\n            new_status = PENDING\n\n        if new_status == DISABLED and task.status == RUNNING:\n            return\n\n        if task.status == DISABLED:\n            if new_status == DONE:\n                task.re_enable()\n\n            # don't allow workers to override a scheduler disable\n            elif task.scheduler_disable_time is not None:\n                return\n\n        if new_status == FAILED and task.can_disable():\n            task.add_failure()\n            if task.has_excessive_failures():\n                task.scheduler_disable_time = datetime.datetime.now()\n                new_status = DISABLED\n                notifications.send_error_email(\n                    'Luigi Scheduler: DISABLED {task} due to excessive failures'.format(task=task.id),\n                    '{task} failed {failures} times in the last {window} seconds, so it is being '\n                    'disabled for {persist} seconds'.format(\n                        failures=self._disable_failures,\n                        task=task.id,\n                        window=self._disable_window,\n                        persist=self._disable_persist,\n                        ))\n        elif new_status == DISABLED:\n            task.scheduler_disable_time = None\n\n        task.status = new_status\n\n    def update(self, worker_id, worker_reference=None):\n        \"\"\" Keep track of whenever the worker was last active \"\"\"\n        worker = self._state.get_worker(worker_id)\n        if worker_reference:\n            worker.reference = worker_reference\n        worker.last_active = time.time()\n\n    def _update_priority(self, task, prio, worker):\n        \"\"\" Update priority of the given task\n\n        Priority can only be increased. If the task doesn't exist, a placeholder\n        task is created to preserve priority when the task is later scheduled.\n        \"\"\"\n        task.priority = prio = max(prio, task.priority)\n        for dep in task.deps or []:\n            t = self._state.get_task(dep)\n            if t is not None and prio > t.priority:\n                self._update_priority(t, prio, worker)\n\n    def add_task(self, worker, task_id, status=PENDING, runnable=True,\n                 deps=None, new_deps=None, expl=None, resources=None,\n                 priority=0, family='', params={}):\n        \"\"\"\n        * Add task identified by task_id if it doesn't exist\n        * If deps is not None, update dependency list\n        * Update status of task\n        * Add additional workers/stakeholders\n        * Update priority when needed\n        \"\"\"\n        self.update(worker)\n\n        task = self._state.get_task(task_id, setdefault=self._make_task(\n                id=task_id, status=PENDING, deps=deps, resources=resources,\n                priority=priority, family=family, params=params))\n\n        # for setting priority, we'll sometimes create tasks with unset family and params\n        if not task.family:\n            task.family = family\n        if not task.params:\n            task.params = params\n\n        if task.remove is not None:\n            task.remove = None  # unmark task for removal so it isn't removed after being added\n\n        if not (task.status == RUNNING and status == PENDING):\n            # don't allow re-scheduling of task while it is running, it must either fail or succeed first\n            if status == PENDING or status != task.status:\n                # Update the DB only if there was a acctual change, to prevent noise.\n                # We also check for status == PENDING b/c that's the default value\n                # (so checking for status != task.status woule lie)\n                self._update_task_history(task_id, status)\n            self.set_status(task, PENDING if status == SUSPENDED else status)\n            if status == FAILED:\n                task.retry = time.time() + self._retry_delay\n\n        if deps is not None:\n            task.deps = set(deps)\n\n        if new_deps is not None:\n            task.deps.update(new_deps)\n\n        task.stakeholders.add(worker)\n        task.resources = resources\n\n        # Task dependencies might not exist yet. Let's create dummy tasks for them for now.\n        # Otherwise the task dependencies might end up being pruned if scheduling takes a long time\n        for dep in task.deps or []:\n            t = self._state.get_task(dep, setdefault=self._make_task(id=dep, status=UNKNOWN, deps=None, priority=priority))\n            t.stakeholders.add(worker)\n\n        self._update_priority(task, priority, worker)\n\n        if runnable:\n            task.workers.add(worker)\n\n        if expl is not None:\n            task.expl = expl\n\n    def add_worker(self, worker, info):\n        self._state.get_worker(worker).add_info(info)\n\n    def update_resources(self, **resources):\n        if self._resources is None:\n            self._resources = {}\n        self._resources.update(resources)\n\n    def _has_resources(self, needed_resources, used_resources):\n        if needed_resources is None:\n            return True\n\n        available_resources = self._resources or {}\n        for resource, amount in needed_resources.items():\n            if amount + used_resources[resource] > available_resources.get(resource, 1):\n                return False\n        return True\n\n    def _used_resources(self):\n        used_resources = collections.defaultdict(int)\n        if self._resources is not None:\n            for task in self._state.get_active_tasks():\n                if task.status == RUNNING and task.resources:\n                    for resource, amount in task.resources.items():\n                        used_resources[resource] += amount\n        return used_resources\n\n    def _rank(self):\n        ''' Return worker's rank function for task scheduling '''\n        dependents = collections.defaultdict(int)\n        def not_done(t):\n            task = self._state.get_task(t, default=None)\n            return task is None or task.status != DONE\n        for task in self._state.get_active_tasks():\n            if task.status != DONE:\n                deps = filter(not_done, task.deps)\n                inverse_num_deps = 1.0 / max(len(deps), 1)\n                for dep in deps:\n                    dependents[dep] += inverse_num_deps\n\n        return lambda task: (task.priority, dependents[task.id], -task.time)\n\n    def _schedulable(self, task):\n        if task.status != PENDING:\n            return False\n        for dep in task.deps:\n            dep_task = self._state.get_task(dep, default=None)\n            if dep_task is None or dep_task.status != DONE:\n                return False\n        return True\n\n    def get_work(self, worker, host=None):\n        # TODO: remove any expired nodes\n\n        # Algo: iterate over all nodes, find the highest priority node no dependencies and available\n        # resources.\n\n        # Resource checking looks both at currently available resources and at which resources would\n        # be available if all running tasks died and we rescheduled all workers greedily. We do both\n        # checks in order to prevent a worker with many low-priority tasks from starving other\n        # workers with higher priority tasks that share the same resources.\n\n        # TODO: remove tasks that can't be done, figure out if the worker has absolutely\n        # nothing it can wait for\n\n        # Return remaining tasks that have no FAILED descendents\n        self.update(worker, {'host': host})\n        best_task = None\n        best_task_id = None\n        locally_pending_tasks = 0\n        running_tasks = []\n\n        used_resources = self._used_resources()\n        greedy_resources = collections.defaultdict(int)\n        n_unique_pending = 0\n        greedy_workers = dict((worker.id, worker.info.get('workers', 1))\n                              for worker in self._state.get_active_workers())\n\n        tasks = list(self._state.get_pending_tasks())\n        tasks.sort(key=self._rank(), reverse=True)\n\n        for task in tasks:\n            if task.status == 'RUNNING' and worker in task.workers:\n                # Return a list of currently running tasks to the client,\n                # makes it easier to troubleshoot\n                other_worker = self._state.get_worker(task.worker_running)\n                more_info = {'task_id': task.id, 'worker': str(other_worker)}\n                if other_worker is not None:\n                    more_info.update(other_worker.info)\n                    running_tasks.append(more_info)\n\n            if task.status == PENDING and worker in task.workers:\n                locally_pending_tasks += 1\n                if len(task.workers) == 1:\n                    n_unique_pending += 1\n\n            if task.status == RUNNING and task.worker_running in greedy_workers:\n                greedy_workers[task.worker_running] -= 1\n                for resource, amount in (task.resources or {}).items():\n                    greedy_resources[resource] += amount\n\n            if not best_task and self._schedulable(task) and self._has_resources(task.resources, greedy_resources):\n                if worker in task.workers and self._has_resources(task.resources, used_resources):\n                    best_task = task\n                    best_task_id = task.id\n                else:\n                    for task_worker in task.workers:\n                        if greedy_workers.get(task_worker, 0) > 0:\n                            # use up a worker\n                            greedy_workers[task_worker] -= 1\n\n                            # keep track of the resources used in greedy scheduling\n                            for resource, amount in (task.resources or {}).items():\n                                greedy_resources[resource] += amount\n\n                            break\n\n        if best_task:\n            best_task.status = RUNNING\n            best_task.worker_running = worker\n            best_task.time_running = time.time()\n            self._update_task_history(best_task.id, RUNNING, host=host)\n\n        return {'n_pending_tasks': locally_pending_tasks,\n                'n_unique_pending': n_unique_pending,\n                'task_id': best_task_id,\n                'running_tasks': running_tasks}\n\n    def ping(self, worker):\n        self.update(worker)\n\n    def _upstream_status(self, task_id, upstream_status_table):\n        if task_id in upstream_status_table:\n            return upstream_status_table[task_id]\n        elif self._state.has_task(task_id):\n            task_stack = [task_id]\n\n            while task_stack:\n                dep_id = task_stack.pop()\n                if self._state.has_task(dep_id):\n                    dep = self._state.get_task(dep_id)\n                    if dep_id not in upstream_status_table:\n                        if dep.status == PENDING and dep.deps:\n                            task_stack = task_stack + [dep_id] + list(dep.deps)\n                            upstream_status_table[dep_id] = ''  # will be updated postorder\n                        else:\n                            dep_status = STATUS_TO_UPSTREAM_MAP.get(dep.status, '')\n                            upstream_status_table[dep_id] = dep_status\n                    elif upstream_status_table[dep_id] == '' and dep.deps:\n                        # This is the postorder update step when we set the\n                        # status based on the previously calculated child elements\n                        upstream_status = [upstream_status_table.get(id, '') for id in dep.deps]\n                        upstream_status.append('')  # to handle empty list\n                        status = max(upstream_status, key=UPSTREAM_SEVERITY_KEY)\n                        upstream_status_table[dep_id] = status\n            return upstream_status_table[dep_id]\n\n    def _serialize_task(self, task_id, include_deps=True):\n        task = self._state.get_task(task_id)\n        ret = {\n            'deps': list(task.deps),\n            'status': task.status,\n            'workers': list(task.workers),\n            'worker_running': task.worker_running,\n            'time_running': getattr(task, \"time_running\", None),\n            'start_time': task.time,\n            'params': task.params,\n            'name': task.family,\n            'priority': task.priority,\n            'resources': task.resources,\n        }\n        if include_deps:\n            ret['deps'] = list(task.deps)\n        return ret\n\n    def graph(self):\n        self.prune()\n        serialized = {}\n        for task in self._state.get_active_tasks():\n            serialized[task.id] = self._serialize_task(task.id)\n        return serialized\n\n    def _recurse_deps(self, task_id, serialized):\n        if task_id not in serialized:\n            task = self._state.get_task(task_id)\n            if task is None or not task.family:\n                logger.warn('Missing task for id [%s]', task_id)\n\n                # try to infer family and params from task_id\n                try:\n                    family, _, param_str = task_id.rstrip(')').partition('(')\n                    params = dict(param.split('=') for param in param_str.split(', '))\n                except:\n                    family, params = '', {}\n                serialized[task_id] = {\n                    'deps': [],\n                    'status': UNKNOWN,\n                    'workers': [],\n                    'start_time': UNKNOWN,\n                    'params': params,\n                    'name': family,\n                    'priority': 0,\n                }\n            else:\n                serialized[task_id] = self._serialize_task(task_id)\n                for dep in task.deps:\n                    self._recurse_deps(dep, serialized)\n\n    def dep_graph(self, task_id):\n        self.prune()\n        serialized = {}\n        if self._state.has_task(task_id):\n            self._recurse_deps(task_id, serialized)\n        return serialized\n\n    def task_list(self, status, upstream_status):\n        ''' query for a subset of tasks by status '''\n        self.prune()\n        result = {}\n        upstream_status_table = {}  # used to memoize upstream status\n        for task in self._state.get_active_tasks():\n            if not status or task.status == status:\n                if (task.status != PENDING or not upstream_status or\n                    upstream_status == self._upstream_status(task.id, upstream_status_table)):\n                    serialized = self._serialize_task(task.id, False)\n                    result[task.id] = serialized\n        return result\n\n    def worker_list(self, include_running=True):\n        self.prune()\n        workers = [\n            dict(\n                name=worker.id,\n                last_active=worker.last_active,\n                started=getattr(worker, 'started', None),\n                **worker.info\n            ) for worker in self._state.get_active_workers()]\n        workers.sort(key=lambda worker: worker['started'], reverse=True)\n        if include_running:\n            running = collections.defaultdict(dict)\n            num_pending = collections.defaultdict(int)\n            num_uniques = collections.defaultdict(int)\n            for task in self._state.get_pending_tasks():\n                if task.status == RUNNING and task.worker_running:\n                    running[task.worker_running][task.id] = self._serialize_task(task.id, False)\n                elif task.status == PENDING:\n                    for worker in task.workers:\n                        num_pending[worker] += 1\n                    if len(task.workers) == 1:\n                        num_uniques[list(task.workers)[0]] += 1\n            for worker in workers:\n                tasks = running[worker['name']]\n                worker['num_running'] = len(tasks)\n                worker['num_pending'] = num_pending[worker['name']]\n                worker['num_uniques'] = num_uniques[worker['name']]\n                worker['running'] = tasks\n        return workers\n\n    def inverse_dependencies(self, task_id):\n        self.prune()\n        serialized = {}\n        if self._state.has_task(task_id):\n            self._traverse_inverse_deps(task_id, serialized)\n        return serialized\n\n    def _traverse_inverse_deps(self, task_id, serialized):\n        stack = [task_id]\n        serialized[task_id] = self._serialize_task(task_id)\n        while len(stack) > 0:\n            curr_id = stack.pop()\n            for task in self._state.get_active_tasks():\n                if curr_id in task.deps:\n                    serialized[curr_id][\"deps\"].append(task.id)\n                    if task.id not in serialized:\n                        serialized[task.id] = self._serialize_task(task.id)\n                        serialized[task.id][\"deps\"] = []\n                        stack.append(task.id)\n\n    def task_search(self, task_str):\n        ''' query for a subset of tasks by task_id '''\n        self.prune()\n        result = collections.defaultdict(dict)\n        for task in self._state.get_active_tasks():\n            if task.id.find(task_str) != -1:\n                serialized = self._serialize_task(task.id, False)\n                result[task.status][task.id] = serialized\n        return result\n\n    def re_enable_task(self, task_id):\n        serialized = {}\n        task = self._state.get_task(task_id)\n        if task and task.status == DISABLED and task.scheduler_disable_time:\n            task.re_enable()\n            serialized = self._serialize_task(task_id)\n        return serialized\n\n    def fetch_error(self, task_id):\n        if self._state.has_task(task_id):\n            return {\"taskId\": task_id, \"error\": self._state.get_task(task_id).expl}\n        else:\n            return {\"taskId\": task_id, \"error\": \"\"}\n\n    def _update_task_history(self, task_id, status, host=None):\n        try:\n            if status == DONE or status == FAILED:\n                successful = (status == DONE)\n                self._task_history.task_finished(task_id, successful)\n            elif status == PENDING:\n                self._task_history.task_scheduled(task_id)\n            elif status == RUNNING:\n                self._task_history.task_started(task_id, host)\n        except:\n            logger.warning(\"Error saving Task history\", exc_info=1)\n\n    @property\n    def task_history(self):\n        # Used by server.py to expose the calls\n        return self._task_history\n",
          "file_after": "# Copyright (c) 2012 Spotify AB\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"); you may not\n# use this file except in compliance with the License. You may obtain a copy of\n# the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n# License for the specific language governing permissions and limitations under\n# the License.\n\nimport collections\nimport datetime\nimport functools\nimport notifications\nimport os\nimport logging\nimport time\nimport cPickle as pickle\nimport task_history as history\nlogger = logging.getLogger(\"luigi.server\")\n\nfrom task_status import PENDING, FAILED, DONE, RUNNING, SUSPENDED, UNKNOWN, DISABLED\n\n\nclass Scheduler(object):\n    ''' Abstract base class\n\n    Note that the methods all take string arguments, not Task objects...\n    '''\n    add_task = NotImplemented\n    get_work = NotImplemented\n    ping = NotImplemented\n\nUPSTREAM_RUNNING = 'UPSTREAM_RUNNING'\nUPSTREAM_MISSING_INPUT = 'UPSTREAM_MISSING_INPUT'\nUPSTREAM_FAILED = 'UPSTREAM_FAILED'\nUPSTREAM_DISABLED = 'UPSTREAM_DISABLED'\n\nUPSTREAM_SEVERITY_ORDER = (\n    '',\n    UPSTREAM_RUNNING,\n    UPSTREAM_MISSING_INPUT,\n    UPSTREAM_FAILED,\n    UPSTREAM_DISABLED,\n)\nUPSTREAM_SEVERITY_KEY = lambda st: UPSTREAM_SEVERITY_ORDER.index(st)\nSTATUS_TO_UPSTREAM_MAP = {\n    FAILED: UPSTREAM_FAILED,\n    RUNNING: UPSTREAM_RUNNING,\n    PENDING: UPSTREAM_MISSING_INPUT,\n    DISABLED: UPSTREAM_DISABLED,\n}\n\n\nclass Failures(object):\n    \"\"\" This class tracks the number of failures in a given time window\n\n    Failures added are marked with the current timestamp, and this class counts\n    the number of failures in a sliding time window ending at the present.\n\n    \"\"\"\n\n    def __init__(self, window):\n        \"\"\" Initialize with the given window\n\n        :param window: how long to track failures for, as a datetime.timedelta\n        \"\"\"\n        self.window = window\n        self.failures = collections.deque()\n\n    def add_failure(self):\n        \"\"\" Add a failure event with the current timestamp \"\"\"\n        self.failures.append(datetime.datetime.now())\n\n    def num_failures(self):\n        \"\"\" Return the number of failures in the window \"\"\"\n        min_time = datetime.datetime.now() - self.window\n        while self.failures and self.failures[0] < min_time:\n            self.failures.popleft()\n        return len(self.failures)\n\n    def clear(self):\n        \"\"\" Clear the failure queue \"\"\"\n        self.failures.clear()\n\n\nclass Task(object):\n    def __init__(self, id, status, deps, resources={}, priority=0, family='', params={},\n                 disable_failures=None, disable_window=None):\n        self.id = id\n        self.stakeholders = set()  # workers ids that are somehow related to this task (i.e. don't prune while any of these workers are still active)\n        self.workers = set()  # workers ids that can perform task - task is 'BROKEN' if none of these workers are active\n        if deps is None:\n            self.deps = set()\n        else:\n            self.deps = set(deps)\n        self.status = status  # PENDING, RUNNING, FAILED or DONE\n        self.time = time.time()  # Timestamp when task was first added\n        self.retry = None\n        self.remove = None\n        self.worker_running = None  # the worker id that is currently running the task or None\n        self.time_running = None  # Timestamp when picked up by worker\n        self.expl = None\n        self.priority = priority\n        self.resources = resources\n        self.family = family\n        self.params = params\n        self.disable_failures = disable_failures\n        self.failures = Failures(disable_window)\n        self.scheduler_disable_time = None\n\n    def __repr__(self):\n        return \"Task(%r)\" % vars(self)\n\n    def add_failure(self):\n        self.failures.add_failure()\n\n    def has_excessive_failures(self):\n        return self.failures.num_failures() >= self.disable_failures\n\n    def can_disable(self):\n        return self.disable_failures is not None\n\n    def re_enable(self):\n        self.scheduler_disable_time = None\n        self.status = FAILED\n        self.failures.clear()\n\n    def set_status(self, new_status, disable_failures=None, disable_window=None, disable_persist=None):\n        # not sure why we have SUSPENDED, as it can never be set\n        if new_status == SUSPENDED:\n            new_status = PENDING\n\n        if new_status == DISABLED and self.status == RUNNING:\n            return\n\n        if self.status == DISABLED:\n            if new_status == DONE:\n                self.re_enable()\n\n            # don't allow workers to override a scheduler disable\n            elif self.scheduler_disable_time is not None:\n                return\n\n        if new_status == FAILED and self.can_disable():\n            self.add_failure()\n            if self.has_excessive_failures():\n                self.scheduler_disable_time = datetime.datetime.now()\n                new_status = DISABLED\n                notifications.send_error_email(\n                    'Luigi Scheduler: DISABLED {task} due to excessive failures'.format(task=self.id),\n                    '{task} failed {failures} times in the last {window} seconds, so it is being '\n                    'disabled for {persist} seconds'.format(\n                        failures=disable_failures,\n                        task=self.id,\n                        window=disable_window,\n                        persist=disable_persist,\n                        ))\n        elif new_status == DISABLED:\n            self.scheduler_disable_time = None\n\n        self.status = new_status\n\n\n\nclass Worker(object):\n    \"\"\" Structure for tracking worker activity and keeping their references \"\"\"\n    def __init__(self, id, last_active=None):\n        self.id = id\n        self.reference = None  # reference to the worker in the real world. (Currently a dict containing just the host)\n        self.last_active = last_active  # seconds since epoch\n        self.started = time.time()  # seconds since epoch\n        self.info = {}\n\n    def add_info(self, info):\n        self.info.update(info)\n\n    def __str__(self):\n        return self.id\n\n\nclass SimpleTaskState(object):\n    ''' Keep track of the current state and handle persistance\n\n    The point of this class is to enable other ways to keep state, eg. by using a database\n    These will be implemented by creating an abstract base class that this and other classes\n    inherit from.\n    '''\n\n    def __init__(self, state_path):\n        self._state_path = state_path\n        self._tasks = {}  # map from id to a Task object\n        self._active_workers = {}  # map from id to a Worker object\n\n    def dump(self):\n        state = (self._tasks, self._active_workers)\n        try:\n            with open(self._state_path, 'w') as fobj:\n                pickle.dump(state, fobj)\n        except IOError:\n            logger.warning(\"Failed saving scheduler state\", exc_info=1)\n        else:\n            logger.info(\"Saved state in %s\", self._state_path)\n\n    # prone to lead to crashes when old state is unpickled with updated code. TODO some kind of version control?\n    def load(self):\n        if os.path.exists(self._state_path):\n            logger.info(\"Attempting to load state from %s\", self._state_path)\n            try:\n                with open(self._state_path) as fobj:\n                    state = pickle.load(fobj)\n            except:\n                logger.exception(\"Error when loading state. Starting from clean slate.\")\n                return\n\n            self._tasks, self._active_workers = state\n\n            # Convert from old format\n            # TODO: this is really ugly, we need something more future-proof\n            # Every time we add an attribute to the Worker class, this code needs to be updated\n            for k, v in self._active_workers.iteritems():\n                if isinstance(v, float):\n                    self._active_workers[k] = Worker(id=k, last_active=v)\n        else:\n            logger.info(\"No prior state file exists at %s. Starting with clean slate\", self._state_path)\n\n    def get_active_tasks(self):\n        for task in self._tasks.itervalues():\n            yield task\n\n    def get_pending_tasks(self):\n        for task in self._tasks.itervalues():\n            if task.status in [PENDING, RUNNING]:\n                yield task\n\n    def get_task(self, task_id, default=None, setdefault=None):\n        if setdefault:\n            return self._tasks.setdefault(task_id, setdefault)\n        else:\n            return self._tasks.get(task_id, default)\n\n    def has_task(self, task_id):\n        return task_id in self._tasks\n\n    def inactivate_tasks(self, delete_tasks):\n        # The terminology is a bit confusing: we used to \"delete\" tasks when they became inactive,\n        # but with a pluggable state storage, you might very well want to keep some history of\n        # older tasks as well. That's why we call it \"inactivate\" (as in the verb)\n        for task in delete_tasks:\n            self._tasks.pop(task)\n\n    def get_active_workers(self, last_active_lt=None):\n        for worker in self._active_workers.itervalues():\n            if last_active_lt is not None and worker.last_active >= last_active_lt:\n                continue\n            yield worker\n\n    def get_worker_ids(self):\n        return self._active_workers.keys() # only used for unit tests\n\n    def get_worker(self, worker_id):\n        return self._active_workers.setdefault(worker_id, Worker(worker_id))\n\n    def inactivate_workers(self, delete_workers):\n        # Mark workers as inactive\n        for worker in delete_workers:\n            self._active_workers.pop(worker)\n\n        # remove workers from tasks\n        for task in self.get_active_tasks():\n            task.stakeholders.difference_update(delete_workers)\n            task.workers.difference_update(delete_workers)\n\n\nclass CentralPlannerScheduler(Scheduler):\n    ''' Async scheduler that can handle multiple workers etc\n\n    Can be run locally or on a server (using RemoteScheduler + server.Server).\n    '''\n\n    def __init__(self, retry_delay=900.0, remove_delay=600.0, worker_disconnect_delay=60.0,\n                 state_path='/var/lib/luigi-server/state.pickle', task_history=None,\n                 resources=None, disable_persist=0, disable_window=0, disable_failures=None):\n        '''\n        (all arguments are in seconds)\n        Keyword Arguments:\n        retry_delay -- How long after a Task fails to try it again, or -1 to never retry\n        remove_delay -- How long after a Task finishes to remove it from the scheduler\n        state_path -- Path to state file (tasks and active workers)\n        worker_disconnect_delay -- If a worker hasn't communicated for this long, remove it from active workers\n        '''\n        self._retry_delay = retry_delay\n        self._remove_delay = remove_delay\n        self._worker_disconnect_delay = worker_disconnect_delay\n        self._task_history = task_history or history.NopHistory()\n        self._state = SimpleTaskState(state_path)\n\n        self._task_history = task_history or history.NopHistory()\n        self._resources = resources\n        self._disable_failures = disable_failures\n        self._disable_window = disable_window\n        self._make_task = functools.partial(\n            Task, disable_failures=disable_failures,\n            disable_window=datetime.timedelta(seconds=disable_window))\n        self._disable_persist = disable_persist\n        self._disable_time = datetime.timedelta(seconds=disable_persist)\n\n    def load(self):\n        self._state.load()\n\n    def dump(self):\n        self._state.dump()\n\n    def prune(self):\n        logger.info(\"Starting pruning of task graph\")\n        # Delete workers that haven't said anything for a while (probably killed)\n        delete_workers = []\n        for worker in self._state.get_active_workers(last_active_lt=time.time() - self._worker_disconnect_delay):\n            logger.info(\"Worker %s timed out (no contact for >=%ss)\", worker, self._worker_disconnect_delay)\n            delete_workers.append(worker.id)\n\n        self._state.inactivate_workers(delete_workers)\n\n        delete_workers = set(delete_workers)\n\n        remove_tasks = []\n        for task in self._state.get_active_tasks():\n            # Mark tasks with no remaining active stakeholders for deletion\n            if not task.stakeholders:\n                if task.remove is None:\n                    logger.info(\"Task %r has stakeholders %r but none remain connected -> will remove task in %s seconds\", task.id, task.stakeholders, self._remove_delay)\n                    task.remove = time.time() + self._remove_delay\n\n            # If a running worker disconnects, tag all its jobs as FAILED and subject it to the same retry logic\n            if task.status == RUNNING and task.worker_running and task.worker_running not in task.stakeholders:\n                logger.info(\"Task %r is marked as running by disconnected worker %r -> marking as FAILED with retry delay of %rs\", task.id, task.worker_running, self._retry_delay)\n                task.worker_running = None\n                task.set_status(FAILED, self._disable_failures, self._disable_window, self._disable_persist)\n                task.retry = time.time() + self._retry_delay\n\n            if task.status == DISABLED and task.scheduler_disable_time:\n                # re-enable task after the disable time expires\n                if datetime.datetime.now() - task.scheduler_disable_time > self._disable_time:\n                    task.re_enable()\n\n            # Remove tasks that have no stakeholders\n            if task.remove and time.time() > task.remove:\n                logger.info(\"Removing task %r (no connected stakeholders)\", task.id)\n                remove_tasks.append(task.id)\n\n            # Reset FAILED tasks to PENDING if max timeout is reached, and retry delay is >= 0\n            if task.status == FAILED and self._retry_delay >= 0 and task.retry < time.time():\n                task.set_status(PENDING)\n\n        self._state.inactivate_tasks(remove_tasks)\n\n        logger.info(\"Done pruning task graph\")\n\n    def update(self, worker_id, worker_reference=None):\n        \"\"\" Keep track of whenever the worker was last active \"\"\"\n        worker = self._state.get_worker(worker_id)\n        if worker_reference:\n            worker.reference = worker_reference\n        worker.last_active = time.time()\n\n    def _update_priority(self, task, prio, worker):\n        \"\"\" Update priority of the given task\n\n        Priority can only be increased. If the task doesn't exist, a placeholder\n        task is created to preserve priority when the task is later scheduled.\n        \"\"\"\n        task.priority = prio = max(prio, task.priority)\n        for dep in task.deps or []:\n            t = self._state.get_task(dep)\n            if t is not None and prio > t.priority:\n                self._update_priority(t, prio, worker)\n\n    def add_task(self, worker, task_id, status=PENDING, runnable=True,\n                 deps=None, new_deps=None, expl=None, resources=None,\n                 priority=0, family='', params={}):\n        \"\"\"\n        * Add task identified by task_id if it doesn't exist\n        * If deps is not None, update dependency list\n        * Update status of task\n        * Add additional workers/stakeholders\n        * Update priority when needed\n        \"\"\"\n        self.update(worker)\n\n        task = self._state.get_task(task_id, setdefault=self._make_task(\n                id=task_id, status=PENDING, deps=deps, resources=resources,\n                priority=priority, family=family, params=params))\n\n        # for setting priority, we'll sometimes create tasks with unset family and params\n        if not task.family:\n            task.family = family\n        if not task.params:\n            task.params = params\n\n        if task.remove is not None:\n            task.remove = None  # unmark task for removal so it isn't removed after being added\n\n        if not (task.status == RUNNING and status == PENDING):\n            # don't allow re-scheduling of task while it is running, it must either fail or succeed first\n            if status == PENDING or status != task.status:\n                # Update the DB only if there was a acctual change, to prevent noise.\n                # We also check for status == PENDING b/c that's the default value\n                # (so checking for status != task.status woule lie)\n                self._update_task_history(task_id, status)\n            task.set_status(PENDING if status == SUSPENDED else status,\n                            self._disable_failures, self._disable_window, self._disable_persist)\n            if status == FAILED:\n                task.retry = time.time() + self._retry_delay\n\n        if deps is not None:\n            task.deps = set(deps)\n\n        if new_deps is not None:\n            task.deps.update(new_deps)\n\n        task.stakeholders.add(worker)\n        task.resources = resources\n\n        # Task dependencies might not exist yet. Let's create dummy tasks for them for now.\n        # Otherwise the task dependencies might end up being pruned if scheduling takes a long time\n        for dep in task.deps or []:\n            t = self._state.get_task(dep, setdefault=self._make_task(id=dep, status=UNKNOWN, deps=None, priority=priority))\n            t.stakeholders.add(worker)\n\n        self._update_priority(task, priority, worker)\n\n        if runnable:\n            task.workers.add(worker)\n\n        if expl is not None:\n            task.expl = expl\n\n    def add_worker(self, worker, info):\n        self._state.get_worker(worker).add_info(info)\n\n    def update_resources(self, **resources):\n        if self._resources is None:\n            self._resources = {}\n        self._resources.update(resources)\n\n    def _has_resources(self, needed_resources, used_resources):\n        if needed_resources is None:\n            return True\n\n        available_resources = self._resources or {}\n        for resource, amount in needed_resources.items():\n            if amount + used_resources[resource] > available_resources.get(resource, 1):\n                return False\n        return True\n\n    def _used_resources(self):\n        used_resources = collections.defaultdict(int)\n        if self._resources is not None:\n            for task in self._state.get_active_tasks():\n                if task.status == RUNNING and task.resources:\n                    for resource, amount in task.resources.items():\n                        used_resources[resource] += amount\n        return used_resources\n\n    def _rank(self):\n        ''' Return worker's rank function for task scheduling '''\n        dependents = collections.defaultdict(int)\n        def not_done(t):\n            task = self._state.get_task(t, default=None)\n            return task is None or task.status != DONE\n        for task in self._state.get_active_tasks():\n            if task.status != DONE:\n                deps = filter(not_done, task.deps)\n                inverse_num_deps = 1.0 / max(len(deps), 1)\n                for dep in deps:\n                    dependents[dep] += inverse_num_deps\n\n        return lambda task: (task.priority, dependents[task.id], -task.time)\n\n    def _schedulable(self, task):\n        if task.status != PENDING:\n            return False\n        for dep in task.deps:\n            dep_task = self._state.get_task(dep, default=None)\n            if dep_task is None or dep_task.status != DONE:\n                return False\n        return True\n\n    def get_work(self, worker, host=None):\n        # TODO: remove any expired nodes\n\n        # Algo: iterate over all nodes, find the highest priority node no dependencies and available\n        # resources.\n\n        # Resource checking looks both at currently available resources and at which resources would\n        # be available if all running tasks died and we rescheduled all workers greedily. We do both\n        # checks in order to prevent a worker with many low-priority tasks from starving other\n        # workers with higher priority tasks that share the same resources.\n\n        # TODO: remove tasks that can't be done, figure out if the worker has absolutely\n        # nothing it can wait for\n\n        # Return remaining tasks that have no FAILED descendents\n        self.update(worker, {'host': host})\n        best_task = None\n        best_task_id = None\n        locally_pending_tasks = 0\n        running_tasks = []\n\n        used_resources = self._used_resources()\n        greedy_resources = collections.defaultdict(int)\n        n_unique_pending = 0\n        greedy_workers = dict((worker.id, worker.info.get('workers', 1))\n                              for worker in self._state.get_active_workers())\n\n        tasks = list(self._state.get_pending_tasks())\n        tasks.sort(key=self._rank(), reverse=True)\n\n        for task in tasks:\n            if task.status == 'RUNNING' and worker in task.workers:\n                # Return a list of currently running tasks to the client,\n                # makes it easier to troubleshoot\n                other_worker = self._state.get_worker(task.worker_running)\n                more_info = {'task_id': task.id, 'worker': str(other_worker)}\n                if other_worker is not None:\n                    more_info.update(other_worker.info)\n                    running_tasks.append(more_info)\n\n            if task.status == PENDING and worker in task.workers:\n                locally_pending_tasks += 1\n                if len(task.workers) == 1:\n                    n_unique_pending += 1\n\n            if task.status == RUNNING and task.worker_running in greedy_workers:\n                greedy_workers[task.worker_running] -= 1\n                for resource, amount in (task.resources or {}).items():\n                    greedy_resources[resource] += amount\n\n            if not best_task and self._schedulable(task) and self._has_resources(task.resources, greedy_resources):\n                if worker in task.workers and self._has_resources(task.resources, used_resources):\n                    best_task = task\n                    best_task_id = task.id\n                else:\n                    for task_worker in task.workers:\n                        if greedy_workers.get(task_worker, 0) > 0:\n                            # use up a worker\n                            greedy_workers[task_worker] -= 1\n\n                            # keep track of the resources used in greedy scheduling\n                            for resource, amount in (task.resources or {}).items():\n                                greedy_resources[resource] += amount\n\n                            break\n\n        if best_task:\n            best_task.status = RUNNING\n            best_task.worker_running = worker\n            best_task.time_running = time.time()\n            self._update_task_history(best_task.id, RUNNING, host=host)\n\n        return {'n_pending_tasks': locally_pending_tasks,\n                'n_unique_pending': n_unique_pending,\n                'task_id': best_task_id,\n                'running_tasks': running_tasks}\n\n    def ping(self, worker):\n        self.update(worker)\n\n    def _upstream_status(self, task_id, upstream_status_table):\n        if task_id in upstream_status_table:\n            return upstream_status_table[task_id]\n        elif self._state.has_task(task_id):\n            task_stack = [task_id]\n\n            while task_stack:\n                dep_id = task_stack.pop()\n                if self._state.has_task(dep_id):\n                    dep = self._state.get_task(dep_id)\n                    if dep_id not in upstream_status_table:\n                        if dep.status == PENDING and dep.deps:\n                            task_stack = task_stack + [dep_id] + list(dep.deps)\n                            upstream_status_table[dep_id] = ''  # will be updated postorder\n                        else:\n                            dep_status = STATUS_TO_UPSTREAM_MAP.get(dep.status, '')\n                            upstream_status_table[dep_id] = dep_status\n                    elif upstream_status_table[dep_id] == '' and dep.deps:\n                        # This is the postorder update step when we set the\n                        # status based on the previously calculated child elements\n                        upstream_status = [upstream_status_table.get(id, '') for id in dep.deps]\n                        upstream_status.append('')  # to handle empty list\n                        status = max(upstream_status, key=UPSTREAM_SEVERITY_KEY)\n                        upstream_status_table[dep_id] = status\n            return upstream_status_table[dep_id]\n\n    def _serialize_task(self, task_id, include_deps=True):\n        task = self._state.get_task(task_id)\n        ret = {\n            'deps': list(task.deps),\n            'status': task.status,\n            'workers': list(task.workers),\n            'worker_running': task.worker_running,\n            'time_running': getattr(task, \"time_running\", None),\n            'start_time': task.time,\n            'params': task.params,\n            'name': task.family,\n            'priority': task.priority,\n            'resources': task.resources,\n        }\n        if include_deps:\n            ret['deps'] = list(task.deps)\n        return ret\n\n    def graph(self):\n        self.prune()\n        serialized = {}\n        for task in self._state.get_active_tasks():\n            serialized[task.id] = self._serialize_task(task.id)\n        return serialized\n\n    def _recurse_deps(self, task_id, serialized):\n        if task_id not in serialized:\n            task = self._state.get_task(task_id)\n            if task is None or not task.family:\n                logger.warn('Missing task for id [%s]', task_id)\n\n                # try to infer family and params from task_id\n                try:\n                    family, _, param_str = task_id.rstrip(')').partition('(')\n                    params = dict(param.split('=') for param in param_str.split(', '))\n                except:\n                    family, params = '', {}\n                serialized[task_id] = {\n                    'deps': [],\n                    'status': UNKNOWN,\n                    'workers': [],\n                    'start_time': UNKNOWN,\n                    'params': params,\n                    'name': family,\n                    'priority': 0,\n                }\n            else:\n                serialized[task_id] = self._serialize_task(task_id)\n                for dep in task.deps:\n                    self._recurse_deps(dep, serialized)\n\n    def dep_graph(self, task_id):\n        self.prune()\n        serialized = {}\n        if self._state.has_task(task_id):\n            self._recurse_deps(task_id, serialized)\n        return serialized\n\n    def task_list(self, status, upstream_status):\n        ''' query for a subset of tasks by status '''\n        self.prune()\n        result = {}\n        upstream_status_table = {}  # used to memoize upstream status\n        for task in self._state.get_active_tasks():\n            if not status or task.status == status:\n                if (task.status != PENDING or not upstream_status or\n                    upstream_status == self._upstream_status(task.id, upstream_status_table)):\n                    serialized = self._serialize_task(task.id, False)\n                    result[task.id] = serialized\n        return result\n\n    def worker_list(self, include_running=True):\n        self.prune()\n        workers = [\n            dict(\n                name=worker.id,\n                last_active=worker.last_active,\n                started=getattr(worker, 'started', None),\n                **worker.info\n            ) for worker in self._state.get_active_workers()]\n        workers.sort(key=lambda worker: worker['started'], reverse=True)\n        if include_running:\n            running = collections.defaultdict(dict)\n            num_pending = collections.defaultdict(int)\n            num_uniques = collections.defaultdict(int)\n            for task in self._state.get_pending_tasks():\n                if task.status == RUNNING and task.worker_running:\n                    running[task.worker_running][task.id] = self._serialize_task(task.id, False)\n                elif task.status == PENDING:\n                    for worker in task.workers:\n                        num_pending[worker] += 1\n                    if len(task.workers) == 1:\n                        num_uniques[list(task.workers)[0]] += 1\n            for worker in workers:\n                tasks = running[worker['name']]\n                worker['num_running'] = len(tasks)\n                worker['num_pending'] = num_pending[worker['name']]\n                worker['num_uniques'] = num_uniques[worker['name']]\n                worker['running'] = tasks\n        return workers\n\n    def inverse_dependencies(self, task_id):\n        self.prune()\n        serialized = {}\n        if self._state.has_task(task_id):\n            self._traverse_inverse_deps(task_id, serialized)\n        return serialized\n\n    def _traverse_inverse_deps(self, task_id, serialized):\n        stack = [task_id]\n        serialized[task_id] = self._serialize_task(task_id)\n        while len(stack) > 0:\n            curr_id = stack.pop()\n            for task in self._state.get_active_tasks():\n                if curr_id in task.deps:\n                    serialized[curr_id][\"deps\"].append(task.id)\n                    if task.id not in serialized:\n                        serialized[task.id] = self._serialize_task(task.id)\n                        serialized[task.id][\"deps\"] = []\n                        stack.append(task.id)\n\n    def task_search(self, task_str):\n        ''' query for a subset of tasks by task_id '''\n        self.prune()\n        result = collections.defaultdict(dict)\n        for task in self._state.get_active_tasks():\n            if task.id.find(task_str) != -1:\n                serialized = self._serialize_task(task.id, False)\n                result[task.status][task.id] = serialized\n        return result\n\n    def re_enable_task(self, task_id):\n        serialized = {}\n        task = self._state.get_task(task_id)\n        if task and task.status == DISABLED and task.scheduler_disable_time:\n            task.re_enable()\n            serialized = self._serialize_task(task_id)\n        return serialized\n\n    def fetch_error(self, task_id):\n        if self._state.has_task(task_id):\n            return {\"taskId\": task_id, \"error\": self._state.get_task(task_id).expl}\n        else:\n            return {\"taskId\": task_id, \"error\": \"\"}\n\n    def _update_task_history(self, task_id, status, host=None):\n        try:\n            if status == DONE or status == FAILED:\n                successful = (status == DONE)\n                self._task_history.task_finished(task_id, successful)\n            elif status == PENDING:\n                self._task_history.task_scheduled(task_id)\n            elif status == RUNNING:\n                self._task_history.task_started(task_id, host)\n        except:\n            logger.warning(\"Error saving Task history\", exc_info=1)\n\n    @property\n    def task_history(self):\n        # Used by server.py to expose the calls\n        return self._task_history\n",
          "file_patch": "@@ -130,6 +130,42 @@ class Task(object):\n         self.status = FAILED\n         self.failures.clear()\n \n+    def set_status(self, new_status, disable_failures=None, disable_window=None, disable_persist=None):\n+        # not sure why we have SUSPENDED, as it can never be set\n+        if new_status == SUSPENDED:\n+            new_status = PENDING\n+\n+        if new_status == DISABLED and self.status == RUNNING:\n+            return\n+\n+        if self.status == DISABLED:\n+            if new_status == DONE:\n+                self.re_enable()\n+\n+            # don't allow workers to override a scheduler disable\n+            elif self.scheduler_disable_time is not None:\n+                return\n+\n+        if new_status == FAILED and self.can_disable():\n+            self.add_failure()\n+            if self.has_excessive_failures():\n+                self.scheduler_disable_time = datetime.datetime.now()\n+                new_status = DISABLED\n+                notifications.send_error_email(\n+                    'Luigi Scheduler: DISABLED {task} due to excessive failures'.format(task=self.id),\n+                    '{task} failed {failures} times in the last {window} seconds, so it is being '\n+                    'disabled for {persist} seconds'.format(\n+                        failures=disable_failures,\n+                        task=self.id,\n+                        window=disable_window,\n+                        persist=disable_persist,\n+                        ))\n+        elif new_status == DISABLED:\n+            self.scheduler_disable_time = None\n+\n+        self.status = new_status\n+\n+\n \n class Worker(object):\n     \"\"\" Structure for tracking worker activity and keeping their references \"\"\"\n@@ -303,7 +339,7 @@ class CentralPlannerScheduler(Scheduler):\n             if task.status == RUNNING and task.worker_running and task.worker_running not in task.stakeholders:\n                 logger.info(\"Task %r is marked as running by disconnected worker %r -> marking as FAILED with retry delay of %rs\", task.id, task.worker_running, self._retry_delay)\n                 task.worker_running = None\n-                self.set_status(task, FAILED)\n+                task.set_status(FAILED, self._disable_failures, self._disable_window, self._disable_persist)\n                 task.retry = time.time() + self._retry_delay\n \n             if task.status == DISABLED and task.scheduler_disable_time:\n@@ -318,47 +354,12 @@ class CentralPlannerScheduler(Scheduler):\n \n             # Reset FAILED tasks to PENDING if max timeout is reached, and retry delay is >= 0\n             if task.status == FAILED and self._retry_delay >= 0 and task.retry < time.time():\n-                self.set_status(task, PENDING)\n+                task.set_status(PENDING)\n \n         self._state.inactivate_tasks(remove_tasks)\n \n         logger.info(\"Done pruning task graph\")\n \n-    def set_status(self, task, new_status):\n-        # not sure why we have SUSPENDED, as it can never be set\n-        if new_status == SUSPENDED:\n-            new_status = PENDING\n-\n-        if new_status == DISABLED and task.status == RUNNING:\n-            return\n-\n-        if task.status == DISABLED:\n-            if new_status == DONE:\n-                task.re_enable()\n-\n-            # don't allow workers to override a scheduler disable\n-            elif task.scheduler_disable_time is not None:\n-                return\n-\n-        if new_status == FAILED and task.can_disable():\n-            task.add_failure()\n-            if task.has_excessive_failures():\n-                task.scheduler_disable_time = datetime.datetime.now()\n-                new_status = DISABLED\n-                notifications.send_error_email(\n-                    'Luigi Scheduler: DISABLED {task} due to excessive failures'.format(task=task.id),\n-                    '{task} failed {failures} times in the last {window} seconds, so it is being '\n-                    'disabled for {persist} seconds'.format(\n-                        failures=self._disable_failures,\n-                        task=task.id,\n-                        window=self._disable_window,\n-                        persist=self._disable_persist,\n-                        ))\n-        elif new_status == DISABLED:\n-            task.scheduler_disable_time = None\n-\n-        task.status = new_status\n-\n     def update(self, worker_id, worker_reference=None):\n         \"\"\" Keep track of whenever the worker was last active \"\"\"\n         worker = self._state.get_worker(worker_id)\n@@ -410,7 +411,8 @@ class CentralPlannerScheduler(Scheduler):\n                 # We also check for status == PENDING b/c that's the default value\n                 # (so checking for status != task.status woule lie)\n                 self._update_task_history(task_id, status)\n-            self.set_status(task, PENDING if status == SUSPENDED else status)\n+            task.set_status(PENDING if status == SUSPENDED else status,\n+                            self._disable_failures, self._disable_window, self._disable_persist)\n             if status == FAILED:\n                 task.retry = time.time() + self._retry_delay\n \n",
          "files_name_in_blame_commit": [
            "scheduler.py"
          ]
        }
      }
    }
  }
}