{
  "id": "58",
  "blame_commit": {
    "commit": {
      "commit_id": "a5103909bb43f6e9d655587949b54cbd95abbb18",
      "commit_message": "[RELNOTES] Refactor RNNs to rely on atomic cells. (#7943)\n\n* Refactor RNNs to rely on atomic cells.\r\n\r\n* Add RNN docstrings back.\r\n\r\n* Fix Theano/CNTK RNN dropout\r\n\r\n* Disable dropout in CNTK dynamic RNNs.\r\n\r\n* Standardize input dropout masks in RNNs.\r\n\r\n* Skip RNN dropout test for CNTK.\r\n\r\n* Remove legacy constraints\r\n\r\n* Increase stacked RNN test coverage",
      "commit_author": "Fran\u00e7ois Chollet",
      "commit_date": "2017-09-21 17:02:23",
      "commit_parent": "71a791cb71ef7080ea150152633a90e7821070af"
    },
    "function": {
      "function_name": "build",
      "function_code_before": "def build(self, input_shape):\n    if isinstance(input_shape, list):\n        input_shape = input_shape[0]\n    batch_size = input_shape[0] if self.stateful else None\n    self.input_dim = input_shape[2]\n    self.input_spec[0] = InputSpec(shape=(batch_size, None, self.input_dim))\n    self.states = [None]\n    if self.stateful:\n        self.reset_states()\n    self.kernel = self.add_weight(shape=(self.input_dim, self.units), name='kernel', initializer=self.kernel_initializer, regularizer=self.kernel_regularizer, constraint=self.kernel_constraint)\n    self.recurrent_kernel = self.add_weight(shape=(self.units, self.units), name='recurrent_kernel', initializer=self.recurrent_initializer, regularizer=self.recurrent_regularizer, constraint=self.recurrent_constraint)\n    if self.use_bias:\n        self.bias = self.add_weight(shape=(self.units,), name='bias', initializer=self.bias_initializer, regularizer=self.bias_regularizer, constraint=self.bias_constraint)\n    else:\n        self.bias = None\n    self.built = True",
      "function_code_after": "def build(self, input_shape):\n    for cell in self.cells:\n        if isinstance(cell, Layer):\n            cell.build(input_shape)\n        if hasattr(cell.state_size, '__len__'):\n            output_dim = cell.state_size[0]\n        else:\n            output_dim = cell.state_size\n        input_shape = (input_shape[0], input_shape[1], output_dim)\n    self.built = True",
      "function_before_start_line": 513,
      "function_before_end_line": 544,
      "function_after_start_line": 82,
      "function_after_end_line": 91,
      "function_before_token_count": 219,
      "function_after_token_count": 73,
      "functions_name_modified_file": [
        "trainable_weights",
        "recurrent_regularizer",
        "kernel_regularizer",
        "kernel_constraint",
        "reset_states",
        "_generate_recurrent_dropout_mask",
        "recurrent_constraint",
        "bias_constraint",
        "dropout",
        "losses",
        "recurrent_activation",
        "get_config",
        "set_weights",
        "recurrent_initializer",
        "get_weights",
        "unit_forget_bias",
        "states",
        "units",
        "_generate_dropout_mask",
        "build",
        "call",
        "non_trainable_weights",
        "get_losses_for",
        "bias_initializer",
        "kernel_initializer",
        "use_bias",
        "state_size",
        "recurrent_dropout",
        "bias_regularizer",
        "__init__",
        "compute_mask",
        "implementation",
        "__call__",
        "from_config",
        "compute_output_shape",
        "get_initial_state",
        "activation"
      ],
      "functions_name_all_files": [
        "_assert_sparse_module",
        "_normalize_axis",
        "pool3d",
        "ctc_update_log_p",
        "kernel_constraint",
        "logsumexp",
        "_generate_recurrent_dropout_mask",
        "cumprod",
        "_contain_seqence_axis",
        "dropout",
        "resize_volumes",
        "tanh",
        "reverse",
        "_assert_has_capability",
        "test_dropout",
        "activity_regularizer",
        "map_fn",
        "backward",
        "batch_normalization",
        "_get_dynamic_axis_num",
        "ctc_cost",
        "_reshape_sequence",
        "test_from_config",
        "step",
        "forward",
        "get_weights",
        "in_top_k",
        "unit_forget_bias",
        "_preprocess_conv3d_input",
        "rnn",
        "stack",
        "test_regularizer",
        "eval",
        "test_state_reuse",
        "ones",
        "cos",
        "get_variable_shape",
        "_remove_dims",
        "foldl",
        "dtype",
        "test_return_sequences",
        "equal",
        "hard_sigmoid",
        "_preprocess_conv2d_input",
        "_reduce_on_axis",
        "bias_regularizer",
        "__init__",
        "compute_mask",
        "sqrt",
        "random_binomial",
        "spatial_2d_padding",
        "implementation",
        "shape",
        "get_constants",
        "expand_dims",
        "categorical_crossentropy",
        "conv2d",
        "compute_output_shape",
        "test_stateful_invalid_use",
        "test_statefulness",
        "_preprocess_conv3d_kernel",
        "batch_set_value",
        "cast",
        "greater",
        "int_shape",
        "ctc_label_dense_to_sparse",
        "batch_dot",
        "trainable_weights",
        "moving_average_update",
        "depthwise_conv2d",
        "kernel_regularizer",
        "pattern_broadcast",
        "test_stacked_rnn_attributes",
        "concatenate",
        "recurrent_constraint",
        "reshape",
        "constant",
        "manual_variable_initialization",
        "losses",
        "_preprocess_deconv3d_output_shape",
        "get_config",
        "set_learning_phase",
        "local_conv1d",
        "bias_add",
        "truncated_normal",
        "eye",
        "print_tensor",
        "softplus",
        "ctc_path_probs",
        "test_masking_layer",
        "test_specify_initial_state_non_keras_tensor",
        "states",
        "test_implementation_mode",
        "local_conv2d",
        "test_minimal_rnn_cell_non_layer",
        "less_equal",
        "get_uid",
        "std",
        "spatial_3d_padding",
        "build",
        "merge",
        "call",
        "in_train_phase",
        "test_minimal_rnn_cell_layer",
        "permute_dimensions",
        "updates",
        "get_losses_for",
        "l2_normalize",
        "less",
        "normalize_batch_in_training",
        "_static_rnn",
        "recurrent_dropout",
        "test_specify_initial_state_keras_tensor",
        "ones_like",
        "_convert_dtype_string",
        "test_dynamic_behavior",
        "test_return_state",
        "sigmoid",
        "prod",
        "random_normal_variable",
        "_preprocess_conv2d_kernel",
        "get_num_dynamic_axis",
        "sign",
        "_preprocess_deconv_output_shape",
        "random_uniform",
        "get_session",
        "_padding",
        "conv3d",
        "update_add",
        "_convert_string_dtype",
        "repeat_elements",
        "pool2d",
        "activation",
        "infer_outputs",
        "separable_conv2d",
        "log",
        "identity",
        "recurrent_regularizer",
        "ctc_interleave_blanks",
        "stop_gradient",
        "ctc_batch_cost",
        "reset_states",
        "sin",
        "relu",
        "switch",
        "_preprocess_border_mode",
        "count_params",
        "_is_explicit_shape",
        "var",
        "any",
        "_preprocess_conv3d_filter_shape",
        "batch_get_value",
        "bias_constraint",
        "test_masking_correctness",
        "clear_session",
        "recurrent_activation",
        "function",
        "update_sub",
        "_old_batch_normalization",
        "set_weights",
        "random_normal",
        "zeros_like",
        "classification_error",
        "cumsum",
        "test_minimal_rnn_cell_non_layer_multiple_states",
        "recurrent_initializer",
        "test_reset_states_with_values",
        "has_seq_axis",
        "softsign",
        "binary_crossentropy",
        "is_placeholder",
        "_reshape_batch",
        "units",
        "preprocess_input",
        "softmax",
        "test_specify_state_with_masking",
        "batch_flatten",
        "_get_cntk_version",
        "conv3d_transpose",
        "elu",
        "non_trainable_weights",
        "square",
        "sparse_categorical_crossentropy",
        "kernel_initializer",
        "_preprocess_padding",
        "squeeze",
        "constraints",
        "_preprocess_conv2d_image_shape",
        "AtrousConvolution1D",
        "__call__",
        "abs",
        "learning_phase",
        "placeholder",
        "_is_input_shape_compatible",
        "all",
        "mean",
        "clip",
        "conv2d_transpose",
        "ctc_decode",
        "dot",
        "in_test_phase",
        "variable",
        "exp",
        "one_hot",
        "AtrousConvolution2D",
        "_postprocess_conv2d_output",
        "resize_images",
        "max",
        "sum",
        "_old_normalize_batch_in_training",
        "_prepare_name",
        "_to_tensor",
        "minimum",
        "_moments",
        "foldr",
        "is_keras_tensor",
        "argmax",
        "_reshape_dummy_dim",
        "zeros",
        "set_value",
        "reset_uids",
        "_preprocess_conv3d_volume_shape",
        "_arguments_validation",
        "test_initial_states_as_other_inputs",
        "tile",
        "flatten",
        "pow",
        "to_dense",
        "_preprocess_conv2d_filter_shape",
        "get_updates_for",
        "argmin",
        "conv1d",
        "round",
        "_generate_dropout_mask",
        "random_uniform_variable",
        "rnn_test",
        "temporal_padding",
        "set_session",
        "repeat",
        "bias_initializer",
        "get_value",
        "is_sparse",
        "use_bias",
        "state_size",
        "transpose",
        "ndim",
        "gradients",
        "update",
        "from_config",
        "_postprocess_conv3d_output",
        "name_scope",
        "arange",
        "min",
        "greater_equal",
        "not_equal",
        "get_initial_state",
        "ctc_create_skip_idxs",
        "gather",
        "maximum"
      ],
      "functions_name_co_evolved_modified_file": [
        "trainable_weights",
        "recurrent_regularizer",
        "kernel_constraint",
        "kernel_regularizer",
        "reset_states",
        "recurrent_constraint",
        "_generate_recurrent_dropout_mask",
        "dropout",
        "bias_constraint",
        "losses",
        "recurrent_activation",
        "get_config",
        "set_weights",
        "step",
        "recurrent_initializer",
        "get_weights",
        "unit_forget_bias",
        "states",
        "units",
        "preprocess_input",
        "_generate_dropout_mask",
        "call",
        "_time_distributed_dense",
        "bias_initializer",
        "get_losses_for",
        "non_trainable_weights",
        "kernel_initializer",
        "use_bias",
        "state_size",
        "recurrent_dropout",
        "bias_regularizer",
        "__init__",
        "implementation",
        "__call__",
        "from_config",
        "get_constants",
        "compute_output_shape",
        "get_initial_state",
        "activation"
      ],
      "functions_name_co_evolved_all_files": [
        "trainable_weights",
        "recurrent_regularizer",
        "kernel_constraint",
        "kernel_regularizer",
        "test_stacked_rnn_attributes",
        "reset_states",
        "recurrent_constraint",
        "_generate_recurrent_dropout_mask",
        "test_unroll_true_throws_exception_with_one_timestep",
        "dropout",
        "bias_constraint",
        "losses",
        "test_masking_correctness",
        "recurrent_activation",
        "get_config",
        "test_dropout",
        "set_weights",
        "test_minimal_rnn_cell_non_layer_multiple_states",
        "step",
        "recurrent_initializer",
        "test_masking_layer",
        "get_weights",
        "unit_forget_bias",
        "states",
        "rnn",
        "test_minimal_rnn_cell_non_layer",
        "test_implementation_mode",
        "units",
        "preprocess_input",
        "test_regularizer",
        "_generate_dropout_mask",
        "call",
        "test_minimal_rnn_cell_layer",
        "_time_distributed_dense",
        "bias_initializer",
        "get_losses_for",
        "non_trainable_weights",
        "kernel_initializer",
        "use_bias",
        "_static_rnn",
        "state_size",
        "recurrent_dropout",
        "constraints",
        "bias_regularizer",
        "__init__",
        "implementation",
        "compute_mask",
        "__call__",
        "from_config",
        "get_constants",
        "compute_output_shape",
        "test_stateful_invalid_use",
        "test_statefulness",
        "get_initial_state",
        "activation"
      ]
    },
    "file": {
      "file_name": "recurrent.py",
      "file_nloc": 1666,
      "file_complexity": 263,
      "file_token_count": 8604,
      "file_before": "# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import\nimport numpy as np\n\nfrom .. import backend as K\nfrom .. import activations\nfrom .. import initializers\nfrom .. import regularizers\nfrom .. import constraints\nfrom ..engine import Layer\nfrom ..engine import InputSpec\nfrom ..legacy import interfaces\n\n\ndef _time_distributed_dense(x, w, b=None, dropout=None,\n                            input_dim=None, output_dim=None,\n                            timesteps=None, training=None):\n    \"\"\"Apply `y . w + b` for every temporal slice y of x.\n\n    # Arguments\n        x: input tensor.\n        w: weight matrix.\n        b: optional bias vector.\n        dropout: whether to apply dropout (same dropout mask\n            for every temporal slice of the input).\n        input_dim: integer; optional dimensionality of the input.\n        output_dim: integer; optional dimensionality of the output.\n        timesteps: integer; optional number of timesteps.\n        training: training phase tensor or boolean.\n\n    # Returns\n        Output tensor.\n    \"\"\"\n    if not input_dim:\n        input_dim = K.shape(x)[2]\n    if not timesteps:\n        timesteps = K.shape(x)[1]\n    if not output_dim:\n        output_dim = K.shape(w)[1]\n\n    if dropout is not None and 0. < dropout < 1.:\n        # apply the same dropout pattern at every timestep\n        ones = K.ones_like(K.reshape(x[:, 0, :], (-1, input_dim)))\n        dropout_matrix = K.dropout(ones, dropout)\n        expanded_dropout_matrix = K.repeat(dropout_matrix, timesteps)\n        x = K.in_train_phase(x * expanded_dropout_matrix, x, training=training)\n\n    # collapse time dimension and batch dimension together\n    x = K.reshape(x, (-1, input_dim))\n    x = K.dot(x, w)\n    if b is not None:\n        x = K.bias_add(x, b)\n    # reshape to 3D tensor\n    if K.backend() == 'tensorflow':\n        x = K.reshape(x, K.stack([-1, timesteps, output_dim]))\n        x.set_shape([None, None, output_dim])\n    else:\n        x = K.reshape(x, (-1, timesteps, output_dim))\n    return x\n\n\nclass Recurrent(Layer):\n    \"\"\"Abstract base class for recurrent layers.\n\n    Do not use in a model -- it's not a valid layer!\n    Use its children classes `LSTM`, `GRU` and `SimpleRNN` instead.\n\n    All recurrent layers (`LSTM`, `GRU`, `SimpleRNN`) also\n    follow the specifications of this class and accept\n    the keyword arguments listed below.\n\n    # Example\n\n    ```python\n        # as the first layer in a Sequential model\n        model = Sequential()\n        model.add(LSTM(32, input_shape=(10, 64)))\n        # now model.output_shape == (None, 32)\n        # note: `None` is the batch dimension.\n\n        # for subsequent layers, no need to specify the input size:\n        model.add(LSTM(16))\n\n        # to stack recurrent layers, you must use return_sequences=True\n        # on any recurrent layer that feeds into another recurrent layer.\n        # note that you only need to specify the input size on the first layer.\n        model = Sequential()\n        model.add(LSTM(64, input_dim=64, input_length=10, return_sequences=True))\n        model.add(LSTM(32, return_sequences=True))\n        model.add(LSTM(10))\n    ```\n\n    # Arguments\n        weights: list of Numpy arrays to set as initial weights.\n            The list should have 3 elements, of shapes:\n            `[(input_dim, output_dim), (output_dim, output_dim), (output_dim,)]`.\n        return_sequences: Boolean. Whether to return the last output\n            in the output sequence, or the full sequence.\n        return_state: Boolean. Whether to return the last state\n            in addition to the output.\n        go_backwards: Boolean (default False).\n            If True, process the input sequence backwards and return the\n            reversed sequence.\n        stateful: Boolean (default False). If True, the last state\n            for each sample at index i in a batch will be used as initial\n            state for the sample of index i in the following batch.\n        unroll: Boolean (default False).\n            If True, the network will be unrolled,\n            else a symbolic loop will be used.\n            Unrolling can speed-up a RNN,\n            although it tends to be more memory-intensive.\n            Unrolling is only suitable for short sequences.\n        implementation: one of {0, 1, or 2}.\n            If set to 0, the RNN will use\n            an implementation that uses fewer, larger matrix products,\n            thus running faster on CPU but consuming more memory.\n            If set to 1, the RNN will use more matrix products,\n            but smaller ones, thus running slower\n            (may actually be faster on GPU) while consuming less memory.\n            If set to 2 (LSTM/GRU only),\n            the RNN will combine the input gate,\n            the forget gate and the output gate into a single matrix,\n            enabling more time-efficient parallelization on the GPU.\n            Note: RNN dropout must be shared for all gates,\n            resulting in a slightly reduced regularization.\n        input_dim: dimensionality of the input (integer).\n            This argument (or alternatively, the keyword argument `input_shape`)\n            is required when using this layer as the first layer in a model.\n        input_length: Length of input sequences, to be specified\n            when it is constant.\n            This argument is required if you are going to connect\n            `Flatten` then `Dense` layers upstream\n            (without it, the shape of the dense outputs cannot be computed).\n            Note that if the recurrent layer is not the first layer\n            in your model, you would need to specify the input length\n            at the level of the first layer\n            (e.g. via the `input_shape` argument)\n\n    # Input shapes\n        3D tensor with shape `(batch_size, timesteps, input_dim)`,\n        (Optional) 2D tensors with shape `(batch_size, output_dim)`.\n\n    # Output shape\n        - if `return_state`: a list of tensors. The first tensor is\n            the output. The remaining tensors are the last states,\n            each with shape `(batch_size, units)`.\n        - if `return_sequences`: 3D tensor with shape\n            `(batch_size, timesteps, units)`.\n        - else, 2D tensor with shape `(batch_size, units)`.\n\n    # Masking\n        This layer supports masking for input data with a variable number\n        of timesteps. To introduce masks to your data,\n        use an [Embedding](embeddings.md) layer with the `mask_zero` parameter\n        set to `True`.\n\n    # Note on using statefulness in RNNs\n        You can set RNN layers to be 'stateful', which means that the states\n        computed for the samples in one batch will be reused as initial states\n        for the samples in the next batch. This assumes a one-to-one mapping\n        between samples in different successive batches.\n\n        To enable statefulness:\n            - specify `stateful=True` in the layer constructor.\n            - specify a fixed batch size for your model, by passing\n                if sequential model:\n                  `batch_input_shape=(...)` to the first layer in your model.\n                else for functional model with 1 or more Input layers:\n                  `batch_shape=(...)` to all the first layers in your model.\n                This is the expected shape of your inputs\n                *including the batch size*.\n                It should be a tuple of integers, e.g. `(32, 10, 100)`.\n            - specify `shuffle=False` when calling fit().\n\n        To reset the states of your model, call `.reset_states()` on either\n        a specific layer, or on your entire model.\n\n    # Note on specifying the initial state of RNNs\n        You can specify the initial state of RNN layers symbolically by\n        calling them with the keyword argument `initial_state`. The value of\n        `initial_state` should be a tensor or list of tensors representing\n        the initial state of the RNN layer.\n\n        You can specify the initial state of RNN layers numerically by\n        calling `reset_states` with the keyword argument `states`. The value of\n        `states` should be a numpy array or list of numpy arrays representing\n        the initial state of the RNN layer.\n    \"\"\"\n\n    def __init__(self, return_sequences=False,\n                 return_state=False,\n                 go_backwards=False,\n                 stateful=False,\n                 unroll=False,\n                 implementation=0,\n                 **kwargs):\n        super(Recurrent, self).__init__(**kwargs)\n        self.return_sequences = return_sequences\n        self.return_state = return_state\n        self.go_backwards = go_backwards\n\n        self.stateful = stateful\n        self.unroll = unroll\n        self.implementation = implementation\n        self.supports_masking = True\n        self.input_spec = [InputSpec(ndim=3)]\n        self.state_spec = None\n        self.dropout = 0\n        self.recurrent_dropout = 0\n\n    def compute_output_shape(self, input_shape):\n        if isinstance(input_shape, list):\n            input_shape = input_shape[0]\n\n        if self.return_sequences:\n            output_shape = (input_shape[0], input_shape[1], self.units)\n        else:\n            output_shape = (input_shape[0], self.units)\n\n        if self.return_state:\n            state_shape = [(input_shape[0], self.units) for _ in self.states]\n            return [output_shape] + state_shape\n        else:\n            return output_shape\n\n    def compute_mask(self, inputs, mask):\n        if isinstance(mask, list):\n            mask = mask[0]\n        output_mask = mask if self.return_sequences else None\n        if self.return_state:\n            state_mask = [None for _ in self.states]\n            return [output_mask] + state_mask\n        else:\n            return output_mask\n\n    def step(self, inputs, states):\n        raise NotImplementedError\n\n    def get_constants(self, inputs, training=None):\n        return []\n\n    def get_initial_state(self, inputs):\n        # build an all-zero tensor of shape (samples, output_dim)\n        initial_state = K.zeros_like(inputs)  # (samples, timesteps, input_dim)\n        initial_state = K.sum(initial_state, axis=(1, 2))  # (samples,)\n        initial_state = K.expand_dims(initial_state)  # (samples, 1)\n        initial_state = K.tile(initial_state, [1, self.units])  # (samples, output_dim)\n        initial_state = [initial_state for _ in range(len(self.states))]\n        return initial_state\n\n    def preprocess_input(self, inputs, training=None):\n        return inputs\n\n    def __call__(self, inputs, initial_state=None, **kwargs):\n\n        # If there are multiple inputs, then\n        # they should be the main input and `initial_state`\n        # e.g. when loading model from file\n        if isinstance(inputs, (list, tuple)) and len(inputs) > 1 and initial_state is None:\n            initial_state = inputs[1:]\n            inputs = inputs[0]\n\n        # If `initial_state` is specified,\n        # and if it a Keras tensor,\n        # then add it to the inputs and temporarily\n        # modify the input spec to include the state.\n        if initial_state is None:\n            return super(Recurrent, self).__call__(inputs, **kwargs)\n\n        if not isinstance(initial_state, (list, tuple)):\n            initial_state = [initial_state]\n\n        is_keras_tensor = hasattr(initial_state[0], '_keras_history')\n        for tensor in initial_state:\n            if hasattr(tensor, '_keras_history') != is_keras_tensor:\n                raise ValueError('The initial state of an RNN layer cannot be'\n                                 ' specified with a mix of Keras tensors and'\n                                 ' non-Keras tensors')\n\n        if is_keras_tensor:\n            # Compute the full input spec, including state\n            input_spec = self.input_spec\n            state_spec = self.state_spec\n            if not isinstance(input_spec, list):\n                input_spec = [input_spec]\n            if not isinstance(state_spec, list):\n                state_spec = [state_spec]\n            self.input_spec = input_spec + state_spec\n\n            # Compute the full inputs, including state\n            inputs = [inputs] + list(initial_state)\n\n            # Perform the call\n            output = super(Recurrent, self).__call__(inputs, **kwargs)\n\n            # Restore original input spec\n            self.input_spec = input_spec\n            return output\n        else:\n            kwargs['initial_state'] = initial_state\n            return super(Recurrent, self).__call__(inputs, **kwargs)\n\n    def call(self, inputs, mask=None, training=None, initial_state=None):\n        # input shape: `(samples, time (padded with zeros), input_dim)`\n        # note that the .build() method of subclasses MUST define\n        # self.input_spec and self.state_spec with complete input shapes.\n        if isinstance(inputs, list):\n            initial_state = inputs[1:]\n            inputs = inputs[0]\n        elif initial_state is not None:\n            pass\n        elif self.stateful:\n            initial_state = self.states\n        else:\n            initial_state = self.get_initial_state(inputs)\n\n        if isinstance(mask, list):\n            mask = mask[0]\n\n        if len(initial_state) != len(self.states):\n            raise ValueError('Layer has ' + str(len(self.states)) +\n                             ' states but was passed ' +\n                             str(len(initial_state)) +\n                             ' initial states.')\n        input_shape = K.int_shape(inputs)\n        timesteps = input_shape[1]\n        if self.unroll and timesteps in [None, 1]:\n            raise ValueError('Cannot unroll a RNN if the '\n                             'time dimension is undefined or equal to 1. \\n'\n                             '- If using a Sequential model, '\n                             'specify the time dimension by passing '\n                             'an `input_shape` or `batch_input_shape` '\n                             'argument to your first layer. If your '\n                             'first layer is an Embedding, you can '\n                             'also use the `input_length` argument.\\n'\n                             '- If using the functional API, specify '\n                             'the time dimension by passing a `shape` '\n                             'or `batch_shape` argument to your Input layer.')\n        constants = self.get_constants(inputs, training=None)\n        preprocessed_input = self.preprocess_input(inputs, training=None)\n        last_output, outputs, states = K.rnn(self.step,\n                                             preprocessed_input,\n                                             initial_state,\n                                             go_backwards=self.go_backwards,\n                                             mask=mask,\n                                             constants=constants,\n                                             unroll=self.unroll,\n                                             input_length=timesteps)\n        if self.stateful:\n            updates = []\n            for i in range(len(states)):\n                updates.append((self.states[i], states[i]))\n            self.add_update(updates, inputs)\n\n        # Properly set learning phase\n        if 0 < self.dropout + self.recurrent_dropout:\n            last_output._uses_learning_phase = True\n            outputs._uses_learning_phase = True\n\n        if self.return_sequences:\n            output = outputs\n        else:\n            output = last_output\n\n        if self.return_state:\n            if not isinstance(states, (list, tuple)):\n                states = [states]\n            else:\n                states = list(states)\n            return [output] + states\n        else:\n            return output\n\n    def reset_states(self, states=None):\n        if not self.stateful:\n            raise AttributeError('Layer must be stateful.')\n        batch_size = self.input_spec[0].shape[0]\n        if not batch_size:\n            raise ValueError('If a RNN is stateful, it needs to know '\n                             'its batch size. Specify the batch size '\n                             'of your input tensors: \\n'\n                             '- If using a Sequential model, '\n                             'specify the batch size by passing '\n                             'a `batch_input_shape` '\n                             'argument to your first layer.\\n'\n                             '- If using the functional API, specify '\n                             'the time dimension by passing a '\n                             '`batch_shape` argument to your Input layer.')\n        # initialize state if None\n        if self.states[0] is None:\n            self.states = [K.zeros((batch_size, self.units))\n                           for _ in self.states]\n        elif states is None:\n            for state in self.states:\n                K.set_value(state, np.zeros((batch_size, self.units)))\n        else:\n            if not isinstance(states, (list, tuple)):\n                states = [states]\n            if len(states) != len(self.states):\n                raise ValueError('Layer ' + self.name + ' expects ' +\n                                 str(len(self.states)) + ' states, '\n                                 'but it received ' + str(len(states)) +\n                                 ' state values. Input received: ' +\n                                 str(states))\n            for index, (value, state) in enumerate(zip(states, self.states)):\n                if value.shape != (batch_size, self.units):\n                    raise ValueError('State ' + str(index) +\n                                     ' is incompatible with layer ' +\n                                     self.name + ': expected shape=' +\n                                     str((batch_size, self.units)) +\n                                     ', found shape=' + str(value.shape))\n                K.set_value(state, value)\n\n    def get_config(self):\n        config = {'return_sequences': self.return_sequences,\n                  'return_state': self.return_state,\n                  'go_backwards': self.go_backwards,\n                  'stateful': self.stateful,\n                  'unroll': self.unroll,\n                  'implementation': self.implementation}\n        base_config = super(Recurrent, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\n\nclass SimpleRNN(Recurrent):\n    \"\"\"Fully-connected RNN where the output is to be fed back to input.\n\n    # Arguments\n        units: Positive integer, dimensionality of the output space.\n        activation: Activation function to use\n            (see [activations](../activations.md)).\n            If you pass None, no activation is applied\n            (ie. \"linear\" activation: `a(x) = x`).\n        use_bias: Boolean, whether the layer uses a bias vector.\n        kernel_initializer: Initializer for the `kernel` weights matrix,\n            used for the linear transformation of the inputs.\n            (see [initializers](../initializers.md)).\n        recurrent_initializer: Initializer for the `recurrent_kernel`\n            weights matrix,\n            used for the linear transformation of the recurrent state.\n            (see [initializers](../initializers.md)).\n        bias_initializer: Initializer for the bias vector\n            (see [initializers](../initializers.md)).\n        kernel_regularizer: Regularizer function applied to\n            the `kernel` weights matrix\n            (see [regularizer](../regularizers.md)).\n        recurrent_regularizer: Regularizer function applied to\n            the `recurrent_kernel` weights matrix\n            (see [regularizer](../regularizers.md)).\n        bias_regularizer: Regularizer function applied to the bias vector\n            (see [regularizer](../regularizers.md)).\n        activity_regularizer: Regularizer function applied to\n            the output of the layer (its \"activation\").\n            (see [regularizer](../regularizers.md)).\n        kernel_constraint: Constraint function applied to\n            the `kernel` weights matrix\n            (see [constraints](../constraints.md)).\n        recurrent_constraint: Constraint function applied to\n            the `recurrent_kernel` weights matrix\n            (see [constraints](../constraints.md)).\n        bias_constraint: Constraint function applied to the bias vector\n            (see [constraints](../constraints.md)).\n        dropout: Float between 0 and 1.\n            Fraction of the units to drop for\n            the linear transformation of the inputs.\n        recurrent_dropout: Float between 0 and 1.\n            Fraction of the units to drop for\n            the linear transformation of the recurrent state.\n\n    # References\n        - [A Theoretically Grounded Application of Dropout in Recurrent Neural Networks](http://arxiv.org/abs/1512.05287)\n    \"\"\"\n\n    @interfaces.legacy_recurrent_support\n    def __init__(self, units,\n                 activation='tanh',\n                 use_bias=True,\n                 kernel_initializer='glorot_uniform',\n                 recurrent_initializer='orthogonal',\n                 bias_initializer='zeros',\n                 kernel_regularizer=None,\n                 recurrent_regularizer=None,\n                 bias_regularizer=None,\n                 activity_regularizer=None,\n                 kernel_constraint=None,\n                 recurrent_constraint=None,\n                 bias_constraint=None,\n                 dropout=0.,\n                 recurrent_dropout=0.,\n                 **kwargs):\n        super(SimpleRNN, self).__init__(**kwargs)\n        self.units = units\n        self.activation = activations.get(activation)\n        self.use_bias = use_bias\n\n        self.kernel_initializer = initializers.get(kernel_initializer)\n        self.recurrent_initializer = initializers.get(recurrent_initializer)\n        self.bias_initializer = initializers.get(bias_initializer)\n\n        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n        self.recurrent_regularizer = regularizers.get(recurrent_regularizer)\n        self.bias_regularizer = regularizers.get(bias_regularizer)\n        self.activity_regularizer = regularizers.get(activity_regularizer)\n\n        self.kernel_constraint = constraints.get(kernel_constraint)\n        self.recurrent_constraint = constraints.get(recurrent_constraint)\n        self.bias_constraint = constraints.get(bias_constraint)\n\n        self.dropout = min(1., max(0., dropout))\n        self.recurrent_dropout = min(1., max(0., recurrent_dropout))\n        self.state_spec = InputSpec(shape=(None, self.units))\n\n    def build(self, input_shape):\n        if isinstance(input_shape, list):\n            input_shape = input_shape[0]\n\n        batch_size = input_shape[0] if self.stateful else None\n        self.input_dim = input_shape[2]\n        self.input_spec[0] = InputSpec(shape=(batch_size, None, self.input_dim))\n\n        self.states = [None]\n        if self.stateful:\n            self.reset_states()\n\n        self.kernel = self.add_weight(shape=(self.input_dim, self.units),\n                                      name='kernel',\n                                      initializer=self.kernel_initializer,\n                                      regularizer=self.kernel_regularizer,\n                                      constraint=self.kernel_constraint)\n        self.recurrent_kernel = self.add_weight(\n            shape=(self.units, self.units),\n            name='recurrent_kernel',\n            initializer=self.recurrent_initializer,\n            regularizer=self.recurrent_regularizer,\n            constraint=self.recurrent_constraint)\n        if self.use_bias:\n            self.bias = self.add_weight(shape=(self.units,),\n                                        name='bias',\n                                        initializer=self.bias_initializer,\n                                        regularizer=self.bias_regularizer,\n                                        constraint=self.bias_constraint)\n        else:\n            self.bias = None\n        self.built = True\n\n    def preprocess_input(self, inputs, training=None):\n        if self.implementation > 0:\n            return inputs\n        else:\n            input_shape = K.int_shape(inputs)\n            input_dim = input_shape[2]\n            timesteps = input_shape[1]\n            return _time_distributed_dense(inputs,\n                                           self.kernel,\n                                           self.bias,\n                                           self.dropout,\n                                           input_dim,\n                                           self.units,\n                                           timesteps,\n                                           training=training)\n\n    def step(self, inputs, states):\n        if self.implementation == 0:\n            h = inputs\n        else:\n            if 0 < self.dropout < 1:\n                h = K.dot(inputs * states[1], self.kernel)\n            else:\n                h = K.dot(inputs, self.kernel)\n            if self.bias is not None:\n                h = K.bias_add(h, self.bias)\n\n        prev_output = states[0]\n        if 0 < self.recurrent_dropout < 1:\n            prev_output *= states[2]\n        output = h + K.dot(prev_output, self.recurrent_kernel)\n        if self.activation is not None:\n            output = self.activation(output)\n\n        # Properly set learning phase on output tensor.\n        if 0 < self.dropout + self.recurrent_dropout:\n            output._uses_learning_phase = True\n        return output, [output]\n\n    def get_constants(self, inputs, training=None):\n        constants = []\n        if self.implementation != 0 and 0 < self.dropout < 1:\n            input_shape = K.int_shape(inputs)\n            input_dim = input_shape[-1]\n            ones = K.ones_like(K.reshape(inputs[:, 0, 0], (-1, 1)))\n            ones = K.tile(ones, (1, int(input_dim)))\n\n            def dropped_inputs():\n                return K.dropout(ones, self.dropout)\n\n            dp_mask = K.in_train_phase(dropped_inputs,\n                                       ones,\n                                       training=training)\n            constants.append(dp_mask)\n        else:\n            constants.append(K.cast_to_floatx(1.))\n\n        if 0 < self.recurrent_dropout < 1:\n            ones = K.ones_like(K.reshape(inputs[:, 0, 0], (-1, 1)))\n            ones = K.tile(ones, (1, self.units))\n\n            def dropped_inputs():\n                return K.dropout(ones, self.recurrent_dropout)\n            rec_dp_mask = K.in_train_phase(dropped_inputs,\n                                           ones,\n                                           training=training)\n            constants.append(rec_dp_mask)\n        else:\n            constants.append(K.cast_to_floatx(1.))\n        return constants\n\n    def get_config(self):\n        config = {'units': self.units,\n                  'activation': activations.serialize(self.activation),\n                  'use_bias': self.use_bias,\n                  'kernel_initializer': initializers.serialize(self.kernel_initializer),\n                  'recurrent_initializer': initializers.serialize(self.recurrent_initializer),\n                  'bias_initializer': initializers.serialize(self.bias_initializer),\n                  'kernel_regularizer': regularizers.serialize(self.kernel_regularizer),\n                  'recurrent_regularizer': regularizers.serialize(self.recurrent_regularizer),\n                  'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n                  'activity_regularizer': regularizers.serialize(self.activity_regularizer),\n                  'kernel_constraint': constraints.serialize(self.kernel_constraint),\n                  'recurrent_constraint': constraints.serialize(self.recurrent_constraint),\n                  'bias_constraint': constraints.serialize(self.bias_constraint),\n                  'dropout': self.dropout,\n                  'recurrent_dropout': self.recurrent_dropout}\n        base_config = super(SimpleRNN, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\n\nclass GRU(Recurrent):\n    \"\"\"Gated Recurrent Unit - Cho et al. 2014.\n\n    # Arguments\n        units: Positive integer, dimensionality of the output space.\n        activation: Activation function to use\n            (see [activations](../activations.md)).\n            If you pass None, no activation is applied\n            (ie. \"linear\" activation: `a(x) = x`).\n        recurrent_activation: Activation function to use\n            for the recurrent step\n            (see [activations](../activations.md)).\n        use_bias: Boolean, whether the layer uses a bias vector.\n        kernel_initializer: Initializer for the `kernel` weights matrix,\n            used for the linear transformation of the inputs.\n            (see [initializers](../initializers.md)).\n        recurrent_initializer: Initializer for the `recurrent_kernel`\n            weights matrix,\n            used for the linear transformation of the recurrent state.\n            (see [initializers](../initializers.md)).\n        bias_initializer: Initializer for the bias vector\n            (see [initializers](../initializers.md)).\n        kernel_regularizer: Regularizer function applied to\n            the `kernel` weights matrix\n            (see [regularizer](../regularizers.md)).\n        recurrent_regularizer: Regularizer function applied to\n            the `recurrent_kernel` weights matrix\n            (see [regularizer](../regularizers.md)).\n        bias_regularizer: Regularizer function applied to the bias vector\n            (see [regularizer](../regularizers.md)).\n        activity_regularizer: Regularizer function applied to\n            the output of the layer (its \"activation\").\n            (see [regularizer](../regularizers.md)).\n        kernel_constraint: Constraint function applied to\n            the `kernel` weights matrix\n            (see [constraints](../constraints.md)).\n        recurrent_constraint: Constraint function applied to\n            the `recurrent_kernel` weights matrix\n            (see [constraints](../constraints.md)).\n        bias_constraint: Constraint function applied to the bias vector\n            (see [constraints](../constraints.md)).\n        dropout: Float between 0 and 1.\n            Fraction of the units to drop for\n            the linear transformation of the inputs.\n        recurrent_dropout: Float between 0 and 1.\n            Fraction of the units to drop for\n            the linear transformation of the recurrent state.\n\n    # References\n        - [On the Properties of Neural Machine Translation: Encoder-Decoder Approaches](https://arxiv.org/abs/1409.1259)\n        - [Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling](http://arxiv.org/abs/1412.3555v1)\n        - [A Theoretically Grounded Application of Dropout in Recurrent Neural Networks](http://arxiv.org/abs/1512.05287)\n    \"\"\"\n\n    @interfaces.legacy_recurrent_support\n    def __init__(self, units,\n                 activation='tanh',\n                 recurrent_activation='hard_sigmoid',\n                 use_bias=True,\n                 kernel_initializer='glorot_uniform',\n                 recurrent_initializer='orthogonal',\n                 bias_initializer='zeros',\n                 kernel_regularizer=None,\n                 recurrent_regularizer=None,\n                 bias_regularizer=None,\n                 activity_regularizer=None,\n                 kernel_constraint=None,\n                 recurrent_constraint=None,\n                 bias_constraint=None,\n                 dropout=0.,\n                 recurrent_dropout=0.,\n                 **kwargs):\n        super(GRU, self).__init__(**kwargs)\n        self.units = units\n        self.activation = activations.get(activation)\n        self.recurrent_activation = activations.get(recurrent_activation)\n        self.use_bias = use_bias\n\n        self.kernel_initializer = initializers.get(kernel_initializer)\n        self.recurrent_initializer = initializers.get(recurrent_initializer)\n        self.bias_initializer = initializers.get(bias_initializer)\n\n        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n        self.recurrent_regularizer = regularizers.get(recurrent_regularizer)\n        self.bias_regularizer = regularizers.get(bias_regularizer)\n        self.activity_regularizer = regularizers.get(activity_regularizer)\n\n        self.kernel_constraint = constraints.get(kernel_constraint)\n        self.recurrent_constraint = constraints.get(recurrent_constraint)\n        self.bias_constraint = constraints.get(bias_constraint)\n\n        self.dropout = min(1., max(0., dropout))\n        self.recurrent_dropout = min(1., max(0., recurrent_dropout))\n        self.state_spec = InputSpec(shape=(None, self.units))\n\n    def build(self, input_shape):\n        if isinstance(input_shape, list):\n            input_shape = input_shape[0]\n\n        batch_size = input_shape[0] if self.stateful else None\n        self.input_dim = input_shape[2]\n        self.input_spec[0] = InputSpec(shape=(batch_size, None, self.input_dim))\n\n        self.states = [None]\n        if self.stateful:\n            self.reset_states()\n\n        self.kernel = self.add_weight(shape=(self.input_dim, self.units * 3),\n                                      name='kernel',\n                                      initializer=self.kernel_initializer,\n                                      regularizer=self.kernel_regularizer,\n                                      constraint=self.kernel_constraint)\n        self.recurrent_kernel = self.add_weight(\n            shape=(self.units, self.units * 3),\n            name='recurrent_kernel',\n            initializer=self.recurrent_initializer,\n            regularizer=self.recurrent_regularizer,\n            constraint=self.recurrent_constraint)\n\n        if self.use_bias:\n            self.bias = self.add_weight(shape=(self.units * 3,),\n                                        name='bias',\n                                        initializer=self.bias_initializer,\n                                        regularizer=self.bias_regularizer,\n                                        constraint=self.bias_constraint)\n        else:\n            self.bias = None\n\n        self.kernel_z = self.kernel[:, :self.units]\n        self.recurrent_kernel_z = self.recurrent_kernel[:, :self.units]\n        self.kernel_r = self.kernel[:, self.units: self.units * 2]\n        self.recurrent_kernel_r = self.recurrent_kernel[:,\n                                                        self.units:\n                                                        self.units * 2]\n        self.kernel_h = self.kernel[:, self.units * 2:]\n        self.recurrent_kernel_h = self.recurrent_kernel[:, self.units * 2:]\n\n        if self.use_bias:\n            self.bias_z = self.bias[:self.units]\n            self.bias_r = self.bias[self.units: self.units * 2]\n            self.bias_h = self.bias[self.units * 2:]\n        else:\n            self.bias_z = None\n            self.bias_r = None\n            self.bias_h = None\n        self.built = True\n\n    def preprocess_input(self, inputs, training=None):\n        if self.implementation == 0:\n            input_shape = K.int_shape(inputs)\n            input_dim = input_shape[2]\n            timesteps = input_shape[1]\n\n            x_z = _time_distributed_dense(inputs, self.kernel_z, self.bias_z,\n                                          self.dropout, input_dim, self.units,\n                                          timesteps, training=training)\n            x_r = _time_distributed_dense(inputs, self.kernel_r, self.bias_r,\n                                          self.dropout, input_dim, self.units,\n                                          timesteps, training=training)\n            x_h = _time_distributed_dense(inputs, self.kernel_h, self.bias_h,\n                                          self.dropout, input_dim, self.units,\n                                          timesteps, training=training)\n            return K.concatenate([x_z, x_r, x_h], axis=2)\n        else:\n            return inputs\n\n    def get_constants(self, inputs, training=None):\n        constants = []\n        if self.implementation != 0 and 0 < self.dropout < 1:\n            input_shape = K.int_shape(inputs)\n            input_dim = input_shape[-1]\n            ones = K.ones_like(K.reshape(inputs[:, 0, 0], (-1, 1)))\n            ones = K.tile(ones, (1, int(input_dim)))\n\n            def dropped_inputs():\n                return K.dropout(ones, self.dropout)\n\n            dp_mask = [K.in_train_phase(dropped_inputs,\n                                        ones,\n                                        training=training) for _ in range(3)]\n            constants.append(dp_mask)\n        else:\n            constants.append([K.cast_to_floatx(1.) for _ in range(3)])\n\n        if 0 < self.recurrent_dropout < 1:\n            ones = K.ones_like(K.reshape(inputs[:, 0, 0], (-1, 1)))\n            ones = K.tile(ones, (1, self.units))\n\n            def dropped_inputs():\n                return K.dropout(ones, self.recurrent_dropout)\n            rec_dp_mask = [K.in_train_phase(dropped_inputs,\n                                            ones,\n                                            training=training) for _ in range(3)]\n            constants.append(rec_dp_mask)\n        else:\n            constants.append([K.cast_to_floatx(1.) for _ in range(3)])\n        return constants\n\n    def step(self, inputs, states):\n        h_tm1 = states[0]  # previous memory\n        dp_mask = states[1]  # dropout matrices for recurrent units\n        rec_dp_mask = states[2]\n\n        if self.implementation == 2:\n            matrix_x = K.dot(inputs * dp_mask[0], self.kernel)\n            if self.use_bias:\n                matrix_x = K.bias_add(matrix_x, self.bias)\n            matrix_inner = K.dot(h_tm1 * rec_dp_mask[0],\n                                 self.recurrent_kernel[:, :2 * self.units])\n\n            x_z = matrix_x[:, :self.units]\n            x_r = matrix_x[:, self.units: 2 * self.units]\n            recurrent_z = matrix_inner[:, :self.units]\n            recurrent_r = matrix_inner[:, self.units: 2 * self.units]\n\n            z = self.recurrent_activation(x_z + recurrent_z)\n            r = self.recurrent_activation(x_r + recurrent_r)\n\n            x_h = matrix_x[:, 2 * self.units:]\n            recurrent_h = K.dot(r * h_tm1 * rec_dp_mask[0],\n                                self.recurrent_kernel[:, 2 * self.units:])\n            hh = self.activation(x_h + recurrent_h)\n        else:\n            if self.implementation == 0:\n                x_z = inputs[:, :self.units]\n                x_r = inputs[:, self.units: 2 * self.units]\n                x_h = inputs[:, 2 * self.units:]\n            elif self.implementation == 1:\n                x_z = K.dot(inputs * dp_mask[0], self.kernel_z)\n                x_r = K.dot(inputs * dp_mask[1], self.kernel_r)\n                x_h = K.dot(inputs * dp_mask[2], self.kernel_h)\n                if self.use_bias:\n                    x_z = K.bias_add(x_z, self.bias_z)\n                    x_r = K.bias_add(x_r, self.bias_r)\n                    x_h = K.bias_add(x_h, self.bias_h)\n            else:\n                raise ValueError('Unknown `implementation` mode.')\n            z = self.recurrent_activation(x_z + K.dot(h_tm1 * rec_dp_mask[0],\n                                                      self.recurrent_kernel_z))\n            r = self.recurrent_activation(x_r + K.dot(h_tm1 * rec_dp_mask[1],\n                                                      self.recurrent_kernel_r))\n\n            hh = self.activation(x_h + K.dot(r * h_tm1 * rec_dp_mask[2],\n                                             self.recurrent_kernel_h))\n        h = z * h_tm1 + (1 - z) * hh\n        if 0 < self.dropout + self.recurrent_dropout:\n            h._uses_learning_phase = True\n        return h, [h]\n\n    def get_config(self):\n        config = {'units': self.units,\n                  'activation': activations.serialize(self.activation),\n                  'recurrent_activation': activations.serialize(self.recurrent_activation),\n                  'use_bias': self.use_bias,\n                  'kernel_initializer': initializers.serialize(self.kernel_initializer),\n                  'recurrent_initializer': initializers.serialize(self.recurrent_initializer),\n                  'bias_initializer': initializers.serialize(self.bias_initializer),\n                  'kernel_regularizer': regularizers.serialize(self.kernel_regularizer),\n                  'recurrent_regularizer': regularizers.serialize(self.recurrent_regularizer),\n                  'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n                  'activity_regularizer': regularizers.serialize(self.activity_regularizer),\n                  'kernel_constraint': constraints.serialize(self.kernel_constraint),\n                  'recurrent_constraint': constraints.serialize(self.recurrent_constraint),\n                  'bias_constraint': constraints.serialize(self.bias_constraint),\n                  'dropout': self.dropout,\n                  'recurrent_dropout': self.recurrent_dropout}\n        base_config = super(GRU, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\n\nclass LSTM(Recurrent):\n    \"\"\"Long-Short Term Memory unit - Hochreiter 1997.\n\n    For a step-by-step description of the algorithm, see\n    [this tutorial](http://deeplearning.net/tutorial/lstm.html).\n\n    # Arguments\n        units: Positive integer, dimensionality of the output space.\n        activation: Activation function to use\n            (see [activations](../activations.md)).\n            If you pass None, no activation is applied\n            (ie. \"linear\" activation: `a(x) = x`).\n        recurrent_activation: Activation function to use\n            for the recurrent step\n            (see [activations](../activations.md)).\n        use_bias: Boolean, whether the layer uses a bias vector.\n        kernel_initializer: Initializer for the `kernel` weights matrix,\n            used for the linear transformation of the inputs.\n            (see [initializers](../initializers.md)).\n        recurrent_initializer: Initializer for the `recurrent_kernel`\n            weights matrix,\n            used for the linear transformation of the recurrent state.\n            (see [initializers](../initializers.md)).\n        bias_initializer: Initializer for the bias vector\n            (see [initializers](../initializers.md)).\n        unit_forget_bias: Boolean.\n            If True, add 1 to the bias of the forget gate at initialization.\n            Setting it to true will also force `bias_initializer=\"zeros\"`.\n            This is recommended in [Jozefowicz et al.](http://www.jmlr.org/proceedings/papers/v37/jozefowicz15.pdf)\n        kernel_regularizer: Regularizer function applied to\n            the `kernel` weights matrix\n            (see [regularizer](../regularizers.md)).\n        recurrent_regularizer: Regularizer function applied to\n            the `recurrent_kernel` weights matrix\n            (see [regularizer](../regularizers.md)).\n        bias_regularizer: Regularizer function applied to the bias vector\n            (see [regularizer](../regularizers.md)).\n        activity_regularizer: Regularizer function applied to\n            the output of the layer (its \"activation\").\n            (see [regularizer](../regularizers.md)).\n        kernel_constraint: Constraint function applied to\n            the `kernel` weights matrix\n            (see [constraints](../constraints.md)).\n        recurrent_constraint: Constraint function applied to\n            the `recurrent_kernel` weights matrix\n            (see [constraints](../constraints.md)).\n        bias_constraint: Constraint function applied to the bias vector\n            (see [constraints](../constraints.md)).\n        dropout: Float between 0 and 1.\n            Fraction of the units to drop for\n            the linear transformation of the inputs.\n        recurrent_dropout: Float between 0 and 1.\n            Fraction of the units to drop for\n            the linear transformation of the recurrent state.\n\n    # References\n        - [Long short-term memory](http://www.bioinf.jku.at/publications/older/2604.pdf) (original 1997 paper)\n        - [Learning to forget: Continual prediction with LSTM](http://www.mitpressjournals.org/doi/pdf/10.1162/089976600300015015)\n        - [Supervised sequence labeling with recurrent neural networks](http://www.cs.toronto.edu/~graves/preprint.pdf)\n        - [A Theoretically Grounded Application of Dropout in Recurrent Neural Networks](http://arxiv.org/abs/1512.05287)\n    \"\"\"\n    @interfaces.legacy_recurrent_support\n    def __init__(self, units,\n                 activation='tanh',\n                 recurrent_activation='hard_sigmoid',\n                 use_bias=True,\n                 kernel_initializer='glorot_uniform',\n                 recurrent_initializer='orthogonal',\n                 bias_initializer='zeros',\n                 unit_forget_bias=True,\n                 kernel_regularizer=None,\n                 recurrent_regularizer=None,\n                 bias_regularizer=None,\n                 activity_regularizer=None,\n                 kernel_constraint=None,\n                 recurrent_constraint=None,\n                 bias_constraint=None,\n                 dropout=0.,\n                 recurrent_dropout=0.,\n                 **kwargs):\n        super(LSTM, self).__init__(**kwargs)\n        self.units = units\n        self.activation = activations.get(activation)\n        self.recurrent_activation = activations.get(recurrent_activation)\n        self.use_bias = use_bias\n\n        self.kernel_initializer = initializers.get(kernel_initializer)\n        self.recurrent_initializer = initializers.get(recurrent_initializer)\n        self.bias_initializer = initializers.get(bias_initializer)\n        self.unit_forget_bias = unit_forget_bias\n\n        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n        self.recurrent_regularizer = regularizers.get(recurrent_regularizer)\n        self.bias_regularizer = regularizers.get(bias_regularizer)\n        self.activity_regularizer = regularizers.get(activity_regularizer)\n\n        self.kernel_constraint = constraints.get(kernel_constraint)\n        self.recurrent_constraint = constraints.get(recurrent_constraint)\n        self.bias_constraint = constraints.get(bias_constraint)\n\n        self.dropout = min(1., max(0., dropout))\n        self.recurrent_dropout = min(1., max(0., recurrent_dropout))\n        self.state_spec = [InputSpec(shape=(None, self.units)),\n                           InputSpec(shape=(None, self.units))]\n\n    def build(self, input_shape):\n        if isinstance(input_shape, list):\n            input_shape = input_shape[0]\n\n        batch_size = input_shape[0] if self.stateful else None\n        self.input_dim = input_shape[2]\n        self.input_spec[0] = InputSpec(shape=(batch_size, None, self.input_dim))\n\n        self.states = [None, None]\n        if self.stateful:\n            self.reset_states()\n\n        self.kernel = self.add_weight(shape=(self.input_dim, self.units * 4),\n                                      name='kernel',\n                                      initializer=self.kernel_initializer,\n                                      regularizer=self.kernel_regularizer,\n                                      constraint=self.kernel_constraint)\n        self.recurrent_kernel = self.add_weight(\n            shape=(self.units, self.units * 4),\n            name='recurrent_kernel',\n            initializer=self.recurrent_initializer,\n            regularizer=self.recurrent_regularizer,\n            constraint=self.recurrent_constraint)\n\n        if self.use_bias:\n            if self.unit_forget_bias:\n                def bias_initializer(shape, *args, **kwargs):\n                    return K.concatenate([\n                        self.bias_initializer((self.units,), *args, **kwargs),\n                        initializers.Ones()((self.units,), *args, **kwargs),\n                        self.bias_initializer((self.units * 2,), *args, **kwargs),\n                    ])\n            else:\n                bias_initializer = self.bias_initializer\n            self.bias = self.add_weight(shape=(self.units * 4,),\n                                        name='bias',\n                                        initializer=bias_initializer,\n                                        regularizer=self.bias_regularizer,\n                                        constraint=self.bias_constraint)\n        else:\n            self.bias = None\n\n        self.kernel_i = self.kernel[:, :self.units]\n        self.kernel_f = self.kernel[:, self.units: self.units * 2]\n        self.kernel_c = self.kernel[:, self.units * 2: self.units * 3]\n        self.kernel_o = self.kernel[:, self.units * 3:]\n\n        self.recurrent_kernel_i = self.recurrent_kernel[:, :self.units]\n        self.recurrent_kernel_f = self.recurrent_kernel[:, self.units: self.units * 2]\n        self.recurrent_kernel_c = self.recurrent_kernel[:, self.units * 2: self.units * 3]\n        self.recurrent_kernel_o = self.recurrent_kernel[:, self.units * 3:]\n\n        if self.use_bias:\n            self.bias_i = self.bias[:self.units]\n            self.bias_f = self.bias[self.units: self.units * 2]\n            self.bias_c = self.bias[self.units * 2: self.units * 3]\n            self.bias_o = self.bias[self.units * 3:]\n        else:\n            self.bias_i = None\n            self.bias_f = None\n            self.bias_c = None\n            self.bias_o = None\n        self.built = True\n\n    def preprocess_input(self, inputs, training=None):\n        if self.implementation == 0:\n            input_shape = K.int_shape(inputs)\n            input_dim = input_shape[2]\n            timesteps = input_shape[1]\n\n            x_i = _time_distributed_dense(inputs, self.kernel_i, self.bias_i,\n                                          self.dropout, input_dim, self.units,\n                                          timesteps, training=training)\n            x_f = _time_distributed_dense(inputs, self.kernel_f, self.bias_f,\n                                          self.dropout, input_dim, self.units,\n                                          timesteps, training=training)\n            x_c = _time_distributed_dense(inputs, self.kernel_c, self.bias_c,\n                                          self.dropout, input_dim, self.units,\n                                          timesteps, training=training)\n            x_o = _time_distributed_dense(inputs, self.kernel_o, self.bias_o,\n                                          self.dropout, input_dim, self.units,\n                                          timesteps, training=training)\n            return K.concatenate([x_i, x_f, x_c, x_o], axis=2)\n        else:\n            return inputs\n\n    def get_constants(self, inputs, training=None):\n        constants = []\n        if self.implementation != 0 and 0 < self.dropout < 1:\n            input_shape = K.int_shape(inputs)\n            input_dim = input_shape[-1]\n            ones = K.ones_like(K.reshape(inputs[:, 0, 0], (-1, 1)))\n            ones = K.tile(ones, (1, int(input_dim)))\n\n            def dropped_inputs():\n                return K.dropout(ones, self.dropout)\n\n            dp_mask = [K.in_train_phase(dropped_inputs,\n                                        ones,\n                                        training=training) for _ in range(4)]\n            constants.append(dp_mask)\n        else:\n            constants.append([K.cast_to_floatx(1.) for _ in range(4)])\n\n        if 0 < self.recurrent_dropout < 1:\n            ones = K.ones_like(K.reshape(inputs[:, 0, 0], (-1, 1)))\n            ones = K.tile(ones, (1, self.units))\n\n            def dropped_inputs():\n                return K.dropout(ones, self.recurrent_dropout)\n            rec_dp_mask = [K.in_train_phase(dropped_inputs,\n                                            ones,\n                                            training=training) for _ in range(4)]\n            constants.append(rec_dp_mask)\n        else:\n            constants.append([K.cast_to_floatx(1.) for _ in range(4)])\n        return constants\n\n    def step(self, inputs, states):\n        h_tm1 = states[0]\n        c_tm1 = states[1]\n        dp_mask = states[2]\n        rec_dp_mask = states[3]\n\n        if self.implementation == 2:\n            z = K.dot(inputs * dp_mask[0], self.kernel)\n            z += K.dot(h_tm1 * rec_dp_mask[0], self.recurrent_kernel)\n            if self.use_bias:\n                z = K.bias_add(z, self.bias)\n\n            z0 = z[:, :self.units]\n            z1 = z[:, self.units: 2 * self.units]\n            z2 = z[:, 2 * self.units: 3 * self.units]\n            z3 = z[:, 3 * self.units:]\n\n            i = self.recurrent_activation(z0)\n            f = self.recurrent_activation(z1)\n            c = f * c_tm1 + i * self.activation(z2)\n            o = self.recurrent_activation(z3)\n        else:\n            if self.implementation == 0:\n                x_i = inputs[:, :self.units]\n                x_f = inputs[:, self.units: 2 * self.units]\n                x_c = inputs[:, 2 * self.units: 3 * self.units]\n                x_o = inputs[:, 3 * self.units:]\n            elif self.implementation == 1:\n                x_i = K.dot(inputs * dp_mask[0], self.kernel_i) + self.bias_i\n                x_f = K.dot(inputs * dp_mask[1], self.kernel_f) + self.bias_f\n                x_c = K.dot(inputs * dp_mask[2], self.kernel_c) + self.bias_c\n                x_o = K.dot(inputs * dp_mask[3], self.kernel_o) + self.bias_o\n            else:\n                raise ValueError('Unknown `implementation` mode.')\n\n            i = self.recurrent_activation(x_i + K.dot(h_tm1 * rec_dp_mask[0],\n                                                      self.recurrent_kernel_i))\n            f = self.recurrent_activation(x_f + K.dot(h_tm1 * rec_dp_mask[1],\n                                                      self.recurrent_kernel_f))\n            c = f * c_tm1 + i * self.activation(x_c + K.dot(h_tm1 * rec_dp_mask[2],\n                                                            self.recurrent_kernel_c))\n            o = self.recurrent_activation(x_o + K.dot(h_tm1 * rec_dp_mask[3],\n                                                      self.recurrent_kernel_o))\n        h = o * self.activation(c)\n        if 0 < self.dropout + self.recurrent_dropout:\n            h._uses_learning_phase = True\n        return h, [h, c]\n\n    def get_config(self):\n        config = {'units': self.units,\n                  'activation': activations.serialize(self.activation),\n                  'recurrent_activation': activations.serialize(self.recurrent_activation),\n                  'use_bias': self.use_bias,\n                  'kernel_initializer': initializers.serialize(self.kernel_initializer),\n                  'recurrent_initializer': initializers.serialize(self.recurrent_initializer),\n                  'bias_initializer': initializers.serialize(self.bias_initializer),\n                  'unit_forget_bias': self.unit_forget_bias,\n                  'kernel_regularizer': regularizers.serialize(self.kernel_regularizer),\n                  'recurrent_regularizer': regularizers.serialize(self.recurrent_regularizer),\n                  'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n                  'activity_regularizer': regularizers.serialize(self.activity_regularizer),\n                  'kernel_constraint': constraints.serialize(self.kernel_constraint),\n                  'recurrent_constraint': constraints.serialize(self.recurrent_constraint),\n                  'bias_constraint': constraints.serialize(self.bias_constraint),\n                  'dropout': self.dropout,\n                  'recurrent_dropout': self.recurrent_dropout}\n        base_config = super(LSTM, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n",
      "file_after": "# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import\nimport numpy as np\nimport functools\nimport warnings\n\nfrom .. import backend as K\nfrom .. import activations\nfrom .. import initializers\nfrom .. import regularizers\nfrom .. import constraints\nfrom ..engine import Layer\nfrom ..engine import InputSpec\nfrom ..utils.generic_utils import has_arg\n\n# Legacy support.\nfrom ..legacy.layers import Recurrent\nfrom ..legacy import interfaces\n\n\nclass StackedRNNCells(Layer):\n    \"\"\"Wrapper allowing a stack of RNN cells to behave as a single cell.\n\n    # Arguments:\n        cells: List of RNN cell instances.\n    \"\"\"\n\n    def __init__(self, cells, **kwargs):\n        for cell in cells:\n            if not hasattr(cell, 'call'):\n                raise ValueError('All cells must have a `call` method. '\n                                 'received cells:', cells)\n            if not hasattr(cell, 'state_size'):\n                raise ValueError('All cells must have a '\n                                 '`state_size` attribute. '\n                                 'received cells:', cells)\n        self.cells = cells\n        super(StackedRNNCells, self).__init__(**kwargs)\n\n    @property\n    def state_size(self):\n        # States are a flat list\n        # in reverse order of the cell stack.\n        # This allows to preserve the requirement\n        # `stack.state_size[0] == output_dim`.\n        # e.g. states of a 2-layer LSTM would be\n        # `[h2, c2, h1, c1]`\n        # (assuming one LSTM has states [h, c])\n        state_size = []\n        for cell in self.cells[::-1]:\n            if hasattr(cell.state_size, '__len__'):\n                state_size += list(cell.state_size)\n            else:\n                state_size.append(cell.state_size)\n        return tuple(state_size)\n\n    def call(self, inputs, states, **kwargs):\n        # Recover per-cell states.\n        nested_states = []\n        for cell in self.cells[::-1]:\n            if hasattr(cell.state_size, '__len__'):\n                nested_states.append(states[:len(cell.state_size)])\n                states = states[len(cell.state_size):]\n            else:\n                nested_states.append([states[0]])\n                states = states[1:]\n        nested_states = nested_states[::-1]\n\n        # Call the cells in order and store the returned states.\n        new_nested_states = []\n        for cell, states in zip(self.cells, nested_states):\n            inputs, states = cell.call(inputs, states, **kwargs)\n            new_nested_states.append(states)\n\n        # Format the new states as a flat list\n        # in reverse cell order.\n        states = []\n        for cell_states in new_nested_states[::-1]:\n            states += cell_states\n        return inputs, states\n\n    def build(self, input_shape):\n        for cell in self.cells:\n            if isinstance(cell, Layer):\n                cell.build(input_shape)\n            if hasattr(cell.state_size, '__len__'):\n                output_dim = cell.state_size[0]\n            else:\n                output_dim = cell.state_size\n            input_shape = (input_shape[0], input_shape[1], output_dim)\n        self.built = True\n\n    def get_config(self):\n        cells = []\n        for cell in self.cells:\n            cells.append({'class_name': cell.__class__.__name__,\n                          'config': cell.get_config()})\n        config = {'cells': cells}\n        base_config = super(StackedRNNCells, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\n    @classmethod\n    def from_config(cls, config, custom_objects=None):\n        from . import deserialize as deserialize_layer\n        cells = []\n        for cell_config in config.pop('cells'):\n            cells.append(deserialize_layer(cell_config,\n                                           custom_objects=custom_objects))\n        return cls(cells, **config)\n\n    @property\n    def trainable_weights(self):\n        if not self.trainable:\n            return []\n        weights = []\n        for cell in self.cells:\n            if isinstance(cell, Layer):\n                weights += cell.trainable_weights\n        return weights\n\n    @property\n    def non_trainable_weights(self):\n        weights = []\n        for cell in self.cells:\n            if isinstance(cell, Layer):\n                weights += cell.non_trainable_weights\n        if not self.trainable:\n            trainable_weights = []\n            for cell in self.cells:\n                if isinstance(cell, Layer):\n                    trainable_weights += cell.trainable_weights\n            return trainable_weights + weights\n        return weights\n\n    def get_weights(self):\n        \"\"\"Retrieves the weights of the model.\n\n        # Returns\n            A flat list of Numpy arrays.\n        \"\"\"\n        weights = []\n        for cell in self.cells:\n            if isinstance(cell, Layer):\n                weights += cell.weights\n        return K.batch_get_value(weights)\n\n    def set_weights(self, weights):\n        \"\"\"Sets the weights of the model.\n\n        # Arguments\n            weights: A list of Numpy arrays with shapes and types matching\n                the output of `model.get_weights()`.\n        \"\"\"\n        tuples = []\n        for cell in self.cells:\n            if isinstance(cell, Layer):\n                num_param = len(cell.weights)\n                weights = weights[:num_param]\n                for sw, w in zip(cell.weights, weights):\n                    tuples.append((sw, w))\n                weights = weights[num_param:]\n        K.batch_set_value(tuples)\n\n    @property\n    def losses(self):\n        losses = []\n        for cell in self.cells:\n            if isinstance(cell, Layer):\n                cell_losses = cell.losses\n                losses += cell_losses\n        return losses\n\n    def get_losses_for(self, inputs=None):\n        losses = []\n        for cell in self.cells:\n            if isinstance(cell, Layer):\n                cell_losses = cell.get_losses_for(inputs)\n                losses += cell_losses\n        return losses\n\n\nclass RNN(Layer):\n    \"\"\"Base class for recurrent layers.\n\n    # Arguments\n        cell: A RNN cell instance. A RNN cell is a class that has:\n            - a `call(input_at_t, states_at_t)` method, returning\n                `(output_at_t, states_at_t_plus_1)`.\n            - a `state_size` attribute. This can be a single integer\n                (single state) in which case it is\n                the size of the recurrent state\n                (which should be the same as the size of the cell output).\n                This can also be a list/tuple of integers\n                (one size per state). In this case, the first entry\n                (`state_size[0]`) should be the same as\n                the size of the cell output.\n            It is also possible for `cell` to be a list of RNN cell instances,\n            in which cases the cells get stacked on after the other in the RNN,\n            implementing an efficient stacked RNN.\n        return_sequences: Boolean. Whether to return the last output.\n            in the output sequence, or the full sequence.\n        return_state: Boolean. Whether to return the last state\n            in addition to the output.\n        go_backwards: Boolean (default False).\n            If True, process the input sequence backwards and return the\n            reversed sequence.\n        stateful: Boolean (default False). If True, the last state\n            for each sample at index i in a batch will be used as initial\n            state for the sample of index i in the following batch.\n        unroll: Boolean (default False).\n            If True, the network will be unrolled,\n            else a symbolic loop will be used.\n            Unrolling can speed-up a RNN,\n            although it tends to be more memory-intensive.\n            Unrolling is only suitable for short sequences.\n        input_dim: dimensionality of the input (integer).\n            This argument (or alternatively, the keyword argument `input_shape`)\n            is required when using this layer as the first layer in a model.\n        input_length: Length of input sequences, to be specified\n            when it is constant.\n            This argument is required if you are going to connect\n            `Flatten` then `Dense` layers upstream\n            (without it, the shape of the dense outputs cannot be computed).\n            Note that if the recurrent layer is not the first layer\n            in your model, you would need to specify the input length\n            at the level of the first layer\n            (e.g. via the `input_shape` argument)\n\n    # Input shapes\n        3D tensor with shape `(batch_size, timesteps, input_dim)`,\n        (Optional) 2D tensors with shape `(batch_size, output_dim)`.\n\n    # Output shape\n        - if `return_state`: a list of tensors. The first tensor is\n            the output. The remaining tensors are the last states,\n            each with shape `(batch_size, units)`.\n        - if `return_sequences`: 3D tensor with shape\n            `(batch_size, timesteps, units)`.\n        - else, 2D tensor with shape `(batch_size, units)`.\n\n    # Masking\n        This layer supports masking for input data with a variable number\n        of timesteps. To introduce masks to your data,\n        use an [Embedding](embeddings.md) layer with the `mask_zero` parameter\n        set to `True`.\n\n    # Note on using statefulness in RNNs\n        You can set RNN layers to be 'stateful', which means that the states\n        computed for the samples in one batch will be reused as initial states\n        for the samples in the next batch. This assumes a one-to-one mapping\n        between samples in different successive batches.\n\n        To enable statefulness:\n            - specify `stateful=True` in the layer constructor.\n            - specify a fixed batch size for your model, by passing\n                if sequential model:\n                  `batch_input_shape=(...)` to the first layer in your model.\n                else for functional model with 1 or more Input layers:\n                  `batch_shape=(...)` to all the first layers in your model.\n                This is the expected shape of your inputs\n                *including the batch size*.\n                It should be a tuple of integers, e.g. `(32, 10, 100)`.\n            - specify `shuffle=False` when calling fit().\n\n        To reset the states of your model, call `.reset_states()` on either\n        a specific layer, or on your entire model.\n\n    # Note on specifying the initial state of RNNs\n        You can specify the initial state of RNN layers symbolically by\n        calling them with the keyword argument `initial_state`. The value of\n        `initial_state` should be a tensor or list of tensors representing\n        the initial state of the RNN layer.\n\n        You can specify the initial state of RNN layers numerically by\n        calling `reset_states` with the keyword argument `states`. The value of\n        `states` should be a numpy array or list of numpy arrays representing\n        the initial state of the RNN layer.\n\n    # Examples\n\n    ```python\n    # First, let's define a RNN Cell, as a layer subclass.\n\n    class MinimalRNNCell(keras.layers.Layer):\n\n        def __init__(self, units, **kwargs):\n            self.units = units\n            self.state_size = units\n            super(MinimalRNNCell, self).__init__(**kwargs)\n\n        def build(self, input_shape):\n            self.kernel = self.add_weight(shape=(input_shape[-1], self.units),\n                                          initializer='uniform',\n                                          name='kernel')\n            self.recurrent_kernel = self.add_weight(\n                shape=(self.units, self.units),\n                initializer='uniform',\n                name='recurrent_kernel')\n            self.built = True\n\n        def call(self, inputs, states):\n            prev_output = states[0]\n            h = K.dot(inputs, self.kernel)\n            output = h + K.dot(prev_output, self.recurrent_kernel)\n            return output, [output]\n\n    # Let's use this cell in a RNN layer:\n\n    cell = MinimalRNNCell(32)\n    x = keras.Input((None, 5))\n    layer = RNN(cell)\n    y = layer(x)\n\n    # Here's how to use the cell to build a stacked RNN:\n\n    cells = [MinimalRNNCell(32), MinimalRNNCell(64)]\n    x = keras.Input((None, 5))\n    layer = RNN(cells)\n    y = layer(x)\n    ```\n    \"\"\"\n\n    def __init__(self, cell,\n                 return_sequences=False,\n                 return_state=False,\n                 go_backwards=False,\n                 stateful=False,\n                 unroll=False,\n                 **kwargs):\n        if isinstance(cell, (list, tuple)):\n            cell = StackedRNNCells(cell)\n        if not hasattr(cell, 'call'):\n            raise ValueError('`cell` should have a `call` method. '\n                             'The RNN was passed:', cell)\n        if not hasattr(cell, 'state_size'):\n            raise ValueError('The RNN cell should have '\n                             'an attribute `state_size` '\n                             '(tuple of integers, '\n                             'one integer per RNN state).')\n        super(RNN, self).__init__(**kwargs)\n        self.cell = cell\n        self.return_sequences = return_sequences\n        self.return_state = return_state\n        self.go_backwards = go_backwards\n        self.stateful = stateful\n        self.unroll = unroll\n\n        self.supports_masking = True\n        self.input_spec = [InputSpec(ndim=3)]\n        if hasattr(self.cell.state_size, '__len__'):\n            self.state_spec = [InputSpec(shape=(None, dim))\n                               for dim in self.cell.state_size]\n        else:\n            self.state_spec = InputSpec(shape=(None, self.cell.state_size))\n        self._states = None\n\n    @property\n    def states(self):\n        if self._states is None:\n            if isinstance(self.cell.state_size, int):\n                num_states = 1\n            else:\n                num_states = len(self.cell.state_size)\n            return [None for _ in range(num_states)]\n        return self._states\n\n    @states.setter\n    def states(self, states):\n        self._states = states\n\n    def compute_output_shape(self, input_shape):\n        if isinstance(input_shape, list):\n            input_shape = input_shape[0]\n\n        if hasattr(self.cell.state_size, '__len__'):\n            output_dim = self.cell.state_size[0]\n        else:\n            output_dim = self.cell.state_size\n\n        if self.return_sequences:\n            output_shape = (input_shape[0], input_shape[1], output_dim)\n        else:\n            output_shape = (input_shape[0], output_dim)\n\n        if self.return_state:\n            state_shape = [(input_shape[0], output_dim) for _ in self.states]\n            return [output_shape] + state_shape\n        else:\n            return output_shape\n\n    def compute_mask(self, inputs, mask):\n        if isinstance(mask, list):\n            mask = mask[0]\n        output_mask = mask if self.return_sequences else None\n        if self.return_state:\n            state_mask = [None for _ in self.states]\n            return [output_mask] + state_mask\n        else:\n            return output_mask\n\n    def build(self, input_shape):\n        if isinstance(input_shape, list):\n            input_shape = input_shape[0]\n\n        batch_size = input_shape[0] if self.stateful else None\n        input_dim = input_shape[-1]\n        self.input_spec[0] = InputSpec(shape=(batch_size, None, input_dim))\n\n        if self.stateful:\n            self.reset_states()\n\n        if isinstance(self.cell, Layer):\n            step_input_shape = (input_shape[0],) + input_shape[2:]\n            self.cell.build(step_input_shape)\n\n    def get_initial_state(self, inputs):\n        # build an all-zero tensor of shape (samples, output_dim)\n        initial_state = K.zeros_like(inputs)  # (samples, timesteps, input_dim)\n        initial_state = K.sum(initial_state, axis=(1, 2))  # (samples,)\n        initial_state = K.expand_dims(initial_state)  # (samples, 1)\n        if hasattr(self.cell.state_size, '__len__'):\n            return [K.tile(initial_state, [1, dim])\n                    for dim in self.cell.state_size]\n        else:\n            return [K.tile(initial_state, [1, self.cell.state_size])]\n\n    def __call__(self, inputs, initial_state=None, **kwargs):\n        # If there are multiple inputs, then\n        # they should be the main input and `initial_state`\n        # e.g. when loading model from file\n        if isinstance(inputs, (list, tuple)) and len(inputs) > 1 and initial_state is None:\n            initial_state = inputs[1:]\n            inputs = inputs[0]\n\n        # If `initial_state` is specified,\n        # and if it a Keras tensor,\n        # then add it to the inputs and temporarily\n        # modify the input spec to include the state.\n        if initial_state is None:\n            return super(RNN, self).__call__(inputs, **kwargs)\n\n        if not isinstance(initial_state, (list, tuple)):\n            initial_state = [initial_state]\n\n        is_keras_tensor = hasattr(initial_state[0], '_keras_history')\n        for tensor in initial_state:\n            if hasattr(tensor, '_keras_history') != is_keras_tensor:\n                raise ValueError('The initial state of an RNN layer cannot be'\n                                 ' specified with a mix of Keras tensors and'\n                                 ' non-Keras tensors')\n\n        if is_keras_tensor:\n            # Compute the full input spec, including state\n            input_spec = self.input_spec\n            state_spec = self.state_spec\n            if not isinstance(input_spec, list):\n                input_spec = [input_spec]\n            if not isinstance(state_spec, list):\n                state_spec = [state_spec]\n            self.input_spec = input_spec + state_spec\n\n            # Compute the full inputs, including state\n            inputs = [inputs] + list(initial_state)\n\n            # Perform the call\n            output = super(RNN, self).__call__(inputs, **kwargs)\n\n            # Restore original input spec\n            self.input_spec = input_spec\n            return output\n        else:\n            kwargs['initial_state'] = initial_state\n            return super(RNN, self).__call__(inputs, **kwargs)\n\n    def call(self, inputs, mask=None, training=None, initial_state=None):\n        # input shape: `(samples, time (padded with zeros), input_dim)`\n        # note that the .build() method of subclasses MUST define\n        # self.input_spec and self.state_spec with complete input shapes.\n        if isinstance(inputs, list):\n            initial_state = inputs[1:]\n            inputs = inputs[0]\n        elif initial_state is not None:\n            pass\n        elif self.stateful:\n            initial_state = self.states\n        else:\n            initial_state = self.get_initial_state(inputs)\n\n        if isinstance(mask, list):\n            mask = mask[0]\n\n        if len(initial_state) != len(self.states):\n            raise ValueError('Layer has ' + str(len(self.states)) +\n                             ' states but was passed ' +\n                             str(len(initial_state)) +\n                             ' initial states.')\n        input_shape = K.int_shape(inputs)\n        timesteps = input_shape[1]\n        if self.unroll and timesteps in [None, 1]:\n            raise ValueError('Cannot unroll a RNN if the '\n                             'time dimension is undefined or equal to 1. \\n'\n                             '- If using a Sequential model, '\n                             'specify the time dimension by passing '\n                             'an `input_shape` or `batch_input_shape` '\n                             'argument to your first layer. If your '\n                             'first layer is an Embedding, you can '\n                             'also use the `input_length` argument.\\n'\n                             '- If using the functional API, specify '\n                             'the time dimension by passing a `shape` '\n                             'or `batch_shape` argument to your Input layer.')\n\n        if has_arg(self.cell.call, 'training'):\n            step = functools.partial(self.cell.call, training=training)\n        else:\n            step = self.cell.call\n        last_output, outputs, states = K.rnn(step,\n                                             inputs,\n                                             initial_state,\n                                             go_backwards=self.go_backwards,\n                                             mask=mask,\n                                             unroll=self.unroll,\n                                             input_length=timesteps)\n        if self.stateful:\n            updates = []\n            for i in range(len(states)):\n                updates.append((self.states[i], states[i]))\n            self.add_update(updates, inputs)\n\n        if self.return_sequences:\n            output = outputs\n        else:\n            output = last_output\n\n        # Properly set learning phase\n        if getattr(last_output, '_uses_learning_phase', False):\n            output._uses_learning_phase = True\n\n        if self.return_state:\n            if not isinstance(states, (list, tuple)):\n                states = [states]\n            else:\n                states = list(states)\n            return [output] + states\n        else:\n            return output\n\n    def reset_states(self, states=None):\n        if not self.stateful:\n            raise AttributeError('Layer must be stateful.')\n        batch_size = self.input_spec[0].shape[0]\n        if not batch_size:\n            raise ValueError('If a RNN is stateful, it needs to know '\n                             'its batch size. Specify the batch size '\n                             'of your input tensors: \\n'\n                             '- If using a Sequential model, '\n                             'specify the batch size by passing '\n                             'a `batch_input_shape` '\n                             'argument to your first layer.\\n'\n                             '- If using the functional API, specify '\n                             'the time dimension by passing a '\n                             '`batch_shape` argument to your Input layer.')\n        # initialize state if None\n        if self.states[0] is None:\n            if hasattr(self.cell.state_size, '__len__'):\n                self.states = [K.zeros((batch_size, dim))\n                               for dim in self.cell.state_size]\n            else:\n                self.states = [K.zeros((batch_size, self.cell.state_size))]\n        elif states is None:\n            if hasattr(self.cell.state_size, '__len__'):\n                for state, dim in zip(self.states, self.cell.state_size):\n                    K.set_value(state, np.zeros((batch_size, dim)))\n            else:\n                K.set_value(self.states[0],\n                            np.zeros((batch_size, self.cell.state_size)))\n        else:\n            if not isinstance(states, (list, tuple)):\n                states = [states]\n            if len(states) != len(self.states):\n                raise ValueError('Layer ' + self.name + ' expects ' +\n                                 str(len(self.states)) + ' states, '\n                                 'but it received ' + str(len(states)) +\n                                 ' state values. Input received: ' +\n                                 str(states))\n            for index, (value, state) in enumerate(zip(states, self.states)):\n                if hasattr(self.cell.state_size, '__len__'):\n                    dim = self.cell.state_size[index]\n                else:\n                    dim = self.cell.state_size\n                if value.shape != (batch_size, dim):\n                    raise ValueError('State ' + str(index) +\n                                     ' is incompatible with layer ' +\n                                     self.name + ': expected shape=' +\n                                     str((batch_size, dim)) +\n                                     ', found shape=' + str(value.shape))\n                # TODO: consider batch calls to `set_value`.\n                K.set_value(state, value)\n\n    def get_config(self):\n        config = {'return_sequences': self.return_sequences,\n                  'return_state': self.return_state,\n                  'go_backwards': self.go_backwards,\n                  'stateful': self.stateful,\n                  'unroll': self.unroll}\n        cell_config = self.cell.get_config()\n        config['cell'] = {'class_name': self.cell.__class__.__name__,\n                          'config': cell_config}\n        base_config = super(RNN, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\n    @classmethod\n    def from_config(cls, config, custom_objects=None):\n        from . import deserialize as deserialize_layer\n        cell = deserialize_layer(config.pop('cell'),\n                                 custom_objects=custom_objects)\n        return cls(cell, **config)\n\n    @property\n    def trainable_weights(self):\n        if isinstance(self.cell, Layer):\n            return self.cell.trainable_weights\n        return []\n\n    @property\n    def non_trainable_weights(self):\n        if isinstance(self.cell, Layer):\n            return self.cell.non_trainable_weights\n        return []\n\n    @property\n    def losses(self):\n        if isinstance(self.cell, Layer):\n            return self.cell.losses\n        return []\n\n    def get_losses_for(self, inputs=None):\n        if isinstance(self.cell, Layer):\n            cell_losses = self.cell.get_losses_for(inputs)\n            return cell_losses + super(RNN, self).get_losses_for(inputs)\n        return super(RNN, self).get_losses_for(inputs)\n\n\nclass SimpleRNNCell(Layer):\n    \"\"\"Cell class for SimpleRNN.\n\n    # Arguments\n        units: Positive integer, dimensionality of the output space.\n        activation: Activation function to use\n            (see [activations](../activations.md)).\n            If you pass None, no activation is applied\n            (ie. \"linear\" activation: `a(x) = x`).\n        use_bias: Boolean, whether the layer uses a bias vector.\n        kernel_initializer: Initializer for the `kernel` weights matrix,\n            used for the linear transformation of the inputs.\n            (see [initializers](../initializers.md)).\n        recurrent_initializer: Initializer for the `recurrent_kernel`\n            weights matrix,\n            used for the linear transformation of the recurrent state.\n            (see [initializers](../initializers.md)).\n        bias_initializer: Initializer for the bias vector\n            (see [initializers](../initializers.md)).\n        kernel_regularizer: Regularizer function applied to\n            the `kernel` weights matrix\n            (see [regularizer](../regularizers.md)).\n        recurrent_regularizer: Regularizer function applied to\n            the `recurrent_kernel` weights matrix\n            (see [regularizer](../regularizers.md)).\n        bias_regularizer: Regularizer function applied to the bias vector\n            (see [regularizer](../regularizers.md)).\n        activity_regularizer: Regularizer function applied to\n            the output of the layer (its \"activation\").\n            (see [regularizer](../regularizers.md)).\n        kernel_constraint: Constraint function applied to\n            the `kernel` weights matrix\n            (see [constraints](../constraints.md)).\n        recurrent_constraint: Constraint function applied to\n            the `recurrent_kernel` weights matrix\n            (see [constraints](../constraints.md)).\n        bias_constraint: Constraint function applied to the bias vector\n            (see [constraints](../constraints.md)).\n        dropout: Float between 0 and 1.\n            Fraction of the units to drop for\n            the linear transformation of the inputs.\n        recurrent_dropout: Float between 0 and 1.\n            Fraction of the units to drop for\n            the linear transformation of the recurrent state.\n    \"\"\"\n\n    def __init__(self, units,\n                 activation='tanh',\n                 use_bias=True,\n                 kernel_initializer='glorot_uniform',\n                 recurrent_initializer='orthogonal',\n                 bias_initializer='zeros',\n                 kernel_regularizer=None,\n                 recurrent_regularizer=None,\n                 bias_regularizer=None,\n                 kernel_constraint=None,\n                 recurrent_constraint=None,\n                 bias_constraint=None,\n                 dropout=0.,\n                 recurrent_dropout=0.,\n                 **kwargs):\n        super(SimpleRNNCell, self).__init__(**kwargs)\n        self.units = units\n        self.activation = activations.get(activation)\n        self.use_bias = use_bias\n\n        self.kernel_initializer = initializers.get(kernel_initializer)\n        self.recurrent_initializer = initializers.get(recurrent_initializer)\n        self.bias_initializer = initializers.get(bias_initializer)\n\n        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n        self.recurrent_regularizer = regularizers.get(recurrent_regularizer)\n        self.bias_regularizer = regularizers.get(bias_regularizer)\n\n        self.kernel_constraint = constraints.get(kernel_constraint)\n        self.recurrent_constraint = constraints.get(recurrent_constraint)\n        self.bias_constraint = constraints.get(bias_constraint)\n\n        self.dropout = min(1., max(0., dropout))\n        self.recurrent_dropout = min(1., max(0., recurrent_dropout))\n        self.state_size = self.units\n        self._dropout_mask = None\n        self._recurrent_dropout_mask = None\n\n    def build(self, input_shape):\n        self.kernel = self.add_weight(shape=(input_shape[-1], self.units),\n                                      name='kernel',\n                                      initializer=self.kernel_initializer,\n                                      regularizer=self.kernel_regularizer,\n                                      constraint=self.kernel_constraint)\n        self.recurrent_kernel = self.add_weight(\n            shape=(self.units, self.units),\n            name='recurrent_kernel',\n            initializer=self.recurrent_initializer,\n            regularizer=self.recurrent_regularizer,\n            constraint=self.recurrent_constraint)\n        if self.use_bias:\n            self.bias = self.add_weight(shape=(self.units,),\n                                        name='bias',\n                                        initializer=self.bias_initializer,\n                                        regularizer=self.bias_regularizer,\n                                        constraint=self.bias_constraint)\n        else:\n            self.bias = None\n        self.built = True\n\n    def _generate_dropout_mask(self, inputs, training=None):\n        if 0 < self.dropout < 1:\n            ones = K.ones_like(K.squeeze(inputs[:, 0:1, :], axis=1))\n\n            def dropped_inputs():\n                return K.dropout(ones, self.dropout)\n\n            self._dropout_mask = K.in_train_phase(\n                dropped_inputs,\n                ones,\n                training=training)\n        else:\n            self._dropout_mask = None\n\n    def _generate_recurrent_dropout_mask(self, inputs, training=None):\n        if 0 < self.recurrent_dropout < 1:\n            ones = K.ones_like(K.reshape(inputs[:, 0, 0], (-1, 1)))\n            ones = K.tile(ones, (1, self.units))\n\n            def dropped_inputs():\n                return K.dropout(ones, self.dropout)\n\n            self._recurrent_dropout_mask = K.in_train_phase(\n                dropped_inputs,\n                ones,\n                training=training)\n        else:\n            self._recurrent_dropout_mask = None\n\n    def call(self, inputs, states, training=None):\n        prev_output = states[0]\n        dp_mask = self._dropout_mask\n        rec_dp_mask = self._recurrent_dropout_mask\n\n        if dp_mask is not None:\n            h = K.dot(inputs * dp_mask, self.kernel)\n        else:\n            h = K.dot(inputs, self.kernel)\n        if self.bias is not None:\n            h = K.bias_add(h, self.bias)\n\n        if rec_dp_mask is not None:\n            prev_output *= rec_dp_mask\n        output = h + K.dot(prev_output, self.recurrent_kernel)\n        if self.activation is not None:\n            output = self.activation(output)\n\n        # Properly set learning phase on output tensor.\n        if 0 < self.dropout + self.recurrent_dropout:\n            if training is None:\n                output._uses_learning_phase = True\n        return output, [output]\n\n\nclass SimpleRNN(RNN):\n    \"\"\"Fully-connected RNN where the output is to be fed back to input.\n\n    # Arguments\n        units: Positive integer, dimensionality of the output space.\n        activation: Activation function to use\n            (see [activations](../activations.md)).\n            If you pass None, no activation is applied\n            (ie. \"linear\" activation: `a(x) = x`).\n        use_bias: Boolean, whether the layer uses a bias vector.\n        kernel_initializer: Initializer for the `kernel` weights matrix,\n            used for the linear transformation of the inputs.\n            (see [initializers](../initializers.md)).\n        recurrent_initializer: Initializer for the `recurrent_kernel`\n            weights matrix,\n            used for the linear transformation of the recurrent state.\n            (see [initializers](../initializers.md)).\n        bias_initializer: Initializer for the bias vector\n            (see [initializers](../initializers.md)).\n        kernel_regularizer: Regularizer function applied to\n            the `kernel` weights matrix\n            (see [regularizer](../regularizers.md)).\n        recurrent_regularizer: Regularizer function applied to\n            the `recurrent_kernel` weights matrix\n            (see [regularizer](../regularizers.md)).\n        bias_regularizer: Regularizer function applied to the bias vector\n            (see [regularizer](../regularizers.md)).\n        activity_regularizer: Regularizer function applied to\n            the output of the layer (its \"activation\").\n            (see [regularizer](../regularizers.md)).\n        kernel_constraint: Constraint function applied to\n            the `kernel` weights matrix\n            (see [constraints](../constraints.md)).\n        recurrent_constraint: Constraint function applied to\n            the `recurrent_kernel` weights matrix\n            (see [constraints](../constraints.md)).\n        bias_constraint: Constraint function applied to the bias vector\n            (see [constraints](../constraints.md)).\n        dropout: Float between 0 and 1.\n            Fraction of the units to drop for\n            the linear transformation of the inputs.\n        recurrent_dropout: Float between 0 and 1.\n            Fraction of the units to drop for\n            the linear transformation of the recurrent state.\n    \"\"\"\n\n    @interfaces.legacy_recurrent_support\n    def __init__(self, units,\n                 activation='tanh',\n                 use_bias=True,\n                 kernel_initializer='glorot_uniform',\n                 recurrent_initializer='orthogonal',\n                 bias_initializer='zeros',\n                 kernel_regularizer=None,\n                 recurrent_regularizer=None,\n                 bias_regularizer=None,\n                 activity_regularizer=None,\n                 kernel_constraint=None,\n                 recurrent_constraint=None,\n                 bias_constraint=None,\n                 dropout=0.,\n                 recurrent_dropout=0.,\n                 **kwargs):\n        if 'implementation' in kwargs:\n            kwargs.pop('implementation')\n            warnings.warn('The `implementation` argument '\n                          'in `SimpleRNN` has been deprecated. '\n                          'Please remove it from your layer call.')\n        if K.backend() == 'cntk':\n            if not kwargs.get('unroll') and (dropout > 0 or recurrent_dropout > 0):\n                warnings.warn(\n                    'RNN dropout is not supported with the CNTK backend '\n                    'when using dynamic RNNs (i.e. non-unrolled). '\n                    'You can either set `unroll=True`, '\n                    'set `dropout` and `recurrent_dropout` to 0, '\n                    'or use a different backend.')\n                dropout = 0.\n                recurrent_dropout = 0.\n\n        cell = SimpleRNNCell(units,\n                             activation=activation,\n                             use_bias=use_bias,\n                             kernel_initializer=kernel_initializer,\n                             recurrent_initializer=recurrent_initializer,\n                             bias_initializer=bias_initializer,\n                             kernel_regularizer=kernel_regularizer,\n                             recurrent_regularizer=recurrent_regularizer,\n                             bias_regularizer=bias_regularizer,\n                             kernel_constraint=kernel_constraint,\n                             recurrent_constraint=recurrent_constraint,\n                             bias_constraint=bias_constraint,\n                             dropout=dropout,\n                             recurrent_dropout=recurrent_dropout)\n        super(SimpleRNN, self).__init__(cell, **kwargs)\n        self.activity_regularizer = regularizers.get(activity_regularizer)\n\n    def call(self, inputs, mask=None, training=None, initial_state=None):\n        self.cell._generate_dropout_mask(inputs, training=training)\n        self.cell._generate_recurrent_dropout_mask(inputs, training=training)\n        return super(SimpleRNN, self).call(inputs,\n                                           mask=mask,\n                                           training=training,\n                                           initial_state=initial_state)\n\n    @property\n    def units(self):\n        return self.cell.units\n\n    @property\n    def activation(self):\n        return self.cell.activation\n\n    @property\n    def use_bias(self):\n        return self.cell.use_bias\n\n    @property\n    def kernel_initializer(self):\n        return self.cell.kernel_initializer\n\n    @property\n    def recurrent_initializer(self):\n        return self.cell.recurrent_initializer\n\n    @property\n    def bias_initializer(self):\n        return self.cell.bias_initializer\n\n    @property\n    def kernel_regularizer(self):\n        return self.cell.kernel_regularizer\n\n    @property\n    def recurrent_regularizer(self):\n        return self.cell.recurrent_regularizer\n\n    @property\n    def bias_regularizer(self):\n        return self.cell.bias_regularizer\n\n    @property\n    def kernel_constraint(self):\n        return self.cell.kernel_constraint\n\n    @property\n    def recurrent_constraint(self):\n        return self.cell.recurrent_constraint\n\n    @property\n    def bias_constraint(self):\n        return self.cell.bias_constraint\n\n    @property\n    def dropout(self):\n        return self.cell.dropout\n\n    @property\n    def recurrent_dropout(self):\n        return self.cell.recurrent_dropout\n\n    def get_config(self):\n        config = {'units': self.units,\n                  'activation': activations.serialize(self.activation),\n                  'use_bias': self.use_bias,\n                  'kernel_initializer': initializers.serialize(self.kernel_initializer),\n                  'recurrent_initializer': initializers.serialize(self.recurrent_initializer),\n                  'bias_initializer': initializers.serialize(self.bias_initializer),\n                  'kernel_regularizer': regularizers.serialize(self.kernel_regularizer),\n                  'recurrent_regularizer': regularizers.serialize(self.recurrent_regularizer),\n                  'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n                  'activity_regularizer': regularizers.serialize(self.activity_regularizer),\n                  'kernel_constraint': constraints.serialize(self.kernel_constraint),\n                  'recurrent_constraint': constraints.serialize(self.recurrent_constraint),\n                  'bias_constraint': constraints.serialize(self.bias_constraint),\n                  'dropout': self.dropout,\n                  'recurrent_dropout': self.recurrent_dropout}\n        base_config = super(SimpleRNN, self).get_config()\n        del base_config['cell']\n        return dict(list(base_config.items()) + list(config.items()))\n\n    @classmethod\n    def from_config(cls, config):\n        if 'implementation' in config:\n            config.pop('implementation')\n        return cls(**config)\n\n\nclass GRUCell(Layer):\n    \"\"\"Cell class for the GRU layer.\n\n    # Arguments\n        units: Positive integer, dimensionality of the output space.\n        activation: Activation function to use\n            (see [activations](../activations.md)).\n            If you pass None, no activation is applied\n            (ie. \"linear\" activation: `a(x) = x`).\n        recurrent_activation: Activation function to use\n            for the recurrent step\n            (see [activations](../activations.md)).\n        use_bias: Boolean, whether the layer uses a bias vector.\n        kernel_initializer: Initializer for the `kernel` weights matrix,\n            used for the linear transformation of the inputs.\n            (see [initializers](../initializers.md)).\n        recurrent_initializer: Initializer for the `recurrent_kernel`\n            weights matrix,\n            used for the linear transformation of the recurrent state.\n            (see [initializers](../initializers.md)).\n        bias_initializer: Initializer for the bias vector\n            (see [initializers](../initializers.md)).\n        kernel_regularizer: Regularizer function applied to\n            the `kernel` weights matrix\n            (see [regularizer](../regularizers.md)).\n        recurrent_regularizer: Regularizer function applied to\n            the `recurrent_kernel` weights matrix\n            (see [regularizer](../regularizers.md)).\n        bias_regularizer: Regularizer function applied to the bias vector\n            (see [regularizer](../regularizers.md)).\n        activity_regularizer: Regularizer function applied to\n            the output of the layer (its \"activation\").\n            (see [regularizer](../regularizers.md)).\n        kernel_constraint: Constraint function applied to\n            the `kernel` weights matrix\n            (see [constraints](../constraints.md)).\n        recurrent_constraint: Constraint function applied to\n            the `recurrent_kernel` weights matrix\n            (see [constraints](../constraints.md)).\n        bias_constraint: Constraint function applied to the bias vector\n            (see [constraints](../constraints.md)).\n        dropout: Float between 0 and 1.\n            Fraction of the units to drop for\n            the linear transformation of the inputs.\n        recurrent_dropout: Float between 0 and 1.\n            Fraction of the units to drop for\n            the linear transformation of the recurrent state.\n        implementation: Implementation mode, either 1 or 2.\n    \"\"\"\n\n    def __init__(self, units,\n                 activation='tanh',\n                 recurrent_activation='hard_sigmoid',\n                 use_bias=True,\n                 kernel_initializer='glorot_uniform',\n                 recurrent_initializer='orthogonal',\n                 bias_initializer='zeros',\n                 kernel_regularizer=None,\n                 recurrent_regularizer=None,\n                 bias_regularizer=None,\n                 kernel_constraint=None,\n                 recurrent_constraint=None,\n                 bias_constraint=None,\n                 dropout=0.,\n                 recurrent_dropout=0.,\n                 implementation=1,\n                 **kwargs):\n        super(GRUCell, self).__init__(**kwargs)\n        self.units = units\n        self.activation = activations.get(activation)\n        self.recurrent_activation = activations.get(recurrent_activation)\n        self.use_bias = use_bias\n\n        self.kernel_initializer = initializers.get(kernel_initializer)\n        self.recurrent_initializer = initializers.get(recurrent_initializer)\n        self.bias_initializer = initializers.get(bias_initializer)\n\n        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n        self.recurrent_regularizer = regularizers.get(recurrent_regularizer)\n        self.bias_regularizer = regularizers.get(bias_regularizer)\n\n        self.kernel_constraint = constraints.get(kernel_constraint)\n        self.recurrent_constraint = constraints.get(recurrent_constraint)\n        self.bias_constraint = constraints.get(bias_constraint)\n\n        self.dropout = min(1., max(0., dropout))\n        self.recurrent_dropout = min(1., max(0., recurrent_dropout))\n        self.implementation = implementation\n        self.state_size = self.units\n        self._dropout_mask = None\n        self._recurrent_dropout_mask = None\n\n    def build(self, input_shape):\n        input_dim = input_shape[-1]\n        self.kernel = self.add_weight(shape=(input_dim, self.units * 3),\n                                      name='kernel',\n                                      initializer=self.kernel_initializer,\n                                      regularizer=self.kernel_regularizer,\n                                      constraint=self.kernel_constraint)\n        self.recurrent_kernel = self.add_weight(\n            shape=(self.units, self.units * 3),\n            name='recurrent_kernel',\n            initializer=self.recurrent_initializer,\n            regularizer=self.recurrent_regularizer,\n            constraint=self.recurrent_constraint)\n\n        if self.use_bias:\n            self.bias = self.add_weight(shape=(self.units * 3,),\n                                        name='bias',\n                                        initializer=self.bias_initializer,\n                                        regularizer=self.bias_regularizer,\n                                        constraint=self.bias_constraint)\n        else:\n            self.bias = None\n\n        self.kernel_z = self.kernel[:, :self.units]\n        self.recurrent_kernel_z = self.recurrent_kernel[:, :self.units]\n        self.kernel_r = self.kernel[:, self.units: self.units * 2]\n        self.recurrent_kernel_r = self.recurrent_kernel[:,\n                                                        self.units:\n                                                        self.units * 2]\n        self.kernel_h = self.kernel[:, self.units * 2:]\n        self.recurrent_kernel_h = self.recurrent_kernel[:, self.units * 2:]\n\n        if self.use_bias:\n            self.bias_z = self.bias[:self.units]\n            self.bias_r = self.bias[self.units: self.units * 2]\n            self.bias_h = self.bias[self.units * 2:]\n        else:\n            self.bias_z = None\n            self.bias_r = None\n            self.bias_h = None\n        self.built = True\n\n    def _generate_dropout_mask(self, inputs, training=None):\n        if 0 < self.dropout < 1:\n            ones = K.ones_like(K.squeeze(inputs[:, 0:1, :], axis=1))\n\n            def dropped_inputs():\n                return K.dropout(ones, self.dropout)\n\n            self._dropout_mask = [K.in_train_phase(\n                dropped_inputs,\n                ones,\n                training=training)\n                for _ in range(3)]\n        else:\n            self._dropout_mask = None\n\n    def _generate_recurrent_dropout_mask(self, inputs, training=None):\n        if 0 < self.recurrent_dropout < 1:\n            ones = K.ones_like(K.reshape(inputs[:, 0, 0], (-1, 1)))\n            ones = K.tile(ones, (1, self.units))\n\n            def dropped_inputs():\n                return K.dropout(ones, self.dropout)\n\n            self._recurrent_dropout_mask = [K.in_train_phase(\n                dropped_inputs,\n                ones,\n                training=training)\n                for _ in range(3)]\n        else:\n            self._recurrent_dropout_mask = None\n\n    def call(self, inputs, states, training=None):\n        h_tm1 = states[0]  # previous memory\n\n        # dropout matrices for input units\n        dp_mask = self._dropout_mask\n        # dropout matrices for recurrent units\n        rec_dp_mask = self._recurrent_dropout_mask\n\n        if self.implementation == 1:\n            if 0. < self.dropout < 1.:\n                inputs_z = inputs * dp_mask[0]\n                inputs_r = inputs * dp_mask[1]\n                inputs_h = inputs * dp_mask[2]\n            else:\n                inputs_z = inputs\n                inputs_r = inputs\n                inputs_h = inputs\n            x_z = K.dot(inputs_z, self.kernel_z)\n            x_r = K.dot(inputs_r, self.kernel_r)\n            x_h = K.dot(inputs_h, self.kernel_h)\n            if self.use_bias:\n                x_z = K.bias_add(x_z, self.bias_z)\n                x_r = K.bias_add(x_r, self.bias_r)\n                x_h = K.bias_add(x_h, self.bias_h)\n\n            if 0. < self.recurrent_dropout < 1.:\n                h_tm1_z = h_tm1 * rec_dp_mask[0]\n                h_tm1_r = h_tm1 * rec_dp_mask[1]\n                h_tm1_h = h_tm1 * rec_dp_mask[2]\n            else:\n                h_tm1_z = h_tm1\n                h_tm1_r = h_tm1\n                h_tm1_h = h_tm1\n            z = self.recurrent_activation(x_z + K.dot(h_tm1_z,\n                                                      self.recurrent_kernel_z))\n            r = self.recurrent_activation(x_r + K.dot(h_tm1_r,\n                                                      self.recurrent_kernel_r))\n\n            hh = self.activation(x_h + K.dot(r * h_tm1_h,\n                                             self.recurrent_kernel_h))\n        else:\n            if 0. < self.dropout < 1.:\n                inputs *= dp_mask[0]\n            matrix_x = K.dot(inputs, self.kernel)\n            if self.use_bias:\n                matrix_x = K.bias_add(matrix_x, self.bias)\n            if 0. < self.recurrent_dropout < 1.:\n                h_tm1 *= rec_dp_mask[0]\n            matrix_inner = K.dot(h_tm1,\n                                 self.recurrent_kernel[:, :2 * self.units])\n\n            x_z = matrix_x[:, :self.units]\n            x_r = matrix_x[:, self.units: 2 * self.units]\n            recurrent_z = matrix_inner[:, :self.units]\n            recurrent_r = matrix_inner[:, self.units: 2 * self.units]\n\n            z = self.recurrent_activation(x_z + recurrent_z)\n            r = self.recurrent_activation(x_r + recurrent_r)\n\n            x_h = matrix_x[:, 2 * self.units:]\n            recurrent_h = K.dot(r * h_tm1,\n                                self.recurrent_kernel[:, 2 * self.units:])\n            hh = self.activation(x_h + recurrent_h)\n        h = z * h_tm1 + (1 - z) * hh\n        if 0 < self.dropout + self.recurrent_dropout:\n            if training is None:\n                h._uses_learning_phase = True\n        return h, [h]\n\n\nclass GRU(RNN):\n    \"\"\"Gated Recurrent Unit - Cho et al. 2014.\n\n    # Arguments\n        units: Positive integer, dimensionality of the output space.\n        activation: Activation function to use\n            (see [activations](../activations.md)).\n            If you pass None, no activation is applied\n            (ie. \"linear\" activation: `a(x) = x`).\n        recurrent_activation: Activation function to use\n            for the recurrent step\n            (see [activations](../activations.md)).\n        use_bias: Boolean, whether the layer uses a bias vector.\n        kernel_initializer: Initializer for the `kernel` weights matrix,\n            used for the linear transformation of the inputs.\n            (see [initializers](../initializers.md)).\n        recurrent_initializer: Initializer for the `recurrent_kernel`\n            weights matrix,\n            used for the linear transformation of the recurrent state.\n            (see [initializers](../initializers.md)).\n        bias_initializer: Initializer for the bias vector\n            (see [initializers](../initializers.md)).\n        kernel_regularizer: Regularizer function applied to\n            the `kernel` weights matrix\n            (see [regularizer](../regularizers.md)).\n        recurrent_regularizer: Regularizer function applied to\n            the `recurrent_kernel` weights matrix\n            (see [regularizer](../regularizers.md)).\n        bias_regularizer: Regularizer function applied to the bias vector\n            (see [regularizer](../regularizers.md)).\n        activity_regularizer: Regularizer function applied to\n            the output of the layer (its \"activation\").\n            (see [regularizer](../regularizers.md)).\n        kernel_constraint: Constraint function applied to\n            the `kernel` weights matrix\n            (see [constraints](../constraints.md)).\n        recurrent_constraint: Constraint function applied to\n            the `recurrent_kernel` weights matrix\n            (see [constraints](../constraints.md)).\n        bias_constraint: Constraint function applied to the bias vector\n            (see [constraints](../constraints.md)).\n        dropout: Float between 0 and 1.\n            Fraction of the units to drop for\n            the linear transformation of the inputs.\n        recurrent_dropout: Float between 0 and 1.\n            Fraction of the units to drop for\n            the linear transformation of the recurrent state.\n        implementation: Implementation mode, either 1 or 2.\n\n    # References\n        - [On the Properties of Neural Machine Translation: Encoder-Decoder Approaches](https://arxiv.org/abs/1409.1259)\n        - [Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling](http://arxiv.org/abs/1412.3555v1)\n        - [A Theoretically Grounded Application of Dropout in Recurrent Neural Networks](http://arxiv.org/abs/1512.05287)\n    \"\"\"\n\n    @interfaces.legacy_recurrent_support\n    def __init__(self, units,\n                 activation='tanh',\n                 recurrent_activation='hard_sigmoid',\n                 use_bias=True,\n                 kernel_initializer='glorot_uniform',\n                 recurrent_initializer='orthogonal',\n                 bias_initializer='zeros',\n                 kernel_regularizer=None,\n                 recurrent_regularizer=None,\n                 bias_regularizer=None,\n                 activity_regularizer=None,\n                 kernel_constraint=None,\n                 recurrent_constraint=None,\n                 bias_constraint=None,\n                 dropout=0.,\n                 recurrent_dropout=0.,\n                 implementation=1,\n                 **kwargs):\n        if implementation == 0:\n            warnings.warn('`implementation=0` has been deprecated, '\n                          'and now defaults to `implementation=1`.'\n                          'Please update your layer call.')\n        if K.backend() == 'cntk':\n            if not kwargs.get('unroll') and (dropout > 0 or recurrent_dropout > 0):\n                warnings.warn(\n                    'RNN dropout is not supported with the CNTK backend '\n                    'when using dynamic RNNs (i.e. non-unrolled). '\n                    'You can either set `unroll=True`, '\n                    'set `dropout` and `recurrent_dropout` to 0, '\n                    'or use a different backend.')\n                dropout = 0.\n                recurrent_dropout = 0.\n\n        cell = GRUCell(units,\n                       activation=activation,\n                       recurrent_activation=recurrent_activation,\n                       use_bias=use_bias,\n                       kernel_initializer=kernel_initializer,\n                       recurrent_initializer=recurrent_initializer,\n                       bias_initializer=bias_initializer,\n                       kernel_regularizer=kernel_regularizer,\n                       recurrent_regularizer=recurrent_regularizer,\n                       bias_regularizer=bias_regularizer,\n                       kernel_constraint=kernel_constraint,\n                       recurrent_constraint=recurrent_constraint,\n                       bias_constraint=bias_constraint,\n                       dropout=dropout,\n                       recurrent_dropout=recurrent_dropout,\n                       implementation=implementation)\n        super(GRU, self).__init__(cell, **kwargs)\n        self.activity_regularizer = regularizers.get(activity_regularizer)\n\n    def call(self, inputs, mask=None, training=None, initial_state=None):\n        self.cell._generate_dropout_mask(inputs, training=training)\n        self.cell._generate_recurrent_dropout_mask(inputs, training=training)\n        return super(GRU, self).call(inputs,\n                                     mask=mask,\n                                     training=training,\n                                     initial_state=initial_state)\n\n    @property\n    def units(self):\n        return self.cell.units\n\n    @property\n    def activation(self):\n        return self.cell.activation\n\n    @property\n    def recurrent_activation(self):\n        return self.cell.recurrent_activation\n\n    @property\n    def use_bias(self):\n        return self.cell.use_bias\n\n    @property\n    def kernel_initializer(self):\n        return self.cell.kernel_initializer\n\n    @property\n    def recurrent_initializer(self):\n        return self.cell.recurrent_initializer\n\n    @property\n    def bias_initializer(self):\n        return self.cell.bias_initializer\n\n    @property\n    def kernel_regularizer(self):\n        return self.cell.kernel_regularizer\n\n    @property\n    def recurrent_regularizer(self):\n        return self.cell.recurrent_regularizer\n\n    @property\n    def bias_regularizer(self):\n        return self.cell.bias_regularizer\n\n    @property\n    def kernel_constraint(self):\n        return self.cell.kernel_constraint\n\n    @property\n    def recurrent_constraint(self):\n        return self.cell.recurrent_constraint\n\n    @property\n    def bias_constraint(self):\n        return self.cell.bias_constraint\n\n    @property\n    def dropout(self):\n        return self.cell.dropout\n\n    @property\n    def recurrent_dropout(self):\n        return self.cell.recurrent_dropout\n\n    @property\n    def implementation(self):\n        return self.cell.implementation\n\n    def get_config(self):\n        config = {'units': self.units,\n                  'activation': activations.serialize(self.activation),\n                  'recurrent_activation': activations.serialize(self.recurrent_activation),\n                  'use_bias': self.use_bias,\n                  'kernel_initializer': initializers.serialize(self.kernel_initializer),\n                  'recurrent_initializer': initializers.serialize(self.recurrent_initializer),\n                  'bias_initializer': initializers.serialize(self.bias_initializer),\n                  'kernel_regularizer': regularizers.serialize(self.kernel_regularizer),\n                  'recurrent_regularizer': regularizers.serialize(self.recurrent_regularizer),\n                  'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n                  'activity_regularizer': regularizers.serialize(self.activity_regularizer),\n                  'kernel_constraint': constraints.serialize(self.kernel_constraint),\n                  'recurrent_constraint': constraints.serialize(self.recurrent_constraint),\n                  'bias_constraint': constraints.serialize(self.bias_constraint),\n                  'dropout': self.dropout,\n                  'recurrent_dropout': self.recurrent_dropout,\n                  'implementation': self.implementation}\n        base_config = super(GRU, self).get_config()\n        del base_config['cell']\n        return dict(list(base_config.items()) + list(config.items()))\n\n    @classmethod\n    def from_config(cls, config):\n        if 'implementation' in config and config['implementation'] == 0:\n            config['implementation'] = 1\n        return cls(**config)\n\n\nclass LSTMCell(Layer):\n    \"\"\"Cell class for the LSTM layer.\n\n    # Arguments\n        units: Positive integer, dimensionality of the output space.\n        activation: Activation function to use\n            (see [activations](../activations.md)).\n            If you pass None, no activation is applied\n            (ie. \"linear\" activation: `a(x) = x`).\n        recurrent_activation: Activation function to use\n            for the recurrent step\n            (see [activations](../activations.md)).\n        use_bias: Boolean, whether the layer uses a bias vector.\n        kernel_initializer: Initializer for the `kernel` weights matrix,\n            used for the linear transformation of the inputs.\n            (see [initializers](../initializers.md)).\n        recurrent_initializer: Initializer for the `recurrent_kernel`\n            weights matrix,\n            used for the linear transformation of the recurrent state.\n            (see [initializers](../initializers.md)).\n        bias_initializer: Initializer for the bias vector\n            (see [initializers](../initializers.md)).\n        unit_forget_bias: Boolean.\n            If True, add 1 to the bias of the forget gate at initialization.\n            Setting it to true will also force `bias_initializer=\"zeros\"`.\n            This is recommended in [Jozefowicz et al.](http://www.jmlr.org/proceedings/papers/v37/jozefowicz15.pdf)\n        kernel_regularizer: Regularizer function applied to\n            the `kernel` weights matrix\n            (see [regularizer](../regularizers.md)).\n        recurrent_regularizer: Regularizer function applied to\n            the `recurrent_kernel` weights matrix\n            (see [regularizer](../regularizers.md)).\n        bias_regularizer: Regularizer function applied to the bias vector\n            (see [regularizer](../regularizers.md)).\n        activity_regularizer: Regularizer function applied to\n            the output of the layer (its \"activation\").\n            (see [regularizer](../regularizers.md)).\n        kernel_constraint: Constraint function applied to\n            the `kernel` weights matrix\n            (see [constraints](../constraints.md)).\n        recurrent_constraint: Constraint function applied to\n            the `recurrent_kernel` weights matrix\n            (see [constraints](../constraints.md)).\n        bias_constraint: Constraint function applied to the bias vector\n            (see [constraints](../constraints.md)).\n        dropout: Float between 0 and 1.\n            Fraction of the units to drop for\n            the linear transformation of the inputs.\n        recurrent_dropout: Float between 0 and 1.\n            Fraction of the units to drop for\n            the linear transformation of the recurrent state.\n        implementation: Implementation mode, either 1 or 2.\n    \"\"\"\n\n    def __init__(self, units,\n                 activation='tanh',\n                 recurrent_activation='hard_sigmoid',\n                 use_bias=True,\n                 kernel_initializer='glorot_uniform',\n                 recurrent_initializer='orthogonal',\n                 bias_initializer='zeros',\n                 unit_forget_bias=True,\n                 kernel_regularizer=None,\n                 recurrent_regularizer=None,\n                 bias_regularizer=None,\n                 kernel_constraint=None,\n                 recurrent_constraint=None,\n                 bias_constraint=None,\n                 dropout=0.,\n                 recurrent_dropout=0.,\n                 implementation=1,\n                 **kwargs):\n        super(LSTMCell, self).__init__(**kwargs)\n        self.units = units\n        self.activation = activations.get(activation)\n        self.recurrent_activation = activations.get(recurrent_activation)\n        self.use_bias = use_bias\n\n        self.kernel_initializer = initializers.get(kernel_initializer)\n        self.recurrent_initializer = initializers.get(recurrent_initializer)\n        self.bias_initializer = initializers.get(bias_initializer)\n        self.unit_forget_bias = unit_forget_bias\n\n        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n        self.recurrent_regularizer = regularizers.get(recurrent_regularizer)\n        self.bias_regularizer = regularizers.get(bias_regularizer)\n\n        self.kernel_constraint = constraints.get(kernel_constraint)\n        self.recurrent_constraint = constraints.get(recurrent_constraint)\n        self.bias_constraint = constraints.get(bias_constraint)\n\n        self.dropout = min(1., max(0., dropout))\n        self.recurrent_dropout = min(1., max(0., recurrent_dropout))\n        self.implementation = implementation\n        self.state_size = (self.units, self.units)\n        self._dropout_mask = None\n        self._recurrent_dropout_mask = None\n\n    def build(self, input_shape):\n        input_dim = input_shape[-1]\n        self.kernel = self.add_weight(shape=(input_dim, self.units * 4),\n                                      name='kernel',\n                                      initializer=self.kernel_initializer,\n                                      regularizer=self.kernel_regularizer,\n                                      constraint=self.kernel_constraint)\n        self.recurrent_kernel = self.add_weight(\n            shape=(self.units, self.units * 4),\n            name='recurrent_kernel',\n            initializer=self.recurrent_initializer,\n            regularizer=self.recurrent_regularizer,\n            constraint=self.recurrent_constraint)\n\n        if self.use_bias:\n            if self.unit_forget_bias:\n                def bias_initializer(shape, *args, **kwargs):\n                    return K.concatenate([\n                        self.bias_initializer((self.units,), *args, **kwargs),\n                        initializers.Ones()((self.units,), *args, **kwargs),\n                        self.bias_initializer((self.units * 2,), *args, **kwargs),\n                    ])\n            else:\n                bias_initializer = self.bias_initializer\n            self.bias = self.add_weight(shape=(self.units * 4,),\n                                        name='bias',\n                                        initializer=bias_initializer,\n                                        regularizer=self.bias_regularizer,\n                                        constraint=self.bias_constraint)\n        else:\n            self.bias = None\n\n        self.kernel_i = self.kernel[:, :self.units]\n        self.kernel_f = self.kernel[:, self.units: self.units * 2]\n        self.kernel_c = self.kernel[:, self.units * 2: self.units * 3]\n        self.kernel_o = self.kernel[:, self.units * 3:]\n\n        self.recurrent_kernel_i = self.recurrent_kernel[:, :self.units]\n        self.recurrent_kernel_f = self.recurrent_kernel[:, self.units: self.units * 2]\n        self.recurrent_kernel_c = self.recurrent_kernel[:, self.units * 2: self.units * 3]\n        self.recurrent_kernel_o = self.recurrent_kernel[:, self.units * 3:]\n\n        if self.use_bias:\n            self.bias_i = self.bias[:self.units]\n            self.bias_f = self.bias[self.units: self.units * 2]\n            self.bias_c = self.bias[self.units * 2: self.units * 3]\n            self.bias_o = self.bias[self.units * 3:]\n        else:\n            self.bias_i = None\n            self.bias_f = None\n            self.bias_c = None\n            self.bias_o = None\n        self.built = True\n\n    def _generate_dropout_mask(self, inputs, training=None):\n        if 0 < self.dropout < 1:\n            ones = K.ones_like(K.squeeze(inputs[:, 0:1, :], axis=1))\n\n            def dropped_inputs():\n                return K.dropout(ones, self.dropout)\n\n            self._dropout_mask = [K.in_train_phase(\n                dropped_inputs,\n                ones,\n                training=training)\n                for _ in range(4)]\n        else:\n            self._dropout_mask = None\n\n    def _generate_recurrent_dropout_mask(self, inputs, training=None):\n        if 0 < self.recurrent_dropout < 1:\n            ones = K.ones_like(K.reshape(inputs[:, 0, 0], (-1, 1)))\n            ones = K.tile(ones, (1, self.units))\n\n            def dropped_inputs():\n                return K.dropout(ones, self.dropout)\n\n            self._recurrent_dropout_mask = [K.in_train_phase(\n                dropped_inputs,\n                ones,\n                training=training)\n                for _ in range(4)]\n        else:\n            self._recurrent_dropout_mask = None\n\n    def call(self, inputs, states, training=None):\n        # dropout matrices for input units\n        dp_mask = self._dropout_mask\n        # dropout matrices for recurrent units\n        rec_dp_mask = self._recurrent_dropout_mask\n\n        h_tm1 = states[0]  # previous memory state\n        c_tm1 = states[1]  # previous carry state\n\n        if self.implementation == 1:\n            if 0 < self.dropout < 1.:\n                inputs_i = inputs * dp_mask[0]\n                inputs_f = inputs * dp_mask[1]\n                inputs_c = inputs * dp_mask[2]\n                inputs_o = inputs * dp_mask[3]\n            else:\n                inputs_i = inputs\n                inputs_f = inputs\n                inputs_c = inputs\n                inputs_o = inputs\n            x_i = K.dot(inputs_i, self.kernel_i) + self.bias_i\n            x_f = K.dot(inputs_f, self.kernel_f) + self.bias_f\n            x_c = K.dot(inputs_c, self.kernel_c) + self.bias_c\n            x_o = K.dot(inputs_o, self.kernel_o) + self.bias_o\n\n            if 0 < self.recurrent_dropout < 1.:\n                h_tm1_i = h_tm1 * rec_dp_mask[0]\n                h_tm1_f = h_tm1 * rec_dp_mask[1]\n                h_tm1_c = h_tm1 * rec_dp_mask[2]\n                h_tm1_o = h_tm1 * rec_dp_mask[3]\n            else:\n                h_tm1_i = h_tm1\n                h_tm1_f = h_tm1\n                h_tm1_c = h_tm1\n                h_tm1_o = h_tm1\n            i = self.recurrent_activation(x_i + K.dot(h_tm1_i,\n                                                      self.recurrent_kernel_i))\n            f = self.recurrent_activation(x_f + K.dot(h_tm1_f,\n                                                      self.recurrent_kernel_f))\n            c = f * c_tm1 + i * self.activation(x_c + K.dot(h_tm1_c,\n                                                            self.recurrent_kernel_c))\n            o = self.recurrent_activation(x_o + K.dot(h_tm1_o,\n                                                      self.recurrent_kernel_o))\n        else:\n            if 0. < self.dropout < 1.:\n                inputs *= dp_mask[0]\n            z = K.dot(inputs, self.kernel)\n            if 0. < self.recurrent_dropout < 1.:\n                h_tm1 *= rec_dp_mask[0]\n            z += K.dot(h_tm1, self.recurrent_kernel)\n            if self.use_bias:\n                z = K.bias_add(z, self.bias)\n\n            z0 = z[:, :self.units]\n            z1 = z[:, self.units: 2 * self.units]\n            z2 = z[:, 2 * self.units: 3 * self.units]\n            z3 = z[:, 3 * self.units:]\n\n            i = self.recurrent_activation(z0)\n            f = self.recurrent_activation(z1)\n            c = f * c_tm1 + i * self.activation(z2)\n            o = self.recurrent_activation(z3)\n\n        h = o * self.activation(c)\n        if 0 < self.dropout + self.recurrent_dropout:\n            if training is None:\n                h._uses_learning_phase = True\n        return h, [h, c]\n\n\nclass LSTM(RNN):\n    \"\"\"Long-Short Term Memory layer - Hochreiter 1997.\n\n    # Arguments\n        units: Positive integer, dimensionality of the output space.\n        activation: Activation function to use\n            (see [activations](../activations.md)).\n            If you pass None, no activation is applied\n            (ie. \"linear\" activation: `a(x) = x`).\n        recurrent_activation: Activation function to use\n            for the recurrent step\n            (see [activations](../activations.md)).\n        use_bias: Boolean, whether the layer uses a bias vector.\n        kernel_initializer: Initializer for the `kernel` weights matrix,\n            used for the linear transformation of the inputs.\n            (see [initializers](../initializers.md)).\n        recurrent_initializer: Initializer for the `recurrent_kernel`\n            weights matrix,\n            used for the linear transformation of the recurrent state.\n            (see [initializers](../initializers.md)).\n        bias_initializer: Initializer for the bias vector\n            (see [initializers](../initializers.md)).\n        unit_forget_bias: Boolean.\n            If True, add 1 to the bias of the forget gate at initialization.\n            Setting it to true will also force `bias_initializer=\"zeros\"`.\n            This is recommended in [Jozefowicz et al.](http://www.jmlr.org/proceedings/papers/v37/jozefowicz15.pdf)\n        kernel_regularizer: Regularizer function applied to\n            the `kernel` weights matrix\n            (see [regularizer](../regularizers.md)).\n        recurrent_regularizer: Regularizer function applied to\n            the `recurrent_kernel` weights matrix\n            (see [regularizer](../regularizers.md)).\n        bias_regularizer: Regularizer function applied to the bias vector\n            (see [regularizer](../regularizers.md)).\n        activity_regularizer: Regularizer function applied to\n            the output of the layer (its \"activation\").\n            (see [regularizer](../regularizers.md)).\n        kernel_constraint: Constraint function applied to\n            the `kernel` weights matrix\n            (see [constraints](../constraints.md)).\n        recurrent_constraint: Constraint function applied to\n            the `recurrent_kernel` weights matrix\n            (see [constraints](../constraints.md)).\n        bias_constraint: Constraint function applied to the bias vector\n            (see [constraints](../constraints.md)).\n        dropout: Float between 0 and 1.\n            Fraction of the units to drop for\n            the linear transformation of the inputs.\n        recurrent_dropout: Float between 0 and 1.\n            Fraction of the units to drop for\n            the linear transformation of the recurrent state.\n        implementation: Implementation mode, either 1 or 2.\n\n    # References\n        - [Long short-term memory](http://www.bioinf.jku.at/publications/older/2604.pdf) (original 1997 paper)\n        - [Learning to forget: Continual prediction with LSTM](http://www.mitpressjournals.org/doi/pdf/10.1162/089976600300015015)\n        - [Supervised sequence labeling with recurrent neural networks](http://www.cs.toronto.edu/~graves/preprint.pdf)\n        - [A Theoretically Grounded Application of Dropout in Recurrent Neural Networks](http://arxiv.org/abs/1512.05287)\n    \"\"\"\n\n    @interfaces.legacy_recurrent_support\n    def __init__(self, units,\n                 activation='tanh',\n                 recurrent_activation='hard_sigmoid',\n                 use_bias=True,\n                 kernel_initializer='glorot_uniform',\n                 recurrent_initializer='orthogonal',\n                 bias_initializer='zeros',\n                 unit_forget_bias=True,\n                 kernel_regularizer=None,\n                 recurrent_regularizer=None,\n                 bias_regularizer=None,\n                 activity_regularizer=None,\n                 kernel_constraint=None,\n                 recurrent_constraint=None,\n                 bias_constraint=None,\n                 dropout=0.,\n                 recurrent_dropout=0.,\n                 implementation=1,\n                 **kwargs):\n        if implementation == 0:\n            warnings.warn('`implementation=0` has been deprecated, '\n                          'and now defaults to `implementation=1`.'\n                          'Please update your layer call.')\n        if K.backend() == 'cntk':\n            if not kwargs.get('unroll') and (dropout > 0 or recurrent_dropout > 0):\n                warnings.warn(\n                    'RNN dropout is not supported with the CNTK backend '\n                    'when using dynamic RNNs (i.e. non-unrolled). '\n                    'You can either set `unroll=True`, '\n                    'set `dropout` and `recurrent_dropout` to 0, '\n                    'or use a different backend.')\n                dropout = 0.\n                recurrent_dropout = 0.\n\n        cell = LSTMCell(units,\n                        activation=activation,\n                        recurrent_activation=recurrent_activation,\n                        use_bias=use_bias,\n                        kernel_initializer=kernel_initializer,\n                        recurrent_initializer=recurrent_initializer,\n                        unit_forget_bias=unit_forget_bias,\n                        bias_initializer=bias_initializer,\n                        kernel_regularizer=kernel_regularizer,\n                        recurrent_regularizer=recurrent_regularizer,\n                        bias_regularizer=bias_regularizer,\n                        kernel_constraint=kernel_constraint,\n                        recurrent_constraint=recurrent_constraint,\n                        bias_constraint=bias_constraint,\n                        dropout=dropout,\n                        recurrent_dropout=recurrent_dropout,\n                        implementation=implementation)\n        super(LSTM, self).__init__(cell, **kwargs)\n        self.activity_regularizer = regularizers.get(activity_regularizer)\n\n    def call(self, inputs, mask=None, training=None, initial_state=None):\n        self.cell._generate_dropout_mask(inputs, training=training)\n        self.cell._generate_recurrent_dropout_mask(inputs, training=training)\n        return super(LSTM, self).call(inputs,\n                                      mask=mask,\n                                      training=training,\n                                      initial_state=initial_state)\n\n    @property\n    def units(self):\n        return self.cell.units\n\n    @property\n    def activation(self):\n        return self.cell.activation\n\n    @property\n    def recurrent_activation(self):\n        return self.cell.recurrent_activation\n\n    @property\n    def use_bias(self):\n        return self.cell.use_bias\n\n    @property\n    def kernel_initializer(self):\n        return self.cell.kernel_initializer\n\n    @property\n    def recurrent_initializer(self):\n        return self.cell.recurrent_initializer\n\n    @property\n    def bias_initializer(self):\n        return self.cell.bias_initializer\n\n    @property\n    def unit_forget_bias(self):\n        return self.cell.unit_forget_bias\n\n    @property\n    def kernel_regularizer(self):\n        return self.cell.kernel_regularizer\n\n    @property\n    def recurrent_regularizer(self):\n        return self.cell.recurrent_regularizer\n\n    @property\n    def bias_regularizer(self):\n        return self.cell.bias_regularizer\n\n    @property\n    def kernel_constraint(self):\n        return self.cell.kernel_constraint\n\n    @property\n    def recurrent_constraint(self):\n        return self.cell.recurrent_constraint\n\n    @property\n    def bias_constraint(self):\n        return self.cell.bias_constraint\n\n    @property\n    def dropout(self):\n        return self.cell.dropout\n\n    @property\n    def recurrent_dropout(self):\n        return self.cell.recurrent_dropout\n\n    @property\n    def implementation(self):\n        return self.cell.implementation\n\n    def get_config(self):\n        config = {'units': self.units,\n                  'activation': activations.serialize(self.activation),\n                  'recurrent_activation': activations.serialize(self.recurrent_activation),\n                  'use_bias': self.use_bias,\n                  'kernel_initializer': initializers.serialize(self.kernel_initializer),\n                  'recurrent_initializer': initializers.serialize(self.recurrent_initializer),\n                  'bias_initializer': initializers.serialize(self.bias_initializer),\n                  'unit_forget_bias': self.unit_forget_bias,\n                  'kernel_regularizer': regularizers.serialize(self.kernel_regularizer),\n                  'recurrent_regularizer': regularizers.serialize(self.recurrent_regularizer),\n                  'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n                  'activity_regularizer': regularizers.serialize(self.activity_regularizer),\n                  'kernel_constraint': constraints.serialize(self.kernel_constraint),\n                  'recurrent_constraint': constraints.serialize(self.recurrent_constraint),\n                  'bias_constraint': constraints.serialize(self.bias_constraint),\n                  'dropout': self.dropout,\n                  'recurrent_dropout': self.recurrent_dropout,\n                  'implementation': self.implementation}\n        base_config = super(LSTM, self).get_config()\n        del base_config['cell']\n        return dict(list(base_config.items()) + list(config.items()))\n\n    @classmethod\n    def from_config(cls, config):\n        if 'implementation' in config and config['implementation'] == 0:\n            config['implementation'] = 1\n        return cls(**config)\n",
      "file_patch": "@@ -1,6 +1,8 @@\n # -*- coding: utf-8 -*-\n from __future__ import absolute_import\n import numpy as np\n+import functools\n+import warnings\n \n from .. import backend as K\n from .. import activations\n@@ -9,92 +11,193 @@ from .. import regularizers\n from .. import constraints\n from ..engine import Layer\n from ..engine import InputSpec\n+from ..utils.generic_utils import has_arg\n+\n+# Legacy support.\n+from ..legacy.layers import Recurrent\n from ..legacy import interfaces\n \n \n-def _time_distributed_dense(x, w, b=None, dropout=None,\n-                            input_dim=None, output_dim=None,\n-                            timesteps=None, training=None):\n-    \"\"\"Apply `y . w + b` for every temporal slice y of x.\n+class StackedRNNCells(Layer):\n+    \"\"\"Wrapper allowing a stack of RNN cells to behave as a single cell.\n \n-    # Arguments\n-        x: input tensor.\n-        w: weight matrix.\n-        b: optional bias vector.\n-        dropout: whether to apply dropout (same dropout mask\n-            for every temporal slice of the input).\n-        input_dim: integer; optional dimensionality of the input.\n-        output_dim: integer; optional dimensionality of the output.\n-        timesteps: integer; optional number of timesteps.\n-        training: training phase tensor or boolean.\n-\n-    # Returns\n-        Output tensor.\n+    # Arguments:\n+        cells: List of RNN cell instances.\n     \"\"\"\n-    if not input_dim:\n-        input_dim = K.shape(x)[2]\n-    if not timesteps:\n-        timesteps = K.shape(x)[1]\n-    if not output_dim:\n-        output_dim = K.shape(w)[1]\n-\n-    if dropout is not None and 0. < dropout < 1.:\n-        # apply the same dropout pattern at every timestep\n-        ones = K.ones_like(K.reshape(x[:, 0, :], (-1, input_dim)))\n-        dropout_matrix = K.dropout(ones, dropout)\n-        expanded_dropout_matrix = K.repeat(dropout_matrix, timesteps)\n-        x = K.in_train_phase(x * expanded_dropout_matrix, x, training=training)\n-\n-    # collapse time dimension and batch dimension together\n-    x = K.reshape(x, (-1, input_dim))\n-    x = K.dot(x, w)\n-    if b is not None:\n-        x = K.bias_add(x, b)\n-    # reshape to 3D tensor\n-    if K.backend() == 'tensorflow':\n-        x = K.reshape(x, K.stack([-1, timesteps, output_dim]))\n-        x.set_shape([None, None, output_dim])\n-    else:\n-        x = K.reshape(x, (-1, timesteps, output_dim))\n-    return x\n-\n-\n-class Recurrent(Layer):\n-    \"\"\"Abstract base class for recurrent layers.\n-\n-    Do not use in a model -- it's not a valid layer!\n-    Use its children classes `LSTM`, `GRU` and `SimpleRNN` instead.\n-\n-    All recurrent layers (`LSTM`, `GRU`, `SimpleRNN`) also\n-    follow the specifications of this class and accept\n-    the keyword arguments listed below.\n-\n-    # Example\n \n-    ```python\n-        # as the first layer in a Sequential model\n-        model = Sequential()\n-        model.add(LSTM(32, input_shape=(10, 64)))\n-        # now model.output_shape == (None, 32)\n-        # note: `None` is the batch dimension.\n-\n-        # for subsequent layers, no need to specify the input size:\n-        model.add(LSTM(16))\n-\n-        # to stack recurrent layers, you must use return_sequences=True\n-        # on any recurrent layer that feeds into another recurrent layer.\n-        # note that you only need to specify the input size on the first layer.\n-        model = Sequential()\n-        model.add(LSTM(64, input_dim=64, input_length=10, return_sequences=True))\n-        model.add(LSTM(32, return_sequences=True))\n-        model.add(LSTM(10))\n-    ```\n+    def __init__(self, cells, **kwargs):\n+        for cell in cells:\n+            if not hasattr(cell, 'call'):\n+                raise ValueError('All cells must have a `call` method. '\n+                                 'received cells:', cells)\n+            if not hasattr(cell, 'state_size'):\n+                raise ValueError('All cells must have a '\n+                                 '`state_size` attribute. '\n+                                 'received cells:', cells)\n+        self.cells = cells\n+        super(StackedRNNCells, self).__init__(**kwargs)\n+\n+    @property\n+    def state_size(self):\n+        # States are a flat list\n+        # in reverse order of the cell stack.\n+        # This allows to preserve the requirement\n+        # `stack.state_size[0] == output_dim`.\n+        # e.g. states of a 2-layer LSTM would be\n+        # `[h2, c2, h1, c1]`\n+        # (assuming one LSTM has states [h, c])\n+        state_size = []\n+        for cell in self.cells[::-1]:\n+            if hasattr(cell.state_size, '__len__'):\n+                state_size += list(cell.state_size)\n+            else:\n+                state_size.append(cell.state_size)\n+        return tuple(state_size)\n+\n+    def call(self, inputs, states, **kwargs):\n+        # Recover per-cell states.\n+        nested_states = []\n+        for cell in self.cells[::-1]:\n+            if hasattr(cell.state_size, '__len__'):\n+                nested_states.append(states[:len(cell.state_size)])\n+                states = states[len(cell.state_size):]\n+            else:\n+                nested_states.append([states[0]])\n+                states = states[1:]\n+        nested_states = nested_states[::-1]\n+\n+        # Call the cells in order and store the returned states.\n+        new_nested_states = []\n+        for cell, states in zip(self.cells, nested_states):\n+            inputs, states = cell.call(inputs, states, **kwargs)\n+            new_nested_states.append(states)\n+\n+        # Format the new states as a flat list\n+        # in reverse cell order.\n+        states = []\n+        for cell_states in new_nested_states[::-1]:\n+            states += cell_states\n+        return inputs, states\n+\n+    def build(self, input_shape):\n+        for cell in self.cells:\n+            if isinstance(cell, Layer):\n+                cell.build(input_shape)\n+            if hasattr(cell.state_size, '__len__'):\n+                output_dim = cell.state_size[0]\n+            else:\n+                output_dim = cell.state_size\n+            input_shape = (input_shape[0], input_shape[1], output_dim)\n+        self.built = True\n+\n+    def get_config(self):\n+        cells = []\n+        for cell in self.cells:\n+            cells.append({'class_name': cell.__class__.__name__,\n+                          'config': cell.get_config()})\n+        config = {'cells': cells}\n+        base_config = super(StackedRNNCells, self).get_config()\n+        return dict(list(base_config.items()) + list(config.items()))\n+\n+    @classmethod\n+    def from_config(cls, config, custom_objects=None):\n+        from . import deserialize as deserialize_layer\n+        cells = []\n+        for cell_config in config.pop('cells'):\n+            cells.append(deserialize_layer(cell_config,\n+                                           custom_objects=custom_objects))\n+        return cls(cells, **config)\n+\n+    @property\n+    def trainable_weights(self):\n+        if not self.trainable:\n+            return []\n+        weights = []\n+        for cell in self.cells:\n+            if isinstance(cell, Layer):\n+                weights += cell.trainable_weights\n+        return weights\n+\n+    @property\n+    def non_trainable_weights(self):\n+        weights = []\n+        for cell in self.cells:\n+            if isinstance(cell, Layer):\n+                weights += cell.non_trainable_weights\n+        if not self.trainable:\n+            trainable_weights = []\n+            for cell in self.cells:\n+                if isinstance(cell, Layer):\n+                    trainable_weights += cell.trainable_weights\n+            return trainable_weights + weights\n+        return weights\n+\n+    def get_weights(self):\n+        \"\"\"Retrieves the weights of the model.\n+\n+        # Returns\n+            A flat list of Numpy arrays.\n+        \"\"\"\n+        weights = []\n+        for cell in self.cells:\n+            if isinstance(cell, Layer):\n+                weights += cell.weights\n+        return K.batch_get_value(weights)\n+\n+    def set_weights(self, weights):\n+        \"\"\"Sets the weights of the model.\n+\n+        # Arguments\n+            weights: A list of Numpy arrays with shapes and types matching\n+                the output of `model.get_weights()`.\n+        \"\"\"\n+        tuples = []\n+        for cell in self.cells:\n+            if isinstance(cell, Layer):\n+                num_param = len(cell.weights)\n+                weights = weights[:num_param]\n+                for sw, w in zip(cell.weights, weights):\n+                    tuples.append((sw, w))\n+                weights = weights[num_param:]\n+        K.batch_set_value(tuples)\n+\n+    @property\n+    def losses(self):\n+        losses = []\n+        for cell in self.cells:\n+            if isinstance(cell, Layer):\n+                cell_losses = cell.losses\n+                losses += cell_losses\n+        return losses\n+\n+    def get_losses_for(self, inputs=None):\n+        losses = []\n+        for cell in self.cells:\n+            if isinstance(cell, Layer):\n+                cell_losses = cell.get_losses_for(inputs)\n+                losses += cell_losses\n+        return losses\n+\n+\n+class RNN(Layer):\n+    \"\"\"Base class for recurrent layers.\n \n     # Arguments\n-        weights: list of Numpy arrays to set as initial weights.\n-            The list should have 3 elements, of shapes:\n-            `[(input_dim, output_dim), (output_dim, output_dim), (output_dim,)]`.\n-        return_sequences: Boolean. Whether to return the last output\n+        cell: A RNN cell instance. A RNN cell is a class that has:\n+            - a `call(input_at_t, states_at_t)` method, returning\n+                `(output_at_t, states_at_t_plus_1)`.\n+            - a `state_size` attribute. This can be a single integer\n+                (single state) in which case it is\n+                the size of the recurrent state\n+                (which should be the same as the size of the cell output).\n+                This can also be a list/tuple of integers\n+                (one size per state). In this case, the first entry\n+                (`state_size[0]`) should be the same as\n+                the size of the cell output.\n+            It is also possible for `cell` to be a list of RNN cell instances,\n+            in which cases the cells get stacked on after the other in the RNN,\n+            implementing an efficient stacked RNN.\n+        return_sequences: Boolean. Whether to return the last output.\n             in the output sequence, or the full sequence.\n         return_state: Boolean. Whether to return the last state\n             in addition to the output.\n@@ -110,19 +213,6 @@ class Recurrent(Layer):\n             Unrolling can speed-up a RNN,\n             although it tends to be more memory-intensive.\n             Unrolling is only suitable for short sequences.\n-        implementation: one of {0, 1, or 2}.\n-            If set to 0, the RNN will use\n-            an implementation that uses fewer, larger matrix products,\n-            thus running faster on CPU but consuming more memory.\n-            If set to 1, the RNN will use more matrix products,\n-            but smaller ones, thus running slower\n-            (may actually be faster on GPU) while consuming less memory.\n-            If set to 2 (LSTM/GRU only),\n-            the RNN will combine the input gate,\n-            the forget gate and the output gate into a single matrix,\n-            enabling more time-efficient parallelization on the GPU.\n-            Note: RNN dropout must be shared for all gates,\n-            resulting in a slightly reduced regularization.\n         input_dim: dimensionality of the input (integer).\n             This argument (or alternatively, the keyword argument `input_shape`)\n             is required when using this layer as the first layer in a model.\n@@ -185,40 +275,115 @@ class Recurrent(Layer):\n         calling `reset_states` with the keyword argument `states`. The value of\n         `states` should be a numpy array or list of numpy arrays representing\n         the initial state of the RNN layer.\n+\n+    # Examples\n+\n+    ```python\n+    # First, let's define a RNN Cell, as a layer subclass.\n+\n+    class MinimalRNNCell(keras.layers.Layer):\n+\n+        def __init__(self, units, **kwargs):\n+            self.units = units\n+            self.state_size = units\n+            super(MinimalRNNCell, self).__init__(**kwargs)\n+\n+        def build(self, input_shape):\n+            self.kernel = self.add_weight(shape=(input_shape[-1], self.units),\n+                                          initializer='uniform',\n+                                          name='kernel')\n+            self.recurrent_kernel = self.add_weight(\n+                shape=(self.units, self.units),\n+                initializer='uniform',\n+                name='recurrent_kernel')\n+            self.built = True\n+\n+        def call(self, inputs, states):\n+            prev_output = states[0]\n+            h = K.dot(inputs, self.kernel)\n+            output = h + K.dot(prev_output, self.recurrent_kernel)\n+            return output, [output]\n+\n+    # Let's use this cell in a RNN layer:\n+\n+    cell = MinimalRNNCell(32)\n+    x = keras.Input((None, 5))\n+    layer = RNN(cell)\n+    y = layer(x)\n+\n+    # Here's how to use the cell to build a stacked RNN:\n+\n+    cells = [MinimalRNNCell(32), MinimalRNNCell(64)]\n+    x = keras.Input((None, 5))\n+    layer = RNN(cells)\n+    y = layer(x)\n+    ```\n     \"\"\"\n \n-    def __init__(self, return_sequences=False,\n+    def __init__(self, cell,\n+                 return_sequences=False,\n                  return_state=False,\n                  go_backwards=False,\n                  stateful=False,\n                  unroll=False,\n-                 implementation=0,\n                  **kwargs):\n-        super(Recurrent, self).__init__(**kwargs)\n+        if isinstance(cell, (list, tuple)):\n+            cell = StackedRNNCells(cell)\n+        if not hasattr(cell, 'call'):\n+            raise ValueError('`cell` should have a `call` method. '\n+                             'The RNN was passed:', cell)\n+        if not hasattr(cell, 'state_size'):\n+            raise ValueError('The RNN cell should have '\n+                             'an attribute `state_size` '\n+                             '(tuple of integers, '\n+                             'one integer per RNN state).')\n+        super(RNN, self).__init__(**kwargs)\n+        self.cell = cell\n         self.return_sequences = return_sequences\n         self.return_state = return_state\n         self.go_backwards = go_backwards\n-\n         self.stateful = stateful\n         self.unroll = unroll\n-        self.implementation = implementation\n+\n         self.supports_masking = True\n         self.input_spec = [InputSpec(ndim=3)]\n-        self.state_spec = None\n-        self.dropout = 0\n-        self.recurrent_dropout = 0\n+        if hasattr(self.cell.state_size, '__len__'):\n+            self.state_spec = [InputSpec(shape=(None, dim))\n+                               for dim in self.cell.state_size]\n+        else:\n+            self.state_spec = InputSpec(shape=(None, self.cell.state_size))\n+        self._states = None\n+\n+    @property\n+    def states(self):\n+        if self._states is None:\n+            if isinstance(self.cell.state_size, int):\n+                num_states = 1\n+            else:\n+                num_states = len(self.cell.state_size)\n+            return [None for _ in range(num_states)]\n+        return self._states\n+\n+    @states.setter\n+    def states(self, states):\n+        self._states = states\n \n     def compute_output_shape(self, input_shape):\n         if isinstance(input_shape, list):\n             input_shape = input_shape[0]\n \n+        if hasattr(self.cell.state_size, '__len__'):\n+            output_dim = self.cell.state_size[0]\n+        else:\n+            output_dim = self.cell.state_size\n+\n         if self.return_sequences:\n-            output_shape = (input_shape[0], input_shape[1], self.units)\n+            output_shape = (input_shape[0], input_shape[1], output_dim)\n         else:\n-            output_shape = (input_shape[0], self.units)\n+            output_shape = (input_shape[0], output_dim)\n \n         if self.return_state:\n-            state_shape = [(input_shape[0], self.units) for _ in self.states]\n+            state_shape = [(input_shape[0], output_dim) for _ in self.states]\n             return [output_shape] + state_shape\n         else:\n             return output_shape\n@@ -233,26 +398,33 @@ class Recurrent(Layer):\n         else:\n             return output_mask\n \n-    def step(self, inputs, states):\n-        raise NotImplementedError\n+    def build(self, input_shape):\n+        if isinstance(input_shape, list):\n+            input_shape = input_shape[0]\n+\n+        batch_size = input_shape[0] if self.stateful else None\n+        input_dim = input_shape[-1]\n+        self.input_spec[0] = InputSpec(shape=(batch_size, None, input_dim))\n \n-    def get_constants(self, inputs, training=None):\n-        return []\n+        if self.stateful:\n+            self.reset_states()\n+\n+        if isinstance(self.cell, Layer):\n+            step_input_shape = (input_shape[0],) + input_shape[2:]\n+            self.cell.build(step_input_shape)\n \n     def get_initial_state(self, inputs):\n         # build an all-zero tensor of shape (samples, output_dim)\n         initial_state = K.zeros_like(inputs)  # (samples, timesteps, input_dim)\n         initial_state = K.sum(initial_state, axis=(1, 2))  # (samples,)\n         initial_state = K.expand_dims(initial_state)  # (samples, 1)\n-        initial_state = K.tile(initial_state, [1, self.units])  # (samples, output_dim)\n-        initial_state = [initial_state for _ in range(len(self.states))]\n-        return initial_state\n-\n-    def preprocess_input(self, inputs, training=None):\n-        return inputs\n+        if hasattr(self.cell.state_size, '__len__'):\n+            return [K.tile(initial_state, [1, dim])\n+                    for dim in self.cell.state_size]\n+        else:\n+            return [K.tile(initial_state, [1, self.cell.state_size])]\n \n     def __call__(self, inputs, initial_state=None, **kwargs):\n-\n         # If there are multiple inputs, then\n         # they should be the main input and `initial_state`\n         # e.g. when loading model from file\n@@ -265,7 +437,7 @@ class Recurrent(Layer):\n         # then add it to the inputs and temporarily\n         # modify the input spec to include the state.\n         if initial_state is None:\n-            return super(Recurrent, self).__call__(inputs, **kwargs)\n+            return super(RNN, self).__call__(inputs, **kwargs)\n \n         if not isinstance(initial_state, (list, tuple)):\n             initial_state = [initial_state]\n@@ -291,14 +463,14 @@ class Recurrent(Layer):\n             inputs = [inputs] + list(initial_state)\n \n             # Perform the call\n-            output = super(Recurrent, self).__call__(inputs, **kwargs)\n+            output = super(RNN, self).__call__(inputs, **kwargs)\n \n             # Restore original input spec\n             self.input_spec = input_spec\n             return output\n         else:\n             kwargs['initial_state'] = initial_state\n-            return super(Recurrent, self).__call__(inputs, **kwargs)\n+            return super(RNN, self).__call__(inputs, **kwargs)\n \n     def call(self, inputs, mask=None, training=None, initial_state=None):\n         # input shape: `(samples, time (padded with zeros), input_dim)`\n@@ -336,14 +508,16 @@ class Recurrent(Layer):\n                              '- If using the functional API, specify '\n                              'the time dimension by passing a `shape` '\n                              'or `batch_shape` argument to your Input layer.')\n-        constants = self.get_constants(inputs, training=None)\n-        preprocessed_input = self.preprocess_input(inputs, training=None)\n-        last_output, outputs, states = K.rnn(self.step,\n-                                             preprocessed_input,\n+\n+        if has_arg(self.cell.call, 'training'):\n+            step = functools.partial(self.cell.call, training=training)\n+        else:\n+            step = self.cell.call\n+        last_output, outputs, states = K.rnn(step,\n+                                             inputs,\n                                              initial_state,\n                                              go_backwards=self.go_backwards,\n                                              mask=mask,\n-                                             constants=constants,\n                                              unroll=self.unroll,\n                                              input_length=timesteps)\n         if self.stateful:\n@@ -352,16 +526,15 @@ class Recurrent(Layer):\n                 updates.append((self.states[i], states[i]))\n             self.add_update(updates, inputs)\n \n-        # Properly set learning phase\n-        if 0 < self.dropout + self.recurrent_dropout:\n-            last_output._uses_learning_phase = True\n-            outputs._uses_learning_phase = True\n-\n         if self.return_sequences:\n             output = outputs\n         else:\n             output = last_output\n \n+        # Properly set learning phase\n+        if getattr(last_output, '_uses_learning_phase', False):\n+            output._uses_learning_phase = True\n+\n         if self.return_state:\n             if not isinstance(states, (list, tuple)):\n                 states = [states]\n@@ -388,11 +561,18 @@ class Recurrent(Layer):\n                              '`batch_shape` argument to your Input layer.')\n         # initialize state if None\n         if self.states[0] is None:\n-            self.states = [K.zeros((batch_size, self.units))\n-                           for _ in self.states]\n+            if hasattr(self.cell.state_size, '__len__'):\n+                self.states = [K.zeros((batch_size, dim))\n+                               for dim in self.cell.state_size]\n+            else:\n+                self.states = [K.zeros((batch_size, self.cell.state_size))]\n         elif states is None:\n-            for state in self.states:\n-                K.set_value(state, np.zeros((batch_size, self.units)))\n+            if hasattr(self.cell.state_size, '__len__'):\n+                for state, dim in zip(self.states, self.cell.state_size):\n+                    K.set_value(state, np.zeros((batch_size, dim)))\n+            else:\n+                K.set_value(self.states[0],\n+                            np.zeros((batch_size, self.cell.state_size)))\n         else:\n             if not isinstance(states, (list, tuple)):\n                 states = [states]\n@@ -403,12 +583,17 @@ class Recurrent(Layer):\n                                  ' state values. Input received: ' +\n                                  str(states))\n             for index, (value, state) in enumerate(zip(states, self.states)):\n-                if value.shape != (batch_size, self.units):\n+                if hasattr(self.cell.state_size, '__len__'):\n+                    dim = self.cell.state_size[index]\n+                else:\n+                    dim = self.cell.state_size\n+                if value.shape != (batch_size, dim):\n                     raise ValueError('State ' + str(index) +\n                                      ' is incompatible with layer ' +\n                                      self.name + ': expected shape=' +\n-                                     str((batch_size, self.units)) +\n+                                     str((batch_size, dim)) +\n                                      ', found shape=' + str(value.shape))\n+                # TODO: consider batch calls to `set_value`.\n                 K.set_value(state, value)\n \n     def get_config(self):\n@@ -416,14 +601,47 @@ class Recurrent(Layer):\n                   'return_state': self.return_state,\n                   'go_backwards': self.go_backwards,\n                   'stateful': self.stateful,\n-                  'unroll': self.unroll,\n-                  'implementation': self.implementation}\n-        base_config = super(Recurrent, self).get_config()\n+                  'unroll': self.unroll}\n+        cell_config = self.cell.get_config()\n+        config['cell'] = {'class_name': self.cell.__class__.__name__,\n+                          'config': cell_config}\n+        base_config = super(RNN, self).get_config()\n         return dict(list(base_config.items()) + list(config.items()))\n \n+    @classmethod\n+    def from_config(cls, config, custom_objects=None):\n+        from . import deserialize as deserialize_layer\n+        cell = deserialize_layer(config.pop('cell'),\n+                                 custom_objects=custom_objects)\n+        return cls(cell, **config)\n+\n+    @property\n+    def trainable_weights(self):\n+        if isinstance(self.cell, Layer):\n+            return self.cell.trainable_weights\n+        return []\n+\n+    @property\n+    def non_trainable_weights(self):\n+        if isinstance(self.cell, Layer):\n+            return self.cell.non_trainable_weights\n+        return []\n+\n+    @property\n+    def losses(self):\n+        if isinstance(self.cell, Layer):\n+            return self.cell.losses\n+        return []\n+\n+    def get_losses_for(self, inputs=None):\n+        if isinstance(self.cell, Layer):\n+            cell_losses = self.cell.get_losses_for(inputs)\n+            return cell_losses + super(RNN, self).get_losses_for(inputs)\n+        return super(RNN, self).get_losses_for(inputs)\n \n-class SimpleRNN(Recurrent):\n-    \"\"\"Fully-connected RNN where the output is to be fed back to input.\n+\n+class SimpleRNNCell(Layer):\n+    \"\"\"Cell class for SimpleRNN.\n \n     # Arguments\n         units: Positive integer, dimensionality of the output space.\n@@ -466,12 +684,8 @@ class SimpleRNN(Recurrent):\n         recurrent_dropout: Float between 0 and 1.\n             Fraction of the units to drop for\n             the linear transformation of the recurrent state.\n-\n-    # References\n-        - [A Theoretically Grounded Application of Dropout in Recurrent Neural Networks](http://arxiv.org/abs/1512.05287)\n     \"\"\"\n \n-    @interfaces.legacy_recurrent_support\n     def __init__(self, units,\n                  activation='tanh',\n                  use_bias=True,\n@@ -481,14 +695,13 @@ class SimpleRNN(Recurrent):\n                  kernel_regularizer=None,\n                  recurrent_regularizer=None,\n                  bias_regularizer=None,\n-                 activity_regularizer=None,\n                  kernel_constraint=None,\n                  recurrent_constraint=None,\n                  bias_constraint=None,\n                  dropout=0.,\n                  recurrent_dropout=0.,\n                  **kwargs):\n-        super(SimpleRNN, self).__init__(**kwargs)\n+        super(SimpleRNNCell, self).__init__(**kwargs)\n         self.units = units\n         self.activation = activations.get(activation)\n         self.use_bias = use_bias\n@@ -500,7 +713,6 @@ class SimpleRNN(Recurrent):\n         self.kernel_regularizer = regularizers.get(kernel_regularizer)\n         self.recurrent_regularizer = regularizers.get(recurrent_regularizer)\n         self.bias_regularizer = regularizers.get(bias_regularizer)\n-        self.activity_regularizer = regularizers.get(activity_regularizer)\n \n         self.kernel_constraint = constraints.get(kernel_constraint)\n         self.recurrent_constraint = constraints.get(recurrent_constraint)\n@@ -508,21 +720,12 @@ class SimpleRNN(Recurrent):\n \n         self.dropout = min(1., max(0., dropout))\n         self.recurrent_dropout = min(1., max(0., recurrent_dropout))\n-        self.state_spec = InputSpec(shape=(None, self.units))\n+        self.state_size = self.units\n+        self._dropout_mask = None\n+        self._recurrent_dropout_mask = None\n \n     def build(self, input_shape):\n-        if isinstance(input_shape, list):\n-            input_shape = input_shape[0]\n-\n-        batch_size = input_shape[0] if self.stateful else None\n-        self.input_dim = input_shape[2]\n-        self.input_spec[0] = InputSpec(shape=(batch_size, None, self.input_dim))\n-\n-        self.states = [None]\n-        if self.stateful:\n-            self.reset_states()\n-\n-        self.kernel = self.add_weight(shape=(self.input_dim, self.units),\n+        self.kernel = self.add_weight(shape=(input_shape[-1], self.units),\n                                       name='kernel',\n                                       initializer=self.kernel_initializer,\n                                       regularizer=self.kernel_regularizer,\n@@ -543,76 +746,219 @@ class SimpleRNN(Recurrent):\n             self.bias = None\n         self.built = True\n \n-    def preprocess_input(self, inputs, training=None):\n-        if self.implementation > 0:\n-            return inputs\n+    def _generate_dropout_mask(self, inputs, training=None):\n+        if 0 < self.dropout < 1:\n+            ones = K.ones_like(K.squeeze(inputs[:, 0:1, :], axis=1))\n+\n+            def dropped_inputs():\n+                return K.dropout(ones, self.dropout)\n+\n+            self._dropout_mask = K.in_train_phase(\n+                dropped_inputs,\n+                ones,\n+                training=training)\n         else:\n-            input_shape = K.int_shape(inputs)\n-            input_dim = input_shape[2]\n-            timesteps = input_shape[1]\n-            return _time_distributed_dense(inputs,\n-                                           self.kernel,\n-                                           self.bias,\n-                                           self.dropout,\n-                                           input_dim,\n-                                           self.units,\n-                                           timesteps,\n-                                           training=training)\n-\n-    def step(self, inputs, states):\n-        if self.implementation == 0:\n-            h = inputs\n+            self._dropout_mask = None\n+\n+    def _generate_recurrent_dropout_mask(self, inputs, training=None):\n+        if 0 < self.recurrent_dropout < 1:\n+            ones = K.ones_like(K.reshape(inputs[:, 0, 0], (-1, 1)))\n+            ones = K.tile(ones, (1, self.units))\n+\n+            def dropped_inputs():\n+                return K.dropout(ones, self.dropout)\n+\n+            self._recurrent_dropout_mask = K.in_train_phase(\n+                dropped_inputs,\n+                ones,\n+                training=training)\n         else:\n-            if 0 < self.dropout < 1:\n-                h = K.dot(inputs * states[1], self.kernel)\n-            else:\n-                h = K.dot(inputs, self.kernel)\n-            if self.bias is not None:\n-                h = K.bias_add(h, self.bias)\n+            self._recurrent_dropout_mask = None\n \n+    def call(self, inputs, states, training=None):\n         prev_output = states[0]\n-        if 0 < self.recurrent_dropout < 1:\n-            prev_output *= states[2]\n+        dp_mask = self._dropout_mask\n+        rec_dp_mask = self._recurrent_dropout_mask\n+\n+        if dp_mask is not None:\n+            h = K.dot(inputs * dp_mask, self.kernel)\n+        else:\n+            h = K.dot(inputs, self.kernel)\n+        if self.bias is not None:\n+            h = K.bias_add(h, self.bias)\n+\n+        if rec_dp_mask is not None:\n+            prev_output *= rec_dp_mask\n         output = h + K.dot(prev_output, self.recurrent_kernel)\n         if self.activation is not None:\n             output = self.activation(output)\n \n         # Properly set learning phase on output tensor.\n         if 0 < self.dropout + self.recurrent_dropout:\n-            output._uses_learning_phase = True\n+            if training is None:\n+                output._uses_learning_phase = True\n         return output, [output]\n \n-    def get_constants(self, inputs, training=None):\n-        constants = []\n-        if self.implementation != 0 and 0 < self.dropout < 1:\n-            input_shape = K.int_shape(inputs)\n-            input_dim = input_shape[-1]\n-            ones = K.ones_like(K.reshape(inputs[:, 0, 0], (-1, 1)))\n-            ones = K.tile(ones, (1, int(input_dim)))\n \n-            def dropped_inputs():\n-                return K.dropout(ones, self.dropout)\n+class SimpleRNN(RNN):\n+    \"\"\"Fully-connected RNN where the output is to be fed back to input.\n \n-            dp_mask = K.in_train_phase(dropped_inputs,\n-                                       ones,\n-                                       training=training)\n-            constants.append(dp_mask)\n-        else:\n-            constants.append(K.cast_to_floatx(1.))\n+    # Arguments\n+        units: Positive integer, dimensionality of the output space.\n+        activation: Activation function to use\n+            (see [activations](../activations.md)).\n+            If you pass None, no activation is applied\n+            (ie. \"linear\" activation: `a(x) = x`).\n+        use_bias: Boolean, whether the layer uses a bias vector.\n+        kernel_initializer: Initializer for the `kernel` weights matrix,\n+            used for the linear transformation of the inputs.\n+            (see [initializers](../initializers.md)).\n+        recurrent_initializer: Initializer for the `recurrent_kernel`\n+            weights matrix,\n+            used for the linear transformation of the recurrent state.\n+            (see [initializers](../initializers.md)).\n+        bias_initializer: Initializer for the bias vector\n+            (see [initializers](../initializers.md)).\n+        kernel_regularizer: Regularizer function applied to\n+            the `kernel` weights matrix\n+            (see [regularizer](../regularizers.md)).\n+        recurrent_regularizer: Regularizer function applied to\n+            the `recurrent_kernel` weights matrix\n+            (see [regularizer](../regularizers.md)).\n+        bias_regularizer: Regularizer function applied to the bias vector\n+            (see [regularizer](../regularizers.md)).\n+        activity_regularizer: Regularizer function applied to\n+            the output of the layer (its \"activation\").\n+            (see [regularizer](../regularizers.md)).\n+        kernel_constraint: Constraint function applied to\n+            the `kernel` weights matrix\n+            (see [constraints](../constraints.md)).\n+        recurrent_constraint: Constraint function applied to\n+            the `recurrent_kernel` weights matrix\n+            (see [constraints](../constraints.md)).\n+        bias_constraint: Constraint function applied to the bias vector\n+            (see [constraints](../constraints.md)).\n+        dropout: Float between 0 and 1.\n+            Fraction of the units to drop for\n+            the linear transformation of the inputs.\n+        recurrent_dropout: Float between 0 and 1.\n+            Fraction of the units to drop for\n+            the linear transformation of the recurrent state.\n+    \"\"\"\n \n-        if 0 < self.recurrent_dropout < 1:\n-            ones = K.ones_like(K.reshape(inputs[:, 0, 0], (-1, 1)))\n-            ones = K.tile(ones, (1, self.units))\n+    @interfaces.legacy_recurrent_support\n+    def __init__(self, units,\n+                 activation='tanh',\n+                 use_bias=True,\n+                 kernel_initializer='glorot_uniform',\n+                 recurrent_initializer='orthogonal',\n+                 bias_initializer='zeros',\n+                 kernel_regularizer=None,\n+                 recurrent_regularizer=None,\n+                 bias_regularizer=None,\n+                 activity_regularizer=None,\n+                 kernel_constraint=None,\n+                 recurrent_constraint=None,\n+                 bias_constraint=None,\n+                 dropout=0.,\n+                 recurrent_dropout=0.,\n+                 **kwargs):\n+        if 'implementation' in kwargs:\n+            kwargs.pop('implementation')\n+            warnings.warn('The `implementation` argument '\n+                          'in `SimpleRNN` has been deprecated. '\n+                          'Please remove it from your layer call.')\n+        if K.backend() == 'cntk':\n+            if not kwargs.get('unroll') and (dropout > 0 or recurrent_dropout > 0):\n+                warnings.warn(\n+                    'RNN dropout is not supported with the CNTK backend '\n+                    'when using dynamic RNNs (i.e. non-unrolled). '\n+                    'You can either set `unroll=True`, '\n+                    'set `dropout` and `recurrent_dropout` to 0, '\n+                    'or use a different backend.')\n+                dropout = 0.\n+                recurrent_dropout = 0.\n+\n+        cell = SimpleRNNCell(units,\n+                             activation=activation,\n+                             use_bias=use_bias,\n+                             kernel_initializer=kernel_initializer,\n+                             recurrent_initializer=recurrent_initializer,\n+                             bias_initializer=bias_initializer,\n+                             kernel_regularizer=kernel_regularizer,\n+                             recurrent_regularizer=recurrent_regularizer,\n+                             bias_regularizer=bias_regularizer,\n+                             kernel_constraint=kernel_constraint,\n+                             recurrent_constraint=recurrent_constraint,\n+                             bias_constraint=bias_constraint,\n+                             dropout=dropout,\n+                             recurrent_dropout=recurrent_dropout)\n+        super(SimpleRNN, self).__init__(cell, **kwargs)\n+        self.activity_regularizer = regularizers.get(activity_regularizer)\n \n-            def dropped_inputs():\n-                return K.dropout(ones, self.recurrent_dropout)\n-            rec_dp_mask = K.in_train_phase(dropped_inputs,\n-                                           ones,\n-                                           training=training)\n-            constants.append(rec_dp_mask)\n-        else:\n-            constants.append(K.cast_to_floatx(1.))\n-        return constants\n+    def call(self, inputs, mask=None, training=None, initial_state=None):\n+        self.cell._generate_dropout_mask(inputs, training=training)\n+        self.cell._generate_recurrent_dropout_mask(inputs, training=training)\n+        return super(SimpleRNN, self).call(inputs,\n+                                           mask=mask,\n+                                           training=training,\n+                                           initial_state=initial_state)\n+\n+    @property\n+    def units(self):\n+        return self.cell.units\n+\n+    @property\n+    def activation(self):\n+        return self.cell.activation\n+\n+    @property\n+    def use_bias(self):\n+        return self.cell.use_bias\n+\n+    @property\n+    def kernel_initializer(self):\n+        return self.cell.kernel_initializer\n+\n+    @property\n+    def recurrent_initializer(self):\n+        return self.cell.recurrent_initializer\n+\n+    @property\n+    def bias_initializer(self):\n+        return self.cell.bias_initializer\n+\n+    @property\n+    def kernel_regularizer(self):\n+        return self.cell.kernel_regularizer\n+\n+    @property\n+    def recurrent_regularizer(self):\n+        return self.cell.recurrent_regularizer\n+\n+    @property\n+    def bias_regularizer(self):\n+        return self.cell.bias_regularizer\n+\n+    @property\n+    def kernel_constraint(self):\n+        return self.cell.kernel_constraint\n+\n+    @property\n+    def recurrent_constraint(self):\n+        return self.cell.recurrent_constraint\n+\n+    @property\n+    def bias_constraint(self):\n+        return self.cell.bias_constraint\n+\n+    @property\n+    def dropout(self):\n+        return self.cell.dropout\n+\n+    @property\n+    def recurrent_dropout(self):\n+        return self.cell.recurrent_dropout\n \n     def get_config(self):\n         config = {'units': self.units,\n@@ -631,11 +977,18 @@ class SimpleRNN(Recurrent):\n                   'dropout': self.dropout,\n                   'recurrent_dropout': self.recurrent_dropout}\n         base_config = super(SimpleRNN, self).get_config()\n+        del base_config['cell']\n         return dict(list(base_config.items()) + list(config.items()))\n \n+    @classmethod\n+    def from_config(cls, config):\n+        if 'implementation' in config:\n+            config.pop('implementation')\n+        return cls(**config)\n \n-class GRU(Recurrent):\n-    \"\"\"Gated Recurrent Unit - Cho et al. 2014.\n+\n+class GRUCell(Layer):\n+    \"\"\"Cell class for the GRU layer.\n \n     # Arguments\n         units: Positive integer, dimensionality of the output space.\n@@ -681,14 +1034,9 @@ class GRU(Recurrent):\n         recurrent_dropout: Float between 0 and 1.\n             Fraction of the units to drop for\n             the linear transformation of the recurrent state.\n-\n-    # References\n-        - [On the Properties of Neural Machine Translation: Encoder-Decoder Approaches](https://arxiv.org/abs/1409.1259)\n-        - [Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling](http://arxiv.org/abs/1412.3555v1)\n-        - [A Theoretically Grounded Application of Dropout in Recurrent Neural Networks](http://arxiv.org/abs/1512.05287)\n+        implementation: Implementation mode, either 1 or 2.\n     \"\"\"\n \n-    @interfaces.legacy_recurrent_support\n     def __init__(self, units,\n                  activation='tanh',\n                  recurrent_activation='hard_sigmoid',\n@@ -699,14 +1047,14 @@ class GRU(Recurrent):\n                  kernel_regularizer=None,\n                  recurrent_regularizer=None,\n                  bias_regularizer=None,\n-                 activity_regularizer=None,\n                  kernel_constraint=None,\n                  recurrent_constraint=None,\n                  bias_constraint=None,\n                  dropout=0.,\n                  recurrent_dropout=0.,\n+                 implementation=1,\n                  **kwargs):\n-        super(GRU, self).__init__(**kwargs)\n+        super(GRUCell, self).__init__(**kwargs)\n         self.units = units\n         self.activation = activations.get(activation)\n         self.recurrent_activation = activations.get(recurrent_activation)\n@@ -719,7 +1067,6 @@ class GRU(Recurrent):\n         self.kernel_regularizer = regularizers.get(kernel_regularizer)\n         self.recurrent_regularizer = regularizers.get(recurrent_regularizer)\n         self.bias_regularizer = regularizers.get(bias_regularizer)\n-        self.activity_regularizer = regularizers.get(activity_regularizer)\n \n         self.kernel_constraint = constraints.get(kernel_constraint)\n         self.recurrent_constraint = constraints.get(recurrent_constraint)\n@@ -727,21 +1074,14 @@ class GRU(Recurrent):\n \n         self.dropout = min(1., max(0., dropout))\n         self.recurrent_dropout = min(1., max(0., recurrent_dropout))\n-        self.state_spec = InputSpec(shape=(None, self.units))\n+        self.implementation = implementation\n+        self.state_size = self.units\n+        self._dropout_mask = None\n+        self._recurrent_dropout_mask = None\n \n     def build(self, input_shape):\n-        if isinstance(input_shape, list):\n-            input_shape = input_shape[0]\n-\n-        batch_size = input_shape[0] if self.stateful else None\n-        self.input_dim = input_shape[2]\n-        self.input_spec[0] = InputSpec(shape=(batch_size, None, self.input_dim))\n-\n-        self.states = [None]\n-        if self.stateful:\n-            self.reset_states()\n-\n-        self.kernel = self.add_weight(shape=(self.input_dim, self.units * 3),\n+        input_dim = input_shape[-1]\n+        self.kernel = self.add_weight(shape=(input_dim, self.units * 3),\n                                       name='kernel',\n                                       initializer=self.kernel_initializer,\n                                       regularizer=self.kernel_regularizer,\n@@ -781,67 +1121,86 @@ class GRU(Recurrent):\n             self.bias_h = None\n         self.built = True\n \n-    def preprocess_input(self, inputs, training=None):\n-        if self.implementation == 0:\n-            input_shape = K.int_shape(inputs)\n-            input_dim = input_shape[2]\n-            timesteps = input_shape[1]\n-\n-            x_z = _time_distributed_dense(inputs, self.kernel_z, self.bias_z,\n-                                          self.dropout, input_dim, self.units,\n-                                          timesteps, training=training)\n-            x_r = _time_distributed_dense(inputs, self.kernel_r, self.bias_r,\n-                                          self.dropout, input_dim, self.units,\n-                                          timesteps, training=training)\n-            x_h = _time_distributed_dense(inputs, self.kernel_h, self.bias_h,\n-                                          self.dropout, input_dim, self.units,\n-                                          timesteps, training=training)\n-            return K.concatenate([x_z, x_r, x_h], axis=2)\n-        else:\n-            return inputs\n-\n-    def get_constants(self, inputs, training=None):\n-        constants = []\n-        if self.implementation != 0 and 0 < self.dropout < 1:\n-            input_shape = K.int_shape(inputs)\n-            input_dim = input_shape[-1]\n-            ones = K.ones_like(K.reshape(inputs[:, 0, 0], (-1, 1)))\n-            ones = K.tile(ones, (1, int(input_dim)))\n+    def _generate_dropout_mask(self, inputs, training=None):\n+        if 0 < self.dropout < 1:\n+            ones = K.ones_like(K.squeeze(inputs[:, 0:1, :], axis=1))\n \n             def dropped_inputs():\n                 return K.dropout(ones, self.dropout)\n \n-            dp_mask = [K.in_train_phase(dropped_inputs,\n-                                        ones,\n-                                        training=training) for _ in range(3)]\n-            constants.append(dp_mask)\n+            self._dropout_mask = [K.in_train_phase(\n+                dropped_inputs,\n+                ones,\n+                training=training)\n+                for _ in range(3)]\n         else:\n-            constants.append([K.cast_to_floatx(1.) for _ in range(3)])\n+            self._dropout_mask = None\n \n+    def _generate_recurrent_dropout_mask(self, inputs, training=None):\n         if 0 < self.recurrent_dropout < 1:\n             ones = K.ones_like(K.reshape(inputs[:, 0, 0], (-1, 1)))\n             ones = K.tile(ones, (1, self.units))\n \n             def dropped_inputs():\n-                return K.dropout(ones, self.recurrent_dropout)\n-            rec_dp_mask = [K.in_train_phase(dropped_inputs,\n-                                            ones,\n-                                            training=training) for _ in range(3)]\n-            constants.append(rec_dp_mask)\n+                return K.dropout(ones, self.dropout)\n+\n+            self._recurrent_dropout_mask = [K.in_train_phase(\n+                dropped_inputs,\n+                ones,\n+                training=training)\n+                for _ in range(3)]\n         else:\n-            constants.append([K.cast_to_floatx(1.) for _ in range(3)])\n-        return constants\n+            self._recurrent_dropout_mask = None\n \n-    def step(self, inputs, states):\n+    def call(self, inputs, states, training=None):\n         h_tm1 = states[0]  # previous memory\n-        dp_mask = states[1]  # dropout matrices for recurrent units\n-        rec_dp_mask = states[2]\n \n-        if self.implementation == 2:\n-            matrix_x = K.dot(inputs * dp_mask[0], self.kernel)\n+        # dropout matrices for input units\n+        dp_mask = self._dropout_mask\n+        # dropout matrices for recurrent units\n+        rec_dp_mask = self._recurrent_dropout_mask\n+\n+        if self.implementation == 1:\n+            if 0. < self.dropout < 1.:\n+                inputs_z = inputs * dp_mask[0]\n+                inputs_r = inputs * dp_mask[1]\n+                inputs_h = inputs * dp_mask[2]\n+            else:\n+                inputs_z = inputs\n+                inputs_r = inputs\n+                inputs_h = inputs\n+            x_z = K.dot(inputs_z, self.kernel_z)\n+            x_r = K.dot(inputs_r, self.kernel_r)\n+            x_h = K.dot(inputs_h, self.kernel_h)\n+            if self.use_bias:\n+                x_z = K.bias_add(x_z, self.bias_z)\n+                x_r = K.bias_add(x_r, self.bias_r)\n+                x_h = K.bias_add(x_h, self.bias_h)\n+\n+            if 0. < self.recurrent_dropout < 1.:\n+                h_tm1_z = h_tm1 * rec_dp_mask[0]\n+                h_tm1_r = h_tm1 * rec_dp_mask[1]\n+                h_tm1_h = h_tm1 * rec_dp_mask[2]\n+            else:\n+                h_tm1_z = h_tm1\n+                h_tm1_r = h_tm1\n+                h_tm1_h = h_tm1\n+            z = self.recurrent_activation(x_z + K.dot(h_tm1_z,\n+                                                      self.recurrent_kernel_z))\n+            r = self.recurrent_activation(x_r + K.dot(h_tm1_r,\n+                                                      self.recurrent_kernel_r))\n+\n+            hh = self.activation(x_h + K.dot(r * h_tm1_h,\n+                                             self.recurrent_kernel_h))\n+        else:\n+            if 0. < self.dropout < 1.:\n+                inputs *= dp_mask[0]\n+            matrix_x = K.dot(inputs, self.kernel)\n             if self.use_bias:\n                 matrix_x = K.bias_add(matrix_x, self.bias)\n-            matrix_inner = K.dot(h_tm1 * rec_dp_mask[0],\n+            if 0. < self.recurrent_dropout < 1.:\n+                h_tm1 *= rec_dp_mask[0]\n+            matrix_inner = K.dot(h_tm1,\n                                  self.recurrent_kernel[:, :2 * self.units])\n \n             x_z = matrix_x[:, :self.units]\n@@ -853,36 +1212,196 @@ class GRU(Recurrent):\n             r = self.recurrent_activation(x_r + recurrent_r)\n \n             x_h = matrix_x[:, 2 * self.units:]\n-            recurrent_h = K.dot(r * h_tm1 * rec_dp_mask[0],\n+            recurrent_h = K.dot(r * h_tm1,\n                                 self.recurrent_kernel[:, 2 * self.units:])\n             hh = self.activation(x_h + recurrent_h)\n-        else:\n-            if self.implementation == 0:\n-                x_z = inputs[:, :self.units]\n-                x_r = inputs[:, self.units: 2 * self.units]\n-                x_h = inputs[:, 2 * self.units:]\n-            elif self.implementation == 1:\n-                x_z = K.dot(inputs * dp_mask[0], self.kernel_z)\n-                x_r = K.dot(inputs * dp_mask[1], self.kernel_r)\n-                x_h = K.dot(inputs * dp_mask[2], self.kernel_h)\n-                if self.use_bias:\n-                    x_z = K.bias_add(x_z, self.bias_z)\n-                    x_r = K.bias_add(x_r, self.bias_r)\n-                    x_h = K.bias_add(x_h, self.bias_h)\n-            else:\n-                raise ValueError('Unknown `implementation` mode.')\n-            z = self.recurrent_activation(x_z + K.dot(h_tm1 * rec_dp_mask[0],\n-                                                      self.recurrent_kernel_z))\n-            r = self.recurrent_activation(x_r + K.dot(h_tm1 * rec_dp_mask[1],\n-                                                      self.recurrent_kernel_r))\n-\n-            hh = self.activation(x_h + K.dot(r * h_tm1 * rec_dp_mask[2],\n-                                             self.recurrent_kernel_h))\n         h = z * h_tm1 + (1 - z) * hh\n         if 0 < self.dropout + self.recurrent_dropout:\n-            h._uses_learning_phase = True\n+            if training is None:\n+                h._uses_learning_phase = True\n         return h, [h]\n \n+\n+class GRU(RNN):\n+    \"\"\"Gated Recurrent Unit - Cho et al. 2014.\n+\n+    # Arguments\n+        units: Positive integer, dimensionality of the output space.\n+        activation: Activation function to use\n+            (see [activations](../activations.md)).\n+            If you pass None, no activation is applied\n+            (ie. \"linear\" activation: `a(x) = x`).\n+        recurrent_activation: Activation function to use\n+            for the recurrent step\n+            (see [activations](../activations.md)).\n+        use_bias: Boolean, whether the layer uses a bias vector.\n+        kernel_initializer: Initializer for the `kernel` weights matrix,\n+            used for the linear transformation of the inputs.\n+            (see [initializers](../initializers.md)).\n+        recurrent_initializer: Initializer for the `recurrent_kernel`\n+            weights matrix,\n+            used for the linear transformation of the recurrent state.\n+            (see [initializers](../initializers.md)).\n+        bias_initializer: Initializer for the bias vector\n+            (see [initializers](../initializers.md)).\n+        kernel_regularizer: Regularizer function applied to\n+            the `kernel` weights matrix\n+            (see [regularizer](../regularizers.md)).\n+        recurrent_regularizer: Regularizer function applied to\n+            the `recurrent_kernel` weights matrix\n+            (see [regularizer](../regularizers.md)).\n+        bias_regularizer: Regularizer function applied to the bias vector\n+            (see [regularizer](../regularizers.md)).\n+        activity_regularizer: Regularizer function applied to\n+            the output of the layer (its \"activation\").\n+            (see [regularizer](../regularizers.md)).\n+        kernel_constraint: Constraint function applied to\n+            the `kernel` weights matrix\n+            (see [constraints](../constraints.md)).\n+        recurrent_constraint: Constraint function applied to\n+            the `recurrent_kernel` weights matrix\n+            (see [constraints](../constraints.md)).\n+        bias_constraint: Constraint function applied to the bias vector\n+            (see [constraints](../constraints.md)).\n+        dropout: Float between 0 and 1.\n+            Fraction of the units to drop for\n+            the linear transformation of the inputs.\n+        recurrent_dropout: Float between 0 and 1.\n+            Fraction of the units to drop for\n+            the linear transformation of the recurrent state.\n+        implementation: Implementation mode, either 1 or 2.\n+\n+    # References\n+        - [On the Properties of Neural Machine Translation: Encoder-Decoder Approaches](https://arxiv.org/abs/1409.1259)\n+        - [Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling](http://arxiv.org/abs/1412.3555v1)\n+        - [A Theoretically Grounded Application of Dropout in Recurrent Neural Networks](http://arxiv.org/abs/1512.05287)\n+    \"\"\"\n+\n+    @interfaces.legacy_recurrent_support\n+    def __init__(self, units,\n+                 activation='tanh',\n+                 recurrent_activation='hard_sigmoid',\n+                 use_bias=True,\n+                 kernel_initializer='glorot_uniform',\n+                 recurrent_initializer='orthogonal',\n+                 bias_initializer='zeros',\n+                 kernel_regularizer=None,\n+                 recurrent_regularizer=None,\n+                 bias_regularizer=None,\n+                 activity_regularizer=None,\n+                 kernel_constraint=None,\n+                 recurrent_constraint=None,\n+                 bias_constraint=None,\n+                 dropout=0.,\n+                 recurrent_dropout=0.,\n+                 implementation=1,\n+                 **kwargs):\n+        if implementation == 0:\n+            warnings.warn('`implementation=0` has been deprecated, '\n+                          'and now defaults to `implementation=1`.'\n+                          'Please update your layer call.')\n+        if K.backend() == 'cntk':\n+            if not kwargs.get('unroll') and (dropout > 0 or recurrent_dropout > 0):\n+                warnings.warn(\n+                    'RNN dropout is not supported with the CNTK backend '\n+                    'when using dynamic RNNs (i.e. non-unrolled). '\n+                    'You can either set `unroll=True`, '\n+                    'set `dropout` and `recurrent_dropout` to 0, '\n+                    'or use a different backend.')\n+                dropout = 0.\n+                recurrent_dropout = 0.\n+\n+        cell = GRUCell(units,\n+                       activation=activation,\n+                       recurrent_activation=recurrent_activation,\n+                       use_bias=use_bias,\n+                       kernel_initializer=kernel_initializer,\n+                       recurrent_initializer=recurrent_initializer,\n+                       bias_initializer=bias_initializer,\n+                       kernel_regularizer=kernel_regularizer,\n+                       recurrent_regularizer=recurrent_regularizer,\n+                       bias_regularizer=bias_regularizer,\n+                       kernel_constraint=kernel_constraint,\n+                       recurrent_constraint=recurrent_constraint,\n+                       bias_constraint=bias_constraint,\n+                       dropout=dropout,\n+                       recurrent_dropout=recurrent_dropout,\n+                       implementation=implementation)\n+        super(GRU, self).__init__(cell, **kwargs)\n+        self.activity_regularizer = regularizers.get(activity_regularizer)\n+\n+    def call(self, inputs, mask=None, training=None, initial_state=None):\n+        self.cell._generate_dropout_mask(inputs, training=training)\n+        self.cell._generate_recurrent_dropout_mask(inputs, training=training)\n+        return super(GRU, self).call(inputs,\n+                                     mask=mask,\n+                                     training=training,\n+                                     initial_state=initial_state)\n+\n+    @property\n+    def units(self):\n+        return self.cell.units\n+\n+    @property\n+    def activation(self):\n+        return self.cell.activation\n+\n+    @property\n+    def recurrent_activation(self):\n+        return self.cell.recurrent_activation\n+\n+    @property\n+    def use_bias(self):\n+        return self.cell.use_bias\n+\n+    @property\n+    def kernel_initializer(self):\n+        return self.cell.kernel_initializer\n+\n+    @property\n+    def recurrent_initializer(self):\n+        return self.cell.recurrent_initializer\n+\n+    @property\n+    def bias_initializer(self):\n+        return self.cell.bias_initializer\n+\n+    @property\n+    def kernel_regularizer(self):\n+        return self.cell.kernel_regularizer\n+\n+    @property\n+    def recurrent_regularizer(self):\n+        return self.cell.recurrent_regularizer\n+\n+    @property\n+    def bias_regularizer(self):\n+        return self.cell.bias_regularizer\n+\n+    @property\n+    def kernel_constraint(self):\n+        return self.cell.kernel_constraint\n+\n+    @property\n+    def recurrent_constraint(self):\n+        return self.cell.recurrent_constraint\n+\n+    @property\n+    def bias_constraint(self):\n+        return self.cell.bias_constraint\n+\n+    @property\n+    def dropout(self):\n+        return self.cell.dropout\n+\n+    @property\n+    def recurrent_dropout(self):\n+        return self.cell.recurrent_dropout\n+\n+    @property\n+    def implementation(self):\n+        return self.cell.implementation\n+\n     def get_config(self):\n         config = {'units': self.units,\n                   'activation': activations.serialize(self.activation),\n@@ -899,16 +1418,21 @@ class GRU(Recurrent):\n                   'recurrent_constraint': constraints.serialize(self.recurrent_constraint),\n                   'bias_constraint': constraints.serialize(self.bias_constraint),\n                   'dropout': self.dropout,\n-                  'recurrent_dropout': self.recurrent_dropout}\n+                  'recurrent_dropout': self.recurrent_dropout,\n+                  'implementation': self.implementation}\n         base_config = super(GRU, self).get_config()\n+        del base_config['cell']\n         return dict(list(base_config.items()) + list(config.items()))\n \n+    @classmethod\n+    def from_config(cls, config):\n+        if 'implementation' in config and config['implementation'] == 0:\n+            config['implementation'] = 1\n+        return cls(**config)\n \n-class LSTM(Recurrent):\n-    \"\"\"Long-Short Term Memory unit - Hochreiter 1997.\n \n-    For a step-by-step description of the algorithm, see\n-    [this tutorial](http://deeplearning.net/tutorial/lstm.html).\n+class LSTMCell(Layer):\n+    \"\"\"Cell class for the LSTM layer.\n \n     # Arguments\n         units: Positive integer, dimensionality of the output space.\n@@ -958,14 +1482,9 @@ class LSTM(Recurrent):\n         recurrent_dropout: Float between 0 and 1.\n             Fraction of the units to drop for\n             the linear transformation of the recurrent state.\n-\n-    # References\n-        - [Long short-term memory](http://www.bioinf.jku.at/publications/older/2604.pdf) (original 1997 paper)\n-        - [Learning to forget: Continual prediction with LSTM](http://www.mitpressjournals.org/doi/pdf/10.1162/089976600300015015)\n-        - [Supervised sequence labeling with recurrent neural networks](http://www.cs.toronto.edu/~graves/preprint.pdf)\n-        - [A Theoretically Grounded Application of Dropout in Recurrent Neural Networks](http://arxiv.org/abs/1512.05287)\n+        implementation: Implementation mode, either 1 or 2.\n     \"\"\"\n-    @interfaces.legacy_recurrent_support\n+\n     def __init__(self, units,\n                  activation='tanh',\n                  recurrent_activation='hard_sigmoid',\n@@ -977,14 +1496,14 @@ class LSTM(Recurrent):\n                  kernel_regularizer=None,\n                  recurrent_regularizer=None,\n                  bias_regularizer=None,\n-                 activity_regularizer=None,\n                  kernel_constraint=None,\n                  recurrent_constraint=None,\n                  bias_constraint=None,\n                  dropout=0.,\n                  recurrent_dropout=0.,\n+                 implementation=1,\n                  **kwargs):\n-        super(LSTM, self).__init__(**kwargs)\n+        super(LSTMCell, self).__init__(**kwargs)\n         self.units = units\n         self.activation = activations.get(activation)\n         self.recurrent_activation = activations.get(recurrent_activation)\n@@ -998,7 +1517,6 @@ class LSTM(Recurrent):\n         self.kernel_regularizer = regularizers.get(kernel_regularizer)\n         self.recurrent_regularizer = regularizers.get(recurrent_regularizer)\n         self.bias_regularizer = regularizers.get(bias_regularizer)\n-        self.activity_regularizer = regularizers.get(activity_regularizer)\n \n         self.kernel_constraint = constraints.get(kernel_constraint)\n         self.recurrent_constraint = constraints.get(recurrent_constraint)\n@@ -1006,22 +1524,14 @@ class LSTM(Recurrent):\n \n         self.dropout = min(1., max(0., dropout))\n         self.recurrent_dropout = min(1., max(0., recurrent_dropout))\n-        self.state_spec = [InputSpec(shape=(None, self.units)),\n-                           InputSpec(shape=(None, self.units))]\n+        self.implementation = implementation\n+        self.state_size = (self.units, self.units)\n+        self._dropout_mask = None\n+        self._recurrent_dropout_mask = None\n \n     def build(self, input_shape):\n-        if isinstance(input_shape, list):\n-            input_shape = input_shape[0]\n-\n-        batch_size = input_shape[0] if self.stateful else None\n-        self.input_dim = input_shape[2]\n-        self.input_spec[0] = InputSpec(shape=(batch_size, None, self.input_dim))\n-\n-        self.states = [None, None]\n-        if self.stateful:\n-            self.reset_states()\n-\n-        self.kernel = self.add_weight(shape=(self.input_dim, self.units * 4),\n+        input_dim = input_shape[-1]\n+        self.kernel = self.add_weight(shape=(input_dim, self.units * 4),\n                                       name='kernel',\n                                       initializer=self.kernel_initializer,\n                                       regularizer=self.kernel_regularizer,\n@@ -1073,69 +1583,87 @@ class LSTM(Recurrent):\n             self.bias_o = None\n         self.built = True\n \n-    def preprocess_input(self, inputs, training=None):\n-        if self.implementation == 0:\n-            input_shape = K.int_shape(inputs)\n-            input_dim = input_shape[2]\n-            timesteps = input_shape[1]\n-\n-            x_i = _time_distributed_dense(inputs, self.kernel_i, self.bias_i,\n-                                          self.dropout, input_dim, self.units,\n-                                          timesteps, training=training)\n-            x_f = _time_distributed_dense(inputs, self.kernel_f, self.bias_f,\n-                                          self.dropout, input_dim, self.units,\n-                                          timesteps, training=training)\n-            x_c = _time_distributed_dense(inputs, self.kernel_c, self.bias_c,\n-                                          self.dropout, input_dim, self.units,\n-                                          timesteps, training=training)\n-            x_o = _time_distributed_dense(inputs, self.kernel_o, self.bias_o,\n-                                          self.dropout, input_dim, self.units,\n-                                          timesteps, training=training)\n-            return K.concatenate([x_i, x_f, x_c, x_o], axis=2)\n-        else:\n-            return inputs\n-\n-    def get_constants(self, inputs, training=None):\n-        constants = []\n-        if self.implementation != 0 and 0 < self.dropout < 1:\n-            input_shape = K.int_shape(inputs)\n-            input_dim = input_shape[-1]\n-            ones = K.ones_like(K.reshape(inputs[:, 0, 0], (-1, 1)))\n-            ones = K.tile(ones, (1, int(input_dim)))\n+    def _generate_dropout_mask(self, inputs, training=None):\n+        if 0 < self.dropout < 1:\n+            ones = K.ones_like(K.squeeze(inputs[:, 0:1, :], axis=1))\n \n             def dropped_inputs():\n                 return K.dropout(ones, self.dropout)\n \n-            dp_mask = [K.in_train_phase(dropped_inputs,\n-                                        ones,\n-                                        training=training) for _ in range(4)]\n-            constants.append(dp_mask)\n+            self._dropout_mask = [K.in_train_phase(\n+                dropped_inputs,\n+                ones,\n+                training=training)\n+                for _ in range(4)]\n         else:\n-            constants.append([K.cast_to_floatx(1.) for _ in range(4)])\n+            self._dropout_mask = None\n \n+    def _generate_recurrent_dropout_mask(self, inputs, training=None):\n         if 0 < self.recurrent_dropout < 1:\n             ones = K.ones_like(K.reshape(inputs[:, 0, 0], (-1, 1)))\n             ones = K.tile(ones, (1, self.units))\n \n             def dropped_inputs():\n-                return K.dropout(ones, self.recurrent_dropout)\n-            rec_dp_mask = [K.in_train_phase(dropped_inputs,\n-                                            ones,\n-                                            training=training) for _ in range(4)]\n-            constants.append(rec_dp_mask)\n+                return K.dropout(ones, self.dropout)\n+\n+            self._recurrent_dropout_mask = [K.in_train_phase(\n+                dropped_inputs,\n+                ones,\n+                training=training)\n+                for _ in range(4)]\n+        else:\n+            self._recurrent_dropout_mask = None\n+\n+    def call(self, inputs, states, training=None):\n+        # dropout matrices for input units\n+        dp_mask = self._dropout_mask\n+        # dropout matrices for recurrent units\n+        rec_dp_mask = self._recurrent_dropout_mask\n+\n+        h_tm1 = states[0]  # previous memory state\n+        c_tm1 = states[1]  # previous carry state\n+\n+        if self.implementation == 1:\n+            if 0 < self.dropout < 1.:\n+                inputs_i = inputs * dp_mask[0]\n+                inputs_f = inputs * dp_mask[1]\n+                inputs_c = inputs * dp_mask[2]\n+                inputs_o = inputs * dp_mask[3]\n+            else:\n+                inputs_i = inputs\n+                inputs_f = inputs\n+                inputs_c = inputs\n+                inputs_o = inputs\n+            x_i = K.dot(inputs_i, self.kernel_i) + self.bias_i\n+            x_f = K.dot(inputs_f, self.kernel_f) + self.bias_f\n+            x_c = K.dot(inputs_c, self.kernel_c) + self.bias_c\n+            x_o = K.dot(inputs_o, self.kernel_o) + self.bias_o\n+\n+            if 0 < self.recurrent_dropout < 1.:\n+                h_tm1_i = h_tm1 * rec_dp_mask[0]\n+                h_tm1_f = h_tm1 * rec_dp_mask[1]\n+                h_tm1_c = h_tm1 * rec_dp_mask[2]\n+                h_tm1_o = h_tm1 * rec_dp_mask[3]\n+            else:\n+                h_tm1_i = h_tm1\n+                h_tm1_f = h_tm1\n+                h_tm1_c = h_tm1\n+                h_tm1_o = h_tm1\n+            i = self.recurrent_activation(x_i + K.dot(h_tm1_i,\n+                                                      self.recurrent_kernel_i))\n+            f = self.recurrent_activation(x_f + K.dot(h_tm1_f,\n+                                                      self.recurrent_kernel_f))\n+            c = f * c_tm1 + i * self.activation(x_c + K.dot(h_tm1_c,\n+                                                            self.recurrent_kernel_c))\n+            o = self.recurrent_activation(x_o + K.dot(h_tm1_o,\n+                                                      self.recurrent_kernel_o))\n         else:\n-            constants.append([K.cast_to_floatx(1.) for _ in range(4)])\n-        return constants\n-\n-    def step(self, inputs, states):\n-        h_tm1 = states[0]\n-        c_tm1 = states[1]\n-        dp_mask = states[2]\n-        rec_dp_mask = states[3]\n-\n-        if self.implementation == 2:\n-            z = K.dot(inputs * dp_mask[0], self.kernel)\n-            z += K.dot(h_tm1 * rec_dp_mask[0], self.recurrent_kernel)\n+            if 0. < self.dropout < 1.:\n+                inputs *= dp_mask[0]\n+            z = K.dot(inputs, self.kernel)\n+            if 0. < self.recurrent_dropout < 1.:\n+                h_tm1 *= rec_dp_mask[0]\n+            z += K.dot(h_tm1, self.recurrent_kernel)\n             if self.use_bias:\n                 z = K.bias_add(z, self.bias)\n \n@@ -1148,33 +1676,205 @@ class LSTM(Recurrent):\n             f = self.recurrent_activation(z1)\n             c = f * c_tm1 + i * self.activation(z2)\n             o = self.recurrent_activation(z3)\n-        else:\n-            if self.implementation == 0:\n-                x_i = inputs[:, :self.units]\n-                x_f = inputs[:, self.units: 2 * self.units]\n-                x_c = inputs[:, 2 * self.units: 3 * self.units]\n-                x_o = inputs[:, 3 * self.units:]\n-            elif self.implementation == 1:\n-                x_i = K.dot(inputs * dp_mask[0], self.kernel_i) + self.bias_i\n-                x_f = K.dot(inputs * dp_mask[1], self.kernel_f) + self.bias_f\n-                x_c = K.dot(inputs * dp_mask[2], self.kernel_c) + self.bias_c\n-                x_o = K.dot(inputs * dp_mask[3], self.kernel_o) + self.bias_o\n-            else:\n-                raise ValueError('Unknown `implementation` mode.')\n \n-            i = self.recurrent_activation(x_i + K.dot(h_tm1 * rec_dp_mask[0],\n-                                                      self.recurrent_kernel_i))\n-            f = self.recurrent_activation(x_f + K.dot(h_tm1 * rec_dp_mask[1],\n-                                                      self.recurrent_kernel_f))\n-            c = f * c_tm1 + i * self.activation(x_c + K.dot(h_tm1 * rec_dp_mask[2],\n-                                                            self.recurrent_kernel_c))\n-            o = self.recurrent_activation(x_o + K.dot(h_tm1 * rec_dp_mask[3],\n-                                                      self.recurrent_kernel_o))\n         h = o * self.activation(c)\n         if 0 < self.dropout + self.recurrent_dropout:\n-            h._uses_learning_phase = True\n+            if training is None:\n+                h._uses_learning_phase = True\n         return h, [h, c]\n \n+\n+class LSTM(RNN):\n+    \"\"\"Long-Short Term Memory layer - Hochreiter 1997.\n+\n+    # Arguments\n+        units: Positive integer, dimensionality of the output space.\n+        activation: Activation function to use\n+            (see [activations](../activations.md)).\n+            If you pass None, no activation is applied\n+            (ie. \"linear\" activation: `a(x) = x`).\n+        recurrent_activation: Activation function to use\n+            for the recurrent step\n+            (see [activations](../activations.md)).\n+        use_bias: Boolean, whether the layer uses a bias vector.\n+        kernel_initializer: Initializer for the `kernel` weights matrix,\n+            used for the linear transformation of the inputs.\n+            (see [initializers](../initializers.md)).\n+        recurrent_initializer: Initializer for the `recurrent_kernel`\n+            weights matrix,\n+            used for the linear transformation of the recurrent state.\n+            (see [initializers](../initializers.md)).\n+        bias_initializer: Initializer for the bias vector\n+            (see [initializers](../initializers.md)).\n+        unit_forget_bias: Boolean.\n+            If True, add 1 to the bias of the forget gate at initialization.\n+            Setting it to true will also force `bias_initializer=\"zeros\"`.\n+            This is recommended in [Jozefowicz et al.](http://www.jmlr.org/proceedings/papers/v37/jozefowicz15.pdf)\n+        kernel_regularizer: Regularizer function applied to\n+            the `kernel` weights matrix\n+            (see [regularizer](../regularizers.md)).\n+        recurrent_regularizer: Regularizer function applied to\n+            the `recurrent_kernel` weights matrix\n+            (see [regularizer](../regularizers.md)).\n+        bias_regularizer: Regularizer function applied to the bias vector\n+            (see [regularizer](../regularizers.md)).\n+        activity_regularizer: Regularizer function applied to\n+            the output of the layer (its \"activation\").\n+            (see [regularizer](../regularizers.md)).\n+        kernel_constraint: Constraint function applied to\n+            the `kernel` weights matrix\n+            (see [constraints](../constraints.md)).\n+        recurrent_constraint: Constraint function applied to\n+            the `recurrent_kernel` weights matrix\n+            (see [constraints](../constraints.md)).\n+        bias_constraint: Constraint function applied to the bias vector\n+            (see [constraints](../constraints.md)).\n+        dropout: Float between 0 and 1.\n+            Fraction of the units to drop for\n+            the linear transformation of the inputs.\n+        recurrent_dropout: Float between 0 and 1.\n+            Fraction of the units to drop for\n+            the linear transformation of the recurrent state.\n+        implementation: Implementation mode, either 1 or 2.\n+\n+    # References\n+        - [Long short-term memory](http://www.bioinf.jku.at/publications/older/2604.pdf) (original 1997 paper)\n+        - [Learning to forget: Continual prediction with LSTM](http://www.mitpressjournals.org/doi/pdf/10.1162/089976600300015015)\n+        - [Supervised sequence labeling with recurrent neural networks](http://www.cs.toronto.edu/~graves/preprint.pdf)\n+        - [A Theoretically Grounded Application of Dropout in Recurrent Neural Networks](http://arxiv.org/abs/1512.05287)\n+    \"\"\"\n+\n+    @interfaces.legacy_recurrent_support\n+    def __init__(self, units,\n+                 activation='tanh',\n+                 recurrent_activation='hard_sigmoid',\n+                 use_bias=True,\n+                 kernel_initializer='glorot_uniform',\n+                 recurrent_initializer='orthogonal',\n+                 bias_initializer='zeros',\n+                 unit_forget_bias=True,\n+                 kernel_regularizer=None,\n+                 recurrent_regularizer=None,\n+                 bias_regularizer=None,\n+                 activity_regularizer=None,\n+                 kernel_constraint=None,\n+                 recurrent_constraint=None,\n+                 bias_constraint=None,\n+                 dropout=0.,\n+                 recurrent_dropout=0.,\n+                 implementation=1,\n+                 **kwargs):\n+        if implementation == 0:\n+            warnings.warn('`implementation=0` has been deprecated, '\n+                          'and now defaults to `implementation=1`.'\n+                          'Please update your layer call.')\n+        if K.backend() == 'cntk':\n+            if not kwargs.get('unroll') and (dropout > 0 or recurrent_dropout > 0):\n+                warnings.warn(\n+                    'RNN dropout is not supported with the CNTK backend '\n+                    'when using dynamic RNNs (i.e. non-unrolled). '\n+                    'You can either set `unroll=True`, '\n+                    'set `dropout` and `recurrent_dropout` to 0, '\n+                    'or use a different backend.')\n+                dropout = 0.\n+                recurrent_dropout = 0.\n+\n+        cell = LSTMCell(units,\n+                        activation=activation,\n+                        recurrent_activation=recurrent_activation,\n+                        use_bias=use_bias,\n+                        kernel_initializer=kernel_initializer,\n+                        recurrent_initializer=recurrent_initializer,\n+                        unit_forget_bias=unit_forget_bias,\n+                        bias_initializer=bias_initializer,\n+                        kernel_regularizer=kernel_regularizer,\n+                        recurrent_regularizer=recurrent_regularizer,\n+                        bias_regularizer=bias_regularizer,\n+                        kernel_constraint=kernel_constraint,\n+                        recurrent_constraint=recurrent_constraint,\n+                        bias_constraint=bias_constraint,\n+                        dropout=dropout,\n+                        recurrent_dropout=recurrent_dropout,\n+                        implementation=implementation)\n+        super(LSTM, self).__init__(cell, **kwargs)\n+        self.activity_regularizer = regularizers.get(activity_regularizer)\n+\n+    def call(self, inputs, mask=None, training=None, initial_state=None):\n+        self.cell._generate_dropout_mask(inputs, training=training)\n+        self.cell._generate_recurrent_dropout_mask(inputs, training=training)\n+        return super(LSTM, self).call(inputs,\n+                                      mask=mask,\n+                                      training=training,\n+                                      initial_state=initial_state)\n+\n+    @property\n+    def units(self):\n+        return self.cell.units\n+\n+    @property\n+    def activation(self):\n+        return self.cell.activation\n+\n+    @property\n+    def recurrent_activation(self):\n+        return self.cell.recurrent_activation\n+\n+    @property\n+    def use_bias(self):\n+        return self.cell.use_bias\n+\n+    @property\n+    def kernel_initializer(self):\n+        return self.cell.kernel_initializer\n+\n+    @property\n+    def recurrent_initializer(self):\n+        return self.cell.recurrent_initializer\n+\n+    @property\n+    def bias_initializer(self):\n+        return self.cell.bias_initializer\n+\n+    @property\n+    def unit_forget_bias(self):\n+        return self.cell.unit_forget_bias\n+\n+    @property\n+    def kernel_regularizer(self):\n+        return self.cell.kernel_regularizer\n+\n+    @property\n+    def recurrent_regularizer(self):\n+        return self.cell.recurrent_regularizer\n+\n+    @property\n+    def bias_regularizer(self):\n+        return self.cell.bias_regularizer\n+\n+    @property\n+    def kernel_constraint(self):\n+        return self.cell.kernel_constraint\n+\n+    @property\n+    def recurrent_constraint(self):\n+        return self.cell.recurrent_constraint\n+\n+    @property\n+    def bias_constraint(self):\n+        return self.cell.bias_constraint\n+\n+    @property\n+    def dropout(self):\n+        return self.cell.dropout\n+\n+    @property\n+    def recurrent_dropout(self):\n+        return self.cell.recurrent_dropout\n+\n+    @property\n+    def implementation(self):\n+        return self.cell.implementation\n+\n     def get_config(self):\n         config = {'units': self.units,\n                   'activation': activations.serialize(self.activation),\n@@ -1192,6 +1892,14 @@ class LSTM(Recurrent):\n                   'recurrent_constraint': constraints.serialize(self.recurrent_constraint),\n                   'bias_constraint': constraints.serialize(self.bias_constraint),\n                   'dropout': self.dropout,\n-                  'recurrent_dropout': self.recurrent_dropout}\n+                  'recurrent_dropout': self.recurrent_dropout,\n+                  'implementation': self.implementation}\n         base_config = super(LSTM, self).get_config()\n+        del base_config['cell']\n         return dict(list(base_config.items()) + list(config.items()))\n+\n+    @classmethod\n+    def from_config(cls, config):\n+        if 'implementation' in config and config['implementation'] == 0:\n+            config['implementation'] = 1\n+        return cls(**config)\n",
      "files_name_in_blame_commit": [
        "layers.py",
        "lstm_benchmark.py",
        "recurrent_test.py",
        "tensorflow_backend.py",
        "wrappers.py",
        "cntk_backend.py",
        "theano_backend.py",
        "recurrent.py"
      ]
    }
  },
  "commits_modify_file_before_fix": {
    "size": 198
  },
  "recursive_blame_commits": {
    "recursive_blame_function_lines": {
      "513": {
        "commit_id": "295bfe4e3ae7e98655b3630a9f83b2df4a82234f",
        "line_code": "    def build(self, input_shape):",
        "commit_date": "2016-03-31 11:35:27",
        "valid": 1
      },
      "514": {
        "commit_id": "a214f4e64d9e1f09a6ffdcad5e9595d0534e08f0",
        "line_code": "        if isinstance(input_shape, list):",
        "commit_date": "2017-03-03 20:14:22",
        "valid": 1
      },
      "515": {
        "commit_id": "a214f4e64d9e1f09a6ffdcad5e9595d0534e08f0",
        "line_code": "            input_shape = input_shape[0]",
        "commit_date": "2017-03-03 20:14:22",
        "valid": 1
      },
      "516": {
        "commit_id": "e45bce14b7b55e2fef99e1cd18a0a06681e6956b",
        "line_code": "",
        "commit_date": "2017-03-04 16:27:03",
        "valid": 0
      },
      "517": {
        "commit_id": "16343b3261da08a8aa52f209a8609ce4b51c7fdf",
        "line_code": "        batch_size = input_shape[0] if self.stateful else None",
        "commit_date": "2017-03-10 23:45:20",
        "valid": 1
      },
      "518": {
        "commit_id": "e45bce14b7b55e2fef99e1cd18a0a06681e6956b",
        "line_code": "        self.input_dim = input_shape[2]",
        "commit_date": "2017-03-04 16:27:03",
        "valid": 1
      },
      "519": {
        "commit_id": "365f621b24631a03f995e3b30e1800d327e42fc1",
        "line_code": "        self.input_spec[0] = InputSpec(shape=(batch_size, None, self.input_dim))",
        "commit_date": "2017-04-24 20:20:04",
        "valid": 1
      },
      "520": {
        "commit_id": "e45bce14b7b55e2fef99e1cd18a0a06681e6956b",
        "line_code": "",
        "commit_date": "2017-03-04 16:27:03",
        "valid": 0
      },
      "521": {
        "commit_id": "e45bce14b7b55e2fef99e1cd18a0a06681e6956b",
        "line_code": "        self.states = [None]",
        "commit_date": "2017-03-04 16:27:03",
        "valid": 1
      },
      "522": {
        "commit_id": "47ed18a3af8be20dce91286a331d4671074ee0ca",
        "line_code": "        if self.stateful:",
        "commit_date": "2015-11-26 10:42:52",
        "valid": 1
      },
      "523": {
        "commit_id": "07ffc76b9318ab1197ce61838992960e9abb3c22",
        "line_code": "            self.reset_states()",
        "commit_date": "2015-12-08 10:43:06",
        "valid": 1
      },
      "524": {
        "commit_id": "37a1db225420851cc668600c49697d9a2057f098",
        "line_code": "",
        "commit_date": "2015-03-27 17:59:42",
        "valid": 0
      },
      "525": {
        "commit_id": "8830c53135dcad4825e2f8805c78523931c3bbed",
        "line_code": "        self.kernel = self.add_weight(shape=(self.input_dim, self.units),",
        "commit_date": "2017-04-18 11:34:24",
        "valid": 1
      },
      "526": {
        "commit_id": "03a7eb89e27b70f2ca0ac932ef4bace7569d6fab",
        "line_code": "                                      name='kernel',",
        "commit_date": "2017-02-13 16:55:19",
        "valid": 1
      },
      "527": {
        "commit_id": "03a7eb89e27b70f2ca0ac932ef4bace7569d6fab",
        "line_code": "                                      initializer=self.kernel_initializer,",
        "commit_date": "2017-02-13 16:55:19",
        "valid": 1
      },
      "528": {
        "commit_id": "03a7eb89e27b70f2ca0ac932ef4bace7569d6fab",
        "line_code": "                                      regularizer=self.kernel_regularizer,",
        "commit_date": "2017-02-13 16:55:19",
        "valid": 1
      },
      "529": {
        "commit_id": "03a7eb89e27b70f2ca0ac932ef4bace7569d6fab",
        "line_code": "                                      constraint=self.kernel_constraint)",
        "commit_date": "2017-02-13 16:55:19",
        "valid": 1
      },
      "530": {
        "commit_id": "03a7eb89e27b70f2ca0ac932ef4bace7569d6fab",
        "line_code": "        self.recurrent_kernel = self.add_weight(",
        "commit_date": "2017-02-13 16:55:19",
        "valid": 1
      },
      "531": {
        "commit_id": "8830c53135dcad4825e2f8805c78523931c3bbed",
        "line_code": "            shape=(self.units, self.units),",
        "commit_date": "2017-04-18 11:34:24",
        "valid": 1
      },
      "532": {
        "commit_id": "03a7eb89e27b70f2ca0ac932ef4bace7569d6fab",
        "line_code": "            name='recurrent_kernel',",
        "commit_date": "2017-02-13 16:55:19",
        "valid": 1
      },
      "533": {
        "commit_id": "03a7eb89e27b70f2ca0ac932ef4bace7569d6fab",
        "line_code": "            initializer=self.recurrent_initializer,",
        "commit_date": "2017-02-13 16:55:19",
        "valid": 1
      },
      "534": {
        "commit_id": "03a7eb89e27b70f2ca0ac932ef4bace7569d6fab",
        "line_code": "            regularizer=self.recurrent_regularizer,",
        "commit_date": "2017-02-13 16:55:19",
        "valid": 1
      },
      "535": {
        "commit_id": "03a7eb89e27b70f2ca0ac932ef4bace7569d6fab",
        "line_code": "            constraint=self.recurrent_constraint)",
        "commit_date": "2017-02-13 16:55:19",
        "valid": 1
      },
      "536": {
        "commit_id": "03a7eb89e27b70f2ca0ac932ef4bace7569d6fab",
        "line_code": "        if self.use_bias:",
        "commit_date": "2017-02-13 16:55:19",
        "valid": 1
      },
      "537": {
        "commit_id": "8830c53135dcad4825e2f8805c78523931c3bbed",
        "line_code": "            self.bias = self.add_weight(shape=(self.units,),",
        "commit_date": "2017-04-18 11:34:24",
        "valid": 1
      },
      "538": {
        "commit_id": "03a7eb89e27b70f2ca0ac932ef4bace7569d6fab",
        "line_code": "                                        name='bias',",
        "commit_date": "2017-02-13 16:55:19",
        "valid": 1
      },
      "539": {
        "commit_id": "03a7eb89e27b70f2ca0ac932ef4bace7569d6fab",
        "line_code": "                                        initializer=self.bias_initializer,",
        "commit_date": "2017-02-13 16:55:19",
        "valid": 1
      },
      "540": {
        "commit_id": "03a7eb89e27b70f2ca0ac932ef4bace7569d6fab",
        "line_code": "                                        regularizer=self.bias_regularizer,",
        "commit_date": "2017-02-13 16:55:19",
        "valid": 1
      },
      "541": {
        "commit_id": "03a7eb89e27b70f2ca0ac932ef4bace7569d6fab",
        "line_code": "                                        constraint=self.bias_constraint)",
        "commit_date": "2017-02-13 16:55:19",
        "valid": 1
      },
      "542": {
        "commit_id": "03a7eb89e27b70f2ca0ac932ef4bace7569d6fab",
        "line_code": "        else:",
        "commit_date": "2017-02-13 16:55:19",
        "valid": 1
      },
      "543": {
        "commit_id": "03a7eb89e27b70f2ca0ac932ef4bace7569d6fab",
        "line_code": "            self.bias = None",
        "commit_date": "2017-02-13 16:55:19",
        "valid": 1
      },
      "544": {
        "commit_id": "d663fda862df1c831e7f93f1e3feb2e189a1b9ef",
        "line_code": "        self.built = True",
        "commit_date": "2017-02-16 15:26:58",
        "valid": 1
      }
    },
    "commits": {
      "365f621b24631a03f995e3b30e1800d327e42fc1": {
        "commit": {
          "commit_id": "365f621b24631a03f995e3b30e1800d327e42fc1",
          "commit_message": "Fix Specifying Initial States of RNN Layers (#5795)\n\n* fix specify state\r\n\r\n* Added documentation for `reset_states`\r\n\r\n* Remove unneeded check\r\n\r\n* Update Documentation\r\n\r\n* pep8\r\n\r\n* Fix when initial_states is a tensor\r\n\r\n* modify tests for non-list initial states.\r\n\r\n* use initial_state instead of initial_states\r\n\r\n* pep8\r\n\r\n* change get_initial_states to get_initial_state in ConvLSTM2D\r\n\r\n* Check for Keras Tensors in Recurrent\r\n\r\n* check if initial_state is passed to call\r\n\r\n* pep8\r\n\r\n* Move state_spec definition to __init__\r\n\r\n* Fix reset states\r\n\r\n* fix masking when specifying state\r\n\r\n* added masking test for RNNs with specified state\r\n\r\n* pep8\r\n\r\n* remove unnecessary blank line",
          "commit_author": "Joshua Chin",
          "commit_date": "2017-04-24 20:20:04",
          "commit_parent": "9295efb21686b4ea5a55dfa1944c4e30bd27794e"
        },
        "function": {
          "function_name": "build",
          "function_code_before": "def build(self, input_shape):\n    if isinstance(input_shape, list):\n        input_shape = input_shape[0]\n    batch_size = input_shape[0] if self.stateful else None\n    self.input_dim = input_shape[2]\n    self.input_spec = InputSpec(shape=(batch_size, None, self.input_dim))\n    self.state_spec = InputSpec(shape=(batch_size, self.units))\n    self.states = [None]\n    if self.stateful:\n        self.reset_states()\n    self.kernel = self.add_weight(shape=(self.input_dim, self.units), name='kernel', initializer=self.kernel_initializer, regularizer=self.kernel_regularizer, constraint=self.kernel_constraint)\n    self.recurrent_kernel = self.add_weight(shape=(self.units, self.units), name='recurrent_kernel', initializer=self.recurrent_initializer, regularizer=self.recurrent_regularizer, constraint=self.recurrent_constraint)\n    if self.use_bias:\n        self.bias = self.add_weight(shape=(self.units,), name='bias', initializer=self.bias_initializer, regularizer=self.bias_regularizer, constraint=self.bias_constraint)\n    else:\n        self.bias = None\n    self.built = True",
          "function_code_after": "def build(self, input_shape):\n    if isinstance(input_shape, list):\n        input_shape = input_shape[0]\n    batch_size = input_shape[0] if self.stateful else None\n    self.input_dim = input_shape[2]\n    self.input_spec[0] = InputSpec(shape=(batch_size, None, self.input_dim))\n    self.states = [None]\n    if self.stateful:\n        self.reset_states()\n    self.kernel = self.add_weight(shape=(self.input_dim, self.units), name='kernel', initializer=self.kernel_initializer, regularizer=self.kernel_regularizer, constraint=self.kernel_constraint)\n    self.recurrent_kernel = self.add_weight(shape=(self.units, self.units), name='recurrent_kernel', initializer=self.recurrent_initializer, regularizer=self.recurrent_regularizer, constraint=self.recurrent_constraint)\n    if self.use_bias:\n        self.bias = self.add_weight(shape=(self.units,), name='bias', initializer=self.bias_initializer, regularizer=self.bias_regularizer, constraint=self.bias_constraint)\n    else:\n        self.bias = None\n    self.built = True",
          "function_before_start_line": 461,
          "function_before_end_line": 493,
          "function_after_start_line": 475,
          "function_after_end_line": 506,
          "function_before_token_count": 232,
          "function_after_token_count": 219,
          "functions_name_modified_file": [
            "__init__",
            "compute_mask",
            "__call__",
            "build",
            "call",
            "get_constants",
            "_time_distributed_dense",
            "compute_output_shape",
            "step",
            "reset_states",
            "get_initial_state",
            "preprocess_input",
            "get_config"
          ],
          "functions_name_all_files": [
            "reset_states",
            "test_dropout",
            "get_config",
            "test_from_config",
            "step",
            "test_reset_states_with_values",
            "test_masking_layer",
            "input_conv",
            "test_implementation_mode",
            "test_regularizer",
            "preprocess_input",
            "test_specify_state_with_masking",
            "rnn_test",
            "build",
            "call",
            "_time_distributed_dense",
            "test_return_sequences",
            "test_specify_initial_state_keras_tensor",
            "test_dynamic_behavior",
            "reccurent_conv",
            "__init__",
            "compute_mask",
            "__call__",
            "get_constants",
            "compute_output_shape",
            "test_statefulness",
            "test_specify_initial_state_non_keras_tensor",
            "get_initial_state"
          ],
          "functions_name_co_evolved_modified_file": [
            "__init__",
            "compute_mask",
            "__call__",
            "call",
            "reset_states",
            "get_initial_states",
            "get_initial_state"
          ],
          "functions_name_co_evolved_all_files": [
            "__init__",
            "compute_mask",
            "test_specify_state_with_masking",
            "__call__",
            "call",
            "reset_states",
            "test_specify_initial_state",
            "test_reset_states_with_values",
            "get_initial_states",
            "test_specify_initial_state_keras_tensor",
            "test_specify_initial_state_non_keras_tensor",
            "get_initial_state"
          ]
        },
        "file": {
          "file_name": "recurrent.py",
          "file_nloc": 1020,
          "file_complexity": 128,
          "file_token_count": 6386,
          "file_before": "# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import\nimport numpy as np\n\nfrom .. import backend as K\nfrom .. import activations\nfrom .. import initializers\nfrom .. import regularizers\nfrom .. import constraints\nfrom ..engine import Layer\nfrom ..engine import InputSpec\nfrom ..legacy import interfaces\n\n\ndef _time_distributed_dense(x, w, b=None, dropout=None,\n                            input_dim=None, output_dim=None,\n                            timesteps=None, training=None):\n    \"\"\"Apply `y . w + b` for every temporal slice y of x.\n\n    # Arguments\n        x: input tensor.\n        w: weight matrix.\n        b: optional bias vector.\n        dropout: wether to apply dropout (same dropout mask\n            for every temporal slice of the input).\n        input_dim: integer; optional dimensionality of the input.\n        output_dim: integer; optional dimensionality of the output.\n        timesteps: integer; optional number of timesteps.\n        training: training phase tensor or boolean.\n\n    # Returns\n        Output tensor.\n    \"\"\"\n    if not input_dim:\n        input_dim = K.shape(x)[2]\n    if not timesteps:\n        timesteps = K.shape(x)[1]\n    if not output_dim:\n        output_dim = K.shape(w)[1]\n\n    if dropout is not None and 0. < dropout < 1.:\n        # apply the same dropout pattern at every timestep\n        ones = K.ones_like(K.reshape(x[:, 0, :], (-1, input_dim)))\n        dropout_matrix = K.dropout(ones, dropout)\n        expanded_dropout_matrix = K.repeat(dropout_matrix, timesteps)\n        x = K.in_train_phase(x * expanded_dropout_matrix, x, training=training)\n\n    # collapse time dimension and batch dimension together\n    x = K.reshape(x, (-1, input_dim))\n    x = K.dot(x, w)\n    if b is not None:\n        x = K.bias_add(x, b)\n    # reshape to 3D tensor\n    if K.backend() == 'tensorflow':\n        x = K.reshape(x, K.stack([-1, timesteps, output_dim]))\n        x.set_shape([None, None, output_dim])\n    else:\n        x = K.reshape(x, (-1, timesteps, output_dim))\n    return x\n\n\nclass Recurrent(Layer):\n    \"\"\"Abstract base class for recurrent layers.\n\n    Do not use in a model -- it's not a valid layer!\n    Use its children classes `LSTM`, `GRU` and `SimpleRNN` instead.\n\n    All recurrent layers (`LSTM`, `GRU`, `SimpleRNN`) also\n    follow the specifications of this class and accept\n    the keyword arguments listed below.\n\n    # Example\n\n    ```python\n        # as the first layer in a Sequential model\n        model = Sequential()\n        model.add(LSTM(32, input_shape=(10, 64)))\n        # now model.output_shape == (None, 32)\n        # note: `None` is the batch dimension.\n\n        # for subsequent layers, no need to specify the input size:\n        model.add(LSTM(16))\n\n        # to stack recurrent layers, you must use return_sequences=True\n        # on any recurrent layer that feeds into another recurrent layer.\n        # note that you only need to specify the input size on the first layer.\n        model = Sequential()\n        model.add(LSTM(64, input_dim=64, input_length=10, return_sequences=True))\n        model.add(LSTM(32, return_sequences=True))\n        model.add(LSTM(10))\n    ```\n\n    # Arguments\n        weights: list of Numpy arrays to set as initial weights.\n            The list should have 3 elements, of shapes:\n            `[(input_dim, output_dim), (output_dim, output_dim), (output_dim,)]`.\n        return_sequences: Boolean. Whether to return the last output\n            in the output sequence, or the full sequence.\n        go_backwards: Boolean (default False).\n            If True, process the input sequence backwards and return the\n            reversed sequence.\n        stateful: Boolean (default False). If True, the last state\n            for each sample at index i in a batch will be used as initial\n            state for the sample of index i in the following batch.\n        unroll: Boolean (default False).\n            If True, the network will be unrolled,\n            else a symbolic loop will be used.\n            Unrolling can speed-up a RNN,\n            although it tends to be more memory-intensive.\n            Unrolling is only suitable for short sequences.\n        implementation: one of {0, 1, or 2}.\n            If set to 0, the RNN will use\n            an implementation that uses fewer, larger matrix products,\n            thus running faster on CPU but consuming more memory.\n            If set to 1, the RNN will use more matrix products,\n            but smaller ones, thus running slower\n            (may actually be faster on GPU) while consuming less memory.\n            If set to 2 (LSTM/GRU only),\n            the RNN will combine the input gate,\n            the forget gate and the output gate into a single matrix,\n            enabling more time-efficient parallelization on the GPU.\n            Note: RNN dropout must be shared for all gates,\n            resulting in a slightly reduced regularization.\n        input_dim: dimensionality of the input (integer).\n            This argument (or alternatively, the keyword argument `input_shape`)\n            is required when using this layer as the first layer in a model.\n        input_length: Length of input sequences, to be specified\n            when it is constant.\n            This argument is required if you are going to connect\n            `Flatten` then `Dense` layers upstream\n            (without it, the shape of the dense outputs cannot be computed).\n            Note that if the recurrent layer is not the first layer\n            in your model, you would need to specify the input length\n            at the level of the first layer\n            (e.g. via the `input_shape` argument)\n\n    # Input shapes\n        3D tensor with shape `(batch_size, timesteps, input_dim)`,\n        (Optional) 2D tensors with shape `(batch_size, output_dim)`.\n\n    # Output shape\n        - if `return_sequences`: 3D tensor with shape\n            `(batch_size, timesteps, units)`.\n        - else, 2D tensor with shape `(batch_size, units)`.\n\n    # Masking\n        This layer supports masking for input data with a variable number\n        of timesteps. To introduce masks to your data,\n        use an [Embedding](embeddings.md) layer with the `mask_zero` parameter\n        set to `True`.\n\n    # Note on using statefulness in RNNs\n        You can set RNN layers to be 'stateful', which means that the states\n        computed for the samples in one batch will be reused as initial states\n        for the samples in the next batch. This assumes a one-to-one mapping\n        between samples in different successive batches.\n\n        To enable statefulness:\n            - specify `stateful=True` in the layer constructor.\n            - specify a fixed batch size for your model, by passing\n                if sequential model:\n                  `batch_input_shape=(...)` to the first layer in your model.\n                else for functional model with 1 or more Input layers:\n                  `batch_shape=(...)` to all the first layers in your model.\n                This is the expected shape of your inputs\n                *including the batch size*.\n                It should be a tuple of integers, e.g. `(32, 10, 100)`.\n            - specify `shuffle=False` when calling fit().\n\n        To reset the states of your model, call `.reset_states()` on either\n        a specific layer, or on your entire model.\n\n    # Note on specifying initial states in RNNs\n        You can specify the initial state of RNN layers by calling them with\n        the keyword argument `initial_state`. The value of `initial_state`\n        should be a tensor or list of tensors representing the initial state\n        of the RNN layer.\n    \"\"\"\n\n    def __init__(self, return_sequences=False,\n                 go_backwards=False,\n                 stateful=False,\n                 unroll=False,\n                 implementation=0,\n                 **kwargs):\n        super(Recurrent, self).__init__(**kwargs)\n        self.return_sequences = return_sequences\n        self.go_backwards = go_backwards\n        self.stateful = stateful\n        self.unroll = unroll\n        self.implementation = implementation\n        self.supports_masking = True\n        self.input_spec = InputSpec(ndim=3)\n        self.state_spec = None\n        self.dropout = 0\n        self.recurrent_dropout = 0\n\n    def compute_output_shape(self, input_shape):\n        if isinstance(input_shape, list):\n            input_shape = input_shape[0]\n        if self.return_sequences:\n            return (input_shape[0], input_shape[1], self.units)\n        else:\n            return (input_shape[0], self.units)\n\n    def compute_mask(self, inputs, mask):\n        if self.return_sequences:\n            return mask\n        else:\n            return None\n\n    def step(self, inputs, states):\n        raise NotImplementedError\n\n    def get_constants(self, inputs, training=None):\n        return []\n\n    def get_initial_states(self, inputs):\n        # build an all-zero tensor of shape (samples, output_dim)\n        initial_state = K.zeros_like(inputs)  # (samples, timesteps, input_dim)\n        initial_state = K.sum(initial_state, axis=(1, 2))  # (samples,)\n        initial_state = K.expand_dims(initial_state)  # (samples, 1)\n        initial_state = K.tile(initial_state, [1, self.units])  # (samples, output_dim)\n        initial_states = [initial_state for _ in range(len(self.states))]\n        return initial_states\n\n    def preprocess_input(self, inputs, training=None):\n        return inputs\n\n    def __call__(self, inputs, initial_state=None, **kwargs):\n        # If `initial_state` is specified,\n        # and if it a Keras tensor,\n        # then add it to the inputs and temporarily\n        # modify the input spec to include the state.\n        if initial_state is not None:\n            if hasattr(initial_state, '_keras_history'):\n                # Compute the full input spec, including state\n                input_spec = self.input_spec\n                state_spec = self.state_spec\n                if not isinstance(state_spec, list):\n                    state_spec = [state_spec]\n                self.input_spec = [input_spec] + state_spec\n\n                # Compute the full inputs, including state\n                if not isinstance(initial_state, (list, tuple)):\n                    initial_state = [initial_state]\n                inputs = [inputs] + list(initial_state)\n\n                # Perform the call\n                output = super(Recurrent, self).__call__(inputs, **kwargs)\n\n                # Restore original input spec\n                self.input_spec = input_spec\n                return output\n            else:\n                kwargs['initial_state'] = initial_state\n        return super(Recurrent, self).__call__(inputs, **kwargs)\n\n    def call(self, inputs, mask=None, initial_state=None, training=None):\n        # input shape: `(samples, time (padded with zeros), input_dim)`\n        # note that the .build() method of subclasses MUST define\n        # self.input_spec and self.state_spec with complete input shapes.\n        if initial_state is not None:\n            if not isinstance(initial_state, (list, tuple)):\n                initial_states = [initial_state]\n            else:\n                initial_states = list(initial_state)\n        if isinstance(inputs, list):\n            initial_states = inputs[1:]\n            inputs = inputs[0]\n        elif self.stateful:\n            initial_states = self.states\n        else:\n            initial_states = self.get_initial_states(inputs)\n\n        if len(initial_states) != len(self.states):\n            raise ValueError('Layer has ' + str(len(self.states)) +\n                             ' states but was passed ' +\n                             str(len(initial_states)) +\n                             ' initial states.')\n        input_shape = K.int_shape(inputs)\n        if self.unroll and input_shape[1] is None:\n            raise ValueError('Cannot unroll a RNN if the '\n                             'time dimension is undefined. \\n'\n                             '- If using a Sequential model, '\n                             'specify the time dimension by passing '\n                             'an `input_shape` or `batch_input_shape` '\n                             'argument to your first layer. If your '\n                             'first layer is an Embedding, you can '\n                             'also use the `input_length` argument.\\n'\n                             '- If using the functional API, specify '\n                             'the time dimension by passing a `shape` '\n                             'or `batch_shape` argument to your Input layer.')\n        constants = self.get_constants(inputs, training=None)\n        preprocessed_input = self.preprocess_input(inputs, training=None)\n        last_output, outputs, states = K.rnn(self.step,\n                                             preprocessed_input,\n                                             initial_states,\n                                             go_backwards=self.go_backwards,\n                                             mask=mask,\n                                             constants=constants,\n                                             unroll=self.unroll,\n                                             input_length=input_shape[1])\n        if self.stateful:\n            updates = []\n            for i in range(len(states)):\n                updates.append((self.states[i], states[i]))\n            self.add_update(updates, inputs)\n\n        # Properly set learning phase\n        if 0 < self.dropout + self.recurrent_dropout:\n            last_output._uses_learning_phase = True\n            outputs._uses_learning_phase = True\n\n        if self.return_sequences:\n            return outputs\n        else:\n            return last_output\n\n    def reset_states(self, states_value=None):\n        if not self.stateful:\n            raise AttributeError('Layer must be stateful.')\n        if not self.input_spec:\n            raise RuntimeError('Layer has never been called '\n                               'and thus has no states.')\n        batch_size = self.input_spec.shape[0]\n        if not batch_size:\n            raise ValueError('If a RNN is stateful, it needs to know '\n                             'its batch size. Specify the batch size '\n                             'of your input tensors: \\n'\n                             '- If using a Sequential model, '\n                             'specify the batch size by passing '\n                             'a `batch_input_shape` '\n                             'argument to your first layer.\\n'\n                             '- If using the functional API, specify '\n                             'the time dimension by passing a '\n                             '`batch_shape` argument to your Input layer.')\n        if states_value is not None:\n            if not isinstance(states_value, (list, tuple)):\n                states_value = [states_value]\n            if len(states_value) != len(self.states):\n                raise ValueError('The layer has ' + str(len(self.states)) +\n                                 ' states, but the `states_value` '\n                                 'argument passed '\n                                 'only has ' + str(len(states_value)) +\n                                 ' entries')\n        if self.states[0] is None:\n            self.states = [K.zeros((batch_size, self.units))\n                           for _ in self.states]\n            if not states_value:\n                return\n        for i, state in enumerate(self.states):\n            if states_value:\n                value = states_value[i]\n                if value.shape != (batch_size, self.units):\n                    raise ValueError(\n                        'Expected state #' + str(i) +\n                        ' to have shape ' + str((batch_size, self.units)) +\n                        ' but got array with shape ' + str(value.shape))\n            else:\n                value = np.zeros((batch_size, self.units))\n            K.set_value(state, value)\n\n    def get_config(self):\n        config = {'return_sequences': self.return_sequences,\n                  'go_backwards': self.go_backwards,\n                  'stateful': self.stateful,\n                  'unroll': self.unroll,\n                  'implementation': self.implementation}\n        base_config = super(Recurrent, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\n\nclass SimpleRNN(Recurrent):\n    \"\"\"Fully-connected RNN where the output is to be fed back to input.\n\n    # Arguments\n        units: Positive integer, dimensionality of the output space.\n        activation: Activation function to use\n            (see [activations](../activations.md)).\n            If you pass None, no activation is applied\n            (ie. \"linear\" activation: `a(x) = x`).\n        use_bias: Boolean, whether the layer uses a bias vector.\n        kernel_initializer: Initializer for the `kernel` weights matrix,\n            used for the linear transformation of the inputs.\n            (see [initializers](../initializers.md)).\n        recurrent_initializer: Initializer for the `recurrent_kernel`\n            weights matrix,\n            used for the linear transformation of the recurrent state.\n            (see [initializers](../initializers.md)).\n        bias_initializer: Initializer for the bias vector\n            (see [initializers](../initializers.md)).\n        kernel_regularizer: Regularizer function applied to\n            the `kernel` weights matrix\n            (see [regularizer](../regularizers.md)).\n        recurrent_regularizer: Regularizer function applied to\n            the `recurrent_kernel` weights matrix\n            (see [regularizer](../regularizers.md)).\n        bias_regularizer: Regularizer function applied to the bias vector\n            (see [regularizer](../regularizers.md)).\n        activity_regularizer: Regularizer function applied to\n            the output of the layer (its \"activation\").\n            (see [regularizer](../regularizers.md)).\n        kernel_constraint: Constraint function applied to\n            the `kernel` weights matrix\n            (see [constraints](../constraints.md)).\n        recurrent_constraint: Constraint function applied to\n            the `recurrent_kernel` weights matrix\n            (see [constraints](../constraints.md)).\n        bias_constraint: Constraint function applied to the bias vector\n            (see [constraints](../constraints.md)).\n        dropout: Float between 0 and 1.\n            Fraction of the units to drop for\n            the linear transformation of the inputs.\n        recurrent_dropout: Float between 0 and 1.\n            Fraction of the units to drop for\n            the linear transformation of the recurrent state.\n\n    # References\n        - [A Theoretically Grounded Application of Dropout in Recurrent Neural Networks](http://arxiv.org/abs/1512.05287)\n    \"\"\"\n\n    @interfaces.legacy_recurrent_support\n    def __init__(self, units,\n                 activation='tanh',\n                 use_bias=True,\n                 kernel_initializer='glorot_uniform',\n                 recurrent_initializer='orthogonal',\n                 bias_initializer='zeros',\n                 kernel_regularizer=None,\n                 recurrent_regularizer=None,\n                 bias_regularizer=None,\n                 activity_regularizer=None,\n                 kernel_constraint=None,\n                 recurrent_constraint=None,\n                 bias_constraint=None,\n                 dropout=0.,\n                 recurrent_dropout=0.,\n                 **kwargs):\n        super(SimpleRNN, self).__init__(**kwargs)\n        self.units = units\n        self.activation = activations.get(activation)\n        self.use_bias = use_bias\n\n        self.kernel_initializer = initializers.get(kernel_initializer)\n        self.recurrent_initializer = initializers.get(recurrent_initializer)\n        self.bias_initializer = initializers.get(bias_initializer)\n\n        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n        self.recurrent_regularizer = regularizers.get(recurrent_regularizer)\n        self.bias_regularizer = regularizers.get(bias_regularizer)\n        self.activity_regularizer = regularizers.get(activity_regularizer)\n\n        self.kernel_constraint = constraints.get(kernel_constraint)\n        self.recurrent_constraint = constraints.get(recurrent_constraint)\n        self.bias_constraint = constraints.get(bias_constraint)\n\n        self.dropout = min(1., max(0., dropout))\n        self.recurrent_dropout = min(1., max(0., recurrent_dropout))\n\n    def build(self, input_shape):\n        if isinstance(input_shape, list):\n            input_shape = input_shape[0]\n\n        batch_size = input_shape[0] if self.stateful else None\n        self.input_dim = input_shape[2]\n        self.input_spec = InputSpec(shape=(batch_size, None, self.input_dim))\n        self.state_spec = InputSpec(shape=(batch_size, self.units))\n\n        self.states = [None]\n        if self.stateful:\n            self.reset_states()\n\n        self.kernel = self.add_weight(shape=(self.input_dim, self.units),\n                                      name='kernel',\n                                      initializer=self.kernel_initializer,\n                                      regularizer=self.kernel_regularizer,\n                                      constraint=self.kernel_constraint)\n        self.recurrent_kernel = self.add_weight(\n            shape=(self.units, self.units),\n            name='recurrent_kernel',\n            initializer=self.recurrent_initializer,\n            regularizer=self.recurrent_regularizer,\n            constraint=self.recurrent_constraint)\n        if self.use_bias:\n            self.bias = self.add_weight(shape=(self.units,),\n                                        name='bias',\n                                        initializer=self.bias_initializer,\n                                        regularizer=self.bias_regularizer,\n                                        constraint=self.bias_constraint)\n        else:\n            self.bias = None\n        self.built = True\n\n    def preprocess_input(self, inputs, training=None):\n        if self.implementation > 0:\n            return inputs\n        else:\n            input_shape = K.int_shape(inputs)\n            input_dim = input_shape[2]\n            timesteps = input_shape[1]\n            return _time_distributed_dense(inputs,\n                                           self.kernel,\n                                           self.bias,\n                                           self.dropout,\n                                           input_dim,\n                                           self.units,\n                                           timesteps,\n                                           training=training)\n\n    def step(self, inputs, states):\n        if self.implementation == 0:\n            h = inputs\n        else:\n            if 0 < self.dropout < 1:\n                h = K.dot(inputs * states[1], self.kernel)\n            else:\n                h = K.dot(inputs, self.kernel)\n            if self.bias is not None:\n                h = K.bias_add(h, self.bias)\n\n        prev_output = states[0]\n        if 0 < self.recurrent_dropout < 1:\n            prev_output *= states[2]\n        output = h + K.dot(prev_output, self.recurrent_kernel)\n        if self.activation is not None:\n            output = self.activation(output)\n\n        # Properly set learning phase on output tensor.\n        if 0 < self.dropout + self.recurrent_dropout:\n            output._uses_learning_phase = True\n        return output, [output]\n\n    def get_constants(self, inputs, training=None):\n        constants = []\n        if self.implementation != 0 and 0 < self.dropout < 1:\n            input_shape = K.int_shape(inputs)\n            input_dim = input_shape[-1]\n            ones = K.ones_like(K.reshape(inputs[:, 0, 0], (-1, 1)))\n            ones = K.tile(ones, (1, int(input_dim)))\n\n            def dropped_inputs():\n                return K.dropout(ones, self.dropout)\n\n            dp_mask = K.in_train_phase(dropped_inputs,\n                                       ones,\n                                       training=training)\n            constants.append(dp_mask)\n        else:\n            constants.append(K.cast_to_floatx(1.))\n\n        if 0 < self.recurrent_dropout < 1:\n            ones = K.ones_like(K.reshape(inputs[:, 0, 0], (-1, 1)))\n            ones = K.tile(ones, (1, self.units))\n\n            def dropped_inputs():\n                return K.dropout(ones, self.recurrent_dropout)\n            rec_dp_mask = K.in_train_phase(dropped_inputs,\n                                           ones,\n                                           training=training)\n            constants.append(rec_dp_mask)\n        else:\n            constants.append(K.cast_to_floatx(1.))\n        return constants\n\n    def get_config(self):\n        config = {'units': self.units,\n                  'activation': activations.serialize(self.activation),\n                  'use_bias': self.use_bias,\n                  'kernel_initializer': initializers.serialize(self.kernel_initializer),\n                  'recurrent_initializer': initializers.serialize(self.recurrent_initializer),\n                  'bias_initializer': initializers.serialize(self.bias_initializer),\n                  'kernel_regularizer': regularizers.serialize(self.kernel_regularizer),\n                  'recurrent_regularizer': regularizers.serialize(self.recurrent_regularizer),\n                  'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n                  'activity_regularizer': regularizers.serialize(self.activity_regularizer),\n                  'kernel_constraint': constraints.serialize(self.kernel_constraint),\n                  'recurrent_constraint': constraints.serialize(self.recurrent_constraint),\n                  'bias_constraint': constraints.serialize(self.bias_constraint),\n                  'dropout': self.dropout,\n                  'recurrent_dropout': self.recurrent_dropout}\n        base_config = super(SimpleRNN, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\n\nclass GRU(Recurrent):\n    \"\"\"Gated Recurrent Unit - Cho et al. 2014.\n\n    # Arguments\n        units: Positive integer, dimensionality of the output space.\n        activation: Activation function to use\n            (see [activations](../activations.md)).\n            If you pass None, no activation is applied\n            (ie. \"linear\" activation: `a(x) = x`).\n        recurrent_activation: Activation function to use\n            for the recurrent step\n            (see [activations](../activations.md)).\n        use_bias: Boolean, whether the layer uses a bias vector.\n        kernel_initializer: Initializer for the `kernel` weights matrix,\n            used for the linear transformation of the inputs.\n            (see [initializers](../initializers.md)).\n        recurrent_initializer: Initializer for the `recurrent_kernel`\n            weights matrix,\n            used for the linear transformation of the recurrent state.\n            (see [initializers](../initializers.md)).\n        bias_initializer: Initializer for the bias vector\n            (see [initializers](../initializers.md)).\n        kernel_regularizer: Regularizer function applied to\n            the `kernel` weights matrix\n            (see [regularizer](../regularizers.md)).\n        recurrent_regularizer: Regularizer function applied to\n            the `recurrent_kernel` weights matrix\n            (see [regularizer](../regularizers.md)).\n        bias_regularizer: Regularizer function applied to the bias vector\n            (see [regularizer](../regularizers.md)).\n        activity_regularizer: Regularizer function applied to\n            the output of the layer (its \"activation\").\n            (see [regularizer](../regularizers.md)).\n        kernel_constraint: Constraint function applied to\n            the `kernel` weights matrix\n            (see [constraints](../constraints.md)).\n        recurrent_constraint: Constraint function applied to\n            the `recurrent_kernel` weights matrix\n            (see [constraints](../constraints.md)).\n        bias_constraint: Constraint function applied to the bias vector\n            (see [constraints](../constraints.md)).\n        dropout: Float between 0 and 1.\n            Fraction of the units to drop for\n            the linear transformation of the inputs.\n        recurrent_dropout: Float between 0 and 1.\n            Fraction of the units to drop for\n            the linear transformation of the recurrent state.\n\n    # References\n        - [On the Properties of Neural Machine Translation: Encoder-Decoder Approaches](https://arxiv.org/abs/1409.1259)\n        - [Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling](http://arxiv.org/abs/1412.3555v1)\n        - [A Theoretically Grounded Application of Dropout in Recurrent Neural Networks](http://arxiv.org/abs/1512.05287)\n    \"\"\"\n\n    @interfaces.legacy_recurrent_support\n    def __init__(self, units,\n                 activation='tanh',\n                 recurrent_activation='hard_sigmoid',\n                 use_bias=True,\n                 kernel_initializer='glorot_uniform',\n                 recurrent_initializer='orthogonal',\n                 bias_initializer='zeros',\n                 kernel_regularizer=None,\n                 recurrent_regularizer=None,\n                 bias_regularizer=None,\n                 activity_regularizer=None,\n                 kernel_constraint=None,\n                 recurrent_constraint=None,\n                 bias_constraint=None,\n                 dropout=0.,\n                 recurrent_dropout=0.,\n                 **kwargs):\n        super(GRU, self).__init__(**kwargs)\n        self.units = units\n        self.activation = activations.get(activation)\n        self.recurrent_activation = activations.get(recurrent_activation)\n        self.use_bias = use_bias\n\n        self.kernel_initializer = initializers.get(kernel_initializer)\n        self.recurrent_initializer = initializers.get(recurrent_initializer)\n        self.bias_initializer = initializers.get(bias_initializer)\n\n        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n        self.recurrent_regularizer = regularizers.get(recurrent_regularizer)\n        self.bias_regularizer = regularizers.get(bias_regularizer)\n        self.activity_regularizer = regularizers.get(activity_regularizer)\n\n        self.kernel_constraint = constraints.get(kernel_constraint)\n        self.recurrent_constraint = constraints.get(recurrent_constraint)\n        self.bias_constraint = constraints.get(bias_constraint)\n\n        self.dropout = min(1., max(0., dropout))\n        self.recurrent_dropout = min(1., max(0., recurrent_dropout))\n\n    def build(self, input_shape):\n        if isinstance(input_shape, list):\n            input_shape = input_shape[0]\n\n        batch_size = input_shape[0] if self.stateful else None\n        self.input_dim = input_shape[2]\n        self.input_spec = InputSpec(shape=(batch_size, None, self.input_dim))\n        self.state_spec = InputSpec(shape=(batch_size, self.units))\n\n        self.states = [None]\n        if self.stateful:\n            self.reset_states()\n\n        self.kernel = self.add_weight(shape=(self.input_dim, self.units * 3),\n                                      name='kernel',\n                                      initializer=self.kernel_initializer,\n                                      regularizer=self.kernel_regularizer,\n                                      constraint=self.kernel_constraint)\n        self.recurrent_kernel = self.add_weight(\n            shape=(self.units, self.units * 3),\n            name='recurrent_kernel',\n            initializer=self.recurrent_initializer,\n            regularizer=self.recurrent_regularizer,\n            constraint=self.recurrent_constraint)\n\n        if self.use_bias:\n            self.bias = self.add_weight(shape=(self.units * 3,),\n                                        name='bias',\n                                        initializer=self.bias_initializer,\n                                        regularizer=self.bias_regularizer,\n                                        constraint=self.bias_constraint)\n        else:\n            self.bias = None\n\n        self.kernel_z = self.kernel[:, :self.units]\n        self.recurrent_kernel_z = self.recurrent_kernel[:, :self.units]\n        self.kernel_r = self.kernel[:, self.units: self.units * 2]\n        self.recurrent_kernel_r = self.recurrent_kernel[:,\n                                                        self.units:\n                                                        self.units * 2]\n        self.kernel_h = self.kernel[:, self.units * 2:]\n        self.recurrent_kernel_h = self.recurrent_kernel[:, self.units * 2:]\n\n        if self.use_bias:\n            self.bias_z = self.bias[:self.units]\n            self.bias_r = self.bias[self.units: self.units * 2]\n            self.bias_h = self.bias[self.units * 2:]\n        else:\n            self.bias_z = None\n            self.bias_r = None\n            self.bias_h = None\n        self.built = True\n\n    def preprocess_input(self, inputs, training=None):\n        if self.implementation == 0:\n            input_shape = K.int_shape(inputs)\n            input_dim = input_shape[2]\n            timesteps = input_shape[1]\n\n            x_z = _time_distributed_dense(inputs, self.kernel_z, self.bias_z,\n                                          self.dropout, input_dim, self.units,\n                                          timesteps, training=training)\n            x_r = _time_distributed_dense(inputs, self.kernel_r, self.bias_r,\n                                          self.dropout, input_dim, self.units,\n                                          timesteps, training=training)\n            x_h = _time_distributed_dense(inputs, self.kernel_h, self.bias_h,\n                                          self.dropout, input_dim, self.units,\n                                          timesteps, training=training)\n            return K.concatenate([x_z, x_r, x_h], axis=2)\n        else:\n            return inputs\n\n    def get_constants(self, inputs, training=None):\n        constants = []\n        if self.implementation != 0 and 0 < self.dropout < 1:\n            input_shape = K.int_shape(inputs)\n            input_dim = input_shape[-1]\n            ones = K.ones_like(K.reshape(inputs[:, 0, 0], (-1, 1)))\n            ones = K.tile(ones, (1, int(input_dim)))\n\n            def dropped_inputs():\n                return K.dropout(ones, self.dropout)\n\n            dp_mask = [K.in_train_phase(dropped_inputs,\n                                        ones,\n                                        training=training) for _ in range(3)]\n            constants.append(dp_mask)\n        else:\n            constants.append([K.cast_to_floatx(1.) for _ in range(3)])\n\n        if 0 < self.recurrent_dropout < 1:\n            ones = K.ones_like(K.reshape(inputs[:, 0, 0], (-1, 1)))\n            ones = K.tile(ones, (1, self.units))\n\n            def dropped_inputs():\n                return K.dropout(ones, self.recurrent_dropout)\n            rec_dp_mask = [K.in_train_phase(dropped_inputs,\n                                            ones,\n                                            training=training) for _ in range(3)]\n            constants.append(rec_dp_mask)\n        else:\n            constants.append([K.cast_to_floatx(1.) for _ in range(3)])\n        return constants\n\n    def step(self, inputs, states):\n        h_tm1 = states[0]  # previous memory\n        dp_mask = states[1]  # dropout matrices for recurrent units\n        rec_dp_mask = states[2]\n\n        if self.implementation == 2:\n            matrix_x = K.dot(inputs * dp_mask[0], self.kernel)\n            if self.use_bias:\n                matrix_x = K.bias_add(matrix_x, self.bias)\n            matrix_inner = K.dot(h_tm1 * rec_dp_mask[0],\n                                 self.recurrent_kernel[:, :2 * self.units])\n\n            x_z = matrix_x[:, :self.units]\n            x_r = matrix_x[:, self.units: 2 * self.units]\n            recurrent_z = matrix_inner[:, :self.units]\n            recurrent_r = matrix_inner[:, self.units: 2 * self.units]\n\n            z = self.recurrent_activation(x_z + recurrent_z)\n            r = self.recurrent_activation(x_r + recurrent_r)\n\n            x_h = matrix_x[:, 2 * self.units:]\n            recurrent_h = K.dot(r * h_tm1 * rec_dp_mask[0],\n                                self.recurrent_kernel[:, 2 * self.units:])\n            hh = self.activation(x_h + recurrent_h)\n        else:\n            if self.implementation == 0:\n                x_z = inputs[:, :self.units]\n                x_r = inputs[:, self.units: 2 * self.units]\n                x_h = inputs[:, 2 * self.units:]\n            elif self.implementation == 1:\n                x_z = K.dot(inputs * dp_mask[0], self.kernel_z)\n                x_r = K.dot(inputs * dp_mask[1], self.kernel_r)\n                x_h = K.dot(inputs * dp_mask[2], self.kernel_h)\n                if self.use_bias:\n                    x_z = K.bias_add(x_z, self.bias_z)\n                    x_r = K.bias_add(x_r, self.bias_r)\n                    x_h = K.bias_add(x_h, self.bias_h)\n            else:\n                raise ValueError('Unknown `implementation` mode.')\n            z = self.recurrent_activation(x_z + K.dot(h_tm1 * rec_dp_mask[0],\n                                                      self.recurrent_kernel_z))\n            r = self.recurrent_activation(x_r + K.dot(h_tm1 * rec_dp_mask[1],\n                                                      self.recurrent_kernel_r))\n\n            hh = self.activation(x_h + K.dot(r * h_tm1 * rec_dp_mask[2],\n                                             self.recurrent_kernel_h))\n        h = z * h_tm1 + (1 - z) * hh\n        if 0 < self.dropout + self.recurrent_dropout:\n            h._uses_learning_phase = True\n        return h, [h]\n\n    def get_config(self):\n        config = {'units': self.units,\n                  'activation': activations.serialize(self.activation),\n                  'recurrent_activation': activations.serialize(self.recurrent_activation),\n                  'use_bias': self.use_bias,\n                  'kernel_initializer': initializers.serialize(self.kernel_initializer),\n                  'recurrent_initializer': initializers.serialize(self.recurrent_initializer),\n                  'bias_initializer': initializers.serialize(self.bias_initializer),\n                  'kernel_regularizer': regularizers.serialize(self.kernel_regularizer),\n                  'recurrent_regularizer': regularizers.serialize(self.recurrent_regularizer),\n                  'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n                  'activity_regularizer': regularizers.serialize(self.activity_regularizer),\n                  'kernel_constraint': constraints.serialize(self.kernel_constraint),\n                  'recurrent_constraint': constraints.serialize(self.recurrent_constraint),\n                  'bias_constraint': constraints.serialize(self.bias_constraint),\n                  'dropout': self.dropout,\n                  'recurrent_dropout': self.recurrent_dropout}\n        base_config = super(GRU, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\n\nclass LSTM(Recurrent):\n    \"\"\"Long-Short Term Memory unit - Hochreiter 1997.\n\n    For a step-by-step description of the algorithm, see\n    [this tutorial](http://deeplearning.net/tutorial/lstm.html).\n\n    # Arguments\n        units: Positive integer, dimensionality of the output space.\n        activation: Activation function to use\n            (see [activations](../activations.md)).\n            If you pass None, no activation is applied\n            (ie. \"linear\" activation: `a(x) = x`).\n        recurrent_activation: Activation function to use\n            for the recurrent step\n            (see [activations](../activations.md)).\n        use_bias: Boolean, whether the layer uses a bias vector.\n        kernel_initializer: Initializer for the `kernel` weights matrix,\n            used for the linear transformation of the inputs.\n            (see [initializers](../initializers.md)).\n        recurrent_initializer: Initializer for the `recurrent_kernel`\n            weights matrix,\n            used for the linear transformation of the recurrent state.\n            (see [initializers](../initializers.md)).\n        bias_initializer: Initializer for the bias vector\n            (see [initializers](../initializers.md)).\n        unit_forget_bias: Boolean.\n            If True, add 1 to the bias of the forget gate at initialization.\n            Setting it to true will also force `bias_initializer=\"zeros\"`.\n            This is recommended in [Jozefowicz et al.](http://www.jmlr.org/proceedings/papers/v37/jozefowicz15.pdf)\n        kernel_regularizer: Regularizer function applied to\n            the `kernel` weights matrix\n            (see [regularizer](../regularizers.md)).\n        recurrent_regularizer: Regularizer function applied to\n            the `recurrent_kernel` weights matrix\n            (see [regularizer](../regularizers.md)).\n        bias_regularizer: Regularizer function applied to the bias vector\n            (see [regularizer](../regularizers.md)).\n        activity_regularizer: Regularizer function applied to\n            the output of the layer (its \"activation\").\n            (see [regularizer](../regularizers.md)).\n        kernel_constraint: Constraint function applied to\n            the `kernel` weights matrix\n            (see [constraints](../constraints.md)).\n        recurrent_constraint: Constraint function applied to\n            the `recurrent_kernel` weights matrix\n            (see [constraints](../constraints.md)).\n        bias_constraint: Constraint function applied to the bias vector\n            (see [constraints](../constraints.md)).\n        dropout: Float between 0 and 1.\n            Fraction of the units to drop for\n            the linear transformation of the inputs.\n        recurrent_dropout: Float between 0 and 1.\n            Fraction of the units to drop for\n            the linear transformation of the recurrent state.\n\n    # References\n        - [Long short-term memory](http://deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf) (original 1997 paper)\n        - [Learning to forget: Continual prediction with LSTM](http://www.mitpressjournals.org/doi/pdf/10.1162/089976600300015015)\n        - [Supervised sequence labeling with recurrent neural networks](http://www.cs.toronto.edu/~graves/preprint.pdf)\n        - [A Theoretically Grounded Application of Dropout in Recurrent Neural Networks](http://arxiv.org/abs/1512.05287)\n    \"\"\"\n    @interfaces.legacy_recurrent_support\n    def __init__(self, units,\n                 activation='tanh',\n                 recurrent_activation='hard_sigmoid',\n                 use_bias=True,\n                 kernel_initializer='glorot_uniform',\n                 recurrent_initializer='orthogonal',\n                 bias_initializer='zeros',\n                 unit_forget_bias=True,\n                 kernel_regularizer=None,\n                 recurrent_regularizer=None,\n                 bias_regularizer=None,\n                 activity_regularizer=None,\n                 kernel_constraint=None,\n                 recurrent_constraint=None,\n                 bias_constraint=None,\n                 dropout=0.,\n                 recurrent_dropout=0.,\n                 **kwargs):\n        super(LSTM, self).__init__(**kwargs)\n        self.units = units\n        self.activation = activations.get(activation)\n        self.recurrent_activation = activations.get(recurrent_activation)\n        self.use_bias = use_bias\n\n        self.kernel_initializer = initializers.get(kernel_initializer)\n        self.recurrent_initializer = initializers.get(recurrent_initializer)\n        self.bias_initializer = initializers.get(bias_initializer)\n        self.unit_forget_bias = unit_forget_bias\n\n        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n        self.recurrent_regularizer = regularizers.get(recurrent_regularizer)\n        self.bias_regularizer = regularizers.get(bias_regularizer)\n        self.activity_regularizer = regularizers.get(activity_regularizer)\n\n        self.kernel_constraint = constraints.get(kernel_constraint)\n        self.recurrent_constraint = constraints.get(recurrent_constraint)\n        self.bias_constraint = constraints.get(bias_constraint)\n\n        self.dropout = min(1., max(0., dropout))\n        self.recurrent_dropout = min(1., max(0., recurrent_dropout))\n\n    def build(self, input_shape):\n        if isinstance(input_shape, list):\n            input_shape = input_shape[0]\n\n        batch_size = input_shape[0] if self.stateful else None\n        self.input_dim = input_shape[2]\n        self.input_spec = InputSpec(shape=(batch_size, None, self.input_dim))\n        self.state_spec = [InputSpec(shape=(batch_size, self.units)),\n                           InputSpec(shape=(batch_size, self.units))]\n\n        self.states = [None, None]\n        if self.stateful:\n            self.reset_states()\n\n        self.kernel = self.add_weight(shape=(self.input_dim, self.units * 4),\n                                      name='kernel',\n                                      initializer=self.kernel_initializer,\n                                      regularizer=self.kernel_regularizer,\n                                      constraint=self.kernel_constraint)\n        self.recurrent_kernel = self.add_weight(\n            shape=(self.units, self.units * 4),\n            name='recurrent_kernel',\n            initializer=self.recurrent_initializer,\n            regularizer=self.recurrent_regularizer,\n            constraint=self.recurrent_constraint)\n\n        if self.use_bias:\n            if self.unit_forget_bias:\n                def bias_initializer(shape, *args, **kwargs):\n                    return K.concatenate([\n                        self.bias_initializer((self.units,), *args, **kwargs),\n                        initializers.Ones()((self.units,), *args, **kwargs),\n                        self.bias_initializer((self.units * 2,), *args, **kwargs),\n                    ])\n            else:\n                bias_initializer = self.bias_initializer\n            self.bias = self.add_weight(shape=(self.units * 4,),\n                                        name='bias',\n                                        initializer=bias_initializer,\n                                        regularizer=self.bias_regularizer,\n                                        constraint=self.bias_constraint)\n        else:\n            self.bias = None\n\n        self.kernel_i = self.kernel[:, :self.units]\n        self.kernel_f = self.kernel[:, self.units: self.units * 2]\n        self.kernel_c = self.kernel[:, self.units * 2: self.units * 3]\n        self.kernel_o = self.kernel[:, self.units * 3:]\n\n        self.recurrent_kernel_i = self.recurrent_kernel[:, :self.units]\n        self.recurrent_kernel_f = self.recurrent_kernel[:, self.units: self.units * 2]\n        self.recurrent_kernel_c = self.recurrent_kernel[:, self.units * 2: self.units * 3]\n        self.recurrent_kernel_o = self.recurrent_kernel[:, self.units * 3:]\n\n        if self.use_bias:\n            self.bias_i = self.bias[:self.units]\n            self.bias_f = self.bias[self.units: self.units * 2]\n            self.bias_c = self.bias[self.units * 2: self.units * 3]\n            self.bias_o = self.bias[self.units * 3:]\n        else:\n            self.bias_i = None\n            self.bias_f = None\n            self.bias_c = None\n            self.bias_o = None\n        self.built = True\n\n    def preprocess_input(self, inputs, training=None):\n        if self.implementation == 0:\n            input_shape = K.int_shape(inputs)\n            input_dim = input_shape[2]\n            timesteps = input_shape[1]\n\n            x_i = _time_distributed_dense(inputs, self.kernel_i, self.bias_i,\n                                          self.dropout, input_dim, self.units,\n                                          timesteps, training=training)\n            x_f = _time_distributed_dense(inputs, self.kernel_f, self.bias_f,\n                                          self.dropout, input_dim, self.units,\n                                          timesteps, training=training)\n            x_c = _time_distributed_dense(inputs, self.kernel_c, self.bias_c,\n                                          self.dropout, input_dim, self.units,\n                                          timesteps, training=training)\n            x_o = _time_distributed_dense(inputs, self.kernel_o, self.bias_o,\n                                          self.dropout, input_dim, self.units,\n                                          timesteps, training=training)\n            return K.concatenate([x_i, x_f, x_c, x_o], axis=2)\n        else:\n            return inputs\n\n    def get_constants(self, inputs, training=None):\n        constants = []\n        if self.implementation != 0 and 0 < self.dropout < 1:\n            input_shape = K.int_shape(inputs)\n            input_dim = input_shape[-1]\n            ones = K.ones_like(K.reshape(inputs[:, 0, 0], (-1, 1)))\n            ones = K.tile(ones, (1, int(input_dim)))\n\n            def dropped_inputs():\n                return K.dropout(ones, self.dropout)\n\n            dp_mask = [K.in_train_phase(dropped_inputs,\n                                        ones,\n                                        training=training) for _ in range(4)]\n            constants.append(dp_mask)\n        else:\n            constants.append([K.cast_to_floatx(1.) for _ in range(4)])\n\n        if 0 < self.recurrent_dropout < 1:\n            ones = K.ones_like(K.reshape(inputs[:, 0, 0], (-1, 1)))\n            ones = K.tile(ones, (1, self.units))\n\n            def dropped_inputs():\n                return K.dropout(ones, self.recurrent_dropout)\n            rec_dp_mask = [K.in_train_phase(dropped_inputs,\n                                            ones,\n                                            training=training) for _ in range(4)]\n            constants.append(rec_dp_mask)\n        else:\n            constants.append([K.cast_to_floatx(1.) for _ in range(4)])\n        return constants\n\n    def step(self, inputs, states):\n        h_tm1 = states[0]\n        c_tm1 = states[1]\n        dp_mask = states[2]\n        rec_dp_mask = states[3]\n\n        if self.implementation == 2:\n            z = K.dot(inputs * dp_mask[0], self.kernel)\n            z += K.dot(h_tm1 * rec_dp_mask[0], self.recurrent_kernel)\n            if self.use_bias:\n                z = K.bias_add(z, self.bias)\n\n            z0 = z[:, :self.units]\n            z1 = z[:, self.units: 2 * self.units]\n            z2 = z[:, 2 * self.units: 3 * self.units]\n            z3 = z[:, 3 * self.units:]\n\n            i = self.recurrent_activation(z0)\n            f = self.recurrent_activation(z1)\n            c = f * c_tm1 + i * self.activation(z2)\n            o = self.recurrent_activation(z3)\n        else:\n            if self.implementation == 0:\n                x_i = inputs[:, :self.units]\n                x_f = inputs[:, self.units: 2 * self.units]\n                x_c = inputs[:, 2 * self.units: 3 * self.units]\n                x_o = inputs[:, 3 * self.units:]\n            elif self.implementation == 1:\n                x_i = K.dot(inputs * dp_mask[0], self.kernel_i) + self.bias_i\n                x_f = K.dot(inputs * dp_mask[1], self.kernel_f) + self.bias_f\n                x_c = K.dot(inputs * dp_mask[2], self.kernel_c) + self.bias_c\n                x_o = K.dot(inputs * dp_mask[3], self.kernel_o) + self.bias_o\n            else:\n                raise ValueError('Unknown `implementation` mode.')\n\n            i = self.recurrent_activation(x_i + K.dot(h_tm1 * rec_dp_mask[0],\n                                                      self.recurrent_kernel_i))\n            f = self.recurrent_activation(x_f + K.dot(h_tm1 * rec_dp_mask[1],\n                                                      self.recurrent_kernel_f))\n            c = f * c_tm1 + i * self.activation(x_c + K.dot(h_tm1 * rec_dp_mask[2],\n                                                            self.recurrent_kernel_c))\n            o = self.recurrent_activation(x_o + K.dot(h_tm1 * rec_dp_mask[3],\n                                                      self.recurrent_kernel_o))\n        h = o * self.activation(c)\n        if 0 < self.dropout + self.recurrent_dropout:\n            h._uses_learning_phase = True\n        return h, [h, c]\n\n    def get_config(self):\n        config = {'units': self.units,\n                  'activation': activations.serialize(self.activation),\n                  'recurrent_activation': activations.serialize(self.recurrent_activation),\n                  'use_bias': self.use_bias,\n                  'kernel_initializer': initializers.serialize(self.kernel_initializer),\n                  'recurrent_initializer': initializers.serialize(self.recurrent_initializer),\n                  'bias_initializer': initializers.serialize(self.bias_initializer),\n                  'unit_forget_bias': self.unit_forget_bias,\n                  'kernel_regularizer': regularizers.serialize(self.kernel_regularizer),\n                  'recurrent_regularizer': regularizers.serialize(self.recurrent_regularizer),\n                  'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n                  'activity_regularizer': regularizers.serialize(self.activity_regularizer),\n                  'kernel_constraint': constraints.serialize(self.kernel_constraint),\n                  'recurrent_constraint': constraints.serialize(self.recurrent_constraint),\n                  'bias_constraint': constraints.serialize(self.bias_constraint),\n                  'dropout': self.dropout,\n                  'recurrent_dropout': self.recurrent_dropout}\n        base_config = super(LSTM, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n",
          "file_after": "# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import\nimport numpy as np\n\nfrom .. import backend as K\nfrom .. import activations\nfrom .. import initializers\nfrom .. import regularizers\nfrom .. import constraints\nfrom ..engine import Layer\nfrom ..engine import InputSpec\nfrom ..legacy import interfaces\n\n\ndef _time_distributed_dense(x, w, b=None, dropout=None,\n                            input_dim=None, output_dim=None,\n                            timesteps=None, training=None):\n    \"\"\"Apply `y . w + b` for every temporal slice y of x.\n\n    # Arguments\n        x: input tensor.\n        w: weight matrix.\n        b: optional bias vector.\n        dropout: wether to apply dropout (same dropout mask\n            for every temporal slice of the input).\n        input_dim: integer; optional dimensionality of the input.\n        output_dim: integer; optional dimensionality of the output.\n        timesteps: integer; optional number of timesteps.\n        training: training phase tensor or boolean.\n\n    # Returns\n        Output tensor.\n    \"\"\"\n    if not input_dim:\n        input_dim = K.shape(x)[2]\n    if not timesteps:\n        timesteps = K.shape(x)[1]\n    if not output_dim:\n        output_dim = K.shape(w)[1]\n\n    if dropout is not None and 0. < dropout < 1.:\n        # apply the same dropout pattern at every timestep\n        ones = K.ones_like(K.reshape(x[:, 0, :], (-1, input_dim)))\n        dropout_matrix = K.dropout(ones, dropout)\n        expanded_dropout_matrix = K.repeat(dropout_matrix, timesteps)\n        x = K.in_train_phase(x * expanded_dropout_matrix, x, training=training)\n\n    # collapse time dimension and batch dimension together\n    x = K.reshape(x, (-1, input_dim))\n    x = K.dot(x, w)\n    if b is not None:\n        x = K.bias_add(x, b)\n    # reshape to 3D tensor\n    if K.backend() == 'tensorflow':\n        x = K.reshape(x, K.stack([-1, timesteps, output_dim]))\n        x.set_shape([None, None, output_dim])\n    else:\n        x = K.reshape(x, (-1, timesteps, output_dim))\n    return x\n\n\nclass Recurrent(Layer):\n    \"\"\"Abstract base class for recurrent layers.\n\n    Do not use in a model -- it's not a valid layer!\n    Use its children classes `LSTM`, `GRU` and `SimpleRNN` instead.\n\n    All recurrent layers (`LSTM`, `GRU`, `SimpleRNN`) also\n    follow the specifications of this class and accept\n    the keyword arguments listed below.\n\n    # Example\n\n    ```python\n        # as the first layer in a Sequential model\n        model = Sequential()\n        model.add(LSTM(32, input_shape=(10, 64)))\n        # now model.output_shape == (None, 32)\n        # note: `None` is the batch dimension.\n\n        # for subsequent layers, no need to specify the input size:\n        model.add(LSTM(16))\n\n        # to stack recurrent layers, you must use return_sequences=True\n        # on any recurrent layer that feeds into another recurrent layer.\n        # note that you only need to specify the input size on the first layer.\n        model = Sequential()\n        model.add(LSTM(64, input_dim=64, input_length=10, return_sequences=True))\n        model.add(LSTM(32, return_sequences=True))\n        model.add(LSTM(10))\n    ```\n\n    # Arguments\n        weights: list of Numpy arrays to set as initial weights.\n            The list should have 3 elements, of shapes:\n            `[(input_dim, output_dim), (output_dim, output_dim), (output_dim,)]`.\n        return_sequences: Boolean. Whether to return the last output\n            in the output sequence, or the full sequence.\n        go_backwards: Boolean (default False).\n            If True, process the input sequence backwards and return the\n            reversed sequence.\n        stateful: Boolean (default False). If True, the last state\n            for each sample at index i in a batch will be used as initial\n            state for the sample of index i in the following batch.\n        unroll: Boolean (default False).\n            If True, the network will be unrolled,\n            else a symbolic loop will be used.\n            Unrolling can speed-up a RNN,\n            although it tends to be more memory-intensive.\n            Unrolling is only suitable for short sequences.\n        implementation: one of {0, 1, or 2}.\n            If set to 0, the RNN will use\n            an implementation that uses fewer, larger matrix products,\n            thus running faster on CPU but consuming more memory.\n            If set to 1, the RNN will use more matrix products,\n            but smaller ones, thus running slower\n            (may actually be faster on GPU) while consuming less memory.\n            If set to 2 (LSTM/GRU only),\n            the RNN will combine the input gate,\n            the forget gate and the output gate into a single matrix,\n            enabling more time-efficient parallelization on the GPU.\n            Note: RNN dropout must be shared for all gates,\n            resulting in a slightly reduced regularization.\n        input_dim: dimensionality of the input (integer).\n            This argument (or alternatively, the keyword argument `input_shape`)\n            is required when using this layer as the first layer in a model.\n        input_length: Length of input sequences, to be specified\n            when it is constant.\n            This argument is required if you are going to connect\n            `Flatten` then `Dense` layers upstream\n            (without it, the shape of the dense outputs cannot be computed).\n            Note that if the recurrent layer is not the first layer\n            in your model, you would need to specify the input length\n            at the level of the first layer\n            (e.g. via the `input_shape` argument)\n\n    # Input shapes\n        3D tensor with shape `(batch_size, timesteps, input_dim)`,\n        (Optional) 2D tensors with shape `(batch_size, output_dim)`.\n\n    # Output shape\n        - if `return_sequences`: 3D tensor with shape\n            `(batch_size, timesteps, units)`.\n        - else, 2D tensor with shape `(batch_size, units)`.\n\n    # Masking\n        This layer supports masking for input data with a variable number\n        of timesteps. To introduce masks to your data,\n        use an [Embedding](embeddings.md) layer with the `mask_zero` parameter\n        set to `True`.\n\n    # Note on using statefulness in RNNs\n        You can set RNN layers to be 'stateful', which means that the states\n        computed for the samples in one batch will be reused as initial states\n        for the samples in the next batch. This assumes a one-to-one mapping\n        between samples in different successive batches.\n\n        To enable statefulness:\n            - specify `stateful=True` in the layer constructor.\n            - specify a fixed batch size for your model, by passing\n                if sequential model:\n                  `batch_input_shape=(...)` to the first layer in your model.\n                else for functional model with 1 or more Input layers:\n                  `batch_shape=(...)` to all the first layers in your model.\n                This is the expected shape of your inputs\n                *including the batch size*.\n                It should be a tuple of integers, e.g. `(32, 10, 100)`.\n            - specify `shuffle=False` when calling fit().\n\n        To reset the states of your model, call `.reset_states()` on either\n        a specific layer, or on your entire model.\n\n    # Note on specifying the initial state of RNNs\n        You can specify the initial state of RNN layers symbolically by\n        calling them with the keyword argument `initial_state`. The value of\n        `initial_state` should be a tensor or list of tensors representing\n        the initial state of the RNN layer.\n\n        You can specify the initial state of RNN layers numerically by\n        calling `reset_states` with the keyword argument `states`. The value of\n        `states` should be a numpy array or list of numpy arrays representing\n        the initial state of the RNN layer.\n    \"\"\"\n\n    def __init__(self, return_sequences=False,\n                 go_backwards=False,\n                 stateful=False,\n                 unroll=False,\n                 implementation=0,\n                 **kwargs):\n        super(Recurrent, self).__init__(**kwargs)\n        self.return_sequences = return_sequences\n        self.go_backwards = go_backwards\n        self.stateful = stateful\n        self.unroll = unroll\n        self.implementation = implementation\n        self.supports_masking = True\n        self.input_spec = [InputSpec(ndim=3)]\n        self.state_spec = None\n        self.dropout = 0\n        self.recurrent_dropout = 0\n\n    def compute_output_shape(self, input_shape):\n        if isinstance(input_shape, list):\n            input_shape = input_shape[0]\n        if self.return_sequences:\n            return (input_shape[0], input_shape[1], self.units)\n        else:\n            return (input_shape[0], self.units)\n\n    def compute_mask(self, inputs, mask):\n        if self.return_sequences:\n            if isinstance(mask, list):\n                return mask[0]\n            return mask\n        else:\n            return None\n\n    def step(self, inputs, states):\n        raise NotImplementedError\n\n    def get_constants(self, inputs, training=None):\n        return []\n\n    def get_initial_state(self, inputs):\n        # build an all-zero tensor of shape (samples, output_dim)\n        initial_state = K.zeros_like(inputs)  # (samples, timesteps, input_dim)\n        initial_state = K.sum(initial_state, axis=(1, 2))  # (samples,)\n        initial_state = K.expand_dims(initial_state)  # (samples, 1)\n        initial_state = K.tile(initial_state, [1, self.units])  # (samples, output_dim)\n        initial_state = [initial_state for _ in range(len(self.states))]\n        return initial_state\n\n    def preprocess_input(self, inputs, training=None):\n        return inputs\n\n    def __call__(self, inputs, initial_state=None, **kwargs):\n        # If `initial_state` is specified,\n        # and if it a Keras tensor,\n        # then add it to the inputs and temporarily\n        # modify the input spec to include the state.\n        if initial_state is None:\n            return super(Recurrent, self).__call__(inputs, **kwargs)\n\n        if not isinstance(initial_state, (list, tuple)):\n            initial_state = [initial_state]\n\n        is_keras_tensor = hasattr(initial_state[0], '_keras_history')\n        for tensor in initial_state:\n            if hasattr(tensor, '_keras_history') != is_keras_tensor:\n                raise ValueError('The initial state of an RNN layer cannot be'\n                                 ' specified with a mix of Keras tensors and'\n                                 ' non-Keras tensors')\n\n        if is_keras_tensor:\n            # Compute the full input spec, including state\n            input_spec = self.input_spec\n            state_spec = self.state_spec\n            if not isinstance(state_spec, list):\n                state_spec = [state_spec]\n            self.input_spec = input_spec + state_spec\n\n            # Compute the full inputs, including state\n            inputs = [inputs] + list(initial_state)\n\n            # Perform the call\n            output = super(Recurrent, self).__call__(inputs, **kwargs)\n\n            # Restore original input spec\n            self.input_spec = input_spec\n            return output\n        else:\n            kwargs['initial_state'] = initial_state\n            return super(Recurrent, self).__call__(inputs, **kwargs)\n\n    def call(self, inputs, mask=None, training=None, initial_state=None):\n        # input shape: `(samples, time (padded with zeros), input_dim)`\n        # note that the .build() method of subclasses MUST define\n        # self.input_spec and self.state_spec with complete input shapes.\n        if isinstance(inputs, list):\n            initial_state = inputs[1:]\n            inputs = inputs[0]\n        elif initial_state is not None:\n            pass\n        elif self.stateful:\n            initial_state = self.states\n        else:\n            initial_state = self.get_initial_state(inputs)\n\n        if isinstance(mask, list):\n            mask = mask[0]\n\n        if len(initial_state) != len(self.states):\n            raise ValueError('Layer has ' + str(len(self.states)) +\n                             ' states but was passed ' +\n                             str(len(initial_state)) +\n                             ' initial states.')\n        input_shape = K.int_shape(inputs)\n        if self.unroll and input_shape[1] is None:\n            raise ValueError('Cannot unroll a RNN if the '\n                             'time dimension is undefined. \\n'\n                             '- If using a Sequential model, '\n                             'specify the time dimension by passing '\n                             'an `input_shape` or `batch_input_shape` '\n                             'argument to your first layer. If your '\n                             'first layer is an Embedding, you can '\n                             'also use the `input_length` argument.\\n'\n                             '- If using the functional API, specify '\n                             'the time dimension by passing a `shape` '\n                             'or `batch_shape` argument to your Input layer.')\n        constants = self.get_constants(inputs, training=None)\n        preprocessed_input = self.preprocess_input(inputs, training=None)\n        last_output, outputs, states = K.rnn(self.step,\n                                             preprocessed_input,\n                                             initial_state,\n                                             go_backwards=self.go_backwards,\n                                             mask=mask,\n                                             constants=constants,\n                                             unroll=self.unroll,\n                                             input_length=input_shape[1])\n        if self.stateful:\n            updates = []\n            for i in range(len(states)):\n                updates.append((self.states[i], states[i]))\n            self.add_update(updates, inputs)\n\n        # Properly set learning phase\n        if 0 < self.dropout + self.recurrent_dropout:\n            last_output._uses_learning_phase = True\n            outputs._uses_learning_phase = True\n\n        if self.return_sequences:\n            return outputs\n        else:\n            return last_output\n\n    def reset_states(self, states=None):\n        if not self.stateful:\n            raise AttributeError('Layer must be stateful.')\n        batch_size = self.input_spec[0].shape[0]\n        if not batch_size:\n            raise ValueError('If a RNN is stateful, it needs to know '\n                             'its batch size. Specify the batch size '\n                             'of your input tensors: \\n'\n                             '- If using a Sequential model, '\n                             'specify the batch size by passing '\n                             'a `batch_input_shape` '\n                             'argument to your first layer.\\n'\n                             '- If using the functional API, specify '\n                             'the time dimension by passing a '\n                             '`batch_shape` argument to your Input layer.')\n        # initialize state if None\n        if self.states[0] is None:\n            self.states = [K.zeros((batch_size, self.units))\n                           for _ in self.states]\n        elif states is None:\n            for state in self.states:\n                K.set_value(state, np.zeros((batch_size, self.units)))\n        else:\n            if not isinstance(states, (list, tuple)):\n                states = [states]\n            if len(states) != len(self.states):\n                raise ValueError('Layer ' + self.name + ' expects ' +\n                                 str(len(self.states)) + ' states, '\n                                 'but it received ' + str(len(values)) +\n                                 ' state values. Input received: ' +\n                                 str(values))\n            for index, (value, state) in enumerate(zip(states, self.states)):\n                if value.shape != (batch_size, self.units):\n                    raise ValueError('State ' + str(index) +\n                                     ' is incompatible with layer ' +\n                                     self.name + ': expected shape=' +\n                                     str((batch_size, self.units)) +\n                                     ', found shape=' + str(value.shape))\n                K.set_value(state, value)\n\n    def get_config(self):\n        config = {'return_sequences': self.return_sequences,\n                  'go_backwards': self.go_backwards,\n                  'stateful': self.stateful,\n                  'unroll': self.unroll,\n                  'implementation': self.implementation}\n        base_config = super(Recurrent, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\n\nclass SimpleRNN(Recurrent):\n    \"\"\"Fully-connected RNN where the output is to be fed back to input.\n\n    # Arguments\n        units: Positive integer, dimensionality of the output space.\n        activation: Activation function to use\n            (see [activations](../activations.md)).\n            If you pass None, no activation is applied\n            (ie. \"linear\" activation: `a(x) = x`).\n        use_bias: Boolean, whether the layer uses a bias vector.\n        kernel_initializer: Initializer for the `kernel` weights matrix,\n            used for the linear transformation of the inputs.\n            (see [initializers](../initializers.md)).\n        recurrent_initializer: Initializer for the `recurrent_kernel`\n            weights matrix,\n            used for the linear transformation of the recurrent state.\n            (see [initializers](../initializers.md)).\n        bias_initializer: Initializer for the bias vector\n            (see [initializers](../initializers.md)).\n        kernel_regularizer: Regularizer function applied to\n            the `kernel` weights matrix\n            (see [regularizer](../regularizers.md)).\n        recurrent_regularizer: Regularizer function applied to\n            the `recurrent_kernel` weights matrix\n            (see [regularizer](../regularizers.md)).\n        bias_regularizer: Regularizer function applied to the bias vector\n            (see [regularizer](../regularizers.md)).\n        activity_regularizer: Regularizer function applied to\n            the output of the layer (its \"activation\").\n            (see [regularizer](../regularizers.md)).\n        kernel_constraint: Constraint function applied to\n            the `kernel` weights matrix\n            (see [constraints](../constraints.md)).\n        recurrent_constraint: Constraint function applied to\n            the `recurrent_kernel` weights matrix\n            (see [constraints](../constraints.md)).\n        bias_constraint: Constraint function applied to the bias vector\n            (see [constraints](../constraints.md)).\n        dropout: Float between 0 and 1.\n            Fraction of the units to drop for\n            the linear transformation of the inputs.\n        recurrent_dropout: Float between 0 and 1.\n            Fraction of the units to drop for\n            the linear transformation of the recurrent state.\n\n    # References\n        - [A Theoretically Grounded Application of Dropout in Recurrent Neural Networks](http://arxiv.org/abs/1512.05287)\n    \"\"\"\n\n    @interfaces.legacy_recurrent_support\n    def __init__(self, units,\n                 activation='tanh',\n                 use_bias=True,\n                 kernel_initializer='glorot_uniform',\n                 recurrent_initializer='orthogonal',\n                 bias_initializer='zeros',\n                 kernel_regularizer=None,\n                 recurrent_regularizer=None,\n                 bias_regularizer=None,\n                 activity_regularizer=None,\n                 kernel_constraint=None,\n                 recurrent_constraint=None,\n                 bias_constraint=None,\n                 dropout=0.,\n                 recurrent_dropout=0.,\n                 **kwargs):\n        super(SimpleRNN, self).__init__(**kwargs)\n        self.units = units\n        self.activation = activations.get(activation)\n        self.use_bias = use_bias\n\n        self.kernel_initializer = initializers.get(kernel_initializer)\n        self.recurrent_initializer = initializers.get(recurrent_initializer)\n        self.bias_initializer = initializers.get(bias_initializer)\n\n        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n        self.recurrent_regularizer = regularizers.get(recurrent_regularizer)\n        self.bias_regularizer = regularizers.get(bias_regularizer)\n        self.activity_regularizer = regularizers.get(activity_regularizer)\n\n        self.kernel_constraint = constraints.get(kernel_constraint)\n        self.recurrent_constraint = constraints.get(recurrent_constraint)\n        self.bias_constraint = constraints.get(bias_constraint)\n\n        self.dropout = min(1., max(0., dropout))\n        self.recurrent_dropout = min(1., max(0., recurrent_dropout))\n        self.state_spec = InputSpec(shape=(None, self.units))\n\n    def build(self, input_shape):\n        if isinstance(input_shape, list):\n            input_shape = input_shape[0]\n\n        batch_size = input_shape[0] if self.stateful else None\n        self.input_dim = input_shape[2]\n        self.input_spec[0] = InputSpec(shape=(batch_size, None, self.input_dim))\n\n        self.states = [None]\n        if self.stateful:\n            self.reset_states()\n\n        self.kernel = self.add_weight(shape=(self.input_dim, self.units),\n                                      name='kernel',\n                                      initializer=self.kernel_initializer,\n                                      regularizer=self.kernel_regularizer,\n                                      constraint=self.kernel_constraint)\n        self.recurrent_kernel = self.add_weight(\n            shape=(self.units, self.units),\n            name='recurrent_kernel',\n            initializer=self.recurrent_initializer,\n            regularizer=self.recurrent_regularizer,\n            constraint=self.recurrent_constraint)\n        if self.use_bias:\n            self.bias = self.add_weight(shape=(self.units,),\n                                        name='bias',\n                                        initializer=self.bias_initializer,\n                                        regularizer=self.bias_regularizer,\n                                        constraint=self.bias_constraint)\n        else:\n            self.bias = None\n        self.built = True\n\n    def preprocess_input(self, inputs, training=None):\n        if self.implementation > 0:\n            return inputs\n        else:\n            input_shape = K.int_shape(inputs)\n            input_dim = input_shape[2]\n            timesteps = input_shape[1]\n            return _time_distributed_dense(inputs,\n                                           self.kernel,\n                                           self.bias,\n                                           self.dropout,\n                                           input_dim,\n                                           self.units,\n                                           timesteps,\n                                           training=training)\n\n    def step(self, inputs, states):\n        if self.implementation == 0:\n            h = inputs\n        else:\n            if 0 < self.dropout < 1:\n                h = K.dot(inputs * states[1], self.kernel)\n            else:\n                h = K.dot(inputs, self.kernel)\n            if self.bias is not None:\n                h = K.bias_add(h, self.bias)\n\n        prev_output = states[0]\n        if 0 < self.recurrent_dropout < 1:\n            prev_output *= states[2]\n        output = h + K.dot(prev_output, self.recurrent_kernel)\n        if self.activation is not None:\n            output = self.activation(output)\n\n        # Properly set learning phase on output tensor.\n        if 0 < self.dropout + self.recurrent_dropout:\n            output._uses_learning_phase = True\n        return output, [output]\n\n    def get_constants(self, inputs, training=None):\n        constants = []\n        if self.implementation != 0 and 0 < self.dropout < 1:\n            input_shape = K.int_shape(inputs)\n            input_dim = input_shape[-1]\n            ones = K.ones_like(K.reshape(inputs[:, 0, 0], (-1, 1)))\n            ones = K.tile(ones, (1, int(input_dim)))\n\n            def dropped_inputs():\n                return K.dropout(ones, self.dropout)\n\n            dp_mask = K.in_train_phase(dropped_inputs,\n                                       ones,\n                                       training=training)\n            constants.append(dp_mask)\n        else:\n            constants.append(K.cast_to_floatx(1.))\n\n        if 0 < self.recurrent_dropout < 1:\n            ones = K.ones_like(K.reshape(inputs[:, 0, 0], (-1, 1)))\n            ones = K.tile(ones, (1, self.units))\n\n            def dropped_inputs():\n                return K.dropout(ones, self.recurrent_dropout)\n            rec_dp_mask = K.in_train_phase(dropped_inputs,\n                                           ones,\n                                           training=training)\n            constants.append(rec_dp_mask)\n        else:\n            constants.append(K.cast_to_floatx(1.))\n        return constants\n\n    def get_config(self):\n        config = {'units': self.units,\n                  'activation': activations.serialize(self.activation),\n                  'use_bias': self.use_bias,\n                  'kernel_initializer': initializers.serialize(self.kernel_initializer),\n                  'recurrent_initializer': initializers.serialize(self.recurrent_initializer),\n                  'bias_initializer': initializers.serialize(self.bias_initializer),\n                  'kernel_regularizer': regularizers.serialize(self.kernel_regularizer),\n                  'recurrent_regularizer': regularizers.serialize(self.recurrent_regularizer),\n                  'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n                  'activity_regularizer': regularizers.serialize(self.activity_regularizer),\n                  'kernel_constraint': constraints.serialize(self.kernel_constraint),\n                  'recurrent_constraint': constraints.serialize(self.recurrent_constraint),\n                  'bias_constraint': constraints.serialize(self.bias_constraint),\n                  'dropout': self.dropout,\n                  'recurrent_dropout': self.recurrent_dropout}\n        base_config = super(SimpleRNN, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\n\nclass GRU(Recurrent):\n    \"\"\"Gated Recurrent Unit - Cho et al. 2014.\n\n    # Arguments\n        units: Positive integer, dimensionality of the output space.\n        activation: Activation function to use\n            (see [activations](../activations.md)).\n            If you pass None, no activation is applied\n            (ie. \"linear\" activation: `a(x) = x`).\n        recurrent_activation: Activation function to use\n            for the recurrent step\n            (see [activations](../activations.md)).\n        use_bias: Boolean, whether the layer uses a bias vector.\n        kernel_initializer: Initializer for the `kernel` weights matrix,\n            used for the linear transformation of the inputs.\n            (see [initializers](../initializers.md)).\n        recurrent_initializer: Initializer for the `recurrent_kernel`\n            weights matrix,\n            used for the linear transformation of the recurrent state.\n            (see [initializers](../initializers.md)).\n        bias_initializer: Initializer for the bias vector\n            (see [initializers](../initializers.md)).\n        kernel_regularizer: Regularizer function applied to\n            the `kernel` weights matrix\n            (see [regularizer](../regularizers.md)).\n        recurrent_regularizer: Regularizer function applied to\n            the `recurrent_kernel` weights matrix\n            (see [regularizer](../regularizers.md)).\n        bias_regularizer: Regularizer function applied to the bias vector\n            (see [regularizer](../regularizers.md)).\n        activity_regularizer: Regularizer function applied to\n            the output of the layer (its \"activation\").\n            (see [regularizer](../regularizers.md)).\n        kernel_constraint: Constraint function applied to\n            the `kernel` weights matrix\n            (see [constraints](../constraints.md)).\n        recurrent_constraint: Constraint function applied to\n            the `recurrent_kernel` weights matrix\n            (see [constraints](../constraints.md)).\n        bias_constraint: Constraint function applied to the bias vector\n            (see [constraints](../constraints.md)).\n        dropout: Float between 0 and 1.\n            Fraction of the units to drop for\n            the linear transformation of the inputs.\n        recurrent_dropout: Float between 0 and 1.\n            Fraction of the units to drop for\n            the linear transformation of the recurrent state.\n\n    # References\n        - [On the Properties of Neural Machine Translation: Encoder-Decoder Approaches](https://arxiv.org/abs/1409.1259)\n        - [Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling](http://arxiv.org/abs/1412.3555v1)\n        - [A Theoretically Grounded Application of Dropout in Recurrent Neural Networks](http://arxiv.org/abs/1512.05287)\n    \"\"\"\n\n    @interfaces.legacy_recurrent_support\n    def __init__(self, units,\n                 activation='tanh',\n                 recurrent_activation='hard_sigmoid',\n                 use_bias=True,\n                 kernel_initializer='glorot_uniform',\n                 recurrent_initializer='orthogonal',\n                 bias_initializer='zeros',\n                 kernel_regularizer=None,\n                 recurrent_regularizer=None,\n                 bias_regularizer=None,\n                 activity_regularizer=None,\n                 kernel_constraint=None,\n                 recurrent_constraint=None,\n                 bias_constraint=None,\n                 dropout=0.,\n                 recurrent_dropout=0.,\n                 **kwargs):\n        super(GRU, self).__init__(**kwargs)\n        self.units = units\n        self.activation = activations.get(activation)\n        self.recurrent_activation = activations.get(recurrent_activation)\n        self.use_bias = use_bias\n\n        self.kernel_initializer = initializers.get(kernel_initializer)\n        self.recurrent_initializer = initializers.get(recurrent_initializer)\n        self.bias_initializer = initializers.get(bias_initializer)\n\n        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n        self.recurrent_regularizer = regularizers.get(recurrent_regularizer)\n        self.bias_regularizer = regularizers.get(bias_regularizer)\n        self.activity_regularizer = regularizers.get(activity_regularizer)\n\n        self.kernel_constraint = constraints.get(kernel_constraint)\n        self.recurrent_constraint = constraints.get(recurrent_constraint)\n        self.bias_constraint = constraints.get(bias_constraint)\n\n        self.dropout = min(1., max(0., dropout))\n        self.recurrent_dropout = min(1., max(0., recurrent_dropout))\n        self.state_spec = InputSpec(shape=(None, self.units))\n\n    def build(self, input_shape):\n        if isinstance(input_shape, list):\n            input_shape = input_shape[0]\n\n        batch_size = input_shape[0] if self.stateful else None\n        self.input_dim = input_shape[2]\n        self.input_spec[0] = InputSpec(shape=(batch_size, None, self.input_dim))\n\n        self.states = [None]\n        if self.stateful:\n            self.reset_states()\n\n        self.kernel = self.add_weight(shape=(self.input_dim, self.units * 3),\n                                      name='kernel',\n                                      initializer=self.kernel_initializer,\n                                      regularizer=self.kernel_regularizer,\n                                      constraint=self.kernel_constraint)\n        self.recurrent_kernel = self.add_weight(\n            shape=(self.units, self.units * 3),\n            name='recurrent_kernel',\n            initializer=self.recurrent_initializer,\n            regularizer=self.recurrent_regularizer,\n            constraint=self.recurrent_constraint)\n\n        if self.use_bias:\n            self.bias = self.add_weight(shape=(self.units * 3,),\n                                        name='bias',\n                                        initializer=self.bias_initializer,\n                                        regularizer=self.bias_regularizer,\n                                        constraint=self.bias_constraint)\n        else:\n            self.bias = None\n\n        self.kernel_z = self.kernel[:, :self.units]\n        self.recurrent_kernel_z = self.recurrent_kernel[:, :self.units]\n        self.kernel_r = self.kernel[:, self.units: self.units * 2]\n        self.recurrent_kernel_r = self.recurrent_kernel[:,\n                                                        self.units:\n                                                        self.units * 2]\n        self.kernel_h = self.kernel[:, self.units * 2:]\n        self.recurrent_kernel_h = self.recurrent_kernel[:, self.units * 2:]\n\n        if self.use_bias:\n            self.bias_z = self.bias[:self.units]\n            self.bias_r = self.bias[self.units: self.units * 2]\n            self.bias_h = self.bias[self.units * 2:]\n        else:\n            self.bias_z = None\n            self.bias_r = None\n            self.bias_h = None\n        self.built = True\n\n    def preprocess_input(self, inputs, training=None):\n        if self.implementation == 0:\n            input_shape = K.int_shape(inputs)\n            input_dim = input_shape[2]\n            timesteps = input_shape[1]\n\n            x_z = _time_distributed_dense(inputs, self.kernel_z, self.bias_z,\n                                          self.dropout, input_dim, self.units,\n                                          timesteps, training=training)\n            x_r = _time_distributed_dense(inputs, self.kernel_r, self.bias_r,\n                                          self.dropout, input_dim, self.units,\n                                          timesteps, training=training)\n            x_h = _time_distributed_dense(inputs, self.kernel_h, self.bias_h,\n                                          self.dropout, input_dim, self.units,\n                                          timesteps, training=training)\n            return K.concatenate([x_z, x_r, x_h], axis=2)\n        else:\n            return inputs\n\n    def get_constants(self, inputs, training=None):\n        constants = []\n        if self.implementation != 0 and 0 < self.dropout < 1:\n            input_shape = K.int_shape(inputs)\n            input_dim = input_shape[-1]\n            ones = K.ones_like(K.reshape(inputs[:, 0, 0], (-1, 1)))\n            ones = K.tile(ones, (1, int(input_dim)))\n\n            def dropped_inputs():\n                return K.dropout(ones, self.dropout)\n\n            dp_mask = [K.in_train_phase(dropped_inputs,\n                                        ones,\n                                        training=training) for _ in range(3)]\n            constants.append(dp_mask)\n        else:\n            constants.append([K.cast_to_floatx(1.) for _ in range(3)])\n\n        if 0 < self.recurrent_dropout < 1:\n            ones = K.ones_like(K.reshape(inputs[:, 0, 0], (-1, 1)))\n            ones = K.tile(ones, (1, self.units))\n\n            def dropped_inputs():\n                return K.dropout(ones, self.recurrent_dropout)\n            rec_dp_mask = [K.in_train_phase(dropped_inputs,\n                                            ones,\n                                            training=training) for _ in range(3)]\n            constants.append(rec_dp_mask)\n        else:\n            constants.append([K.cast_to_floatx(1.) for _ in range(3)])\n        return constants\n\n    def step(self, inputs, states):\n        h_tm1 = states[0]  # previous memory\n        dp_mask = states[1]  # dropout matrices for recurrent units\n        rec_dp_mask = states[2]\n\n        if self.implementation == 2:\n            matrix_x = K.dot(inputs * dp_mask[0], self.kernel)\n            if self.use_bias:\n                matrix_x = K.bias_add(matrix_x, self.bias)\n            matrix_inner = K.dot(h_tm1 * rec_dp_mask[0],\n                                 self.recurrent_kernel[:, :2 * self.units])\n\n            x_z = matrix_x[:, :self.units]\n            x_r = matrix_x[:, self.units: 2 * self.units]\n            recurrent_z = matrix_inner[:, :self.units]\n            recurrent_r = matrix_inner[:, self.units: 2 * self.units]\n\n            z = self.recurrent_activation(x_z + recurrent_z)\n            r = self.recurrent_activation(x_r + recurrent_r)\n\n            x_h = matrix_x[:, 2 * self.units:]\n            recurrent_h = K.dot(r * h_tm1 * rec_dp_mask[0],\n                                self.recurrent_kernel[:, 2 * self.units:])\n            hh = self.activation(x_h + recurrent_h)\n        else:\n            if self.implementation == 0:\n                x_z = inputs[:, :self.units]\n                x_r = inputs[:, self.units: 2 * self.units]\n                x_h = inputs[:, 2 * self.units:]\n            elif self.implementation == 1:\n                x_z = K.dot(inputs * dp_mask[0], self.kernel_z)\n                x_r = K.dot(inputs * dp_mask[1], self.kernel_r)\n                x_h = K.dot(inputs * dp_mask[2], self.kernel_h)\n                if self.use_bias:\n                    x_z = K.bias_add(x_z, self.bias_z)\n                    x_r = K.bias_add(x_r, self.bias_r)\n                    x_h = K.bias_add(x_h, self.bias_h)\n            else:\n                raise ValueError('Unknown `implementation` mode.')\n            z = self.recurrent_activation(x_z + K.dot(h_tm1 * rec_dp_mask[0],\n                                                      self.recurrent_kernel_z))\n            r = self.recurrent_activation(x_r + K.dot(h_tm1 * rec_dp_mask[1],\n                                                      self.recurrent_kernel_r))\n\n            hh = self.activation(x_h + K.dot(r * h_tm1 * rec_dp_mask[2],\n                                             self.recurrent_kernel_h))\n        h = z * h_tm1 + (1 - z) * hh\n        if 0 < self.dropout + self.recurrent_dropout:\n            h._uses_learning_phase = True\n        return h, [h]\n\n    def get_config(self):\n        config = {'units': self.units,\n                  'activation': activations.serialize(self.activation),\n                  'recurrent_activation': activations.serialize(self.recurrent_activation),\n                  'use_bias': self.use_bias,\n                  'kernel_initializer': initializers.serialize(self.kernel_initializer),\n                  'recurrent_initializer': initializers.serialize(self.recurrent_initializer),\n                  'bias_initializer': initializers.serialize(self.bias_initializer),\n                  'kernel_regularizer': regularizers.serialize(self.kernel_regularizer),\n                  'recurrent_regularizer': regularizers.serialize(self.recurrent_regularizer),\n                  'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n                  'activity_regularizer': regularizers.serialize(self.activity_regularizer),\n                  'kernel_constraint': constraints.serialize(self.kernel_constraint),\n                  'recurrent_constraint': constraints.serialize(self.recurrent_constraint),\n                  'bias_constraint': constraints.serialize(self.bias_constraint),\n                  'dropout': self.dropout,\n                  'recurrent_dropout': self.recurrent_dropout}\n        base_config = super(GRU, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\n\nclass LSTM(Recurrent):\n    \"\"\"Long-Short Term Memory unit - Hochreiter 1997.\n\n    For a step-by-step description of the algorithm, see\n    [this tutorial](http://deeplearning.net/tutorial/lstm.html).\n\n    # Arguments\n        units: Positive integer, dimensionality of the output space.\n        activation: Activation function to use\n            (see [activations](../activations.md)).\n            If you pass None, no activation is applied\n            (ie. \"linear\" activation: `a(x) = x`).\n        recurrent_activation: Activation function to use\n            for the recurrent step\n            (see [activations](../activations.md)).\n        use_bias: Boolean, whether the layer uses a bias vector.\n        kernel_initializer: Initializer for the `kernel` weights matrix,\n            used for the linear transformation of the inputs.\n            (see [initializers](../initializers.md)).\n        recurrent_initializer: Initializer for the `recurrent_kernel`\n            weights matrix,\n            used for the linear transformation of the recurrent state.\n            (see [initializers](../initializers.md)).\n        bias_initializer: Initializer for the bias vector\n            (see [initializers](../initializers.md)).\n        unit_forget_bias: Boolean.\n            If True, add 1 to the bias of the forget gate at initialization.\n            Setting it to true will also force `bias_initializer=\"zeros\"`.\n            This is recommended in [Jozefowicz et al.](http://www.jmlr.org/proceedings/papers/v37/jozefowicz15.pdf)\n        kernel_regularizer: Regularizer function applied to\n            the `kernel` weights matrix\n            (see [regularizer](../regularizers.md)).\n        recurrent_regularizer: Regularizer function applied to\n            the `recurrent_kernel` weights matrix\n            (see [regularizer](../regularizers.md)).\n        bias_regularizer: Regularizer function applied to the bias vector\n            (see [regularizer](../regularizers.md)).\n        activity_regularizer: Regularizer function applied to\n            the output of the layer (its \"activation\").\n            (see [regularizer](../regularizers.md)).\n        kernel_constraint: Constraint function applied to\n            the `kernel` weights matrix\n            (see [constraints](../constraints.md)).\n        recurrent_constraint: Constraint function applied to\n            the `recurrent_kernel` weights matrix\n            (see [constraints](../constraints.md)).\n        bias_constraint: Constraint function applied to the bias vector\n            (see [constraints](../constraints.md)).\n        dropout: Float between 0 and 1.\n            Fraction of the units to drop for\n            the linear transformation of the inputs.\n        recurrent_dropout: Float between 0 and 1.\n            Fraction of the units to drop for\n            the linear transformation of the recurrent state.\n\n    # References\n        - [Long short-term memory](http://deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf) (original 1997 paper)\n        - [Learning to forget: Continual prediction with LSTM](http://www.mitpressjournals.org/doi/pdf/10.1162/089976600300015015)\n        - [Supervised sequence labeling with recurrent neural networks](http://www.cs.toronto.edu/~graves/preprint.pdf)\n        - [A Theoretically Grounded Application of Dropout in Recurrent Neural Networks](http://arxiv.org/abs/1512.05287)\n    \"\"\"\n    @interfaces.legacy_recurrent_support\n    def __init__(self, units,\n                 activation='tanh',\n                 recurrent_activation='hard_sigmoid',\n                 use_bias=True,\n                 kernel_initializer='glorot_uniform',\n                 recurrent_initializer='orthogonal',\n                 bias_initializer='zeros',\n                 unit_forget_bias=True,\n                 kernel_regularizer=None,\n                 recurrent_regularizer=None,\n                 bias_regularizer=None,\n                 activity_regularizer=None,\n                 kernel_constraint=None,\n                 recurrent_constraint=None,\n                 bias_constraint=None,\n                 dropout=0.,\n                 recurrent_dropout=0.,\n                 **kwargs):\n        super(LSTM, self).__init__(**kwargs)\n        self.units = units\n        self.activation = activations.get(activation)\n        self.recurrent_activation = activations.get(recurrent_activation)\n        self.use_bias = use_bias\n\n        self.kernel_initializer = initializers.get(kernel_initializer)\n        self.recurrent_initializer = initializers.get(recurrent_initializer)\n        self.bias_initializer = initializers.get(bias_initializer)\n        self.unit_forget_bias = unit_forget_bias\n\n        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n        self.recurrent_regularizer = regularizers.get(recurrent_regularizer)\n        self.bias_regularizer = regularizers.get(bias_regularizer)\n        self.activity_regularizer = regularizers.get(activity_regularizer)\n\n        self.kernel_constraint = constraints.get(kernel_constraint)\n        self.recurrent_constraint = constraints.get(recurrent_constraint)\n        self.bias_constraint = constraints.get(bias_constraint)\n\n        self.dropout = min(1., max(0., dropout))\n        self.recurrent_dropout = min(1., max(0., recurrent_dropout))\n        self.state_spec = [InputSpec(shape=(None, self.units)),\n                           InputSpec(shape=(None, self.units))]\n\n    def build(self, input_shape):\n        if isinstance(input_shape, list):\n            input_shape = input_shape[0]\n\n        batch_size = input_shape[0] if self.stateful else None\n        self.input_dim = input_shape[2]\n        self.input_spec[0] = InputSpec(shape=(batch_size, None, self.input_dim))\n\n        self.states = [None, None]\n        if self.stateful:\n            self.reset_states()\n\n        self.kernel = self.add_weight(shape=(self.input_dim, self.units * 4),\n                                      name='kernel',\n                                      initializer=self.kernel_initializer,\n                                      regularizer=self.kernel_regularizer,\n                                      constraint=self.kernel_constraint)\n        self.recurrent_kernel = self.add_weight(\n            shape=(self.units, self.units * 4),\n            name='recurrent_kernel',\n            initializer=self.recurrent_initializer,\n            regularizer=self.recurrent_regularizer,\n            constraint=self.recurrent_constraint)\n\n        if self.use_bias:\n            if self.unit_forget_bias:\n                def bias_initializer(shape, *args, **kwargs):\n                    return K.concatenate([\n                        self.bias_initializer((self.units,), *args, **kwargs),\n                        initializers.Ones()((self.units,), *args, **kwargs),\n                        self.bias_initializer((self.units * 2,), *args, **kwargs),\n                    ])\n            else:\n                bias_initializer = self.bias_initializer\n            self.bias = self.add_weight(shape=(self.units * 4,),\n                                        name='bias',\n                                        initializer=bias_initializer,\n                                        regularizer=self.bias_regularizer,\n                                        constraint=self.bias_constraint)\n        else:\n            self.bias = None\n\n        self.kernel_i = self.kernel[:, :self.units]\n        self.kernel_f = self.kernel[:, self.units: self.units * 2]\n        self.kernel_c = self.kernel[:, self.units * 2: self.units * 3]\n        self.kernel_o = self.kernel[:, self.units * 3:]\n\n        self.recurrent_kernel_i = self.recurrent_kernel[:, :self.units]\n        self.recurrent_kernel_f = self.recurrent_kernel[:, self.units: self.units * 2]\n        self.recurrent_kernel_c = self.recurrent_kernel[:, self.units * 2: self.units * 3]\n        self.recurrent_kernel_o = self.recurrent_kernel[:, self.units * 3:]\n\n        if self.use_bias:\n            self.bias_i = self.bias[:self.units]\n            self.bias_f = self.bias[self.units: self.units * 2]\n            self.bias_c = self.bias[self.units * 2: self.units * 3]\n            self.bias_o = self.bias[self.units * 3:]\n        else:\n            self.bias_i = None\n            self.bias_f = None\n            self.bias_c = None\n            self.bias_o = None\n        self.built = True\n\n    def preprocess_input(self, inputs, training=None):\n        if self.implementation == 0:\n            input_shape = K.int_shape(inputs)\n            input_dim = input_shape[2]\n            timesteps = input_shape[1]\n\n            x_i = _time_distributed_dense(inputs, self.kernel_i, self.bias_i,\n                                          self.dropout, input_dim, self.units,\n                                          timesteps, training=training)\n            x_f = _time_distributed_dense(inputs, self.kernel_f, self.bias_f,\n                                          self.dropout, input_dim, self.units,\n                                          timesteps, training=training)\n            x_c = _time_distributed_dense(inputs, self.kernel_c, self.bias_c,\n                                          self.dropout, input_dim, self.units,\n                                          timesteps, training=training)\n            x_o = _time_distributed_dense(inputs, self.kernel_o, self.bias_o,\n                                          self.dropout, input_dim, self.units,\n                                          timesteps, training=training)\n            return K.concatenate([x_i, x_f, x_c, x_o], axis=2)\n        else:\n            return inputs\n\n    def get_constants(self, inputs, training=None):\n        constants = []\n        if self.implementation != 0 and 0 < self.dropout < 1:\n            input_shape = K.int_shape(inputs)\n            input_dim = input_shape[-1]\n            ones = K.ones_like(K.reshape(inputs[:, 0, 0], (-1, 1)))\n            ones = K.tile(ones, (1, int(input_dim)))\n\n            def dropped_inputs():\n                return K.dropout(ones, self.dropout)\n\n            dp_mask = [K.in_train_phase(dropped_inputs,\n                                        ones,\n                                        training=training) for _ in range(4)]\n            constants.append(dp_mask)\n        else:\n            constants.append([K.cast_to_floatx(1.) for _ in range(4)])\n\n        if 0 < self.recurrent_dropout < 1:\n            ones = K.ones_like(K.reshape(inputs[:, 0, 0], (-1, 1)))\n            ones = K.tile(ones, (1, self.units))\n\n            def dropped_inputs():\n                return K.dropout(ones, self.recurrent_dropout)\n            rec_dp_mask = [K.in_train_phase(dropped_inputs,\n                                            ones,\n                                            training=training) for _ in range(4)]\n            constants.append(rec_dp_mask)\n        else:\n            constants.append([K.cast_to_floatx(1.) for _ in range(4)])\n        return constants\n\n    def step(self, inputs, states):\n        h_tm1 = states[0]\n        c_tm1 = states[1]\n        dp_mask = states[2]\n        rec_dp_mask = states[3]\n\n        if self.implementation == 2:\n            z = K.dot(inputs * dp_mask[0], self.kernel)\n            z += K.dot(h_tm1 * rec_dp_mask[0], self.recurrent_kernel)\n            if self.use_bias:\n                z = K.bias_add(z, self.bias)\n\n            z0 = z[:, :self.units]\n            z1 = z[:, self.units: 2 * self.units]\n            z2 = z[:, 2 * self.units: 3 * self.units]\n            z3 = z[:, 3 * self.units:]\n\n            i = self.recurrent_activation(z0)\n            f = self.recurrent_activation(z1)\n            c = f * c_tm1 + i * self.activation(z2)\n            o = self.recurrent_activation(z3)\n        else:\n            if self.implementation == 0:\n                x_i = inputs[:, :self.units]\n                x_f = inputs[:, self.units: 2 * self.units]\n                x_c = inputs[:, 2 * self.units: 3 * self.units]\n                x_o = inputs[:, 3 * self.units:]\n            elif self.implementation == 1:\n                x_i = K.dot(inputs * dp_mask[0], self.kernel_i) + self.bias_i\n                x_f = K.dot(inputs * dp_mask[1], self.kernel_f) + self.bias_f\n                x_c = K.dot(inputs * dp_mask[2], self.kernel_c) + self.bias_c\n                x_o = K.dot(inputs * dp_mask[3], self.kernel_o) + self.bias_o\n            else:\n                raise ValueError('Unknown `implementation` mode.')\n\n            i = self.recurrent_activation(x_i + K.dot(h_tm1 * rec_dp_mask[0],\n                                                      self.recurrent_kernel_i))\n            f = self.recurrent_activation(x_f + K.dot(h_tm1 * rec_dp_mask[1],\n                                                      self.recurrent_kernel_f))\n            c = f * c_tm1 + i * self.activation(x_c + K.dot(h_tm1 * rec_dp_mask[2],\n                                                            self.recurrent_kernel_c))\n            o = self.recurrent_activation(x_o + K.dot(h_tm1 * rec_dp_mask[3],\n                                                      self.recurrent_kernel_o))\n        h = o * self.activation(c)\n        if 0 < self.dropout + self.recurrent_dropout:\n            h._uses_learning_phase = True\n        return h, [h, c]\n\n    def get_config(self):\n        config = {'units': self.units,\n                  'activation': activations.serialize(self.activation),\n                  'recurrent_activation': activations.serialize(self.recurrent_activation),\n                  'use_bias': self.use_bias,\n                  'kernel_initializer': initializers.serialize(self.kernel_initializer),\n                  'recurrent_initializer': initializers.serialize(self.recurrent_initializer),\n                  'bias_initializer': initializers.serialize(self.bias_initializer),\n                  'unit_forget_bias': self.unit_forget_bias,\n                  'kernel_regularizer': regularizers.serialize(self.kernel_regularizer),\n                  'recurrent_regularizer': regularizers.serialize(self.recurrent_regularizer),\n                  'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n                  'activity_regularizer': regularizers.serialize(self.activity_regularizer),\n                  'kernel_constraint': constraints.serialize(self.kernel_constraint),\n                  'recurrent_constraint': constraints.serialize(self.recurrent_constraint),\n                  'bias_constraint': constraints.serialize(self.bias_constraint),\n                  'dropout': self.dropout,\n                  'recurrent_dropout': self.recurrent_dropout}\n        base_config = super(LSTM, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n",
          "file_patch": "@@ -170,11 +170,16 @@ class Recurrent(Layer):\n         To reset the states of your model, call `.reset_states()` on either\n         a specific layer, or on your entire model.\n \n-    # Note on specifying initial states in RNNs\n-        You can specify the initial state of RNN layers by calling them with\n-        the keyword argument `initial_state`. The value of `initial_state`\n-        should be a tensor or list of tensors representing the initial state\n-        of the RNN layer.\n+    # Note on specifying the initial state of RNNs\n+        You can specify the initial state of RNN layers symbolically by\n+        calling them with the keyword argument `initial_state`. The value of\n+        `initial_state` should be a tensor or list of tensors representing\n+        the initial state of the RNN layer.\n+\n+        You can specify the initial state of RNN layers numerically by\n+        calling `reset_states` with the keyword argument `states`. The value of\n+        `states` should be a numpy array or list of numpy arrays representing\n+        the initial state of the RNN layer.\n     \"\"\"\n \n     def __init__(self, return_sequences=False,\n@@ -190,7 +195,7 @@ class Recurrent(Layer):\n         self.unroll = unroll\n         self.implementation = implementation\n         self.supports_masking = True\n-        self.input_spec = InputSpec(ndim=3)\n+        self.input_spec = [InputSpec(ndim=3)]\n         self.state_spec = None\n         self.dropout = 0\n         self.recurrent_dropout = 0\n@@ -205,6 +210,8 @@ class Recurrent(Layer):\n \n     def compute_mask(self, inputs, mask):\n         if self.return_sequences:\n+            if isinstance(mask, list):\n+                return mask[0]\n             return mask\n         else:\n             return None\n@@ -215,14 +222,14 @@ class Recurrent(Layer):\n     def get_constants(self, inputs, training=None):\n         return []\n \n-    def get_initial_states(self, inputs):\n+    def get_initial_state(self, inputs):\n         # build an all-zero tensor of shape (samples, output_dim)\n         initial_state = K.zeros_like(inputs)  # (samples, timesteps, input_dim)\n         initial_state = K.sum(initial_state, axis=(1, 2))  # (samples,)\n         initial_state = K.expand_dims(initial_state)  # (samples, 1)\n         initial_state = K.tile(initial_state, [1, self.units])  # (samples, output_dim)\n-        initial_states = [initial_state for _ in range(len(self.states))]\n-        return initial_states\n+        initial_state = [initial_state for _ in range(len(self.states))]\n+        return initial_state\n \n     def preprocess_input(self, inputs, training=None):\n         return inputs\n@@ -232,51 +239,61 @@ class Recurrent(Layer):\n         # and if it a Keras tensor,\n         # then add it to the inputs and temporarily\n         # modify the input spec to include the state.\n-        if initial_state is not None:\n-            if hasattr(initial_state, '_keras_history'):\n-                # Compute the full input spec, including state\n-                input_spec = self.input_spec\n-                state_spec = self.state_spec\n-                if not isinstance(state_spec, list):\n-                    state_spec = [state_spec]\n-                self.input_spec = [input_spec] + state_spec\n-\n-                # Compute the full inputs, including state\n-                if not isinstance(initial_state, (list, tuple)):\n-                    initial_state = [initial_state]\n-                inputs = [inputs] + list(initial_state)\n-\n-                # Perform the call\n-                output = super(Recurrent, self).__call__(inputs, **kwargs)\n-\n-                # Restore original input spec\n-                self.input_spec = input_spec\n-                return output\n-            else:\n-                kwargs['initial_state'] = initial_state\n-        return super(Recurrent, self).__call__(inputs, **kwargs)\n+        if initial_state is None:\n+            return super(Recurrent, self).__call__(inputs, **kwargs)\n+\n+        if not isinstance(initial_state, (list, tuple)):\n+            initial_state = [initial_state]\n+\n+        is_keras_tensor = hasattr(initial_state[0], '_keras_history')\n+        for tensor in initial_state:\n+            if hasattr(tensor, '_keras_history') != is_keras_tensor:\n+                raise ValueError('The initial state of an RNN layer cannot be'\n+                                 ' specified with a mix of Keras tensors and'\n+                                 ' non-Keras tensors')\n+\n+        if is_keras_tensor:\n+            # Compute the full input spec, including state\n+            input_spec = self.input_spec\n+            state_spec = self.state_spec\n+            if not isinstance(state_spec, list):\n+                state_spec = [state_spec]\n+            self.input_spec = input_spec + state_spec\n+\n+            # Compute the full inputs, including state\n+            inputs = [inputs] + list(initial_state)\n+\n+            # Perform the call\n+            output = super(Recurrent, self).__call__(inputs, **kwargs)\n+\n+            # Restore original input spec\n+            self.input_spec = input_spec\n+            return output\n+        else:\n+            kwargs['initial_state'] = initial_state\n+            return super(Recurrent, self).__call__(inputs, **kwargs)\n \n-    def call(self, inputs, mask=None, initial_state=None, training=None):\n+    def call(self, inputs, mask=None, training=None, initial_state=None):\n         # input shape: `(samples, time (padded with zeros), input_dim)`\n         # note that the .build() method of subclasses MUST define\n         # self.input_spec and self.state_spec with complete input shapes.\n-        if initial_state is not None:\n-            if not isinstance(initial_state, (list, tuple)):\n-                initial_states = [initial_state]\n-            else:\n-                initial_states = list(initial_state)\n         if isinstance(inputs, list):\n-            initial_states = inputs[1:]\n+            initial_state = inputs[1:]\n             inputs = inputs[0]\n+        elif initial_state is not None:\n+            pass\n         elif self.stateful:\n-            initial_states = self.states\n+            initial_state = self.states\n         else:\n-            initial_states = self.get_initial_states(inputs)\n+            initial_state = self.get_initial_state(inputs)\n \n-        if len(initial_states) != len(self.states):\n+        if isinstance(mask, list):\n+            mask = mask[0]\n+\n+        if len(initial_state) != len(self.states):\n             raise ValueError('Layer has ' + str(len(self.states)) +\n                              ' states but was passed ' +\n-                             str(len(initial_states)) +\n+                             str(len(initial_state)) +\n                              ' initial states.')\n         input_shape = K.int_shape(inputs)\n         if self.unroll and input_shape[1] is None:\n@@ -295,7 +312,7 @@ class Recurrent(Layer):\n         preprocessed_input = self.preprocess_input(inputs, training=None)\n         last_output, outputs, states = K.rnn(self.step,\n                                              preprocessed_input,\n-                                             initial_states,\n+                                             initial_state,\n                                              go_backwards=self.go_backwards,\n                                              mask=mask,\n                                              constants=constants,\n@@ -317,13 +334,10 @@ class Recurrent(Layer):\n         else:\n             return last_output\n \n-    def reset_states(self, states_value=None):\n+    def reset_states(self, states=None):\n         if not self.stateful:\n             raise AttributeError('Layer must be stateful.')\n-        if not self.input_spec:\n-            raise RuntimeError('Layer has never been called '\n-                               'and thus has no states.')\n-        batch_size = self.input_spec.shape[0]\n+        batch_size = self.input_spec[0].shape[0]\n         if not batch_size:\n             raise ValueError('If a RNN is stateful, it needs to know '\n                              'its batch size. Specify the batch size '\n@@ -335,31 +349,30 @@ class Recurrent(Layer):\n                              '- If using the functional API, specify '\n                              'the time dimension by passing a '\n                              '`batch_shape` argument to your Input layer.')\n-        if states_value is not None:\n-            if not isinstance(states_value, (list, tuple)):\n-                states_value = [states_value]\n-            if len(states_value) != len(self.states):\n-                raise ValueError('The layer has ' + str(len(self.states)) +\n-                                 ' states, but the `states_value` '\n-                                 'argument passed '\n-                                 'only has ' + str(len(states_value)) +\n-                                 ' entries')\n+        # initialize state if None\n         if self.states[0] is None:\n             self.states = [K.zeros((batch_size, self.units))\n                            for _ in self.states]\n-            if not states_value:\n-                return\n-        for i, state in enumerate(self.states):\n-            if states_value:\n-                value = states_value[i]\n+        elif states is None:\n+            for state in self.states:\n+                K.set_value(state, np.zeros((batch_size, self.units)))\n+        else:\n+            if not isinstance(states, (list, tuple)):\n+                states = [states]\n+            if len(states) != len(self.states):\n+                raise ValueError('Layer ' + self.name + ' expects ' +\n+                                 str(len(self.states)) + ' states, '\n+                                 'but it received ' + str(len(values)) +\n+                                 ' state values. Input received: ' +\n+                                 str(values))\n+            for index, (value, state) in enumerate(zip(states, self.states)):\n                 if value.shape != (batch_size, self.units):\n-                    raise ValueError(\n-                        'Expected state #' + str(i) +\n-                        ' to have shape ' + str((batch_size, self.units)) +\n-                        ' but got array with shape ' + str(value.shape))\n-            else:\n-                value = np.zeros((batch_size, self.units))\n-            K.set_value(state, value)\n+                    raise ValueError('State ' + str(index) +\n+                                     ' is incompatible with layer ' +\n+                                     self.name + ': expected shape=' +\n+                                     str((batch_size, self.units)) +\n+                                     ', found shape=' + str(value.shape))\n+                K.set_value(state, value)\n \n     def get_config(self):\n         config = {'return_sequences': self.return_sequences,\n@@ -457,6 +470,7 @@ class SimpleRNN(Recurrent):\n \n         self.dropout = min(1., max(0., dropout))\n         self.recurrent_dropout = min(1., max(0., recurrent_dropout))\n+        self.state_spec = InputSpec(shape=(None, self.units))\n \n     def build(self, input_shape):\n         if isinstance(input_shape, list):\n@@ -464,8 +478,7 @@ class SimpleRNN(Recurrent):\n \n         batch_size = input_shape[0] if self.stateful else None\n         self.input_dim = input_shape[2]\n-        self.input_spec = InputSpec(shape=(batch_size, None, self.input_dim))\n-        self.state_spec = InputSpec(shape=(batch_size, self.units))\n+        self.input_spec[0] = InputSpec(shape=(batch_size, None, self.input_dim))\n \n         self.states = [None]\n         if self.stateful:\n@@ -676,6 +689,7 @@ class GRU(Recurrent):\n \n         self.dropout = min(1., max(0., dropout))\n         self.recurrent_dropout = min(1., max(0., recurrent_dropout))\n+        self.state_spec = InputSpec(shape=(None, self.units))\n \n     def build(self, input_shape):\n         if isinstance(input_shape, list):\n@@ -683,8 +697,7 @@ class GRU(Recurrent):\n \n         batch_size = input_shape[0] if self.stateful else None\n         self.input_dim = input_shape[2]\n-        self.input_spec = InputSpec(shape=(batch_size, None, self.input_dim))\n-        self.state_spec = InputSpec(shape=(batch_size, self.units))\n+        self.input_spec[0] = InputSpec(shape=(batch_size, None, self.input_dim))\n \n         self.states = [None]\n         if self.stateful:\n@@ -955,6 +968,8 @@ class LSTM(Recurrent):\n \n         self.dropout = min(1., max(0., dropout))\n         self.recurrent_dropout = min(1., max(0., recurrent_dropout))\n+        self.state_spec = [InputSpec(shape=(None, self.units)),\n+                           InputSpec(shape=(None, self.units))]\n \n     def build(self, input_shape):\n         if isinstance(input_shape, list):\n@@ -962,9 +977,7 @@ class LSTM(Recurrent):\n \n         batch_size = input_shape[0] if self.stateful else None\n         self.input_dim = input_shape[2]\n-        self.input_spec = InputSpec(shape=(batch_size, None, self.input_dim))\n-        self.state_spec = [InputSpec(shape=(batch_size, self.units)),\n-                           InputSpec(shape=(batch_size, self.units))]\n+        self.input_spec[0] = InputSpec(shape=(batch_size, None, self.input_dim))\n \n         self.states = [None, None]\n         if self.stateful:\n",
          "files_name_in_blame_commit": [
            "convolutional_recurrent.py",
            "recurrent.py",
            "recurrent_test.py"
          ]
        }
      },
      "8830c53135dcad4825e2f8805c78523931c3bbed": {
        "commit": {
          "commit_id": "8830c53135dcad4825e2f8805c78523931c3bbed",
          "commit_message": "Refactor add_weight to align it with get_variable",
          "commit_author": "Francois Chollet",
          "commit_date": "2017-04-18 11:34:24",
          "commit_parent": "fc4874f82cda48e0928f9eb730a270f49034883e"
        },
        "function": {
          "function_name": "build",
          "function_code_before": "def build(self, input_shape):\n    if isinstance(input_shape, list):\n        input_shape = input_shape[0]\n    batch_size = input_shape[0] if self.stateful else None\n    self.input_dim = input_shape[2]\n    self.input_spec = InputSpec(shape=(batch_size, None, self.input_dim))\n    self.state_spec = InputSpec(shape=(batch_size, self.units))\n    self.states = [None]\n    if self.stateful:\n        self.reset_states()\n    self.kernel = self.add_weight((self.input_dim, self.units), name='kernel', initializer=self.kernel_initializer, regularizer=self.kernel_regularizer, constraint=self.kernel_constraint)\n    self.recurrent_kernel = self.add_weight((self.units, self.units), name='recurrent_kernel', initializer=self.recurrent_initializer, regularizer=self.recurrent_regularizer, constraint=self.recurrent_constraint)\n    if self.use_bias:\n        self.bias = self.add_weight((self.units,), name='bias', initializer=self.bias_initializer, regularizer=self.bias_regularizer, constraint=self.bias_constraint)\n    else:\n        self.bias = None\n    self.built = True",
          "function_code_after": "def build(self, input_shape):\n    if isinstance(input_shape, list):\n        input_shape = input_shape[0]\n    batch_size = input_shape[0] if self.stateful else None\n    self.input_dim = input_shape[2]\n    self.input_spec = InputSpec(shape=(batch_size, None, self.input_dim))\n    self.state_spec = InputSpec(shape=(batch_size, self.units))\n    self.states = [None]\n    if self.stateful:\n        self.reset_states()\n    self.kernel = self.add_weight(shape=(self.input_dim, self.units), name='kernel', initializer=self.kernel_initializer, regularizer=self.kernel_regularizer, constraint=self.kernel_constraint)\n    self.recurrent_kernel = self.add_weight(shape=(self.units, self.units), name='recurrent_kernel', initializer=self.recurrent_initializer, regularizer=self.recurrent_regularizer, constraint=self.recurrent_constraint)\n    if self.use_bias:\n        self.bias = self.add_weight(shape=(self.units,), name='bias', initializer=self.bias_initializer, regularizer=self.bias_regularizer, constraint=self.bias_constraint)\n    else:\n        self.bias = None\n    self.built = True",
          "function_before_start_line": 461,
          "function_before_end_line": 493,
          "function_after_start_line": 461,
          "function_after_end_line": 493,
          "function_before_token_count": 226,
          "function_after_token_count": 232,
          "functions_name_modified_file": [
            "__init__",
            "compute_mask",
            "__call__",
            "build",
            "call",
            "get_constants",
            "_time_distributed_dense",
            "compute_output_shape",
            "step",
            "get_initial_states",
            "reset_states",
            "preprocess_input",
            "get_config"
          ],
          "functions_name_all_files": [
            "output",
            "save_weights",
            "run_internal_graph",
            "get_layer",
            "get_input_shape_at",
            "_updated_config",
            "step",
            "get_weights",
            "stateful",
            "input_conv",
            "_get_noise_shape",
            "embedding_kwargs_preprocessor",
            "_time_distributed_dense",
            "batchnorm_args_preprocessor",
            "add_weight_args_preprocessing",
            "__init__",
            "compute_mask",
            "get_input_at",
            "get_constants",
            "compute_output_shape",
            "built",
            "save",
            "conv3d_args_preprocessor",
            "trainable_weights",
            "get_output_shape_at",
            "recurrent_args_preprocessor",
            "raise_duplicate_arg_error",
            "losses",
            "get_config",
            "_to_snake_case",
            "_fix_unknown_dimension",
            "save_weights_to_hdf5_group",
            "build",
            "call",
            "updates",
            "separable_conv2d_args_preprocessor",
            "get_losses_for",
            "add_weight",
            "zeropadding2d_args_preprocessor",
            "output_shape",
            "add_update",
            "conv1d_args_preprocessor",
            "add_loss",
            "reset_states",
            "_to_list",
            "deconv2d_args_preprocessor",
            "count_params",
            "set_weights",
            "get_output_mask_at",
            "input",
            "load_weights",
            "get_source_inputs",
            "preprocess_input",
            "_get_node_attribute_at_index",
            "non_trainable_weights",
            "conv2d_args_preprocessor",
            "input_mask",
            "input_shape",
            "generator_methods_args_preprocessor",
            "constraints",
            "__call__",
            "_collect_input_shape",
            "generate_legacy_interface",
            "_add_inbound_node",
            "state_updates",
            "get_input_mask_at",
            "weights",
            "preprocess_weights_for_loading",
            "to_yaml",
            "get_updates_for",
            "output_mask",
            "convlstm2d_args_preprocessor",
            "load_weights_from_hdf5_group_by_name",
            "Input",
            "get_initial_states",
            "to_json",
            "input_spec",
            "get_output_at",
            "uses_learning_phase",
            "summary",
            "load_weights_from_hdf5_group",
            "_is_all_none",
            "reccurent_conv",
            "_object_list_uid",
            "from_config",
            "_collect_previous_mask",
            "assert_input_compatibility"
          ],
          "functions_name_co_evolved_modified_file": [],
          "functions_name_co_evolved_all_files": [
            "add_weight_args_preprocessing",
            "add_weight"
          ]
        },
        "file": {
          "file_name": "recurrent.py",
          "file_nloc": 1007,
          "file_complexity": 126,
          "file_token_count": 6277,
          "file_before": "# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import\nimport numpy as np\n\nfrom .. import backend as K\nfrom .. import activations\nfrom .. import initializers\nfrom .. import regularizers\nfrom .. import constraints\nfrom ..engine import Layer\nfrom ..engine import InputSpec\nfrom ..legacy import interfaces\n\n\ndef _time_distributed_dense(x, w, b=None, dropout=None,\n                            input_dim=None, output_dim=None,\n                            timesteps=None, training=None):\n    \"\"\"Apply `y . w + b` for every temporal slice y of x.\n\n    # Arguments\n        x: input tensor.\n        w: weight matrix.\n        b: optional bias vector.\n        dropout: wether to apply dropout (same dropout mask\n            for every temporal slice of the input).\n        input_dim: integer; optional dimensionality of the input.\n        output_dim: integer; optional dimensionality of the output.\n        timesteps: integer; optional number of timesteps.\n        training: training phase tensor or boolean.\n\n    # Returns\n        Output tensor.\n    \"\"\"\n    if not input_dim:\n        input_dim = K.shape(x)[2]\n    if not timesteps:\n        timesteps = K.shape(x)[1]\n    if not output_dim:\n        output_dim = K.shape(w)[1]\n\n    if dropout is not None and 0. < dropout < 1.:\n        # apply the same dropout pattern at every timestep\n        ones = K.ones_like(K.reshape(x[:, 0, :], (-1, input_dim)))\n        dropout_matrix = K.dropout(ones, dropout)\n        expanded_dropout_matrix = K.repeat(dropout_matrix, timesteps)\n        x = K.in_train_phase(x * expanded_dropout_matrix, x, training=training)\n\n    # collapse time dimension and batch dimension together\n    x = K.reshape(x, (-1, input_dim))\n    x = K.dot(x, w)\n    if b is not None:\n        x = K.bias_add(x, b)\n    # reshape to 3D tensor\n    if K.backend() == 'tensorflow':\n        x = K.reshape(x, K.stack([-1, timesteps, output_dim]))\n        x.set_shape([None, None, output_dim])\n    else:\n        x = K.reshape(x, (-1, timesteps, output_dim))\n    return x\n\n\nclass Recurrent(Layer):\n    \"\"\"Abstract base class for recurrent layers.\n\n    Do not use in a model -- it's not a valid layer!\n    Use its children classes `LSTM`, `GRU` and `SimpleRNN` instead.\n\n    All recurrent layers (`LSTM`, `GRU`, `SimpleRNN`) also\n    follow the specifications of this class and accept\n    the keyword arguments listed below.\n\n    # Example\n\n    ```python\n        # as the first layer in a Sequential model\n        model = Sequential()\n        model.add(LSTM(32, input_shape=(10, 64)))\n        # now model.output_shape == (None, 32)\n        # note: `None` is the batch dimension.\n\n        # for subsequent layers, no need to specify the input size:\n        model.add(LSTM(16))\n\n        # to stack recurrent layers, you must use return_sequences=True\n        # on any recurrent layer that feeds into another recurrent layer.\n        # note that you only need to specify the input size on the first layer.\n        model = Sequential()\n        model.add(LSTM(64, input_dim=64, input_length=10, return_sequences=True))\n        model.add(LSTM(32, return_sequences=True))\n        model.add(LSTM(10))\n    ```\n\n    # Arguments\n        weights: list of Numpy arrays to set as initial weights.\n            The list should have 3 elements, of shapes:\n            `[(input_dim, output_dim), (output_dim, output_dim), (output_dim,)]`.\n        return_sequences: Boolean. Whether to return the last output\n            in the output sequence, or the full sequence.\n        go_backwards: Boolean (default False).\n            If True, process the input sequence backwards and return the\n            reversed sequence.\n        stateful: Boolean (default False). If True, the last state\n            for each sample at index i in a batch will be used as initial\n            state for the sample of index i in the following batch.\n        unroll: Boolean (default False).\n            If True, the network will be unrolled,\n            else a symbolic loop will be used.\n            Unrolling can speed-up a RNN,\n            although it tends to be more memory-intensive.\n            Unrolling is only suitable for short sequences.\n        implementation: one of {0, 1, or 2}.\n            If set to 0, the RNN will use\n            an implementation that uses fewer, larger matrix products,\n            thus running faster on CPU but consuming more memory.\n            If set to 1, the RNN will use more matrix products,\n            but smaller ones, thus running slower\n            (may actually be faster on GPU) while consuming less memory.\n            If set to 2 (LSTM/GRU only),\n            the RNN will combine the input gate,\n            the forget gate and the output gate into a single matrix,\n            enabling more time-efficient parallelization on the GPU.\n            Note: RNN dropout must be shared for all gates,\n            resulting in a slightly reduced regularization.\n        input_dim: dimensionality of the input (integer).\n            This argument (or alternatively, the keyword argument `input_shape`)\n            is required when using this layer as the first layer in a model.\n        input_length: Length of input sequences, to be specified\n            when it is constant.\n            This argument is required if you are going to connect\n            `Flatten` then `Dense` layers upstream\n            (without it, the shape of the dense outputs cannot be computed).\n            Note that if the recurrent layer is not the first layer\n            in your model, you would need to specify the input length\n            at the level of the first layer\n            (e.g. via the `input_shape` argument)\n\n    # Input shapes\n        3D tensor with shape `(batch_size, timesteps, input_dim)`,\n        (Optional) 2D tensors with shape `(batch_size, output_dim)`.\n\n    # Output shape\n        - if `return_sequences`: 3D tensor with shape\n            `(batch_size, timesteps, units)`.\n        - else, 2D tensor with shape `(batch_size, units)`.\n\n    # Masking\n        This layer supports masking for input data with a variable number\n        of timesteps. To introduce masks to your data,\n        use an [Embedding](embeddings.md) layer with the `mask_zero` parameter\n        set to `True`.\n\n    # Note on using statefulness in RNNs\n        You can set RNN layers to be 'stateful', which means that the states\n        computed for the samples in one batch will be reused as initial states\n        for the samples in the next batch. This assumes a one-to-one mapping\n        between samples in different successive batches.\n\n        To enable statefulness:\n            - specify `stateful=True` in the layer constructor.\n            - specify a fixed batch size for your model, by passing\n                if sequential model:\n                  `batch_input_shape=(...)` to the first layer in your model.\n                else for functional model with 1 or more Input layers:\n                  `batch_shape=(...)` to all the first layers in your model.\n                This is the expected shape of your inputs\n                *including the batch size*.\n                It should be a tuple of integers, e.g. `(32, 10, 100)`.\n            - specify `shuffle=False` when calling fit().\n\n        To reset the states of your model, call `.reset_states()` on either\n        a specific layer, or on your entire model.\n\n    # Note on specifying initial states in RNNs\n        You can specify the initial state of RNN layers by calling them with\n        the keyword argument `initial_state`. The value of `initial_state`\n        should be a tensor or list of tensors representing the initial state\n        of the RNN layer.\n    \"\"\"\n\n    def __init__(self, return_sequences=False,\n                 go_backwards=False,\n                 stateful=False,\n                 unroll=False,\n                 implementation=0,\n                 **kwargs):\n        super(Recurrent, self).__init__(**kwargs)\n        self.return_sequences = return_sequences\n        self.go_backwards = go_backwards\n        self.stateful = stateful\n        self.unroll = unroll\n        self.implementation = implementation\n        self.supports_masking = True\n        self.input_spec = InputSpec(ndim=3)\n        self.state_spec = None\n        self.dropout = 0\n        self.recurrent_dropout = 0\n\n    def compute_output_shape(self, input_shape):\n        if isinstance(input_shape, list):\n            input_shape = input_shape[0]\n        if self.return_sequences:\n            return (input_shape[0], input_shape[1], self.units)\n        else:\n            return (input_shape[0], self.units)\n\n    def compute_mask(self, inputs, mask):\n        if self.return_sequences:\n            return mask\n        else:\n            return None\n\n    def step(self, inputs, states):\n        raise NotImplementedError\n\n    def get_constants(self, inputs, training=None):\n        return []\n\n    def get_initial_states(self, inputs):\n        # build an all-zero tensor of shape (samples, output_dim)\n        initial_state = K.zeros_like(inputs)  # (samples, timesteps, input_dim)\n        initial_state = K.sum(initial_state, axis=(1, 2))  # (samples,)\n        initial_state = K.expand_dims(initial_state)  # (samples, 1)\n        initial_state = K.tile(initial_state, [1, self.units])  # (samples, output_dim)\n        initial_states = [initial_state for _ in range(len(self.states))]\n        return initial_states\n\n    def preprocess_input(self, inputs, training=None):\n        return inputs\n\n    def __call__(self, inputs, initial_state=None, **kwargs):\n        # If `initial_state` is specified,\n        # and if it a Keras tensor,\n        # then add it to the inputs and temporarily\n        # modify the input spec to include the state.\n        if initial_state is not None:\n            if hasattr(initial_state, '_keras_history'):\n                # Compute the full input spec, including state\n                input_spec = self.input_spec\n                state_spec = self.state_spec\n                if not isinstance(state_spec, list):\n                    state_spec = [state_spec]\n                self.input_spec = [input_spec] + state_spec\n\n                # Compute the full inputs, including state\n                if not isinstance(initial_state, (list, tuple)):\n                    initial_state = [initial_state]\n                inputs = [inputs] + list(initial_state)\n\n                # Perform the call\n                output = super(Recurrent, self).__call__(inputs, **kwargs)\n\n                # Restore original input spec\n                self.input_spec = input_spec\n                return output\n            else:\n                kwargs['initial_state'] = initial_state\n        return super(Recurrent, self).__call__(inputs, **kwargs)\n\n    def call(self, inputs, mask=None, initial_state=None, training=None):\n        # input shape: `(samples, time (padded with zeros), input_dim)`\n        # note that the .build() method of subclasses MUST define\n        # self.input_spec and self.state_spec with complete input shapes.\n        if initial_state is not None:\n            if not isinstance(initial_state, (list, tuple)):\n                initial_states = [initial_state]\n            else:\n                initial_states = list(initial_state)\n        if isinstance(inputs, list):\n            initial_states = inputs[1:]\n            inputs = inputs[0]\n        elif self.stateful:\n            initial_states = self.states\n        else:\n            initial_states = self.get_initial_states(inputs)\n\n        if len(initial_states) != len(self.states):\n            raise ValueError('Layer has ' + str(len(self.states)) +\n                             ' states but was passed ' +\n                             str(len(initial_states)) +\n                             ' initial states.')\n        input_shape = K.int_shape(inputs)\n        if self.unroll and input_shape[1] is None:\n            raise ValueError('Cannot unroll a RNN if the '\n                             'time dimension is undefined. \\n'\n                             '- If using a Sequential model, '\n                             'specify the time dimension by passing '\n                             'an `input_shape` or `batch_input_shape` '\n                             'argument to your first layer. If your '\n                             'first layer is an Embedding, you can '\n                             'also use the `input_length` argument.\\n'\n                             '- If using the functional API, specify '\n                             'the time dimension by passing a `shape` '\n                             'or `batch_shape` argument to your Input layer.')\n        constants = self.get_constants(inputs, training=None)\n        preprocessed_input = self.preprocess_input(inputs, training=None)\n        last_output, outputs, states = K.rnn(self.step,\n                                             preprocessed_input,\n                                             initial_states,\n                                             go_backwards=self.go_backwards,\n                                             mask=mask,\n                                             constants=constants,\n                                             unroll=self.unroll,\n                                             input_length=input_shape[1])\n        if self.stateful:\n            updates = []\n            for i in range(len(states)):\n                updates.append((self.states[i], states[i]))\n            self.add_update(updates, inputs)\n\n        # Properly set learning phase\n        if 0 < self.dropout + self.recurrent_dropout:\n            last_output._uses_learning_phase = True\n            outputs._uses_learning_phase = True\n\n        if self.return_sequences:\n            return outputs\n        else:\n            return last_output\n\n    def reset_states(self, states_value=None):\n        if not self.stateful:\n            raise AttributeError('Layer must be stateful.')\n        if not self.input_spec:\n            raise RuntimeError('Layer has never been called '\n                               'and thus has no states.')\n        batch_size = self.input_spec.shape[0]\n        if not batch_size:\n            raise ValueError('If a RNN is stateful, it needs to know '\n                             'its batch size. Specify the batch size '\n                             'of your input tensors: \\n'\n                             '- If using a Sequential model, '\n                             'specify the batch size by passing '\n                             'a `batch_input_shape` '\n                             'argument to your first layer.\\n'\n                             '- If using the functional API, specify '\n                             'the time dimension by passing a '\n                             '`batch_shape` argument to your Input layer.')\n        if states_value is not None:\n            if not isinstance(states_value, (list, tuple)):\n                states_value = [states_value]\n            if len(states_value) != len(self.states):\n                raise ValueError('The layer has ' + str(len(self.states)) +\n                                 ' states, but the `states_value` '\n                                 'argument passed '\n                                 'only has ' + str(len(states_value)) +\n                                 ' entries')\n        if self.states[0] is None:\n            self.states = [K.zeros((batch_size, self.units))\n                           for _ in self.states]\n            if not states_value:\n                return\n        for i, state in enumerate(self.states):\n            if states_value:\n                value = states_value[i]\n                if value.shape != (batch_size, self.units):\n                    raise ValueError(\n                        'Expected state #' + str(i) +\n                        ' to have shape ' + str((batch_size, self.units)) +\n                        ' but got array with shape ' + str(value.shape))\n            else:\n                value = np.zeros((batch_size, self.units))\n            K.set_value(state, value)\n\n    def get_config(self):\n        config = {'return_sequences': self.return_sequences,\n                  'go_backwards': self.go_backwards,\n                  'stateful': self.stateful,\n                  'unroll': self.unroll,\n                  'implementation': self.implementation}\n        base_config = super(Recurrent, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\n\nclass SimpleRNN(Recurrent):\n    \"\"\"Fully-connected RNN where the output is to be fed back to input.\n\n    # Arguments\n        units: Positive integer, dimensionality of the output space.\n        activation: Activation function to use\n            (see [activations](../activations.md)).\n            If you pass None, no activation is applied\n            (ie. \"linear\" activation: `a(x) = x`).\n        use_bias: Boolean, whether the layer uses a bias vector.\n        kernel_initializer: Initializer for the `kernel` weights matrix,\n            used for the linear transformation of the inputs.\n            (see [initializers](../initializers.md)).\n        recurrent_initializer: Initializer for the `recurrent_kernel`\n            weights matrix,\n            used for the linear transformation of the recurrent state.\n            (see [initializers](../initializers.md)).\n        bias_initializer: Initializer for the bias vector\n            (see [initializers](../initializers.md)).\n        kernel_regularizer: Regularizer function applied to\n            the `kernel` weights matrix\n            (see [regularizer](../regularizers.md)).\n        recurrent_regularizer: Regularizer function applied to\n            the `recurrent_kernel` weights matrix\n            (see [regularizer](../regularizers.md)).\n        bias_regularizer: Regularizer function applied to the bias vector\n            (see [regularizer](../regularizers.md)).\n        activity_regularizer: Regularizer function applied to\n            the output of the layer (its \"activation\").\n            (see [regularizer](../regularizers.md)).\n        kernel_constraint: Constraint function applied to\n            the `kernel` weights matrix\n            (see [constraints](../constraints.md)).\n        recurrent_constraint: Constraint function applied to\n            the `recurrent_kernel` weights matrix\n            (see [constraints](../constraints.md)).\n        bias_constraint: Constraint function applied to the bias vector\n            (see [constraints](../constraints.md)).\n        dropout: Float between 0 and 1.\n            Fraction of the units to drop for\n            the linear transformation of the inputs.\n        recurrent_dropout: Float between 0 and 1.\n            Fraction of the units to drop for\n            the linear transformation of the recurrent state.\n\n    # References\n        - [A Theoretically Grounded Application of Dropout in Recurrent Neural Networks](http://arxiv.org/abs/1512.05287)\n    \"\"\"\n\n    @interfaces.legacy_recurrent_support\n    def __init__(self, units,\n                 activation='tanh',\n                 use_bias=True,\n                 kernel_initializer='glorot_uniform',\n                 recurrent_initializer='orthogonal',\n                 bias_initializer='zeros',\n                 kernel_regularizer=None,\n                 recurrent_regularizer=None,\n                 bias_regularizer=None,\n                 activity_regularizer=None,\n                 kernel_constraint=None,\n                 recurrent_constraint=None,\n                 bias_constraint=None,\n                 dropout=0.,\n                 recurrent_dropout=0.,\n                 **kwargs):\n        super(SimpleRNN, self).__init__(**kwargs)\n        self.units = units\n        self.activation = activations.get(activation)\n        self.use_bias = use_bias\n\n        self.kernel_initializer = initializers.get(kernel_initializer)\n        self.recurrent_initializer = initializers.get(recurrent_initializer)\n        self.bias_initializer = initializers.get(bias_initializer)\n\n        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n        self.recurrent_regularizer = regularizers.get(recurrent_regularizer)\n        self.bias_regularizer = regularizers.get(bias_regularizer)\n        self.activity_regularizer = regularizers.get(activity_regularizer)\n\n        self.kernel_constraint = constraints.get(kernel_constraint)\n        self.recurrent_constraint = constraints.get(recurrent_constraint)\n        self.bias_constraint = constraints.get(bias_constraint)\n\n        self.dropout = min(1., max(0., dropout))\n        self.recurrent_dropout = min(1., max(0., recurrent_dropout))\n\n    def build(self, input_shape):\n        if isinstance(input_shape, list):\n            input_shape = input_shape[0]\n\n        batch_size = input_shape[0] if self.stateful else None\n        self.input_dim = input_shape[2]\n        self.input_spec = InputSpec(shape=(batch_size, None, self.input_dim))\n        self.state_spec = InputSpec(shape=(batch_size, self.units))\n\n        self.states = [None]\n        if self.stateful:\n            self.reset_states()\n\n        self.kernel = self.add_weight((self.input_dim, self.units),\n                                      name='kernel',\n                                      initializer=self.kernel_initializer,\n                                      regularizer=self.kernel_regularizer,\n                                      constraint=self.kernel_constraint)\n        self.recurrent_kernel = self.add_weight(\n            (self.units, self.units),\n            name='recurrent_kernel',\n            initializer=self.recurrent_initializer,\n            regularizer=self.recurrent_regularizer,\n            constraint=self.recurrent_constraint)\n        if self.use_bias:\n            self.bias = self.add_weight((self.units,),\n                                        name='bias',\n                                        initializer=self.bias_initializer,\n                                        regularizer=self.bias_regularizer,\n                                        constraint=self.bias_constraint)\n        else:\n            self.bias = None\n        self.built = True\n\n    def preprocess_input(self, inputs, training=None):\n        if self.implementation > 0:\n            return inputs\n        else:\n            input_shape = K.int_shape(inputs)\n            input_dim = input_shape[2]\n            timesteps = input_shape[1]\n            return _time_distributed_dense(inputs,\n                                           self.kernel,\n                                           self.bias,\n                                           self.dropout,\n                                           input_dim,\n                                           self.units,\n                                           timesteps,\n                                           training=training)\n\n    def step(self, inputs, states):\n        if self.implementation == 0:\n            h = inputs\n        else:\n            if 0 < self.dropout < 1:\n                h = K.dot(inputs * states[1], self.kernel)\n            else:\n                h = K.dot(inputs, self.kernel)\n            if self.bias is not None:\n                h = K.bias_add(h, self.bias)\n\n        prev_output = states[0]\n        if 0 < self.recurrent_dropout < 1:\n            prev_output *= states[2]\n        output = h + K.dot(prev_output, self.recurrent_kernel)\n        if self.activation is not None:\n            output = self.activation(output)\n\n        # Properly set learning phase on output tensor.\n        if 0 < self.dropout + self.recurrent_dropout:\n            output._uses_learning_phase = True\n        return output, [output]\n\n    def get_constants(self, inputs, training=None):\n        constants = []\n        if self.implementation != 0 and 0 < self.dropout < 1:\n            input_shape = K.int_shape(inputs)\n            input_dim = input_shape[-1]\n            ones = K.ones_like(K.reshape(inputs[:, 0, 0], (-1, 1)))\n            ones = K.tile(ones, (1, int(input_dim)))\n\n            def dropped_inputs():\n                return K.dropout(ones, self.dropout)\n\n            dp_mask = K.in_train_phase(dropped_inputs,\n                                       ones,\n                                       training=training)\n            constants.append(dp_mask)\n        else:\n            constants.append(K.cast_to_floatx(1.))\n\n        if 0 < self.recurrent_dropout < 1:\n            ones = K.ones_like(K.reshape(inputs[:, 0, 0], (-1, 1)))\n            ones = K.tile(ones, (1, self.units))\n\n            def dropped_inputs():\n                return K.dropout(ones, self.recurrent_dropout)\n            rec_dp_mask = K.in_train_phase(dropped_inputs,\n                                           ones,\n                                           training=training)\n            constants.append(rec_dp_mask)\n        else:\n            constants.append(K.cast_to_floatx(1.))\n        return constants\n\n    def get_config(self):\n        config = {'units': self.units,\n                  'activation': activations.serialize(self.activation),\n                  'use_bias': self.use_bias,\n                  'kernel_initializer': initializers.serialize(self.kernel_initializer),\n                  'recurrent_initializer': initializers.serialize(self.recurrent_initializer),\n                  'bias_initializer': initializers.serialize(self.bias_initializer),\n                  'kernel_regularizer': regularizers.serialize(self.kernel_regularizer),\n                  'recurrent_regularizer': regularizers.serialize(self.recurrent_regularizer),\n                  'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n                  'activity_regularizer': regularizers.serialize(self.activity_regularizer),\n                  'kernel_constraint': constraints.serialize(self.kernel_constraint),\n                  'recurrent_constraint': constraints.serialize(self.recurrent_constraint),\n                  'bias_constraint': constraints.serialize(self.bias_constraint),\n                  'dropout': self.dropout,\n                  'recurrent_dropout': self.recurrent_dropout}\n        base_config = super(SimpleRNN, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\n\nclass GRU(Recurrent):\n    \"\"\"Gated Recurrent Unit - Cho et al. 2014.\n\n    # Arguments\n        units: Positive integer, dimensionality of the output space.\n        activation: Activation function to use\n            (see [activations](../activations.md)).\n            If you pass None, no activation is applied\n            (ie. \"linear\" activation: `a(x) = x`).\n        recurrent_activation: Activation function to use\n            for the recurrent step\n            (see [activations](../activations.md)).\n        use_bias: Boolean, whether the layer uses a bias vector.\n        kernel_initializer: Initializer for the `kernel` weights matrix,\n            used for the linear transformation of the inputs.\n            (see [initializers](../initializers.md)).\n        recurrent_initializer: Initializer for the `recurrent_kernel`\n            weights matrix,\n            used for the linear transformation of the recurrent state.\n            (see [initializers](../initializers.md)).\n        bias_initializer: Initializer for the bias vector\n            (see [initializers](../initializers.md)).\n        kernel_regularizer: Regularizer function applied to\n            the `kernel` weights matrix\n            (see [regularizer](../regularizers.md)).\n        recurrent_regularizer: Regularizer function applied to\n            the `recurrent_kernel` weights matrix\n            (see [regularizer](../regularizers.md)).\n        bias_regularizer: Regularizer function applied to the bias vector\n            (see [regularizer](../regularizers.md)).\n        activity_regularizer: Regularizer function applied to\n            the output of the layer (its \"activation\").\n            (see [regularizer](../regularizers.md)).\n        kernel_constraint: Constraint function applied to\n            the `kernel` weights matrix\n            (see [constraints](../constraints.md)).\n        recurrent_constraint: Constraint function applied to\n            the `recurrent_kernel` weights matrix\n            (see [constraints](../constraints.md)).\n        bias_constraint: Constraint function applied to the bias vector\n            (see [constraints](../constraints.md)).\n        dropout: Float between 0 and 1.\n            Fraction of the units to drop for\n            the linear transformation of the inputs.\n        recurrent_dropout: Float between 0 and 1.\n            Fraction of the units to drop for\n            the linear transformation of the recurrent state.\n\n    # References\n        - [On the Properties of Neural Machine Translation: Encoder-Decoder Approaches](https://arxiv.org/abs/1409.1259)\n        - [Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling](http://arxiv.org/abs/1412.3555v1)\n        - [A Theoretically Grounded Application of Dropout in Recurrent Neural Networks](http://arxiv.org/abs/1512.05287)\n    \"\"\"\n\n    @interfaces.legacy_recurrent_support\n    def __init__(self, units,\n                 activation='tanh',\n                 recurrent_activation='hard_sigmoid',\n                 use_bias=True,\n                 kernel_initializer='glorot_uniform',\n                 recurrent_initializer='orthogonal',\n                 bias_initializer='zeros',\n                 kernel_regularizer=None,\n                 recurrent_regularizer=None,\n                 bias_regularizer=None,\n                 activity_regularizer=None,\n                 kernel_constraint=None,\n                 recurrent_constraint=None,\n                 bias_constraint=None,\n                 dropout=0.,\n                 recurrent_dropout=0.,\n                 **kwargs):\n        super(GRU, self).__init__(**kwargs)\n        self.units = units\n        self.activation = activations.get(activation)\n        self.recurrent_activation = activations.get(recurrent_activation)\n        self.use_bias = use_bias\n\n        self.kernel_initializer = initializers.get(kernel_initializer)\n        self.recurrent_initializer = initializers.get(recurrent_initializer)\n        self.bias_initializer = initializers.get(bias_initializer)\n\n        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n        self.recurrent_regularizer = regularizers.get(recurrent_regularizer)\n        self.bias_regularizer = regularizers.get(bias_regularizer)\n        self.activity_regularizer = regularizers.get(activity_regularizer)\n\n        self.kernel_constraint = constraints.get(kernel_constraint)\n        self.recurrent_constraint = constraints.get(recurrent_constraint)\n        self.bias_constraint = constraints.get(bias_constraint)\n\n        self.dropout = min(1., max(0., dropout))\n        self.recurrent_dropout = min(1., max(0., recurrent_dropout))\n\n    def build(self, input_shape):\n        if isinstance(input_shape, list):\n            input_shape = input_shape[0]\n\n        batch_size = input_shape[0] if self.stateful else None\n        self.input_dim = input_shape[2]\n        self.input_spec = InputSpec(shape=(batch_size, None, self.input_dim))\n        self.state_spec = InputSpec(shape=(batch_size, self.units))\n\n        self.states = [None]\n        if self.stateful:\n            self.reset_states()\n\n        self.kernel = self.add_weight((self.input_dim, self.units * 3),\n                                      name='kernel',\n                                      initializer=self.kernel_initializer,\n                                      regularizer=self.kernel_regularizer,\n                                      constraint=self.kernel_constraint)\n        self.recurrent_kernel = self.add_weight(\n            (self.units, self.units * 3),\n            name='recurrent_kernel',\n            initializer=self.recurrent_initializer,\n            regularizer=self.recurrent_regularizer,\n            constraint=self.recurrent_constraint)\n\n        if self.use_bias:\n            self.bias = self.add_weight((self.units * 3,),\n                                        name='bias',\n                                        initializer=self.bias_initializer,\n                                        regularizer=self.bias_regularizer,\n                                        constraint=self.bias_constraint)\n        else:\n            self.bias = None\n\n        self.kernel_z = self.kernel[:, :self.units]\n        self.recurrent_kernel_z = self.recurrent_kernel[:, :self.units]\n        self.kernel_r = self.kernel[:, self.units: self.units * 2]\n        self.recurrent_kernel_r = self.recurrent_kernel[:,\n                                                        self.units:\n                                                        self.units * 2]\n        self.kernel_h = self.kernel[:, self.units * 2:]\n        self.recurrent_kernel_h = self.recurrent_kernel[:, self.units * 2:]\n\n        if self.use_bias:\n            self.bias_z = self.bias[:self.units]\n            self.bias_r = self.bias[self.units: self.units * 2]\n            self.bias_h = self.bias[self.units * 2:]\n        else:\n            self.bias_z = None\n            self.bias_r = None\n            self.bias_h = None\n        self.built = True\n\n    def preprocess_input(self, inputs, training=None):\n        if self.implementation == 0:\n            input_shape = K.int_shape(inputs)\n            input_dim = input_shape[2]\n            timesteps = input_shape[1]\n\n            x_z = _time_distributed_dense(inputs, self.kernel_z, self.bias_z,\n                                          self.dropout, input_dim, self.units,\n                                          timesteps, training=training)\n            x_r = _time_distributed_dense(inputs, self.kernel_r, self.bias_r,\n                                          self.dropout, input_dim, self.units,\n                                          timesteps, training=training)\n            x_h = _time_distributed_dense(inputs, self.kernel_h, self.bias_h,\n                                          self.dropout, input_dim, self.units,\n                                          timesteps, training=training)\n            return K.concatenate([x_z, x_r, x_h], axis=2)\n        else:\n            return inputs\n\n    def get_constants(self, inputs, training=None):\n        constants = []\n        if self.implementation != 0 and 0 < self.dropout < 1:\n            input_shape = K.int_shape(inputs)\n            input_dim = input_shape[-1]\n            ones = K.ones_like(K.reshape(inputs[:, 0, 0], (-1, 1)))\n            ones = K.tile(ones, (1, int(input_dim)))\n\n            def dropped_inputs():\n                return K.dropout(ones, self.dropout)\n\n            dp_mask = [K.in_train_phase(dropped_inputs,\n                                        ones,\n                                        training=training) for _ in range(3)]\n            constants.append(dp_mask)\n        else:\n            constants.append([K.cast_to_floatx(1.) for _ in range(3)])\n\n        if 0 < self.recurrent_dropout < 1:\n            ones = K.ones_like(K.reshape(inputs[:, 0, 0], (-1, 1)))\n            ones = K.tile(ones, (1, self.units))\n\n            def dropped_inputs():\n                return K.dropout(ones, self.recurrent_dropout)\n            rec_dp_mask = [K.in_train_phase(dropped_inputs,\n                                            ones,\n                                            training=training) for _ in range(3)]\n            constants.append(rec_dp_mask)\n        else:\n            constants.append([K.cast_to_floatx(1.) for _ in range(3)])\n        return constants\n\n    def step(self, inputs, states):\n        h_tm1 = states[0]  # previous memory\n        dp_mask = states[1]  # dropout matrices for recurrent units\n        rec_dp_mask = states[2]\n\n        if self.implementation == 2:\n            matrix_x = K.dot(inputs * dp_mask[0], self.kernel)\n            if self.use_bias:\n                matrix_x = K.bias_add(matrix_x, self.bias)\n            matrix_inner = K.dot(h_tm1 * rec_dp_mask[0],\n                                 self.recurrent_kernel[:, :2 * self.units])\n\n            x_z = matrix_x[:, :self.units]\n            x_r = matrix_x[:, self.units: 2 * self.units]\n            recurrent_z = matrix_inner[:, :self.units]\n            recurrent_r = matrix_inner[:, self.units: 2 * self.units]\n\n            z = self.recurrent_activation(x_z + recurrent_z)\n            r = self.recurrent_activation(x_r + recurrent_r)\n\n            x_h = matrix_x[:, 2 * self.units:]\n            recurrent_h = K.dot(r * h_tm1 * rec_dp_mask[0],\n                                self.recurrent_kernel[:, 2 * self.units:])\n            hh = self.activation(x_h + recurrent_h)\n        else:\n            if self.implementation == 0:\n                x_z = inputs[:, :self.units]\n                x_r = inputs[:, self.units: 2 * self.units]\n                x_h = inputs[:, 2 * self.units:]\n            elif self.implementation == 1:\n                x_z = K.dot(inputs * dp_mask[0], self.kernel_z)\n                x_r = K.dot(inputs * dp_mask[1], self.kernel_r)\n                x_h = K.dot(inputs * dp_mask[2], self.kernel_h)\n                if self.use_bias:\n                    x_z = K.bias_add(x_z, self.bias_z)\n                    x_r = K.bias_add(x_r, self.bias_r)\n                    x_h = K.bias_add(x_h, self.bias_h)\n            else:\n                raise ValueError('Unknown `implementation` mode.')\n            z = self.recurrent_activation(x_z + K.dot(h_tm1 * rec_dp_mask[0],\n                                                      self.recurrent_kernel_z))\n            r = self.recurrent_activation(x_r + K.dot(h_tm1 * rec_dp_mask[1],\n                                                      self.recurrent_kernel_r))\n\n            hh = self.activation(x_h + K.dot(r * h_tm1 * rec_dp_mask[2],\n                                             self.recurrent_kernel_h))\n        h = z * h_tm1 + (1 - z) * hh\n        if 0 < self.dropout + self.recurrent_dropout:\n            h._uses_learning_phase = True\n        return h, [h]\n\n    def get_config(self):\n        config = {'units': self.units,\n                  'activation': activations.serialize(self.activation),\n                  'recurrent_activation': activations.serialize(self.recurrent_activation),\n                  'use_bias': self.use_bias,\n                  'kernel_initializer': initializers.serialize(self.kernel_initializer),\n                  'recurrent_initializer': initializers.serialize(self.recurrent_initializer),\n                  'bias_initializer': initializers.serialize(self.bias_initializer),\n                  'kernel_regularizer': regularizers.serialize(self.kernel_regularizer),\n                  'recurrent_regularizer': regularizers.serialize(self.recurrent_regularizer),\n                  'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n                  'activity_regularizer': regularizers.serialize(self.activity_regularizer),\n                  'kernel_constraint': constraints.serialize(self.kernel_constraint),\n                  'recurrent_constraint': constraints.serialize(self.recurrent_constraint),\n                  'bias_constraint': constraints.serialize(self.bias_constraint),\n                  'dropout': self.dropout,\n                  'recurrent_dropout': self.recurrent_dropout}\n        base_config = super(GRU, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\n\nclass LSTM(Recurrent):\n    \"\"\"Long-Short Term Memory unit - Hochreiter 1997.\n\n    For a step-by-step description of the algorithm, see\n    [this tutorial](http://deeplearning.net/tutorial/lstm.html).\n\n    # Arguments\n        units: Positive integer, dimensionality of the output space.\n        activation: Activation function to use\n            (see [activations](../activations.md)).\n            If you pass None, no activation is applied\n            (ie. \"linear\" activation: `a(x) = x`).\n        recurrent_activation: Activation function to use\n            for the recurrent step\n            (see [activations](../activations.md)).\n        use_bias: Boolean, whether the layer uses a bias vector.\n        kernel_initializer: Initializer for the `kernel` weights matrix,\n            used for the linear transformation of the inputs.\n            (see [initializers](../initializers.md)).\n        recurrent_initializer: Initializer for the `recurrent_kernel`\n            weights matrix,\n            used for the linear transformation of the recurrent state.\n            (see [initializers](../initializers.md)).\n        bias_initializer: Initializer for the bias vector\n            (see [initializers](../initializers.md)).\n        unit_forget_bias: Boolean.\n            If True, add 1 to the bias of the forget gate at initialization.\n            Setting it to true will also force `bias_initializer=\"zeros\"`.\n            This is recommended in [Jozefowicz et al.](http://www.jmlr.org/proceedings/papers/v37/jozefowicz15.pdf)\n        kernel_regularizer: Regularizer function applied to\n            the `kernel` weights matrix\n            (see [regularizer](../regularizers.md)).\n        recurrent_regularizer: Regularizer function applied to\n            the `recurrent_kernel` weights matrix\n            (see [regularizer](../regularizers.md)).\n        bias_regularizer: Regularizer function applied to the bias vector\n            (see [regularizer](../regularizers.md)).\n        activity_regularizer: Regularizer function applied to\n            the output of the layer (its \"activation\").\n            (see [regularizer](../regularizers.md)).\n        kernel_constraint: Constraint function applied to\n            the `kernel` weights matrix\n            (see [constraints](../constraints.md)).\n        recurrent_constraint: Constraint function applied to\n            the `recurrent_kernel` weights matrix\n            (see [constraints](../constraints.md)).\n        bias_constraint: Constraint function applied to the bias vector\n            (see [constraints](../constraints.md)).\n        dropout: Float between 0 and 1.\n            Fraction of the units to drop for\n            the linear transformation of the inputs.\n        recurrent_dropout: Float between 0 and 1.\n            Fraction of the units to drop for\n            the linear transformation of the recurrent state.\n\n    # References\n        - [Long short-term memory](http://deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf) (original 1997 paper)\n        - [Learning to forget: Continual prediction with LSTM](http://www.mitpressjournals.org/doi/pdf/10.1162/089976600300015015)\n        - [Supervised sequence labeling with recurrent neural networks](http://www.cs.toronto.edu/~graves/preprint.pdf)\n        - [A Theoretically Grounded Application of Dropout in Recurrent Neural Networks](http://arxiv.org/abs/1512.05287)\n    \"\"\"\n    @interfaces.legacy_recurrent_support\n    def __init__(self, units,\n                 activation='tanh',\n                 recurrent_activation='hard_sigmoid',\n                 use_bias=True,\n                 kernel_initializer='glorot_uniform',\n                 recurrent_initializer='orthogonal',\n                 bias_initializer='zeros',\n                 unit_forget_bias=True,\n                 kernel_regularizer=None,\n                 recurrent_regularizer=None,\n                 bias_regularizer=None,\n                 activity_regularizer=None,\n                 kernel_constraint=None,\n                 recurrent_constraint=None,\n                 bias_constraint=None,\n                 dropout=0.,\n                 recurrent_dropout=0.,\n                 **kwargs):\n        super(LSTM, self).__init__(**kwargs)\n        self.units = units\n        self.activation = activations.get(activation)\n        self.recurrent_activation = activations.get(recurrent_activation)\n        self.use_bias = use_bias\n\n        self.kernel_initializer = initializers.get(kernel_initializer)\n        self.recurrent_initializer = initializers.get(recurrent_initializer)\n        self.bias_initializer = initializers.get(bias_initializer)\n        self.unit_forget_bias = unit_forget_bias\n\n        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n        self.recurrent_regularizer = regularizers.get(recurrent_regularizer)\n        self.bias_regularizer = regularizers.get(bias_regularizer)\n        self.activity_regularizer = regularizers.get(activity_regularizer)\n\n        self.kernel_constraint = constraints.get(kernel_constraint)\n        self.recurrent_constraint = constraints.get(recurrent_constraint)\n        self.bias_constraint = constraints.get(bias_constraint)\n\n        self.dropout = min(1., max(0., dropout))\n        self.recurrent_dropout = min(1., max(0., recurrent_dropout))\n\n    def build(self, input_shape):\n        if isinstance(input_shape, list):\n            input_shape = input_shape[0]\n\n        batch_size = input_shape[0] if self.stateful else None\n        self.input_dim = input_shape[2]\n        self.input_spec = InputSpec(shape=(batch_size, None, self.input_dim))\n        self.state_spec = [InputSpec(shape=(batch_size, self.units)),\n                           InputSpec(shape=(batch_size, self.units))]\n\n        self.states = [None, None]\n        if self.stateful:\n            self.reset_states()\n\n        self.kernel = self.add_weight((self.input_dim, self.units * 4),\n                                      name='kernel',\n                                      initializer=self.kernel_initializer,\n                                      regularizer=self.kernel_regularizer,\n                                      constraint=self.kernel_constraint)\n        self.recurrent_kernel = self.add_weight(\n            (self.units, self.units * 4),\n            name='recurrent_kernel',\n            initializer=self.recurrent_initializer,\n            regularizer=self.recurrent_regularizer,\n            constraint=self.recurrent_constraint)\n\n        if self.use_bias:\n            self.bias = self.add_weight((self.units * 4,),\n                                        name='bias',\n                                        initializer=self.bias_initializer,\n                                        regularizer=self.bias_regularizer,\n                                        constraint=self.bias_constraint)\n            if self.unit_forget_bias:\n                bias_value = np.zeros((self.units * 4,))\n                bias_value[self.units: self.units * 2] = 1.\n                K.set_value(self.bias, bias_value)\n        else:\n            self.bias = None\n\n        self.kernel_i = self.kernel[:, :self.units]\n        self.kernel_f = self.kernel[:, self.units: self.units * 2]\n        self.kernel_c = self.kernel[:, self.units * 2: self.units * 3]\n        self.kernel_o = self.kernel[:, self.units * 3:]\n\n        self.recurrent_kernel_i = self.recurrent_kernel[:, :self.units]\n        self.recurrent_kernel_f = self.recurrent_kernel[:, self.units: self.units * 2]\n        self.recurrent_kernel_c = self.recurrent_kernel[:, self.units * 2: self.units * 3]\n        self.recurrent_kernel_o = self.recurrent_kernel[:, self.units * 3:]\n\n        if self.use_bias:\n            self.bias_i = self.bias[:self.units]\n            self.bias_f = self.bias[self.units: self.units * 2]\n            self.bias_c = self.bias[self.units * 2: self.units * 3]\n            self.bias_o = self.bias[self.units * 3:]\n        else:\n            self.bias_i = None\n            self.bias_f = None\n            self.bias_c = None\n            self.bias_o = None\n        self.built = True\n\n    def preprocess_input(self, inputs, training=None):\n        if self.implementation == 0:\n            input_shape = K.int_shape(inputs)\n            input_dim = input_shape[2]\n            timesteps = input_shape[1]\n\n            x_i = _time_distributed_dense(inputs, self.kernel_i, self.bias_i,\n                                          self.dropout, input_dim, self.units,\n                                          timesteps, training=training)\n            x_f = _time_distributed_dense(inputs, self.kernel_f, self.bias_f,\n                                          self.dropout, input_dim, self.units,\n                                          timesteps, training=training)\n            x_c = _time_distributed_dense(inputs, self.kernel_c, self.bias_c,\n                                          self.dropout, input_dim, self.units,\n                                          timesteps, training=training)\n            x_o = _time_distributed_dense(inputs, self.kernel_o, self.bias_o,\n                                          self.dropout, input_dim, self.units,\n                                          timesteps, training=training)\n            return K.concatenate([x_i, x_f, x_c, x_o], axis=2)\n        else:\n            return inputs\n\n    def get_constants(self, inputs, training=None):\n        constants = []\n        if self.implementation != 0 and 0 < self.dropout < 1:\n            input_shape = K.int_shape(inputs)\n            input_dim = input_shape[-1]\n            ones = K.ones_like(K.reshape(inputs[:, 0, 0], (-1, 1)))\n            ones = K.tile(ones, (1, int(input_dim)))\n\n            def dropped_inputs():\n                return K.dropout(ones, self.dropout)\n\n            dp_mask = [K.in_train_phase(dropped_inputs,\n                                        ones,\n                                        training=training) for _ in range(4)]\n            constants.append(dp_mask)\n        else:\n            constants.append([K.cast_to_floatx(1.) for _ in range(4)])\n\n        if 0 < self.recurrent_dropout < 1:\n            ones = K.ones_like(K.reshape(inputs[:, 0, 0], (-1, 1)))\n            ones = K.tile(ones, (1, self.units))\n\n            def dropped_inputs():\n                return K.dropout(ones, self.recurrent_dropout)\n            rec_dp_mask = [K.in_train_phase(dropped_inputs,\n                                            ones,\n                                            training=training) for _ in range(4)]\n            constants.append(rec_dp_mask)\n        else:\n            constants.append([K.cast_to_floatx(1.) for _ in range(4)])\n        return constants\n\n    def step(self, inputs, states):\n        h_tm1 = states[0]\n        c_tm1 = states[1]\n        dp_mask = states[2]\n        rec_dp_mask = states[3]\n\n        if self.implementation == 2:\n            z = K.dot(inputs * dp_mask[0], self.kernel)\n            z += K.dot(h_tm1 * rec_dp_mask[0], self.recurrent_kernel)\n            if self.use_bias:\n                z = K.bias_add(z, self.bias)\n\n            z0 = z[:, :self.units]\n            z1 = z[:, self.units: 2 * self.units]\n            z2 = z[:, 2 * self.units: 3 * self.units]\n            z3 = z[:, 3 * self.units:]\n\n            i = self.recurrent_activation(z0)\n            f = self.recurrent_activation(z1)\n            c = f * c_tm1 + i * self.activation(z2)\n            o = self.recurrent_activation(z3)\n        else:\n            if self.implementation == 0:\n                x_i = inputs[:, :self.units]\n                x_f = inputs[:, self.units: 2 * self.units]\n                x_c = inputs[:, 2 * self.units: 3 * self.units]\n                x_o = inputs[:, 3 * self.units:]\n            elif self.implementation == 1:\n                x_i = K.dot(inputs * dp_mask[0], self.kernel_i) + self.bias_i\n                x_f = K.dot(inputs * dp_mask[1], self.kernel_f) + self.bias_f\n                x_c = K.dot(inputs * dp_mask[2], self.kernel_c) + self.bias_c\n                x_o = K.dot(inputs * dp_mask[3], self.kernel_o) + self.bias_o\n            else:\n                raise ValueError('Unknown `implementation` mode.')\n\n            i = self.recurrent_activation(x_i + K.dot(h_tm1 * rec_dp_mask[0],\n                                                      self.recurrent_kernel_i))\n            f = self.recurrent_activation(x_f + K.dot(h_tm1 * rec_dp_mask[1],\n                                                      self.recurrent_kernel_f))\n            c = f * c_tm1 + i * self.activation(x_c + K.dot(h_tm1 * rec_dp_mask[2],\n                                                            self.recurrent_kernel_c))\n            o = self.recurrent_activation(x_o + K.dot(h_tm1 * rec_dp_mask[3],\n                                                      self.recurrent_kernel_o))\n        h = o * self.activation(c)\n        if 0 < self.dropout + self.recurrent_dropout:\n            h._uses_learning_phase = True\n        return h, [h, c]\n\n    def get_config(self):\n        config = {'units': self.units,\n                  'activation': activations.serialize(self.activation),\n                  'recurrent_activation': activations.serialize(self.recurrent_activation),\n                  'use_bias': self.use_bias,\n                  'kernel_initializer': initializers.serialize(self.kernel_initializer),\n                  'recurrent_initializer': initializers.serialize(self.recurrent_initializer),\n                  'bias_initializer': initializers.serialize(self.bias_initializer),\n                  'unit_forget_bias': self.unit_forget_bias,\n                  'kernel_regularizer': regularizers.serialize(self.kernel_regularizer),\n                  'recurrent_regularizer': regularizers.serialize(self.recurrent_regularizer),\n                  'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n                  'activity_regularizer': regularizers.serialize(self.activity_regularizer),\n                  'kernel_constraint': constraints.serialize(self.kernel_constraint),\n                  'recurrent_constraint': constraints.serialize(self.recurrent_constraint),\n                  'bias_constraint': constraints.serialize(self.bias_constraint),\n                  'dropout': self.dropout,\n                  'recurrent_dropout': self.recurrent_dropout}\n        base_config = super(LSTM, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n",
          "file_after": "# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import\nimport numpy as np\n\nfrom .. import backend as K\nfrom .. import activations\nfrom .. import initializers\nfrom .. import regularizers\nfrom .. import constraints\nfrom ..engine import Layer\nfrom ..engine import InputSpec\nfrom ..legacy import interfaces\n\n\ndef _time_distributed_dense(x, w, b=None, dropout=None,\n                            input_dim=None, output_dim=None,\n                            timesteps=None, training=None):\n    \"\"\"Apply `y . w + b` for every temporal slice y of x.\n\n    # Arguments\n        x: input tensor.\n        w: weight matrix.\n        b: optional bias vector.\n        dropout: wether to apply dropout (same dropout mask\n            for every temporal slice of the input).\n        input_dim: integer; optional dimensionality of the input.\n        output_dim: integer; optional dimensionality of the output.\n        timesteps: integer; optional number of timesteps.\n        training: training phase tensor or boolean.\n\n    # Returns\n        Output tensor.\n    \"\"\"\n    if not input_dim:\n        input_dim = K.shape(x)[2]\n    if not timesteps:\n        timesteps = K.shape(x)[1]\n    if not output_dim:\n        output_dim = K.shape(w)[1]\n\n    if dropout is not None and 0. < dropout < 1.:\n        # apply the same dropout pattern at every timestep\n        ones = K.ones_like(K.reshape(x[:, 0, :], (-1, input_dim)))\n        dropout_matrix = K.dropout(ones, dropout)\n        expanded_dropout_matrix = K.repeat(dropout_matrix, timesteps)\n        x = K.in_train_phase(x * expanded_dropout_matrix, x, training=training)\n\n    # collapse time dimension and batch dimension together\n    x = K.reshape(x, (-1, input_dim))\n    x = K.dot(x, w)\n    if b is not None:\n        x = K.bias_add(x, b)\n    # reshape to 3D tensor\n    if K.backend() == 'tensorflow':\n        x = K.reshape(x, K.stack([-1, timesteps, output_dim]))\n        x.set_shape([None, None, output_dim])\n    else:\n        x = K.reshape(x, (-1, timesteps, output_dim))\n    return x\n\n\nclass Recurrent(Layer):\n    \"\"\"Abstract base class for recurrent layers.\n\n    Do not use in a model -- it's not a valid layer!\n    Use its children classes `LSTM`, `GRU` and `SimpleRNN` instead.\n\n    All recurrent layers (`LSTM`, `GRU`, `SimpleRNN`) also\n    follow the specifications of this class and accept\n    the keyword arguments listed below.\n\n    # Example\n\n    ```python\n        # as the first layer in a Sequential model\n        model = Sequential()\n        model.add(LSTM(32, input_shape=(10, 64)))\n        # now model.output_shape == (None, 32)\n        # note: `None` is the batch dimension.\n\n        # for subsequent layers, no need to specify the input size:\n        model.add(LSTM(16))\n\n        # to stack recurrent layers, you must use return_sequences=True\n        # on any recurrent layer that feeds into another recurrent layer.\n        # note that you only need to specify the input size on the first layer.\n        model = Sequential()\n        model.add(LSTM(64, input_dim=64, input_length=10, return_sequences=True))\n        model.add(LSTM(32, return_sequences=True))\n        model.add(LSTM(10))\n    ```\n\n    # Arguments\n        weights: list of Numpy arrays to set as initial weights.\n            The list should have 3 elements, of shapes:\n            `[(input_dim, output_dim), (output_dim, output_dim), (output_dim,)]`.\n        return_sequences: Boolean. Whether to return the last output\n            in the output sequence, or the full sequence.\n        go_backwards: Boolean (default False).\n            If True, process the input sequence backwards and return the\n            reversed sequence.\n        stateful: Boolean (default False). If True, the last state\n            for each sample at index i in a batch will be used as initial\n            state for the sample of index i in the following batch.\n        unroll: Boolean (default False).\n            If True, the network will be unrolled,\n            else a symbolic loop will be used.\n            Unrolling can speed-up a RNN,\n            although it tends to be more memory-intensive.\n            Unrolling is only suitable for short sequences.\n        implementation: one of {0, 1, or 2}.\n            If set to 0, the RNN will use\n            an implementation that uses fewer, larger matrix products,\n            thus running faster on CPU but consuming more memory.\n            If set to 1, the RNN will use more matrix products,\n            but smaller ones, thus running slower\n            (may actually be faster on GPU) while consuming less memory.\n            If set to 2 (LSTM/GRU only),\n            the RNN will combine the input gate,\n            the forget gate and the output gate into a single matrix,\n            enabling more time-efficient parallelization on the GPU.\n            Note: RNN dropout must be shared for all gates,\n            resulting in a slightly reduced regularization.\n        input_dim: dimensionality of the input (integer).\n            This argument (or alternatively, the keyword argument `input_shape`)\n            is required when using this layer as the first layer in a model.\n        input_length: Length of input sequences, to be specified\n            when it is constant.\n            This argument is required if you are going to connect\n            `Flatten` then `Dense` layers upstream\n            (without it, the shape of the dense outputs cannot be computed).\n            Note that if the recurrent layer is not the first layer\n            in your model, you would need to specify the input length\n            at the level of the first layer\n            (e.g. via the `input_shape` argument)\n\n    # Input shapes\n        3D tensor with shape `(batch_size, timesteps, input_dim)`,\n        (Optional) 2D tensors with shape `(batch_size, output_dim)`.\n\n    # Output shape\n        - if `return_sequences`: 3D tensor with shape\n            `(batch_size, timesteps, units)`.\n        - else, 2D tensor with shape `(batch_size, units)`.\n\n    # Masking\n        This layer supports masking for input data with a variable number\n        of timesteps. To introduce masks to your data,\n        use an [Embedding](embeddings.md) layer with the `mask_zero` parameter\n        set to `True`.\n\n    # Note on using statefulness in RNNs\n        You can set RNN layers to be 'stateful', which means that the states\n        computed for the samples in one batch will be reused as initial states\n        for the samples in the next batch. This assumes a one-to-one mapping\n        between samples in different successive batches.\n\n        To enable statefulness:\n            - specify `stateful=True` in the layer constructor.\n            - specify a fixed batch size for your model, by passing\n                if sequential model:\n                  `batch_input_shape=(...)` to the first layer in your model.\n                else for functional model with 1 or more Input layers:\n                  `batch_shape=(...)` to all the first layers in your model.\n                This is the expected shape of your inputs\n                *including the batch size*.\n                It should be a tuple of integers, e.g. `(32, 10, 100)`.\n            - specify `shuffle=False` when calling fit().\n\n        To reset the states of your model, call `.reset_states()` on either\n        a specific layer, or on your entire model.\n\n    # Note on specifying initial states in RNNs\n        You can specify the initial state of RNN layers by calling them with\n        the keyword argument `initial_state`. The value of `initial_state`\n        should be a tensor or list of tensors representing the initial state\n        of the RNN layer.\n    \"\"\"\n\n    def __init__(self, return_sequences=False,\n                 go_backwards=False,\n                 stateful=False,\n                 unroll=False,\n                 implementation=0,\n                 **kwargs):\n        super(Recurrent, self).__init__(**kwargs)\n        self.return_sequences = return_sequences\n        self.go_backwards = go_backwards\n        self.stateful = stateful\n        self.unroll = unroll\n        self.implementation = implementation\n        self.supports_masking = True\n        self.input_spec = InputSpec(ndim=3)\n        self.state_spec = None\n        self.dropout = 0\n        self.recurrent_dropout = 0\n\n    def compute_output_shape(self, input_shape):\n        if isinstance(input_shape, list):\n            input_shape = input_shape[0]\n        if self.return_sequences:\n            return (input_shape[0], input_shape[1], self.units)\n        else:\n            return (input_shape[0], self.units)\n\n    def compute_mask(self, inputs, mask):\n        if self.return_sequences:\n            return mask\n        else:\n            return None\n\n    def step(self, inputs, states):\n        raise NotImplementedError\n\n    def get_constants(self, inputs, training=None):\n        return []\n\n    def get_initial_states(self, inputs):\n        # build an all-zero tensor of shape (samples, output_dim)\n        initial_state = K.zeros_like(inputs)  # (samples, timesteps, input_dim)\n        initial_state = K.sum(initial_state, axis=(1, 2))  # (samples,)\n        initial_state = K.expand_dims(initial_state)  # (samples, 1)\n        initial_state = K.tile(initial_state, [1, self.units])  # (samples, output_dim)\n        initial_states = [initial_state for _ in range(len(self.states))]\n        return initial_states\n\n    def preprocess_input(self, inputs, training=None):\n        return inputs\n\n    def __call__(self, inputs, initial_state=None, **kwargs):\n        # If `initial_state` is specified,\n        # and if it a Keras tensor,\n        # then add it to the inputs and temporarily\n        # modify the input spec to include the state.\n        if initial_state is not None:\n            if hasattr(initial_state, '_keras_history'):\n                # Compute the full input spec, including state\n                input_spec = self.input_spec\n                state_spec = self.state_spec\n                if not isinstance(state_spec, list):\n                    state_spec = [state_spec]\n                self.input_spec = [input_spec] + state_spec\n\n                # Compute the full inputs, including state\n                if not isinstance(initial_state, (list, tuple)):\n                    initial_state = [initial_state]\n                inputs = [inputs] + list(initial_state)\n\n                # Perform the call\n                output = super(Recurrent, self).__call__(inputs, **kwargs)\n\n                # Restore original input spec\n                self.input_spec = input_spec\n                return output\n            else:\n                kwargs['initial_state'] = initial_state\n        return super(Recurrent, self).__call__(inputs, **kwargs)\n\n    def call(self, inputs, mask=None, initial_state=None, training=None):\n        # input shape: `(samples, time (padded with zeros), input_dim)`\n        # note that the .build() method of subclasses MUST define\n        # self.input_spec and self.state_spec with complete input shapes.\n        if initial_state is not None:\n            if not isinstance(initial_state, (list, tuple)):\n                initial_states = [initial_state]\n            else:\n                initial_states = list(initial_state)\n        if isinstance(inputs, list):\n            initial_states = inputs[1:]\n            inputs = inputs[0]\n        elif self.stateful:\n            initial_states = self.states\n        else:\n            initial_states = self.get_initial_states(inputs)\n\n        if len(initial_states) != len(self.states):\n            raise ValueError('Layer has ' + str(len(self.states)) +\n                             ' states but was passed ' +\n                             str(len(initial_states)) +\n                             ' initial states.')\n        input_shape = K.int_shape(inputs)\n        if self.unroll and input_shape[1] is None:\n            raise ValueError('Cannot unroll a RNN if the '\n                             'time dimension is undefined. \\n'\n                             '- If using a Sequential model, '\n                             'specify the time dimension by passing '\n                             'an `input_shape` or `batch_input_shape` '\n                             'argument to your first layer. If your '\n                             'first layer is an Embedding, you can '\n                             'also use the `input_length` argument.\\n'\n                             '- If using the functional API, specify '\n                             'the time dimension by passing a `shape` '\n                             'or `batch_shape` argument to your Input layer.')\n        constants = self.get_constants(inputs, training=None)\n        preprocessed_input = self.preprocess_input(inputs, training=None)\n        last_output, outputs, states = K.rnn(self.step,\n                                             preprocessed_input,\n                                             initial_states,\n                                             go_backwards=self.go_backwards,\n                                             mask=mask,\n                                             constants=constants,\n                                             unroll=self.unroll,\n                                             input_length=input_shape[1])\n        if self.stateful:\n            updates = []\n            for i in range(len(states)):\n                updates.append((self.states[i], states[i]))\n            self.add_update(updates, inputs)\n\n        # Properly set learning phase\n        if 0 < self.dropout + self.recurrent_dropout:\n            last_output._uses_learning_phase = True\n            outputs._uses_learning_phase = True\n\n        if self.return_sequences:\n            return outputs\n        else:\n            return last_output\n\n    def reset_states(self, states_value=None):\n        if not self.stateful:\n            raise AttributeError('Layer must be stateful.')\n        if not self.input_spec:\n            raise RuntimeError('Layer has never been called '\n                               'and thus has no states.')\n        batch_size = self.input_spec.shape[0]\n        if not batch_size:\n            raise ValueError('If a RNN is stateful, it needs to know '\n                             'its batch size. Specify the batch size '\n                             'of your input tensors: \\n'\n                             '- If using a Sequential model, '\n                             'specify the batch size by passing '\n                             'a `batch_input_shape` '\n                             'argument to your first layer.\\n'\n                             '- If using the functional API, specify '\n                             'the time dimension by passing a '\n                             '`batch_shape` argument to your Input layer.')\n        if states_value is not None:\n            if not isinstance(states_value, (list, tuple)):\n                states_value = [states_value]\n            if len(states_value) != len(self.states):\n                raise ValueError('The layer has ' + str(len(self.states)) +\n                                 ' states, but the `states_value` '\n                                 'argument passed '\n                                 'only has ' + str(len(states_value)) +\n                                 ' entries')\n        if self.states[0] is None:\n            self.states = [K.zeros((batch_size, self.units))\n                           for _ in self.states]\n            if not states_value:\n                return\n        for i, state in enumerate(self.states):\n            if states_value:\n                value = states_value[i]\n                if value.shape != (batch_size, self.units):\n                    raise ValueError(\n                        'Expected state #' + str(i) +\n                        ' to have shape ' + str((batch_size, self.units)) +\n                        ' but got array with shape ' + str(value.shape))\n            else:\n                value = np.zeros((batch_size, self.units))\n            K.set_value(state, value)\n\n    def get_config(self):\n        config = {'return_sequences': self.return_sequences,\n                  'go_backwards': self.go_backwards,\n                  'stateful': self.stateful,\n                  'unroll': self.unroll,\n                  'implementation': self.implementation}\n        base_config = super(Recurrent, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\n\nclass SimpleRNN(Recurrent):\n    \"\"\"Fully-connected RNN where the output is to be fed back to input.\n\n    # Arguments\n        units: Positive integer, dimensionality of the output space.\n        activation: Activation function to use\n            (see [activations](../activations.md)).\n            If you pass None, no activation is applied\n            (ie. \"linear\" activation: `a(x) = x`).\n        use_bias: Boolean, whether the layer uses a bias vector.\n        kernel_initializer: Initializer for the `kernel` weights matrix,\n            used for the linear transformation of the inputs.\n            (see [initializers](../initializers.md)).\n        recurrent_initializer: Initializer for the `recurrent_kernel`\n            weights matrix,\n            used for the linear transformation of the recurrent state.\n            (see [initializers](../initializers.md)).\n        bias_initializer: Initializer for the bias vector\n            (see [initializers](../initializers.md)).\n        kernel_regularizer: Regularizer function applied to\n            the `kernel` weights matrix\n            (see [regularizer](../regularizers.md)).\n        recurrent_regularizer: Regularizer function applied to\n            the `recurrent_kernel` weights matrix\n            (see [regularizer](../regularizers.md)).\n        bias_regularizer: Regularizer function applied to the bias vector\n            (see [regularizer](../regularizers.md)).\n        activity_regularizer: Regularizer function applied to\n            the output of the layer (its \"activation\").\n            (see [regularizer](../regularizers.md)).\n        kernel_constraint: Constraint function applied to\n            the `kernel` weights matrix\n            (see [constraints](../constraints.md)).\n        recurrent_constraint: Constraint function applied to\n            the `recurrent_kernel` weights matrix\n            (see [constraints](../constraints.md)).\n        bias_constraint: Constraint function applied to the bias vector\n            (see [constraints](../constraints.md)).\n        dropout: Float between 0 and 1.\n            Fraction of the units to drop for\n            the linear transformation of the inputs.\n        recurrent_dropout: Float between 0 and 1.\n            Fraction of the units to drop for\n            the linear transformation of the recurrent state.\n\n    # References\n        - [A Theoretically Grounded Application of Dropout in Recurrent Neural Networks](http://arxiv.org/abs/1512.05287)\n    \"\"\"\n\n    @interfaces.legacy_recurrent_support\n    def __init__(self, units,\n                 activation='tanh',\n                 use_bias=True,\n                 kernel_initializer='glorot_uniform',\n                 recurrent_initializer='orthogonal',\n                 bias_initializer='zeros',\n                 kernel_regularizer=None,\n                 recurrent_regularizer=None,\n                 bias_regularizer=None,\n                 activity_regularizer=None,\n                 kernel_constraint=None,\n                 recurrent_constraint=None,\n                 bias_constraint=None,\n                 dropout=0.,\n                 recurrent_dropout=0.,\n                 **kwargs):\n        super(SimpleRNN, self).__init__(**kwargs)\n        self.units = units\n        self.activation = activations.get(activation)\n        self.use_bias = use_bias\n\n        self.kernel_initializer = initializers.get(kernel_initializer)\n        self.recurrent_initializer = initializers.get(recurrent_initializer)\n        self.bias_initializer = initializers.get(bias_initializer)\n\n        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n        self.recurrent_regularizer = regularizers.get(recurrent_regularizer)\n        self.bias_regularizer = regularizers.get(bias_regularizer)\n        self.activity_regularizer = regularizers.get(activity_regularizer)\n\n        self.kernel_constraint = constraints.get(kernel_constraint)\n        self.recurrent_constraint = constraints.get(recurrent_constraint)\n        self.bias_constraint = constraints.get(bias_constraint)\n\n        self.dropout = min(1., max(0., dropout))\n        self.recurrent_dropout = min(1., max(0., recurrent_dropout))\n\n    def build(self, input_shape):\n        if isinstance(input_shape, list):\n            input_shape = input_shape[0]\n\n        batch_size = input_shape[0] if self.stateful else None\n        self.input_dim = input_shape[2]\n        self.input_spec = InputSpec(shape=(batch_size, None, self.input_dim))\n        self.state_spec = InputSpec(shape=(batch_size, self.units))\n\n        self.states = [None]\n        if self.stateful:\n            self.reset_states()\n\n        self.kernel = self.add_weight(shape=(self.input_dim, self.units),\n                                      name='kernel',\n                                      initializer=self.kernel_initializer,\n                                      regularizer=self.kernel_regularizer,\n                                      constraint=self.kernel_constraint)\n        self.recurrent_kernel = self.add_weight(\n            shape=(self.units, self.units),\n            name='recurrent_kernel',\n            initializer=self.recurrent_initializer,\n            regularizer=self.recurrent_regularizer,\n            constraint=self.recurrent_constraint)\n        if self.use_bias:\n            self.bias = self.add_weight(shape=(self.units,),\n                                        name='bias',\n                                        initializer=self.bias_initializer,\n                                        regularizer=self.bias_regularizer,\n                                        constraint=self.bias_constraint)\n        else:\n            self.bias = None\n        self.built = True\n\n    def preprocess_input(self, inputs, training=None):\n        if self.implementation > 0:\n            return inputs\n        else:\n            input_shape = K.int_shape(inputs)\n            input_dim = input_shape[2]\n            timesteps = input_shape[1]\n            return _time_distributed_dense(inputs,\n                                           self.kernel,\n                                           self.bias,\n                                           self.dropout,\n                                           input_dim,\n                                           self.units,\n                                           timesteps,\n                                           training=training)\n\n    def step(self, inputs, states):\n        if self.implementation == 0:\n            h = inputs\n        else:\n            if 0 < self.dropout < 1:\n                h = K.dot(inputs * states[1], self.kernel)\n            else:\n                h = K.dot(inputs, self.kernel)\n            if self.bias is not None:\n                h = K.bias_add(h, self.bias)\n\n        prev_output = states[0]\n        if 0 < self.recurrent_dropout < 1:\n            prev_output *= states[2]\n        output = h + K.dot(prev_output, self.recurrent_kernel)\n        if self.activation is not None:\n            output = self.activation(output)\n\n        # Properly set learning phase on output tensor.\n        if 0 < self.dropout + self.recurrent_dropout:\n            output._uses_learning_phase = True\n        return output, [output]\n\n    def get_constants(self, inputs, training=None):\n        constants = []\n        if self.implementation != 0 and 0 < self.dropout < 1:\n            input_shape = K.int_shape(inputs)\n            input_dim = input_shape[-1]\n            ones = K.ones_like(K.reshape(inputs[:, 0, 0], (-1, 1)))\n            ones = K.tile(ones, (1, int(input_dim)))\n\n            def dropped_inputs():\n                return K.dropout(ones, self.dropout)\n\n            dp_mask = K.in_train_phase(dropped_inputs,\n                                       ones,\n                                       training=training)\n            constants.append(dp_mask)\n        else:\n            constants.append(K.cast_to_floatx(1.))\n\n        if 0 < self.recurrent_dropout < 1:\n            ones = K.ones_like(K.reshape(inputs[:, 0, 0], (-1, 1)))\n            ones = K.tile(ones, (1, self.units))\n\n            def dropped_inputs():\n                return K.dropout(ones, self.recurrent_dropout)\n            rec_dp_mask = K.in_train_phase(dropped_inputs,\n                                           ones,\n                                           training=training)\n            constants.append(rec_dp_mask)\n        else:\n            constants.append(K.cast_to_floatx(1.))\n        return constants\n\n    def get_config(self):\n        config = {'units': self.units,\n                  'activation': activations.serialize(self.activation),\n                  'use_bias': self.use_bias,\n                  'kernel_initializer': initializers.serialize(self.kernel_initializer),\n                  'recurrent_initializer': initializers.serialize(self.recurrent_initializer),\n                  'bias_initializer': initializers.serialize(self.bias_initializer),\n                  'kernel_regularizer': regularizers.serialize(self.kernel_regularizer),\n                  'recurrent_regularizer': regularizers.serialize(self.recurrent_regularizer),\n                  'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n                  'activity_regularizer': regularizers.serialize(self.activity_regularizer),\n                  'kernel_constraint': constraints.serialize(self.kernel_constraint),\n                  'recurrent_constraint': constraints.serialize(self.recurrent_constraint),\n                  'bias_constraint': constraints.serialize(self.bias_constraint),\n                  'dropout': self.dropout,\n                  'recurrent_dropout': self.recurrent_dropout}\n        base_config = super(SimpleRNN, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\n\nclass GRU(Recurrent):\n    \"\"\"Gated Recurrent Unit - Cho et al. 2014.\n\n    # Arguments\n        units: Positive integer, dimensionality of the output space.\n        activation: Activation function to use\n            (see [activations](../activations.md)).\n            If you pass None, no activation is applied\n            (ie. \"linear\" activation: `a(x) = x`).\n        recurrent_activation: Activation function to use\n            for the recurrent step\n            (see [activations](../activations.md)).\n        use_bias: Boolean, whether the layer uses a bias vector.\n        kernel_initializer: Initializer for the `kernel` weights matrix,\n            used for the linear transformation of the inputs.\n            (see [initializers](../initializers.md)).\n        recurrent_initializer: Initializer for the `recurrent_kernel`\n            weights matrix,\n            used for the linear transformation of the recurrent state.\n            (see [initializers](../initializers.md)).\n        bias_initializer: Initializer for the bias vector\n            (see [initializers](../initializers.md)).\n        kernel_regularizer: Regularizer function applied to\n            the `kernel` weights matrix\n            (see [regularizer](../regularizers.md)).\n        recurrent_regularizer: Regularizer function applied to\n            the `recurrent_kernel` weights matrix\n            (see [regularizer](../regularizers.md)).\n        bias_regularizer: Regularizer function applied to the bias vector\n            (see [regularizer](../regularizers.md)).\n        activity_regularizer: Regularizer function applied to\n            the output of the layer (its \"activation\").\n            (see [regularizer](../regularizers.md)).\n        kernel_constraint: Constraint function applied to\n            the `kernel` weights matrix\n            (see [constraints](../constraints.md)).\n        recurrent_constraint: Constraint function applied to\n            the `recurrent_kernel` weights matrix\n            (see [constraints](../constraints.md)).\n        bias_constraint: Constraint function applied to the bias vector\n            (see [constraints](../constraints.md)).\n        dropout: Float between 0 and 1.\n            Fraction of the units to drop for\n            the linear transformation of the inputs.\n        recurrent_dropout: Float between 0 and 1.\n            Fraction of the units to drop for\n            the linear transformation of the recurrent state.\n\n    # References\n        - [On the Properties of Neural Machine Translation: Encoder-Decoder Approaches](https://arxiv.org/abs/1409.1259)\n        - [Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling](http://arxiv.org/abs/1412.3555v1)\n        - [A Theoretically Grounded Application of Dropout in Recurrent Neural Networks](http://arxiv.org/abs/1512.05287)\n    \"\"\"\n\n    @interfaces.legacy_recurrent_support\n    def __init__(self, units,\n                 activation='tanh',\n                 recurrent_activation='hard_sigmoid',\n                 use_bias=True,\n                 kernel_initializer='glorot_uniform',\n                 recurrent_initializer='orthogonal',\n                 bias_initializer='zeros',\n                 kernel_regularizer=None,\n                 recurrent_regularizer=None,\n                 bias_regularizer=None,\n                 activity_regularizer=None,\n                 kernel_constraint=None,\n                 recurrent_constraint=None,\n                 bias_constraint=None,\n                 dropout=0.,\n                 recurrent_dropout=0.,\n                 **kwargs):\n        super(GRU, self).__init__(**kwargs)\n        self.units = units\n        self.activation = activations.get(activation)\n        self.recurrent_activation = activations.get(recurrent_activation)\n        self.use_bias = use_bias\n\n        self.kernel_initializer = initializers.get(kernel_initializer)\n        self.recurrent_initializer = initializers.get(recurrent_initializer)\n        self.bias_initializer = initializers.get(bias_initializer)\n\n        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n        self.recurrent_regularizer = regularizers.get(recurrent_regularizer)\n        self.bias_regularizer = regularizers.get(bias_regularizer)\n        self.activity_regularizer = regularizers.get(activity_regularizer)\n\n        self.kernel_constraint = constraints.get(kernel_constraint)\n        self.recurrent_constraint = constraints.get(recurrent_constraint)\n        self.bias_constraint = constraints.get(bias_constraint)\n\n        self.dropout = min(1., max(0., dropout))\n        self.recurrent_dropout = min(1., max(0., recurrent_dropout))\n\n    def build(self, input_shape):\n        if isinstance(input_shape, list):\n            input_shape = input_shape[0]\n\n        batch_size = input_shape[0] if self.stateful else None\n        self.input_dim = input_shape[2]\n        self.input_spec = InputSpec(shape=(batch_size, None, self.input_dim))\n        self.state_spec = InputSpec(shape=(batch_size, self.units))\n\n        self.states = [None]\n        if self.stateful:\n            self.reset_states()\n\n        self.kernel = self.add_weight(shape=(self.input_dim, self.units * 3),\n                                      name='kernel',\n                                      initializer=self.kernel_initializer,\n                                      regularizer=self.kernel_regularizer,\n                                      constraint=self.kernel_constraint)\n        self.recurrent_kernel = self.add_weight(\n            shape=(self.units, self.units * 3),\n            name='recurrent_kernel',\n            initializer=self.recurrent_initializer,\n            regularizer=self.recurrent_regularizer,\n            constraint=self.recurrent_constraint)\n\n        if self.use_bias:\n            self.bias = self.add_weight(shape=(self.units * 3,),\n                                        name='bias',\n                                        initializer=self.bias_initializer,\n                                        regularizer=self.bias_regularizer,\n                                        constraint=self.bias_constraint)\n        else:\n            self.bias = None\n\n        self.kernel_z = self.kernel[:, :self.units]\n        self.recurrent_kernel_z = self.recurrent_kernel[:, :self.units]\n        self.kernel_r = self.kernel[:, self.units: self.units * 2]\n        self.recurrent_kernel_r = self.recurrent_kernel[:,\n                                                        self.units:\n                                                        self.units * 2]\n        self.kernel_h = self.kernel[:, self.units * 2:]\n        self.recurrent_kernel_h = self.recurrent_kernel[:, self.units * 2:]\n\n        if self.use_bias:\n            self.bias_z = self.bias[:self.units]\n            self.bias_r = self.bias[self.units: self.units * 2]\n            self.bias_h = self.bias[self.units * 2:]\n        else:\n            self.bias_z = None\n            self.bias_r = None\n            self.bias_h = None\n        self.built = True\n\n    def preprocess_input(self, inputs, training=None):\n        if self.implementation == 0:\n            input_shape = K.int_shape(inputs)\n            input_dim = input_shape[2]\n            timesteps = input_shape[1]\n\n            x_z = _time_distributed_dense(inputs, self.kernel_z, self.bias_z,\n                                          self.dropout, input_dim, self.units,\n                                          timesteps, training=training)\n            x_r = _time_distributed_dense(inputs, self.kernel_r, self.bias_r,\n                                          self.dropout, input_dim, self.units,\n                                          timesteps, training=training)\n            x_h = _time_distributed_dense(inputs, self.kernel_h, self.bias_h,\n                                          self.dropout, input_dim, self.units,\n                                          timesteps, training=training)\n            return K.concatenate([x_z, x_r, x_h], axis=2)\n        else:\n            return inputs\n\n    def get_constants(self, inputs, training=None):\n        constants = []\n        if self.implementation != 0 and 0 < self.dropout < 1:\n            input_shape = K.int_shape(inputs)\n            input_dim = input_shape[-1]\n            ones = K.ones_like(K.reshape(inputs[:, 0, 0], (-1, 1)))\n            ones = K.tile(ones, (1, int(input_dim)))\n\n            def dropped_inputs():\n                return K.dropout(ones, self.dropout)\n\n            dp_mask = [K.in_train_phase(dropped_inputs,\n                                        ones,\n                                        training=training) for _ in range(3)]\n            constants.append(dp_mask)\n        else:\n            constants.append([K.cast_to_floatx(1.) for _ in range(3)])\n\n        if 0 < self.recurrent_dropout < 1:\n            ones = K.ones_like(K.reshape(inputs[:, 0, 0], (-1, 1)))\n            ones = K.tile(ones, (1, self.units))\n\n            def dropped_inputs():\n                return K.dropout(ones, self.recurrent_dropout)\n            rec_dp_mask = [K.in_train_phase(dropped_inputs,\n                                            ones,\n                                            training=training) for _ in range(3)]\n            constants.append(rec_dp_mask)\n        else:\n            constants.append([K.cast_to_floatx(1.) for _ in range(3)])\n        return constants\n\n    def step(self, inputs, states):\n        h_tm1 = states[0]  # previous memory\n        dp_mask = states[1]  # dropout matrices for recurrent units\n        rec_dp_mask = states[2]\n\n        if self.implementation == 2:\n            matrix_x = K.dot(inputs * dp_mask[0], self.kernel)\n            if self.use_bias:\n                matrix_x = K.bias_add(matrix_x, self.bias)\n            matrix_inner = K.dot(h_tm1 * rec_dp_mask[0],\n                                 self.recurrent_kernel[:, :2 * self.units])\n\n            x_z = matrix_x[:, :self.units]\n            x_r = matrix_x[:, self.units: 2 * self.units]\n            recurrent_z = matrix_inner[:, :self.units]\n            recurrent_r = matrix_inner[:, self.units: 2 * self.units]\n\n            z = self.recurrent_activation(x_z + recurrent_z)\n            r = self.recurrent_activation(x_r + recurrent_r)\n\n            x_h = matrix_x[:, 2 * self.units:]\n            recurrent_h = K.dot(r * h_tm1 * rec_dp_mask[0],\n                                self.recurrent_kernel[:, 2 * self.units:])\n            hh = self.activation(x_h + recurrent_h)\n        else:\n            if self.implementation == 0:\n                x_z = inputs[:, :self.units]\n                x_r = inputs[:, self.units: 2 * self.units]\n                x_h = inputs[:, 2 * self.units:]\n            elif self.implementation == 1:\n                x_z = K.dot(inputs * dp_mask[0], self.kernel_z)\n                x_r = K.dot(inputs * dp_mask[1], self.kernel_r)\n                x_h = K.dot(inputs * dp_mask[2], self.kernel_h)\n                if self.use_bias:\n                    x_z = K.bias_add(x_z, self.bias_z)\n                    x_r = K.bias_add(x_r, self.bias_r)\n                    x_h = K.bias_add(x_h, self.bias_h)\n            else:\n                raise ValueError('Unknown `implementation` mode.')\n            z = self.recurrent_activation(x_z + K.dot(h_tm1 * rec_dp_mask[0],\n                                                      self.recurrent_kernel_z))\n            r = self.recurrent_activation(x_r + K.dot(h_tm1 * rec_dp_mask[1],\n                                                      self.recurrent_kernel_r))\n\n            hh = self.activation(x_h + K.dot(r * h_tm1 * rec_dp_mask[2],\n                                             self.recurrent_kernel_h))\n        h = z * h_tm1 + (1 - z) * hh\n        if 0 < self.dropout + self.recurrent_dropout:\n            h._uses_learning_phase = True\n        return h, [h]\n\n    def get_config(self):\n        config = {'units': self.units,\n                  'activation': activations.serialize(self.activation),\n                  'recurrent_activation': activations.serialize(self.recurrent_activation),\n                  'use_bias': self.use_bias,\n                  'kernel_initializer': initializers.serialize(self.kernel_initializer),\n                  'recurrent_initializer': initializers.serialize(self.recurrent_initializer),\n                  'bias_initializer': initializers.serialize(self.bias_initializer),\n                  'kernel_regularizer': regularizers.serialize(self.kernel_regularizer),\n                  'recurrent_regularizer': regularizers.serialize(self.recurrent_regularizer),\n                  'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n                  'activity_regularizer': regularizers.serialize(self.activity_regularizer),\n                  'kernel_constraint': constraints.serialize(self.kernel_constraint),\n                  'recurrent_constraint': constraints.serialize(self.recurrent_constraint),\n                  'bias_constraint': constraints.serialize(self.bias_constraint),\n                  'dropout': self.dropout,\n                  'recurrent_dropout': self.recurrent_dropout}\n        base_config = super(GRU, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\n\nclass LSTM(Recurrent):\n    \"\"\"Long-Short Term Memory unit - Hochreiter 1997.\n\n    For a step-by-step description of the algorithm, see\n    [this tutorial](http://deeplearning.net/tutorial/lstm.html).\n\n    # Arguments\n        units: Positive integer, dimensionality of the output space.\n        activation: Activation function to use\n            (see [activations](../activations.md)).\n            If you pass None, no activation is applied\n            (ie. \"linear\" activation: `a(x) = x`).\n        recurrent_activation: Activation function to use\n            for the recurrent step\n            (see [activations](../activations.md)).\n        use_bias: Boolean, whether the layer uses a bias vector.\n        kernel_initializer: Initializer for the `kernel` weights matrix,\n            used for the linear transformation of the inputs.\n            (see [initializers](../initializers.md)).\n        recurrent_initializer: Initializer for the `recurrent_kernel`\n            weights matrix,\n            used for the linear transformation of the recurrent state.\n            (see [initializers](../initializers.md)).\n        bias_initializer: Initializer for the bias vector\n            (see [initializers](../initializers.md)).\n        unit_forget_bias: Boolean.\n            If True, add 1 to the bias of the forget gate at initialization.\n            Setting it to true will also force `bias_initializer=\"zeros\"`.\n            This is recommended in [Jozefowicz et al.](http://www.jmlr.org/proceedings/papers/v37/jozefowicz15.pdf)\n        kernel_regularizer: Regularizer function applied to\n            the `kernel` weights matrix\n            (see [regularizer](../regularizers.md)).\n        recurrent_regularizer: Regularizer function applied to\n            the `recurrent_kernel` weights matrix\n            (see [regularizer](../regularizers.md)).\n        bias_regularizer: Regularizer function applied to the bias vector\n            (see [regularizer](../regularizers.md)).\n        activity_regularizer: Regularizer function applied to\n            the output of the layer (its \"activation\").\n            (see [regularizer](../regularizers.md)).\n        kernel_constraint: Constraint function applied to\n            the `kernel` weights matrix\n            (see [constraints](../constraints.md)).\n        recurrent_constraint: Constraint function applied to\n            the `recurrent_kernel` weights matrix\n            (see [constraints](../constraints.md)).\n        bias_constraint: Constraint function applied to the bias vector\n            (see [constraints](../constraints.md)).\n        dropout: Float between 0 and 1.\n            Fraction of the units to drop for\n            the linear transformation of the inputs.\n        recurrent_dropout: Float between 0 and 1.\n            Fraction of the units to drop for\n            the linear transformation of the recurrent state.\n\n    # References\n        - [Long short-term memory](http://deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf) (original 1997 paper)\n        - [Learning to forget: Continual prediction with LSTM](http://www.mitpressjournals.org/doi/pdf/10.1162/089976600300015015)\n        - [Supervised sequence labeling with recurrent neural networks](http://www.cs.toronto.edu/~graves/preprint.pdf)\n        - [A Theoretically Grounded Application of Dropout in Recurrent Neural Networks](http://arxiv.org/abs/1512.05287)\n    \"\"\"\n    @interfaces.legacy_recurrent_support\n    def __init__(self, units,\n                 activation='tanh',\n                 recurrent_activation='hard_sigmoid',\n                 use_bias=True,\n                 kernel_initializer='glorot_uniform',\n                 recurrent_initializer='orthogonal',\n                 bias_initializer='zeros',\n                 unit_forget_bias=True,\n                 kernel_regularizer=None,\n                 recurrent_regularizer=None,\n                 bias_regularizer=None,\n                 activity_regularizer=None,\n                 kernel_constraint=None,\n                 recurrent_constraint=None,\n                 bias_constraint=None,\n                 dropout=0.,\n                 recurrent_dropout=0.,\n                 **kwargs):\n        super(LSTM, self).__init__(**kwargs)\n        self.units = units\n        self.activation = activations.get(activation)\n        self.recurrent_activation = activations.get(recurrent_activation)\n        self.use_bias = use_bias\n\n        self.kernel_initializer = initializers.get(kernel_initializer)\n        self.recurrent_initializer = initializers.get(recurrent_initializer)\n        self.bias_initializer = initializers.get(bias_initializer)\n        self.unit_forget_bias = unit_forget_bias\n\n        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n        self.recurrent_regularizer = regularizers.get(recurrent_regularizer)\n        self.bias_regularizer = regularizers.get(bias_regularizer)\n        self.activity_regularizer = regularizers.get(activity_regularizer)\n\n        self.kernel_constraint = constraints.get(kernel_constraint)\n        self.recurrent_constraint = constraints.get(recurrent_constraint)\n        self.bias_constraint = constraints.get(bias_constraint)\n\n        self.dropout = min(1., max(0., dropout))\n        self.recurrent_dropout = min(1., max(0., recurrent_dropout))\n\n    def build(self, input_shape):\n        if isinstance(input_shape, list):\n            input_shape = input_shape[0]\n\n        batch_size = input_shape[0] if self.stateful else None\n        self.input_dim = input_shape[2]\n        self.input_spec = InputSpec(shape=(batch_size, None, self.input_dim))\n        self.state_spec = [InputSpec(shape=(batch_size, self.units)),\n                           InputSpec(shape=(batch_size, self.units))]\n\n        self.states = [None, None]\n        if self.stateful:\n            self.reset_states()\n\n        self.kernel = self.add_weight(shape=(self.input_dim, self.units * 4),\n                                      name='kernel',\n                                      initializer=self.kernel_initializer,\n                                      regularizer=self.kernel_regularizer,\n                                      constraint=self.kernel_constraint)\n        self.recurrent_kernel = self.add_weight(\n            shape=(self.units, self.units * 4),\n            name='recurrent_kernel',\n            initializer=self.recurrent_initializer,\n            regularizer=self.recurrent_regularizer,\n            constraint=self.recurrent_constraint)\n\n        if self.use_bias:\n            self.bias = self.add_weight(shape=(self.units * 4,),\n                                        name='bias',\n                                        initializer=self.bias_initializer,\n                                        regularizer=self.bias_regularizer,\n                                        constraint=self.bias_constraint)\n            if self.unit_forget_bias:\n                bias_value = np.zeros((self.units * 4,))\n                bias_value[self.units: self.units * 2] = 1.\n                K.set_value(self.bias, bias_value)\n        else:\n            self.bias = None\n\n        self.kernel_i = self.kernel[:, :self.units]\n        self.kernel_f = self.kernel[:, self.units: self.units * 2]\n        self.kernel_c = self.kernel[:, self.units * 2: self.units * 3]\n        self.kernel_o = self.kernel[:, self.units * 3:]\n\n        self.recurrent_kernel_i = self.recurrent_kernel[:, :self.units]\n        self.recurrent_kernel_f = self.recurrent_kernel[:, self.units: self.units * 2]\n        self.recurrent_kernel_c = self.recurrent_kernel[:, self.units * 2: self.units * 3]\n        self.recurrent_kernel_o = self.recurrent_kernel[:, self.units * 3:]\n\n        if self.use_bias:\n            self.bias_i = self.bias[:self.units]\n            self.bias_f = self.bias[self.units: self.units * 2]\n            self.bias_c = self.bias[self.units * 2: self.units * 3]\n            self.bias_o = self.bias[self.units * 3:]\n        else:\n            self.bias_i = None\n            self.bias_f = None\n            self.bias_c = None\n            self.bias_o = None\n        self.built = True\n\n    def preprocess_input(self, inputs, training=None):\n        if self.implementation == 0:\n            input_shape = K.int_shape(inputs)\n            input_dim = input_shape[2]\n            timesteps = input_shape[1]\n\n            x_i = _time_distributed_dense(inputs, self.kernel_i, self.bias_i,\n                                          self.dropout, input_dim, self.units,\n                                          timesteps, training=training)\n            x_f = _time_distributed_dense(inputs, self.kernel_f, self.bias_f,\n                                          self.dropout, input_dim, self.units,\n                                          timesteps, training=training)\n            x_c = _time_distributed_dense(inputs, self.kernel_c, self.bias_c,\n                                          self.dropout, input_dim, self.units,\n                                          timesteps, training=training)\n            x_o = _time_distributed_dense(inputs, self.kernel_o, self.bias_o,\n                                          self.dropout, input_dim, self.units,\n                                          timesteps, training=training)\n            return K.concatenate([x_i, x_f, x_c, x_o], axis=2)\n        else:\n            return inputs\n\n    def get_constants(self, inputs, training=None):\n        constants = []\n        if self.implementation != 0 and 0 < self.dropout < 1:\n            input_shape = K.int_shape(inputs)\n            input_dim = input_shape[-1]\n            ones = K.ones_like(K.reshape(inputs[:, 0, 0], (-1, 1)))\n            ones = K.tile(ones, (1, int(input_dim)))\n\n            def dropped_inputs():\n                return K.dropout(ones, self.dropout)\n\n            dp_mask = [K.in_train_phase(dropped_inputs,\n                                        ones,\n                                        training=training) for _ in range(4)]\n            constants.append(dp_mask)\n        else:\n            constants.append([K.cast_to_floatx(1.) for _ in range(4)])\n\n        if 0 < self.recurrent_dropout < 1:\n            ones = K.ones_like(K.reshape(inputs[:, 0, 0], (-1, 1)))\n            ones = K.tile(ones, (1, self.units))\n\n            def dropped_inputs():\n                return K.dropout(ones, self.recurrent_dropout)\n            rec_dp_mask = [K.in_train_phase(dropped_inputs,\n                                            ones,\n                                            training=training) for _ in range(4)]\n            constants.append(rec_dp_mask)\n        else:\n            constants.append([K.cast_to_floatx(1.) for _ in range(4)])\n        return constants\n\n    def step(self, inputs, states):\n        h_tm1 = states[0]\n        c_tm1 = states[1]\n        dp_mask = states[2]\n        rec_dp_mask = states[3]\n\n        if self.implementation == 2:\n            z = K.dot(inputs * dp_mask[0], self.kernel)\n            z += K.dot(h_tm1 * rec_dp_mask[0], self.recurrent_kernel)\n            if self.use_bias:\n                z = K.bias_add(z, self.bias)\n\n            z0 = z[:, :self.units]\n            z1 = z[:, self.units: 2 * self.units]\n            z2 = z[:, 2 * self.units: 3 * self.units]\n            z3 = z[:, 3 * self.units:]\n\n            i = self.recurrent_activation(z0)\n            f = self.recurrent_activation(z1)\n            c = f * c_tm1 + i * self.activation(z2)\n            o = self.recurrent_activation(z3)\n        else:\n            if self.implementation == 0:\n                x_i = inputs[:, :self.units]\n                x_f = inputs[:, self.units: 2 * self.units]\n                x_c = inputs[:, 2 * self.units: 3 * self.units]\n                x_o = inputs[:, 3 * self.units:]\n            elif self.implementation == 1:\n                x_i = K.dot(inputs * dp_mask[0], self.kernel_i) + self.bias_i\n                x_f = K.dot(inputs * dp_mask[1], self.kernel_f) + self.bias_f\n                x_c = K.dot(inputs * dp_mask[2], self.kernel_c) + self.bias_c\n                x_o = K.dot(inputs * dp_mask[3], self.kernel_o) + self.bias_o\n            else:\n                raise ValueError('Unknown `implementation` mode.')\n\n            i = self.recurrent_activation(x_i + K.dot(h_tm1 * rec_dp_mask[0],\n                                                      self.recurrent_kernel_i))\n            f = self.recurrent_activation(x_f + K.dot(h_tm1 * rec_dp_mask[1],\n                                                      self.recurrent_kernel_f))\n            c = f * c_tm1 + i * self.activation(x_c + K.dot(h_tm1 * rec_dp_mask[2],\n                                                            self.recurrent_kernel_c))\n            o = self.recurrent_activation(x_o + K.dot(h_tm1 * rec_dp_mask[3],\n                                                      self.recurrent_kernel_o))\n        h = o * self.activation(c)\n        if 0 < self.dropout + self.recurrent_dropout:\n            h._uses_learning_phase = True\n        return h, [h, c]\n\n    def get_config(self):\n        config = {'units': self.units,\n                  'activation': activations.serialize(self.activation),\n                  'recurrent_activation': activations.serialize(self.recurrent_activation),\n                  'use_bias': self.use_bias,\n                  'kernel_initializer': initializers.serialize(self.kernel_initializer),\n                  'recurrent_initializer': initializers.serialize(self.recurrent_initializer),\n                  'bias_initializer': initializers.serialize(self.bias_initializer),\n                  'unit_forget_bias': self.unit_forget_bias,\n                  'kernel_regularizer': regularizers.serialize(self.kernel_regularizer),\n                  'recurrent_regularizer': regularizers.serialize(self.recurrent_regularizer),\n                  'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n                  'activity_regularizer': regularizers.serialize(self.activity_regularizer),\n                  'kernel_constraint': constraints.serialize(self.kernel_constraint),\n                  'recurrent_constraint': constraints.serialize(self.recurrent_constraint),\n                  'bias_constraint': constraints.serialize(self.bias_constraint),\n                  'dropout': self.dropout,\n                  'recurrent_dropout': self.recurrent_dropout}\n        base_config = super(LSTM, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n",
          "file_patch": "@@ -471,19 +471,19 @@ class SimpleRNN(Recurrent):\n         if self.stateful:\n             self.reset_states()\n \n-        self.kernel = self.add_weight((self.input_dim, self.units),\n+        self.kernel = self.add_weight(shape=(self.input_dim, self.units),\n                                       name='kernel',\n                                       initializer=self.kernel_initializer,\n                                       regularizer=self.kernel_regularizer,\n                                       constraint=self.kernel_constraint)\n         self.recurrent_kernel = self.add_weight(\n-            (self.units, self.units),\n+            shape=(self.units, self.units),\n             name='recurrent_kernel',\n             initializer=self.recurrent_initializer,\n             regularizer=self.recurrent_regularizer,\n             constraint=self.recurrent_constraint)\n         if self.use_bias:\n-            self.bias = self.add_weight((self.units,),\n+            self.bias = self.add_weight(shape=(self.units,),\n                                         name='bias',\n                                         initializer=self.bias_initializer,\n                                         regularizer=self.bias_regularizer,\n@@ -690,20 +690,20 @@ class GRU(Recurrent):\n         if self.stateful:\n             self.reset_states()\n \n-        self.kernel = self.add_weight((self.input_dim, self.units * 3),\n+        self.kernel = self.add_weight(shape=(self.input_dim, self.units * 3),\n                                       name='kernel',\n                                       initializer=self.kernel_initializer,\n                                       regularizer=self.kernel_regularizer,\n                                       constraint=self.kernel_constraint)\n         self.recurrent_kernel = self.add_weight(\n-            (self.units, self.units * 3),\n+            shape=(self.units, self.units * 3),\n             name='recurrent_kernel',\n             initializer=self.recurrent_initializer,\n             regularizer=self.recurrent_regularizer,\n             constraint=self.recurrent_constraint)\n \n         if self.use_bias:\n-            self.bias = self.add_weight((self.units * 3,),\n+            self.bias = self.add_weight(shape=(self.units * 3,),\n                                         name='bias',\n                                         initializer=self.bias_initializer,\n                                         regularizer=self.bias_regularizer,\n@@ -970,20 +970,20 @@ class LSTM(Recurrent):\n         if self.stateful:\n             self.reset_states()\n \n-        self.kernel = self.add_weight((self.input_dim, self.units * 4),\n+        self.kernel = self.add_weight(shape=(self.input_dim, self.units * 4),\n                                       name='kernel',\n                                       initializer=self.kernel_initializer,\n                                       regularizer=self.kernel_regularizer,\n                                       constraint=self.kernel_constraint)\n         self.recurrent_kernel = self.add_weight(\n-            (self.units, self.units * 4),\n+            shape=(self.units, self.units * 4),\n             name='recurrent_kernel',\n             initializer=self.recurrent_initializer,\n             regularizer=self.recurrent_regularizer,\n             constraint=self.recurrent_constraint)\n \n         if self.use_bias:\n-            self.bias = self.add_weight((self.units * 4,),\n+            self.bias = self.add_weight(shape=(self.units * 4,),\n                                         name='bias',\n                                         initializer=self.bias_initializer,\n                                         regularizer=self.bias_regularizer,\n",
          "files_name_in_blame_commit": [
            "embeddings.py",
            "advanced_activations.py",
            "convolutional_recurrent.py",
            "interfaces.py",
            "topology.py",
            "normalization.py",
            "convolutional.py",
            "recurrent.py",
            "local.py",
            "core.py"
          ]
        }
      },
      "16343b3261da08a8aa52f209a8609ce4b51c7fdf": {
        "commit": {
          "commit_id": "16343b3261da08a8aa52f209a8609ce4b51c7fdf",
          "commit_message": "Pass initial_state as a keyword argument.",
          "commit_author": "Joshua Chin",
          "commit_date": "2017-03-10 23:45:20",
          "commit_parent": "5dbb6121cdb182e047a87f8b4cf99722519d1b74"
        },
        "function": {
          "function_name": "build",
          "function_code_before": "def build(self, input_shape):\n    if isinstance(input_shape, list):\n        input_shape = input_shape[0]\n    self.batch_size = input_shape[0]\n    self.input_dim = input_shape[2]\n    self.states = [None]\n    if self.stateful:\n        self.reset_states()\n    self.kernel = self.add_weight((self.input_dim, self.units), name='kernel', initializer=self.kernel_initializer, regularizer=self.kernel_regularizer, constraint=self.kernel_constraint)\n    self.recurrent_kernel = self.add_weight((self.units, self.units), name='recurrent_kernel', initializer=self.recurrent_initializer, regularizer=self.recurrent_regularizer, constraint=self.recurrent_constraint)\n    if self.use_bias:\n        self.bias = self.add_weight((self.units,), name='bias', initializer=self.bias_initializer, regularizer=self.bias_regularizer, constraint=self.bias_constraint)\n    else:\n        self.bias = None\n    self.built = True",
          "function_code_after": "def build(self, input_shape):\n    if isinstance(input_shape, list):\n        input_shape = input_shape[0]\n    batch_size = input_shape[0] if self.stateful else None\n    self.input_dim = input_shape[2]\n    self.input_spec = InputSpec(shape=(batch_size, None, self.input_dim))\n    self.state_spec = InputSpec(shape=(batch_size, self.units))\n    self.states = [None]\n    if self.stateful:\n        self.reset_states()\n    self.kernel = self.add_weight((self.input_dim, self.units), name='kernel', initializer=self.kernel_initializer, regularizer=self.kernel_regularizer, constraint=self.kernel_constraint)\n    self.recurrent_kernel = self.add_weight((self.units, self.units), name='recurrent_kernel', initializer=self.recurrent_initializer, regularizer=self.recurrent_regularizer, constraint=self.recurrent_constraint)\n    if self.use_bias:\n        self.bias = self.add_weight((self.units,), name='bias', initializer=self.bias_initializer, regularizer=self.bias_regularizer, constraint=self.bias_constraint)\n    else:\n        self.bias = None\n    self.built = True",
          "function_before_start_line": 393,
          "function_before_end_line": 423,
          "function_after_start_line": 439,
          "function_after_end_line": 471,
          "function_before_token_count": 188,
          "function_after_token_count": 226,
          "functions_name_modified_file": [
            "__init__",
            "compute_mask",
            "__call__",
            "build",
            "call",
            "get_constants",
            "_time_distributed_dense",
            "compute_output_shape",
            "step",
            "get_initial_states",
            "reset_states",
            "preprocess_input",
            "get_config"
          ],
          "functions_name_all_files": [
            "reset_states",
            "test_dropout",
            "get_config",
            "test_from_config",
            "test_specify_state",
            "step",
            "test_masking_layer",
            "test_implementation_mode",
            "test_regularizer",
            "preprocess_input",
            "rnn_test",
            "build",
            "call",
            "_time_distributed_dense",
            "test_return_sequences",
            "get_initial_states",
            "test_dynamic_behavior",
            "__init__",
            "compute_mask",
            "__call__",
            "get_constants",
            "compute_output_shape",
            "test_statefulness"
          ],
          "functions_name_co_evolved_modified_file": [
            "__init__",
            "call",
            "reset_states",
            "__call__"
          ],
          "functions_name_co_evolved_all_files": [
            "__init__",
            "__call__",
            "call",
            "test_specify_state",
            "reset_states"
          ]
        },
        "file": {
          "file_name": "recurrent.py",
          "file_nloc": 974,
          "file_complexity": 118,
          "file_token_count": 5970,
          "file_before": "# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import\nimport numpy as np\n\nfrom .. import backend as K\nfrom .. import activations\nfrom .. import initializers\nfrom .. import regularizers\nfrom .. import constraints\nfrom ..engine import Layer\nfrom ..engine import InputSpec\n\n\ndef _time_distributed_dense(x, w, b=None, dropout=None,\n                            input_dim=None, output_dim=None,\n                            timesteps=None, training=None):\n    \"\"\"Apply `y . w + b` for every temporal slice y of x.\n\n    # Arguments\n        x: input tensor.\n        w: weight matrix.\n        b: optional bias vector.\n        dropout: wether to apply dropout (same dropout mask\n            for every temporal slice of the input).\n        input_dim: integer; optional dimensionality of the input.\n        output_dim: integer; optional dimensionality of the output.\n        timesteps: integer; optional number of timesteps.\n        training: training phase tensor or boolean.\n\n    # Returns\n        Output tensor.\n    \"\"\"\n    if not input_dim:\n        input_dim = K.shape(x)[2]\n    if not timesteps:\n        timesteps = K.shape(x)[1]\n    if not output_dim:\n        output_dim = K.shape(w)[1]\n\n    if dropout is not None and 0. < dropout < 1.:\n        # apply the same dropout pattern at every timestep\n        ones = K.ones_like(K.reshape(x[:, 0, :], (-1, input_dim)))\n        dropout_matrix = K.dropout(ones, dropout)\n        expanded_dropout_matrix = K.repeat(dropout_matrix, timesteps)\n        x = K.in_train_phase(x * expanded_dropout_matrix, x, training=training)\n\n    # collapse time dimension and batch dimension together\n    x = K.reshape(x, (-1, input_dim))\n    x = K.dot(x, w)\n    if b is not None:\n        x += b\n    # reshape to 3D tensor\n    if K.backend() == 'tensorflow':\n        x = K.reshape(x, K.stack([-1, timesteps, output_dim]))\n        x.set_shape([None, None, output_dim])\n    else:\n        x = K.reshape(x, (-1, timesteps, output_dim))\n    return x\n\n\nclass Recurrent(Layer):\n    \"\"\"Abstract base class for recurrent layers.\n\n    Do not use in a model -- it's not a valid layer!\n    Use its children classes `LSTM`, `GRU` and `SimpleRNN` instead.\n\n    All recurrent layers (`LSTM`, `GRU`, `SimpleRNN`) also\n    follow the specifications of this class and accept\n    the keyword arguments listed below.\n\n    # Example\n\n    ```python\n        # as the first layer in a Sequential model\n        model = Sequential()\n        model.add(LSTM(32, input_shape=(10, 64)))\n        # now model.output_shape == (None, 32)\n        # note: `None` is the batch dimension.\n\n        # the following is identical:\n        model = Sequential()\n        model.add(LSTM(32, input_dim=64, input_length=10))\n\n        # for subsequent layers, not need to specify the input size:\n        model.add(LSTM(16))\n    ```\n\n    # Arguments\n        weights: list of Numpy arrays to set as initial weights.\n            The list should have 3 elements, of shapes:\n            `[(input_dim, output_dim), (output_dim, output_dim), (output_dim,)]`.\n        return_sequences: Boolean. Whether to return the last output\n            in the output sequence, or the full sequence.\n        go_backwards: Boolean (default False).\n            If True, process the input sequence backwards.\n        stateful: Boolean (default False). If True, the last state\n            for each sample at index i in a batch will be used as initial\n            state for the sample of index i in the following batch.\n        unroll: Boolean (default False).\n            If True, the network will be unrolled,\n            else a symbolic loop will be used.\n            Unrolling can speed-up a RNN,\n            although it tends to be more memory-intensive.\n            Unrolling is only suitable for short sequences.\n        implementation: one of {0, 1, or 2}.\n            If set to 0, the RNN will use\n            an implementation that uses fewer, larger matrix products,\n            thus running faster on CPU but consuming more memory.\n            If set to 1, the RNN will use more matrix products,\n            but smaller ones, thus running slower\n            (may actually be faster on GPU) while consuming less memory.\n            If set to 2 (LSTM/GRU only),\n            the RNN will combine the input gate,\n            the forget gate and the output gate into a single matrix,\n            enabling more time-efficient parallelization on the GPU.\n            Note: RNN dropout must be shared for all gates,\n            resulting in a slightly reduced regularization.\n        input_dim: dimensionality of the input (integer).\n            This argument (or alternatively, the keyword argument `input_shape`)\n            is required when using this layer as the first layer in a model.\n        input_length: Length of input sequences, to be specified\n            when it is constant.\n            This argument is required if you are going to connect\n            `Flatten` then `Dense` layers upstream\n            (without it, the shape of the dense outputs cannot be computed).\n            Note that if the recurrent layer is not the first layer\n            in your model, you would need to specify the input length\n            at the level of the first layer\n            (e.g. via the `input_shape` argument)\n\n    # Input shapes\n        3D tensor with shape `(batch_size, timesteps, input_dim)`,\n        (Optional) 2D tensors with shape `(batch_size, output_dim)`.\n\n    # Output shape\n        - if `return_sequences`: 3D tensor with shape\n            `(batch_size, timesteps, units)`.\n        - else, 2D tensor with shape `(batch_size, units)`.\n\n    # Masking\n        This layer supports masking for input data with a variable number\n        of timesteps. To introduce masks to your data,\n        use an [Embedding](embeddings.md) layer with the `mask_zero` parameter\n        set to `True`.\n\n    # Note on using statefulness in RNNs\n        You can set RNN layers to be 'stateful', which means that the states\n        computed for the samples in one batch will be reused as initial states\n        for the samples in the next batch. This assumes a one-to-one mapping\n        between samples in different successive batches.\n\n        To enable statefulness:\n            - specify `stateful=True` in the layer constructor.\n            - specify a fixed batch size for your model, by passing\n                if sequential model:\n                  `batch_input_shape=(...)` to the first layer in your model.\n                else for functional model with 1 or more Input layers:\n                  `batch_shape=(...)` to all the first layers in your model.\n                This is the expected shape of your inputs\n                *including the batch size*.\n                It should be a tuple of integers, e.g. `(32, 10, 100)`.\n            - specify `shuffle=False` when calling fit().\n\n        To reset the states of your model, call `.reset_states()` on either\n        a specific layer, or on your entire model.\n\n    # Note on specifying initial states in RNNs\n        Most RNN layers can be called with either a single tensor, or a list\n        of tensors.\n        If the an RNN layer is called with a single tensor, that tensor is\n        treated as the input, and the initial states are assigned an\n        appropriate default value.\n        If an RNN layer is called with a list of tensors, the first element is\n        treated as the input, and the remaining tensors are treated as the\n        initial states.\n    \"\"\"\n\n    def __init__(self, return_sequences=False,\n                 go_backwards=False,\n                 stateful=False,\n                 unroll=False,\n                 implementation=0,\n                 **kwargs):\n        super(Recurrent, self).__init__(**kwargs)\n        self.return_sequences = return_sequences\n        self.go_backwards = go_backwards\n        self.stateful = stateful\n        self.unroll = unroll\n        self.implementation = 0\n        self.supports_masking = True\n        self.input_spec = None\n        self.dropout = 0\n        self.recurrent_dropout = 0\n\n    def compute_output_shape(self, input_shape):\n        if isinstance(input_shape, list):\n            input_shape = input_shape[0]\n        if self.return_sequences:\n            return (input_shape[0], input_shape[1], self.units)\n        else:\n            return (input_shape[0], self.units)\n\n    def compute_mask(self, inputs, mask):\n        if self.return_sequences:\n            return mask\n        else:\n            return None\n\n    def step(self, inputs, states):\n        raise NotImplementedError\n\n    def get_constants(self, inputs, training=None):\n        return []\n\n    def get_initial_states(self, inputs):\n        # build an all-zero tensor of shape (samples, output_dim)\n        initial_state = K.zeros_like(inputs)  # (samples, timesteps, input_dim)\n        initial_state = K.sum(initial_state, axis=(1, 2))  # (samples,)\n        initial_state = K.expand_dims(initial_state)  # (samples, 1)\n        initial_state = K.tile(initial_state, [1, self.units])  # (samples, output_dim)\n        initial_states = [initial_state for _ in range(len(self.states))]\n        return initial_states\n\n    def preprocess_input(self, inputs, training=None):\n        return inputs\n\n    def call(self, inputs, mask=None, training=None):\n        # input shape: (nbias_samples, time (padded with zeros), input_dim)\n        # note that the .build() method of subclasses MUST define\n        # self.input_spec with a complete input shape.\n        if isinstance(inputs, list):\n            initial_states = inputs[1:]\n            inputs = inputs[0]\n        elif self.stateful:\n            initial_states = self.states\n        else:\n            initial_states = self.get_initial_states(inputs)\n        input_shape = K.int_shape(inputs)\n        if self.unroll and input_shape[1] is None:\n            raise ValueError('Cannot unroll a RNN if the '\n                             'time dimension is undefined. \\n'\n                             '- If using a Sequential model, '\n                             'specify the time dimension by passing '\n                             'an `input_shape` or `batch_input_shape` '\n                             'argument to your first layer. If your '\n                             'first layer is an Embedding, you can '\n                             'also use the `input_length` argument.\\n'\n                             '- If using the functional API, specify '\n                             'the time dimension by passing a `shape` '\n                             'or `batch_shape` argument to your Input layer.')\n        constants = self.get_constants(inputs, training=None)\n        preprocessed_input = self.preprocess_input(inputs, training=None)\n        last_output, outputs, states = K.rnn(self.step,\n                                             preprocessed_input,\n                                             initial_states,\n                                             go_backwards=self.go_backwards,\n                                             mask=mask,\n                                             constants=constants,\n                                             unroll=self.unroll,\n                                             input_length=input_shape[1])\n        if self.stateful:\n            updates = []\n            for i in range(len(states)):\n                updates.append((self.states[i], states[i]))\n            self.add_update(updates, inputs)\n\n        # Properly set learning phase\n        if 0 < self.dropout + self.recurrent_dropout:\n            last_output._uses_learning_phase = True\n            outputs._uses_learning_phase = True\n\n        if self.return_sequences:\n            return outputs\n        else:\n            return last_output\n\n    def reset_states(self):\n        if not self.stateful:\n            raise AttributeError('Layer must be stateful.')\n        if not self.batch_size:\n            raise ValueError('If a RNN is stateful, it needs to know '\n                             'its batch size. Specify the batch size '\n                             'of your input tensors: \\n'\n                             '- If using a Sequential model, '\n                             'specify the batch size by passing '\n                             'a `batch_input_shape` '\n                             'argument to your first layer.\\n'\n                             '- If using the functional API, specify '\n                             'the time dimension by passing a '\n                             '`batch_shape` argument to your Input layer.')\n        if self.states[0] is not None:\n            for state in self.states:\n                K.set_value(state, np.zeros((self.batch_size, self.units)))\n        else:\n            self.states = [K.zeros((self.batch_size, self.units))\n                           for _ in self.states]\n\n    def get_config(self):\n        config = {'return_sequences': self.return_sequences,\n                  'go_backwards': self.go_backwards,\n                  'stateful': self.stateful,\n                  'unroll': self.unroll,\n                  'implementation': self.implementation}\n        base_config = super(Recurrent, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\n\nclass SimpleRNN(Recurrent):\n    \"\"\"Fully-connected RNN where the output is to be fed back to input.\n\n    # Arguments\n        units: Positive integer, dimensionality of the output space.\n        activation: Activation function to use\n            (see [activations](../activations.md)).\n            If you don't specify anything, no activation is applied\n            (ie. \"linear\" activation: `a(x) = x`).\n        use_bias: Boolean, whether the layer uses a bias vector.\n        kernel_initializer: Initializer for the `kernel` weights matrix,\n            used for the linear transformation of the inputs.\n            (see [initializers](../initializers.md)).\n        recurrent_initializer: Initializer for the `recurrent_kernel`\n            weights matrix,\n            used for the linear transformation of the recurrent state.\n            (see [initializers](../initializers.md)).\n        bias_initializer: Initializer for the bias vector\n            (see [initializers](../initializers.md)).\n        kernel_regularizer: Regularizer function applied to\n            the `kernel` weights matrix\n            (see [regularizer](../regularizers.md)).\n        recurrent_regularizer: Regularizer function applied to\n            the `recurrent_kernel` weights matrix\n            (see [regularizer](../regularizers.md)).\n        bias_regularizer: Regularizer function applied to the bias vector\n            (see [regularizer](../regularizers.md)).\n        activity_regularizer: Regularizer function applied to\n            the output of the layer (its \"activation\").\n            (see [regularizer](../regularizers.md)).\n        kernel_constraint: Constraint function applied to\n            the `kernel` weights matrix\n            (see [constraints](../constraints.md)).\n        recurrent_constraint: Constraint function applied to\n            the `recurrent_kernel` weights matrix\n            (see [constraints](../constraints.md)).\n        bias_constraint: Constraint function applied to the bias vector\n            (see [constraints](../constraints.md)).\n        dropout: Float between 0 and 1.\n            Fraction of the units to drop for\n            the linear transformation of the inputs.\n        recurrent_dropout: Float between 0 and 1.\n            Fraction of the units to drop for\n            the linear transformation of the recurrent state.\n\n    # References\n        - [A Theoretically Grounded Application of Dropout in Recurrent Neural Networks](http://arxiv.org/abs/1512.05287)\n    \"\"\"\n\n    def __init__(self, units,\n                 activation='tanh',\n                 use_bias=True,\n                 kernel_initializer='glorot_uniform',\n                 recurrent_initializer='orthogonal',\n                 bias_initializer='zeros',\n                 kernel_regularizer=None,\n                 recurrent_regularizer=None,\n                 bias_regularizer=None,\n                 activity_regularizer=None,\n                 kernel_constraint=None,\n                 recurrent_constraint=None,\n                 bias_constraint=None,\n                 dropout=0.,\n                 recurrent_dropout=0.,\n                 **kwargs):\n        super(SimpleRNN, self).__init__(**kwargs)\n        self.units = units\n        self.activation = activations.get(activation)\n        self.use_bias = use_bias\n\n        self.kernel_initializer = initializers.get(kernel_initializer)\n        self.recurrent_initializer = initializers.get(recurrent_initializer)\n        self.bias_initializer = initializers.get(bias_initializer)\n\n        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n        self.recurrent_regularizer = regularizers.get(recurrent_regularizer)\n        self.bias_regularizer = regularizers.get(bias_regularizer)\n\n        self.kernel_constraint = constraints.get(kernel_constraint)\n        self.recurrent_constraint = constraints.get(recurrent_constraint)\n        self.bias_constraint = constraints.get(bias_constraint)\n\n        self.dropout = min(1., max(0., dropout))\n        self.recurrent_dropout = min(1., max(0., recurrent_dropout))\n\n    def build(self, input_shape):\n        if isinstance(input_shape, list):\n            input_shape = input_shape[0]\n\n        self.batch_size = input_shape[0]\n        self.input_dim = input_shape[2]\n\n        self.states = [None]\n        if self.stateful:\n            self.reset_states()\n\n        self.kernel = self.add_weight((self.input_dim, self.units),\n                                      name='kernel',\n                                      initializer=self.kernel_initializer,\n                                      regularizer=self.kernel_regularizer,\n                                      constraint=self.kernel_constraint)\n        self.recurrent_kernel = self.add_weight(\n            (self.units, self.units),\n            name='recurrent_kernel',\n            initializer=self.recurrent_initializer,\n            regularizer=self.recurrent_regularizer,\n            constraint=self.recurrent_constraint)\n        if self.use_bias:\n            self.bias = self.add_weight((self.units,),\n                                        name='bias',\n                                        initializer=self.bias_initializer,\n                                        regularizer=self.bias_regularizer,\n                                        constraint=self.bias_constraint)\n        else:\n            self.bias = None\n        self.built = True\n\n    def preprocess_input(self, inputs, training=None):\n        if self.implementation > 0:\n            return inputs\n        else:\n            input_shape = K.int_shape(inputs)\n            input_dim = input_shape[2]\n            timesteps = input_shape[1]\n            return _time_distributed_dense(inputs,\n                                           self.kernel,\n                                           self.bias,\n                                           self.dropout,\n                                           input_dim,\n                                           self.units,\n                                           timesteps,\n                                           training=training)\n\n    def step(self, inputs, states):\n        if self.implementation == 0:\n            h = inputs\n        else:\n            if 0 < self.dropout < 1:\n                h = K.dot(inputs * states[1], self.kernel)\n            else:\n                h = K.dot(inputs, self.kernel)\n            if self.bias is not None:\n                h += self.bias\n\n        prev_output = states[0]\n        if 0 < self.recurrent_dropout < 1:\n            prev_output *= states[2]\n        output = h + K.dot(prev_output, self.recurrent_kernel)\n        if self.activation is not None:\n            output = self.activation(output)\n\n        # Properly set learning phase on output tensor.\n        if 0 < self.dropout + self.recurrent_dropout:\n            output._uses_learning_phase = True\n        return output, [output]\n\n    def get_constants(self, inputs, training=None):\n        constants = []\n        if self.implementation == 0 and 0 < self.dropout < 1:\n            input_shape = K.int_shape(inputs)\n            input_dim = input_shape[-1]\n            ones = K.ones_like(K.reshape(inputs[:, 0, 0], (-1, 1)))\n            ones = K.tile(ones, (1, int(input_dim)))\n\n            def dropped_inputs():\n                return K.dropout(ones, self.dropout)\n\n            dp_mask = K.in_train_phase(dropped_inputs,\n                                       ones,\n                                       training=training)\n            constants.append(dp_mask)\n        else:\n            constants.append(K.cast_to_floatx(1.))\n\n        if 0 < self.recurrent_dropout < 1:\n            ones = K.ones_like(K.reshape(inputs[:, 0, 0], (-1, 1)))\n            ones = K.tile(ones, (1, self.units))\n\n            def dropped_inputs():\n                return K.dropout(ones, self.recurrent_dropout)\n            rec_dp_mask = K.in_train_phase(dropped_inputs,\n                                           ones,\n                                           training=training)\n            constants.append(rec_dp_mask)\n        else:\n            constants.append(K.cast_to_floatx(1.))\n        return constants\n\n    def get_config(self):\n        config = {'units': self.units,\n                  'activation': activations.serialize(self.activation),\n                  'use_bias': self.use_bias,\n                  'kernel_initializer': initializers.serialize(self.kernel_initializer),\n                  'recurrent_initializer': initializers.serialize(self.recurrent_initializer),\n                  'bias_initializer': initializers.serialize(self.bias_initializer),\n                  'kernel_regularizer': regularizers.serialize(self.kernel_regularizer),\n                  'recurrent_regularizer': regularizers.serialize(self.recurrent_regularizer),\n                  'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n                  'kernel_constraint': constraints.serialize(self.kernel_constraint),\n                  'recurrent_constraint': constraints.serialize(self.recurrent_constraint),\n                  'bias_constraint': constraints.serialize(self.bias_constraint),\n                  'dropout': self.dropout,\n                  'recurrent_dropout': self.recurrent_dropout}\n        base_config = super(SimpleRNN, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\n\nclass GRU(Recurrent):\n    \"\"\"Gated Recurrent Unit - Cho et al. 2014.\n\n    # Arguments\n        units: Positive integer, dimensionality of the output space.\n        activation: Activation function to use\n            (see [activations](../activations.md)).\n            If you don't specify anything, no activation is applied\n            (ie. \"linear\" activation: `a(x) = x`).\n        recurrent_activation: Activation function to use\n            for the recurrent step\n            (see [activations](../activations.md)).\n        use_bias: Boolean, whether the layer uses a bias vector.\n        kernel_initializer: Initializer for the `kernel` weights matrix,\n            used for the linear transformation of the inputs.\n            (see [initializers](../initializers.md)).\n        recurrent_initializer: Initializer for the `recurrent_kernel`\n            weights matrix,\n            used for the linear transformation of the recurrent state.\n            (see [initializers](../initializers.md)).\n        bias_initializer: Initializer for the bias vector\n            (see [initializers](../initializers.md)).\n        kernel_regularizer: Regularizer function applied to\n            the `kernel` weights matrix\n            (see [regularizer](../regularizers.md)).\n        recurrent_regularizer: Regularizer function applied to\n            the `recurrent_kernel` weights matrix\n            (see [regularizer](../regularizers.md)).\n        bias_regularizer: Regularizer function applied to the bias vector\n            (see [regularizer](../regularizers.md)).\n        activity_regularizer: Regularizer function applied to\n            the output of the layer (its \"activation\").\n            (see [regularizer](../regularizers.md)).\n        kernel_constraint: Constraint function applied to\n            the `kernel` weights matrix\n            (see [constraints](../constraints.md)).\n        recurrent_constraint: Constraint function applied to\n            the `recurrent_kernel` weights matrix\n            (see [constraints](../constraints.md)).\n        bias_constraint: Constraint function applied to the bias vector\n            (see [constraints](../constraints.md)).\n        dropout: Float between 0 and 1.\n            Fraction of the units to drop for\n            the linear transformation of the inputs.\n        recurrent_dropout: Float between 0 and 1.\n            Fraction of the units to drop for\n            the linear transformation of the recurrent state.\n\n    # References\n        - [On the Properties of Neural Machine Translation: Encoder-Decoder Approaches](https://arxiv.org/abs/1409.1259)\n        - [Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling](http://arxiv.org/abs/1412.3555v1)\n        - [A Theoretically Grounded Application of Dropout in Recurrent Neural Networks](http://arxiv.org/abs/1512.05287)\n    \"\"\"\n\n    def __init__(self, units,\n                 activation='tanh',\n                 recurrent_activation='hard_sigmoid',\n                 use_bias=True,\n                 kernel_initializer='glorot_uniform',\n                 recurrent_initializer='orthogonal',\n                 bias_initializer='zeros',\n                 kernel_regularizer=None,\n                 recurrent_regularizer=None,\n                 bias_regularizer=None,\n                 activity_regularizer=None,\n                 kernel_constraint=None,\n                 recurrent_constraint=None,\n                 bias_constraint=None,\n                 dropout=0.,\n                 recurrent_dropout=0.,\n                 **kwargs):\n        super(GRU, self).__init__(**kwargs)\n        self.units = units\n        self.activation = activations.get(activation)\n        self.recurrent_activation = activations.get(recurrent_activation)\n        self.use_bias = use_bias\n\n        self.kernel_initializer = initializers.get(kernel_initializer)\n        self.recurrent_initializer = initializers.get(recurrent_initializer)\n        self.bias_initializer = initializers.get(bias_initializer)\n\n        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n        self.recurrent_regularizer = regularizers.get(recurrent_regularizer)\n        self.bias_regularizer = regularizers.get(bias_regularizer)\n\n        self.kernel_constraint = constraints.get(kernel_constraint)\n        self.recurrent_constraint = constraints.get(recurrent_constraint)\n        self.bias_constraint = constraints.get(bias_constraint)\n\n        self.dropout = min(1., max(0., dropout))\n        self.recurrent_dropout = min(1., max(0., recurrent_dropout))\n\n    def build(self, input_shape):\n        if isinstance(input_shape, list):\n            input_shape = input_shape[0]\n\n        self.batch_size = input_shape[0]\n        self.input_dim = input_shape[2]\n\n        self.states = [None]\n        if self.stateful:\n            self.reset_states()\n\n        self.kernel = self.add_weight((self.input_dim, self.units * 3),\n                                      name='kernel',\n                                      initializer=self.kernel_initializer,\n                                      regularizer=self.kernel_regularizer,\n                                      constraint=self.kernel_constraint)\n        self.recurrent_kernel = self.add_weight(\n            (self.units, self.units * 3),\n            name='recurrent_kernel',\n            initializer=self.recurrent_initializer,\n            regularizer=self.recurrent_regularizer,\n            constraint=self.recurrent_constraint)\n\n        if self.use_bias:\n            self.bias = self.add_weight((self.units * 3,),\n                                        name='bias',\n                                        initializer='zero',\n                                        regularizer=self.bias_regularizer,\n                                        constraint=self.bias_constraint)\n        else:\n            self.bias = None\n\n        self.kernel_z = self.kernel[:, :self.units]\n        self.recurrent_kernel_z = self.recurrent_kernel[:, :self.units]\n        self.kernel_r = self.kernel[:, self.units: self.units * 2]\n        self.recurrent_kernel_r = self.recurrent_kernel[:,\n                                                        self.units:\n                                                        self.units * 2]\n        self.kernel_h = self.kernel[:, self.units * 2:]\n        self.recurrent_kernel_h = self.recurrent_kernel[:, self.units * 2:]\n\n        if self.use_bias:\n            self.bias_z = self.bias[:self.units]\n            self.bias_r = self.bias[self.units: self.units * 2]\n            self.bias_h = self.bias[self.units * 2:]\n        else:\n            self.bias_z = None\n            self.bias_r = None\n            self.bias_h = None\n        self.built = True\n\n    def preprocess_input(self, inputs, training=None):\n        if self.implementation == 0:\n            input_shape = K.int_shape(inputs)\n            input_dim = input_shape[2]\n            timesteps = input_shape[1]\n\n            x_z = _time_distributed_dense(inputs, self.kernel_z, self.bias_z,\n                                          self.dropout, input_dim, self.units,\n                                          timesteps, training=training)\n            x_r = _time_distributed_dense(inputs, self.kernel_r, self.bias_r,\n                                          self.dropout, input_dim, self.units,\n                                          timesteps, training=training)\n            x_h = _time_distributed_dense(inputs, self.kernel_h, self.bias_h,\n                                          self.dropout, input_dim, self.units,\n                                          timesteps, training=training)\n            return K.concatenate([x_z, x_r, x_h], axis=2)\n        else:\n            return inputs\n\n    def get_constants(self, inputs, training=None):\n        constants = []\n        if self.implementation == 0 and 0 < self.dropout < 1:\n            input_shape = K.int_shape(inputs)\n            input_dim = input_shape[-1]\n            ones = K.ones_like(K.reshape(inputs[:, 0, 0], (-1, 1)))\n            ones = K.tile(ones, (1, int(input_dim)))\n\n            def dropped_inputs():\n                return K.dropout(ones, self.dropout)\n\n            dp_mask = [K.in_train_phase(dropped_inputs,\n                                        ones,\n                                        training=training) for _ in range(3)]\n            constants.append(dp_mask)\n        else:\n            constants.append([K.cast_to_floatx(1.) for _ in range(3)])\n\n        if 0 < self.recurrent_dropout < 1:\n            ones = K.ones_like(K.reshape(inputs[:, 0, 0], (-1, 1)))\n            ones = K.tile(ones, (1, self.units))\n\n            def dropped_inputs():\n                return K.dropout(ones, self.recurrent_dropout)\n            rec_dp_mask = [K.in_train_phase(dropped_inputs,\n                                            ones,\n                                            training=training) for _ in range(3)]\n            constants.append(rec_dp_mask)\n        else:\n            constants.append([K.cast_to_floatx(1.) for _ in range(3)])\n        return constants\n\n    def step(self, inputs, states):\n        h_tm1 = states[0]  # previous memory\n        dp_mask = states[1]  # dropout matrices for recurrent units\n        rec_dp_mask = states[2]\n\n        if self.implementation == 1:\n            matrix_x = K.dot(inputs * dp_mask[0], self.kernel)\n            if self.use_bias:\n                matrix_x += self.bias\n            matrix_inner = K.dot(h_tm1 * rec_dp_mask[0],\n                                 self.recurrent_kernel[:, :2 * self.units])\n\n            x_z = matrix_x[:, :self.units]\n            x_r = matrix_x[:, self.units: 2 * self.units]\n            recurrent_z = matrix_inner[:, :self.units]\n            recurrent_r = matrix_inner[:, self.units: 2 * self.units]\n\n            z = self.recurrent_activation(x_z + recurrent_z)\n            r = self.recurrent_activation(x_r + recurrent_r)\n\n            x_h = matrix_x[:, 2 * self.units:]\n            recurrent_h = K.dot(r * h_tm1 * rec_dp_mask[0],\n                                self.recurrent_kernel[:, 2 * self.units:])\n            hh = self.activation(x_h + recurrent_h)\n        else:\n            if self.implementation == 0:\n                x_z = inputs[:, :self.units]\n                x_r = inputs[:, self.units: 2 * self.units]\n                x_h = inputs[:, 2 * self.units:]\n            elif self.implementation == 2:\n                x_z = K.dot(inputs * dp_mask[0], self.kernel_z)\n                x_r = K.dot(inputs * dp_mask[1], self.kernel_r)\n                x_h = K.dot(inputs * dp_mask[2], self.kernel_h)\n                if self.use_bias:\n                    x_z += self.bias_z\n                    x_r += self.bias_r\n                    x_h += self.bias_h\n            else:\n                raise ValueError('Unknown `implementation` mode.')\n            z = self.recurrent_activation(x_z + K.dot(h_tm1 * rec_dp_mask[0],\n                                                      self.recurrent_kernel_z))\n            r = self.recurrent_activation(x_r + K.dot(h_tm1 * rec_dp_mask[1],\n                                                      self.recurrent_kernel_r))\n\n            hh = self.activation(x_h + K.dot(r * h_tm1 * rec_dp_mask[2],\n                                             self.recurrent_kernel_h))\n        h = z * h_tm1 + (1 - z) * hh\n        if 0 < self.dropout + self.recurrent_dropout:\n            h._uses_learning_phase = True\n        return h, [h]\n\n    def get_config(self):\n        config = {'units': self.units,\n                  'activation': activations.serialize(self.activation),\n                  'recurrent_activation': activations.serialize(self.recurrent_activation),\n                  'use_bias': self.use_bias,\n                  'kernel_initializer': initializers.serialize(self.kernel_initializer),\n                  'recurrent_initializer': initializers.serialize(self.recurrent_initializer),\n                  'bias_initializer': initializers.serialize(self.bias_initializer),\n                  'kernel_regularizer': regularizers.serialize(self.kernel_regularizer),\n                  'recurrent_regularizer': regularizers.serialize(self.recurrent_regularizer),\n                  'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n                  'kernel_constraint': constraints.serialize(self.kernel_constraint),\n                  'recurrent_constraint': constraints.serialize(self.recurrent_constraint),\n                  'bias_constraint': constraints.serialize(self.bias_constraint),\n                  'dropout': self.dropout,\n                  'recurrent_dropout': self.recurrent_dropout}\n        base_config = super(GRU, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\n\nclass LSTM(Recurrent):\n    \"\"\"Long-Short Term Memory unit - Hochreiter 1997.\n\n    For a step-by-step description of the algorithm, see\n    [this tutorial](http://deeplearning.net/tutorial/lstm.html).\n\n    # Arguments\n        units: Positive integer, dimensionality of the output space.\n        activation: Activation function to use\n            (see [activations](../activations.md)).\n            If you don't specify anything, no activation is applied\n            (ie. \"linear\" activation: `a(x) = x`).\n        recurrent_activation: Activation function to use\n            for the recurrent step\n            (see [activations](../activations.md)).\n        use_bias: Boolean, whether the layer uses a bias vector.\n        kernel_initializer: Initializer for the `kernel` weights matrix,\n            used for the linear transformation of the inputs.\n            (see [initializers](../initializers.md)).\n        recurrent_initializer: Initializer for the `recurrent_kernel`\n            weights matrix,\n            used for the linear transformation of the recurrent state.\n            (see [initializers](../initializers.md)).\n        bias_initializer: Initializer for the bias vector\n            (see [initializers](../initializers.md)).\n        unit_forget_bias: Boolean.\n            If True, add 1 to the bias of the forget gate at initialization.\n            Use in combination with `bias_initializer=\"zeros\"`.\n            This is recommended in [Jozefowicz et al.](http://www.jmlr.org/proceedings/papers/v37/jozefowicz15.pdf)\n        kernel_regularizer: Regularizer function applied to\n            the `kernel` weights matrix\n            (see [regularizer](../regularizers.md)).\n        recurrent_regularizer: Regularizer function applied to\n            the `recurrent_kernel` weights matrix\n            (see [regularizer](../regularizers.md)).\n        bias_regularizer: Regularizer function applied to the bias vector\n            (see [regularizer](../regularizers.md)).\n        activity_regularizer: Regularizer function applied to\n            the output of the layer (its \"activation\").\n            (see [regularizer](../regularizers.md)).\n        kernel_constraint: Constraint function applied to\n            the `kernel` weights matrix\n            (see [constraints](../constraints.md)).\n        recurrent_constraint: Constraint function applied to\n            the `recurrent_kernel` weights matrix\n            (see [constraints](../constraints.md)).\n        bias_constraint: Constraint function applied to the bias vector\n            (see [constraints](../constraints.md)).\n        dropout: Float between 0 and 1.\n            Fraction of the units to drop for\n            the linear transformation of the inputs.\n        recurrent_dropout: Float between 0 and 1.\n            Fraction of the units to drop for\n            the linear transformation of the recurrent state.\n\n    # References\n        - [Long short-term memory](http://deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf) (original 1997 paper)\n        - [Learning to forget: Continual prediction with LSTM](http://www.mitpressjournals.org/doi/pdf/10.1162/089976600300015015)\n        - [Supervised sequence labeling with recurrent neural networks](http://www.cs.toronto.edu/~graves/preprint.pdf)\n        - [A Theoretically Grounded Application of Dropout in Recurrent Neural Networks](http://arxiv.org/abs/1512.05287)\n    \"\"\"\n\n    def __init__(self, units,\n                 activation='tanh',\n                 recurrent_activation='hard_sigmoid',\n                 use_bias=True,\n                 kernel_initializer='glorot_uniform',\n                 recurrent_initializer='orthogonal',\n                 bias_initializer='zeros',\n                 unit_forget_bias=True,\n                 kernel_regularizer=None,\n                 recurrent_regularizer=None,\n                 bias_regularizer=None,\n                 activity_regularizer=None,\n                 kernel_constraint=None,\n                 recurrent_constraint=None,\n                 bias_constraint=None,\n                 dropout=0.,\n                 recurrent_dropout=0.,\n                 **kwargs):\n        super(LSTM, self).__init__(**kwargs)\n        self.units = units\n        self.activation = activations.get(activation)\n        self.recurrent_activation = activations.get(recurrent_activation)\n        self.use_bias = use_bias\n\n        self.kernel_initializer = initializers.get(kernel_initializer)\n        self.recurrent_initializer = initializers.get(recurrent_initializer)\n        self.bias_initializer = initializers.get(bias_initializer)\n        self.unit_forget_bias = unit_forget_bias\n\n        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n        self.recurrent_regularizer = regularizers.get(recurrent_regularizer)\n        self.bias_regularizer = regularizers.get(bias_regularizer)\n\n        self.kernel_constraint = constraints.get(kernel_constraint)\n        self.recurrent_constraint = constraints.get(recurrent_constraint)\n        self.bias_constraint = constraints.get(bias_constraint)\n\n        self.dropout = min(1., max(0., dropout))\n        self.recurrent_dropout = min(1., max(0., recurrent_dropout))\n\n    def build(self, input_shape):\n        if isinstance(input_shape, list):\n            input_shape = input_shape[0]\n\n        self.batch_size = input_shape[0]\n        self.input_dim = input_shape[2]\n\n        self.states = [None, None]\n        if self.stateful:\n            self.reset_states()\n\n        self.kernel = self.add_weight((self.input_dim, self.units * 4),\n                                      name='kernel',\n                                      initializer=self.kernel_initializer,\n                                      regularizer=self.kernel_regularizer,\n                                      constraint=self.kernel_constraint)\n        self.recurrent_kernel = self.add_weight(\n            (self.units, self.units * 4),\n            name='recurrent_kernel',\n            initializer=self.recurrent_initializer,\n            regularizer=self.recurrent_regularizer,\n            constraint=self.recurrent_constraint)\n\n        if self.use_bias:\n            self.bias = self.add_weight((self.units * 4,),\n                                        name='bias',\n                                        initializer=self.bias_initializer,\n                                        regularizer=self.bias_regularizer,\n                                        constraint=self.bias_constraint)\n            if self.unit_forget_bias:\n                self.bias += K.concatenate([K.zeros((self.units,)),\n                                           K.ones((self.units,)),\n                                           K.zeros((self.units * 2,))])\n        else:\n            self.bias = None\n\n        self.kernel_i = self.kernel[:, :self.units]\n        self.recurrent_kernel_i = self.recurrent_kernel[:, :self.units]\n        self.kernel_f = self.kernel[:, self.units: self.units * 2]\n        self.recurrent_kernel_f = self.recurrent_kernel[:, self.units: self.units * 2]\n        self.kernel_c = self.kernel[:, self.units * 2: self.units * 3]\n        self.recurrent_kernel_c = self.recurrent_kernel[:, self.units * 2: self.units * 3]\n        self.kernel_o = self.kernel[:, self.units * 3:]\n        self.recurrent_kernel_o = self.recurrent_kernel[:, self.units * 3:]\n\n        if self.use_bias:\n            self.bias_i = self.bias[:self.units]\n            self.bias_f = self.bias[self.units: self.units * 2]\n            self.bias_c = self.bias[self.units * 2: self.units * 3]\n            self.bias_o = self.bias[self.units * 3:]\n        else:\n            self.bias_i = None\n            self.bias_f = None\n            self.bias_c = None\n            self.bias_o = None\n        self.built = True\n\n    def preprocess_input(self, inputs, training=None):\n        if self.implementation == 0:\n            input_shape = K.int_shape(inputs)\n            input_dim = input_shape[2]\n            timesteps = input_shape[1]\n\n            x_i = _time_distributed_dense(inputs, self.kernel_i, self.bias_i,\n                                          self.dropout, input_dim, self.units,\n                                          timesteps, training=training)\n            x_f = _time_distributed_dense(inputs, self.kernel_f, self.bias_f,\n                                          self.dropout, input_dim, self.units,\n                                          timesteps, training=training)\n            x_c = _time_distributed_dense(inputs, self.kernel_c, self.bias_c,\n                                          self.dropout, input_dim, self.units,\n                                          timesteps, training=training)\n            x_o = _time_distributed_dense(inputs, self.kernel_o, self.bias_o,\n                                          self.dropout, input_dim, self.units,\n                                          timesteps, training=training)\n            return K.concatenate([x_i, x_f, x_c, x_o], axis=2)\n        else:\n            return inputs\n\n    def get_constants(self, inputs, training=None):\n        constants = []\n        if self.implementation == 0 and 0 < self.dropout < 1:\n            input_shape = K.int_shape(inputs)\n            input_dim = input_shape[-1]\n            ones = K.ones_like(K.reshape(inputs[:, 0, 0], (-1, 1)))\n            ones = K.tile(ones, (1, int(input_dim)))\n\n            def dropped_inputs():\n                return K.dropout(ones, self.dropout)\n\n            dp_mask = [K.in_train_phase(dropped_inputs,\n                                        ones,\n                                        training=training) for _ in range(4)]\n            constants.append(dp_mask)\n        else:\n            constants.append([K.cast_to_floatx(1.) for _ in range(4)])\n\n        if 0 < self.recurrent_dropout < 1:\n            ones = K.ones_like(K.reshape(inputs[:, 0, 0], (-1, 1)))\n            ones = K.tile(ones, (1, self.units))\n\n            def dropped_inputs():\n                return K.dropout(ones, self.recurrent_dropout)\n            rec_dp_mask = [K.in_train_phase(dropped_inputs,\n                                            ones,\n                                            training=training) for _ in range(4)]\n            constants.append(rec_dp_mask)\n        else:\n            constants.append([K.cast_to_floatx(1.) for _ in range(4)])\n        return constants\n\n    def step(self, inputs, states):\n        h_tm1 = states[0]\n        c_tm1 = states[1]\n        dp_mask = states[2]\n        rec_dp_mask = states[3]\n\n        if self.implementation == 1:\n            z = K.dot(inputs * dp_mask[0], self.kernel)\n            z += K.dot(h_tm1 * rec_dp_mask[0], self.recurrent_kernel)\n            if self.use_bias:\n                z += self.bias\n\n            z0 = z[:, :self.units]\n            z1 = z[:, self.units: 2 * self.units]\n            z2 = z[:, 2 * self.units: 3 * self.units]\n            z3 = z[:, 3 * self.units:]\n\n            i = self.recurrent_activation(z0)\n            f = self.recurrent_activation(z1)\n            c = f * c_tm1 + i * self.activation(z2)\n            o = self.recurrent_activation(z3)\n        else:\n            if self.implementation == 0:\n                x_i = inputs[:, :self.units]\n                x_f = inputs[:, self.units: 2 * self.units]\n                x_c = inputs[:, 2 * self.units: 3 * self.units]\n                x_o = inputs[:, 3 * self.units:]\n            elif self.implementation == 2:\n                x_i = K.dot(inputs * dp_mask[0], self.kernel_i) + self.bias_i\n                x_f = K.dot(inputs * dp_mask[1], self.kernel_f) + self.bias_f\n                x_c = K.dot(inputs * dp_mask[2], self.kernel_c) + self.bias_c\n                x_o = K.dot(inputs * dp_mask[3], self.kernel_o) + self.bias_o\n            else:\n                raise ValueError('Unknown `implementation` mode.')\n\n            i = self.recurrent_activation(x_i + K.dot(h_tm1 * rec_dp_mask[0],\n                                                      self.recurrent_kernel_i))\n            f = self.recurrent_activation(x_f + K.dot(h_tm1 * rec_dp_mask[1],\n                                                      self.recurrent_kernel_f))\n            c = f * c_tm1 + i * self.activation(x_c + K.dot(h_tm1 * rec_dp_mask[2],\n                                                            self.recurrent_kernel_c))\n            o = self.recurrent_activation(x_o + K.dot(h_tm1 * rec_dp_mask[3],\n                                                      self.recurrent_kernel_o))\n        h = o * self.activation(c)\n        if 0 < self.dropout + self.recurrent_dropout:\n            h._uses_learning_phase = True\n        return h, [h, c]\n\n    def get_config(self):\n        config = {'units': self.units,\n                  'activation': activations.serialize(self.activation),\n                  'recurrent_activation': activations.serialize(self.recurrent_activation),\n                  'use_bias': self.use_bias,\n                  'kernel_initializer': initializers.serialize(self.kernel_initializer),\n                  'recurrent_initializer': initializers.serialize(self.recurrent_initializer),\n                  'bias_initializer': initializers.serialize(self.bias_initializer),\n                  'unit_forget_bias': self.unit_forget_bias,\n                  'kernel_regularizer': regularizers.serialize(self.kernel_regularizer),\n                  'recurrent_regularizer': regularizers.serialize(self.recurrent_regularizer),\n                  'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n                  'kernel_constraint': constraints.serialize(self.kernel_constraint),\n                  'recurrent_constraint': constraints.serialize(self.recurrent_constraint),\n                  'bias_constraint': constraints.serialize(self.bias_constraint),\n                  'dropout': self.dropout,\n                  'recurrent_dropout': self.recurrent_dropout}\n        base_config = super(LSTM, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n",
          "file_after": "# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import\nimport numpy as np\n\nfrom .. import backend as K\nfrom .. import activations\nfrom .. import initializers\nfrom .. import regularizers\nfrom .. import constraints\nfrom ..engine import Layer\nfrom ..engine import InputSpec\n\n\ndef _time_distributed_dense(x, w, b=None, dropout=None,\n                            input_dim=None, output_dim=None,\n                            timesteps=None, training=None):\n    \"\"\"Apply `y . w + b` for every temporal slice y of x.\n\n    # Arguments\n        x: input tensor.\n        w: weight matrix.\n        b: optional bias vector.\n        dropout: wether to apply dropout (same dropout mask\n            for every temporal slice of the input).\n        input_dim: integer; optional dimensionality of the input.\n        output_dim: integer; optional dimensionality of the output.\n        timesteps: integer; optional number of timesteps.\n        training: training phase tensor or boolean.\n\n    # Returns\n        Output tensor.\n    \"\"\"\n    if not input_dim:\n        input_dim = K.shape(x)[2]\n    if not timesteps:\n        timesteps = K.shape(x)[1]\n    if not output_dim:\n        output_dim = K.shape(w)[1]\n\n    if dropout is not None and 0. < dropout < 1.:\n        # apply the same dropout pattern at every timestep\n        ones = K.ones_like(K.reshape(x[:, 0, :], (-1, input_dim)))\n        dropout_matrix = K.dropout(ones, dropout)\n        expanded_dropout_matrix = K.repeat(dropout_matrix, timesteps)\n        x = K.in_train_phase(x * expanded_dropout_matrix, x, training=training)\n\n    # collapse time dimension and batch dimension together\n    x = K.reshape(x, (-1, input_dim))\n    x = K.dot(x, w)\n    if b is not None:\n        x += b\n    # reshape to 3D tensor\n    if K.backend() == 'tensorflow':\n        x = K.reshape(x, K.stack([-1, timesteps, output_dim]))\n        x.set_shape([None, None, output_dim])\n    else:\n        x = K.reshape(x, (-1, timesteps, output_dim))\n    return x\n\n\nclass Recurrent(Layer):\n    \"\"\"Abstract base class for recurrent layers.\n\n    Do not use in a model -- it's not a valid layer!\n    Use its children classes `LSTM`, `GRU` and `SimpleRNN` instead.\n\n    All recurrent layers (`LSTM`, `GRU`, `SimpleRNN`) also\n    follow the specifications of this class and accept\n    the keyword arguments listed below.\n\n    # Example\n\n    ```python\n        # as the first layer in a Sequential model\n        model = Sequential()\n        model.add(LSTM(32, input_shape=(10, 64)))\n        # now model.output_shape == (None, 32)\n        # note: `None` is the batch dimension.\n\n        # the following is identical:\n        model = Sequential()\n        model.add(LSTM(32, input_dim=64, input_length=10))\n\n        # for subsequent layers, not need to specify the input size:\n        model.add(LSTM(16))\n    ```\n\n    # Arguments\n        weights: list of Numpy arrays to set as initial weights.\n            The list should have 3 elements, of shapes:\n            `[(input_dim, output_dim), (output_dim, output_dim), (output_dim,)]`.\n        return_sequences: Boolean. Whether to return the last output\n            in the output sequence, or the full sequence.\n        go_backwards: Boolean (default False).\n            If True, process the input sequence backwards.\n        stateful: Boolean (default False). If True, the last state\n            for each sample at index i in a batch will be used as initial\n            state for the sample of index i in the following batch.\n        unroll: Boolean (default False).\n            If True, the network will be unrolled,\n            else a symbolic loop will be used.\n            Unrolling can speed-up a RNN,\n            although it tends to be more memory-intensive.\n            Unrolling is only suitable for short sequences.\n        implementation: one of {0, 1, or 2}.\n            If set to 0, the RNN will use\n            an implementation that uses fewer, larger matrix products,\n            thus running faster on CPU but consuming more memory.\n            If set to 1, the RNN will use more matrix products,\n            but smaller ones, thus running slower\n            (may actually be faster on GPU) while consuming less memory.\n            If set to 2 (LSTM/GRU only),\n            the RNN will combine the input gate,\n            the forget gate and the output gate into a single matrix,\n            enabling more time-efficient parallelization on the GPU.\n            Note: RNN dropout must be shared for all gates,\n            resulting in a slightly reduced regularization.\n        input_dim: dimensionality of the input (integer).\n            This argument (or alternatively, the keyword argument `input_shape`)\n            is required when using this layer as the first layer in a model.\n        input_length: Length of input sequences, to be specified\n            when it is constant.\n            This argument is required if you are going to connect\n            `Flatten` then `Dense` layers upstream\n            (without it, the shape of the dense outputs cannot be computed).\n            Note that if the recurrent layer is not the first layer\n            in your model, you would need to specify the input length\n            at the level of the first layer\n            (e.g. via the `input_shape` argument)\n\n    # Input shapes\n        3D tensor with shape `(batch_size, timesteps, input_dim)`,\n        (Optional) 2D tensors with shape `(batch_size, output_dim)`.\n\n    # Output shape\n        - if `return_sequences`: 3D tensor with shape\n            `(batch_size, timesteps, units)`.\n        - else, 2D tensor with shape `(batch_size, units)`.\n\n    # Masking\n        This layer supports masking for input data with a variable number\n        of timesteps. To introduce masks to your data,\n        use an [Embedding](embeddings.md) layer with the `mask_zero` parameter\n        set to `True`.\n\n    # Note on using statefulness in RNNs\n        You can set RNN layers to be 'stateful', which means that the states\n        computed for the samples in one batch will be reused as initial states\n        for the samples in the next batch. This assumes a one-to-one mapping\n        between samples in different successive batches.\n\n        To enable statefulness:\n            - specify `stateful=True` in the layer constructor.\n            - specify a fixed batch size for your model, by passing\n                if sequential model:\n                  `batch_input_shape=(...)` to the first layer in your model.\n                else for functional model with 1 or more Input layers:\n                  `batch_shape=(...)` to all the first layers in your model.\n                This is the expected shape of your inputs\n                *including the batch size*.\n                It should be a tuple of integers, e.g. `(32, 10, 100)`.\n            - specify `shuffle=False` when calling fit().\n\n        To reset the states of your model, call `.reset_states()` on either\n        a specific layer, or on your entire model.\n\n    # Note on specifying initial states in RNNs\n        You can specify the initial state of RNN layers by calling theme with\n        the keyword argument `initial_state`. The value of `initial_state`\n        should be a tensor or list of tensors representing the initial state\n        of the RNN layer.\n    \"\"\"\n\n    def __init__(self, return_sequences=False,\n                 go_backwards=False,\n                 stateful=False,\n                 unroll=False,\n                 implementation=0,\n                 **kwargs):\n        super(Recurrent, self).__init__(**kwargs)\n        self.return_sequences = return_sequences\n        self.go_backwards = go_backwards\n        self.stateful = stateful\n        self.unroll = unroll\n        self.implementation = 0\n        self.supports_masking = True\n        self.input_spec = InputSpec(ndim=3)\n        self.state_spec = None\n        self.dropout = 0\n        self.recurrent_dropout = 0\n\n    def compute_output_shape(self, input_shape):\n        if isinstance(input_shape, list):\n            input_shape = input_shape[0]\n        if self.return_sequences:\n            return (input_shape[0], input_shape[1], self.units)\n        else:\n            return (input_shape[0], self.units)\n\n    def compute_mask(self, inputs, mask):\n        if self.return_sequences:\n            return mask\n        else:\n            return None\n\n    def step(self, inputs, states):\n        raise NotImplementedError\n\n    def get_constants(self, inputs, training=None):\n        return []\n\n    def get_initial_states(self, inputs):\n        # build an all-zero tensor of shape (samples, output_dim)\n        initial_state = K.zeros_like(inputs)  # (samples, timesteps, input_dim)\n        initial_state = K.sum(initial_state, axis=(1, 2))  # (samples,)\n        initial_state = K.expand_dims(initial_state)  # (samples, 1)\n        initial_state = K.tile(initial_state, [1, self.units])  # (samples, output_dim)\n        initial_states = [initial_state for _ in range(len(self.states))]\n        return initial_states\n\n    def preprocess_input(self, inputs, training=None):\n        return inputs\n\n    def __call__(self, inputs, **kwargs):\n        with K.name_scope(self.name):\n            # Handle laying building (weight creating, input spec locking,\n            # state spec locking)\n            if not self.built:\n                # Raise exceptions in case the input is not compatible\n                # with the input_spec specified in the layer constructor.\n                self.assert_input_compatibility(inputs)\n\n                if hasattr(inputs, '_keras_shape'):\n                    input_shape = inputs._keras_shape\n                elif hasattr(K, 'int_shape'):\n                    input_shape = K.int_shape(inputs)\n                else:\n                    raise ValueError('You tried to call layer \"' + self.name +\n                                        '\". This layer has no information'\n                                        ' about its expected input shape, '\n                                        'and thus cannot be built. '\n                                        'You can build it manually via: '\n                                        '`layer.build(batch_input_shape)`')\n                self.build(input_shape)\n                self.built = True\n\n        # If initial_state is specified, add it to the inputs and temporarily\n        # modify the input spec to include the state\n        if 'initial_state' in kwargs:\n            # Compute the full input spec, including state\n            input_spec = self.input_spec\n            state_spec = self.state_spec\n            if not isinstance(state_spec, list):\n                state_spec = [state_spec]\n            self.input_spec = [input_spec] + state_spec\n\n            # Compute the full inputs, including state\n            initial_state = kwargs.pop('initial_state')\n            if not isinstance(initial_state, list):\n                initial_state = [initial_state]\n            inputs = [inputs] + initial_state\n\n            # Perform the call\n            output = super(Recurrent, self).__call__(inputs, **kwargs)\n\n            # Restore original input spec\n            self.input_spec = input_spec\n            return output\n\n        return super(Recurrent, self).__call__(inputs, **kwargs)\n\n    def call(self, inputs, mask=None, training=None):\n        # input shape: (nbias_samples, time (padded with zeros), input_dim)\n        # note that the .build() method of subclasses MUST define\n        # self.input_spec and self.state_spec with complete input shapes.\n        if isinstance(inputs, list):\n            initial_states = inputs[1:]\n            inputs = inputs[0]\n        elif self.stateful:\n            initial_states = self.states\n        else:\n            initial_states = self.get_initial_states(inputs)\n        input_shape = K.int_shape(inputs)\n        if self.unroll and input_shape[1] is None:\n            raise ValueError('Cannot unroll a RNN if the '\n                             'time dimension is undefined. \\n'\n                             '- If using a Sequential model, '\n                             'specify the time dimension by passing '\n                             'an `input_shape` or `batch_input_shape` '\n                             'argument to your first layer. If your '\n                             'first layer is an Embedding, you can '\n                             'also use the `input_length` argument.\\n'\n                             '- If using the functional API, specify '\n                             'the time dimension by passing a `shape` '\n                             'or `batch_shape` argument to your Input layer.')\n        constants = self.get_constants(inputs, training=None)\n        preprocessed_input = self.preprocess_input(inputs, training=None)\n        last_output, outputs, states = K.rnn(self.step,\n                                             preprocessed_input,\n                                             initial_states,\n                                             go_backwards=self.go_backwards,\n                                             mask=mask,\n                                             constants=constants,\n                                             unroll=self.unroll,\n                                             input_length=input_shape[1])\n        if self.stateful:\n            updates = []\n            for i in range(len(states)):\n                updates.append((self.states[i], states[i]))\n            self.add_update(updates, inputs)\n\n        # Properly set learning phase\n        if 0 < self.dropout + self.recurrent_dropout:\n            last_output._uses_learning_phase = True\n            outputs._uses_learning_phase = True\n\n        if self.return_sequences:\n            return outputs\n        else:\n            return last_output\n\n    def reset_states(self):\n        if not self.stateful:\n            raise AttributeError('Layer must be stateful.')\n        batch_size = self.input_spec.shape[0]\n        if not batch_size:\n            raise ValueError('If a RNN is stateful, it needs to know '\n                             'its batch size. Specify the batch size '\n                             'of your input tensors: \\n'\n                             '- If using a Sequential model, '\n                             'specify the batch size by passing '\n                             'a `batch_input_shape` '\n                             'argument to your first layer.\\n'\n                             '- If using the functional API, specify '\n                             'the time dimension by passing a '\n                             '`batch_shape` argument to your Input layer.')\n        if self.states[0] is not None:\n            for state in self.states:\n                K.set_value(state, np.zeros((batch_size, self.units)))\n        else:\n            self.states = [K.zeros((batch_size, self.units))\n                           for _ in self.states]\n\n    def get_config(self):\n        config = {'return_sequences': self.return_sequences,\n                  'go_backwards': self.go_backwards,\n                  'stateful': self.stateful,\n                  'unroll': self.unroll,\n                  'implementation': self.implementation}\n        base_config = super(Recurrent, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\n\nclass SimpleRNN(Recurrent):\n    \"\"\"Fully-connected RNN where the output is to be fed back to input.\n\n    # Arguments\n        units: Positive integer, dimensionality of the output space.\n        activation: Activation function to use\n            (see [activations](../activations.md)).\n            If you don't specify anything, no activation is applied\n            (ie. \"linear\" activation: `a(x) = x`).\n        use_bias: Boolean, whether the layer uses a bias vector.\n        kernel_initializer: Initializer for the `kernel` weights matrix,\n            used for the linear transformation of the inputs.\n            (see [initializers](../initializers.md)).\n        recurrent_initializer: Initializer for the `recurrent_kernel`\n            weights matrix,\n            used for the linear transformation of the recurrent state.\n            (see [initializers](../initializers.md)).\n        bias_initializer: Initializer for the bias vector\n            (see [initializers](../initializers.md)).\n        kernel_regularizer: Regularizer function applied to\n            the `kernel` weights matrix\n            (see [regularizer](../regularizers.md)).\n        recurrent_regularizer: Regularizer function applied to\n            the `recurrent_kernel` weights matrix\n            (see [regularizer](../regularizers.md)).\n        bias_regularizer: Regularizer function applied to the bias vector\n            (see [regularizer](../regularizers.md)).\n        activity_regularizer: Regularizer function applied to\n            the output of the layer (its \"activation\").\n            (see [regularizer](../regularizers.md)).\n        kernel_constraint: Constraint function applied to\n            the `kernel` weights matrix\n            (see [constraints](../constraints.md)).\n        recurrent_constraint: Constraint function applied to\n            the `recurrent_kernel` weights matrix\n            (see [constraints](../constraints.md)).\n        bias_constraint: Constraint function applied to the bias vector\n            (see [constraints](../constraints.md)).\n        dropout: Float between 0 and 1.\n            Fraction of the units to drop for\n            the linear transformation of the inputs.\n        recurrent_dropout: Float between 0 and 1.\n            Fraction of the units to drop for\n            the linear transformation of the recurrent state.\n\n    # References\n        - [A Theoretically Grounded Application of Dropout in Recurrent Neural Networks](http://arxiv.org/abs/1512.05287)\n    \"\"\"\n\n    def __init__(self, units,\n                 activation='tanh',\n                 use_bias=True,\n                 kernel_initializer='glorot_uniform',\n                 recurrent_initializer='orthogonal',\n                 bias_initializer='zeros',\n                 kernel_regularizer=None,\n                 recurrent_regularizer=None,\n                 bias_regularizer=None,\n                 activity_regularizer=None,\n                 kernel_constraint=None,\n                 recurrent_constraint=None,\n                 bias_constraint=None,\n                 dropout=0.,\n                 recurrent_dropout=0.,\n                 **kwargs):\n        super(SimpleRNN, self).__init__(**kwargs)\n        self.units = units\n        self.activation = activations.get(activation)\n        self.use_bias = use_bias\n\n        self.kernel_initializer = initializers.get(kernel_initializer)\n        self.recurrent_initializer = initializers.get(recurrent_initializer)\n        self.bias_initializer = initializers.get(bias_initializer)\n\n        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n        self.recurrent_regularizer = regularizers.get(recurrent_regularizer)\n        self.bias_regularizer = regularizers.get(bias_regularizer)\n\n        self.kernel_constraint = constraints.get(kernel_constraint)\n        self.recurrent_constraint = constraints.get(recurrent_constraint)\n        self.bias_constraint = constraints.get(bias_constraint)\n\n        self.dropout = min(1., max(0., dropout))\n        self.recurrent_dropout = min(1., max(0., recurrent_dropout))\n\n    def build(self, input_shape):\n        if isinstance(input_shape, list):\n            input_shape = input_shape[0]\n\n        batch_size = input_shape[0] if self.stateful else None\n        self.input_dim = input_shape[2]\n        self.input_spec = InputSpec(shape=(batch_size, None, self.input_dim))\n        self.state_spec = InputSpec(shape=(batch_size, self.units))\n\n        self.states = [None]\n        if self.stateful:\n            self.reset_states()\n\n        self.kernel = self.add_weight((self.input_dim, self.units),\n                                      name='kernel',\n                                      initializer=self.kernel_initializer,\n                                      regularizer=self.kernel_regularizer,\n                                      constraint=self.kernel_constraint)\n        self.recurrent_kernel = self.add_weight(\n            (self.units, self.units),\n            name='recurrent_kernel',\n            initializer=self.recurrent_initializer,\n            regularizer=self.recurrent_regularizer,\n            constraint=self.recurrent_constraint)\n        if self.use_bias:\n            self.bias = self.add_weight((self.units,),\n                                        name='bias',\n                                        initializer=self.bias_initializer,\n                                        regularizer=self.bias_regularizer,\n                                        constraint=self.bias_constraint)\n        else:\n            self.bias = None\n        self.built = True\n\n    def preprocess_input(self, inputs, training=None):\n        if self.implementation > 0:\n            return inputs\n        else:\n            input_shape = K.int_shape(inputs)\n            input_dim = input_shape[2]\n            timesteps = input_shape[1]\n            return _time_distributed_dense(inputs,\n                                           self.kernel,\n                                           self.bias,\n                                           self.dropout,\n                                           input_dim,\n                                           self.units,\n                                           timesteps,\n                                           training=training)\n\n    def step(self, inputs, states):\n        if self.implementation == 0:\n            h = inputs\n        else:\n            if 0 < self.dropout < 1:\n                h = K.dot(inputs * states[1], self.kernel)\n            else:\n                h = K.dot(inputs, self.kernel)\n            if self.bias is not None:\n                h += self.bias\n\n        prev_output = states[0]\n        if 0 < self.recurrent_dropout < 1:\n            prev_output *= states[2]\n        output = h + K.dot(prev_output, self.recurrent_kernel)\n        if self.activation is not None:\n            output = self.activation(output)\n\n        # Properly set learning phase on output tensor.\n        if 0 < self.dropout + self.recurrent_dropout:\n            output._uses_learning_phase = True\n        return output, [output]\n\n    def get_constants(self, inputs, training=None):\n        constants = []\n        if self.implementation == 0 and 0 < self.dropout < 1:\n            input_shape = K.int_shape(inputs)\n            input_dim = input_shape[-1]\n            ones = K.ones_like(K.reshape(inputs[:, 0, 0], (-1, 1)))\n            ones = K.tile(ones, (1, int(input_dim)))\n\n            def dropped_inputs():\n                return K.dropout(ones, self.dropout)\n\n            dp_mask = K.in_train_phase(dropped_inputs,\n                                       ones,\n                                       training=training)\n            constants.append(dp_mask)\n        else:\n            constants.append(K.cast_to_floatx(1.))\n\n        if 0 < self.recurrent_dropout < 1:\n            ones = K.ones_like(K.reshape(inputs[:, 0, 0], (-1, 1)))\n            ones = K.tile(ones, (1, self.units))\n\n            def dropped_inputs():\n                return K.dropout(ones, self.recurrent_dropout)\n            rec_dp_mask = K.in_train_phase(dropped_inputs,\n                                           ones,\n                                           training=training)\n            constants.append(rec_dp_mask)\n        else:\n            constants.append(K.cast_to_floatx(1.))\n        return constants\n\n    def get_config(self):\n        config = {'units': self.units,\n                  'activation': activations.serialize(self.activation),\n                  'use_bias': self.use_bias,\n                  'kernel_initializer': initializers.serialize(self.kernel_initializer),\n                  'recurrent_initializer': initializers.serialize(self.recurrent_initializer),\n                  'bias_initializer': initializers.serialize(self.bias_initializer),\n                  'kernel_regularizer': regularizers.serialize(self.kernel_regularizer),\n                  'recurrent_regularizer': regularizers.serialize(self.recurrent_regularizer),\n                  'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n                  'kernel_constraint': constraints.serialize(self.kernel_constraint),\n                  'recurrent_constraint': constraints.serialize(self.recurrent_constraint),\n                  'bias_constraint': constraints.serialize(self.bias_constraint),\n                  'dropout': self.dropout,\n                  'recurrent_dropout': self.recurrent_dropout}\n        base_config = super(SimpleRNN, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\n\nclass GRU(Recurrent):\n    \"\"\"Gated Recurrent Unit - Cho et al. 2014.\n\n    # Arguments\n        units: Positive integer, dimensionality of the output space.\n        activation: Activation function to use\n            (see [activations](../activations.md)).\n            If you don't specify anything, no activation is applied\n            (ie. \"linear\" activation: `a(x) = x`).\n        recurrent_activation: Activation function to use\n            for the recurrent step\n            (see [activations](../activations.md)).\n        use_bias: Boolean, whether the layer uses a bias vector.\n        kernel_initializer: Initializer for the `kernel` weights matrix,\n            used for the linear transformation of the inputs.\n            (see [initializers](../initializers.md)).\n        recurrent_initializer: Initializer for the `recurrent_kernel`\n            weights matrix,\n            used for the linear transformation of the recurrent state.\n            (see [initializers](../initializers.md)).\n        bias_initializer: Initializer for the bias vector\n            (see [initializers](../initializers.md)).\n        kernel_regularizer: Regularizer function applied to\n            the `kernel` weights matrix\n            (see [regularizer](../regularizers.md)).\n        recurrent_regularizer: Regularizer function applied to\n            the `recurrent_kernel` weights matrix\n            (see [regularizer](../regularizers.md)).\n        bias_regularizer: Regularizer function applied to the bias vector\n            (see [regularizer](../regularizers.md)).\n        activity_regularizer: Regularizer function applied to\n            the output of the layer (its \"activation\").\n            (see [regularizer](../regularizers.md)).\n        kernel_constraint: Constraint function applied to\n            the `kernel` weights matrix\n            (see [constraints](../constraints.md)).\n        recurrent_constraint: Constraint function applied to\n            the `recurrent_kernel` weights matrix\n            (see [constraints](../constraints.md)).\n        bias_constraint: Constraint function applied to the bias vector\n            (see [constraints](../constraints.md)).\n        dropout: Float between 0 and 1.\n            Fraction of the units to drop for\n            the linear transformation of the inputs.\n        recurrent_dropout: Float between 0 and 1.\n            Fraction of the units to drop for\n            the linear transformation of the recurrent state.\n\n    # References\n        - [On the Properties of Neural Machine Translation: Encoder-Decoder Approaches](https://arxiv.org/abs/1409.1259)\n        - [Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling](http://arxiv.org/abs/1412.3555v1)\n        - [A Theoretically Grounded Application of Dropout in Recurrent Neural Networks](http://arxiv.org/abs/1512.05287)\n    \"\"\"\n\n    def __init__(self, units,\n                 activation='tanh',\n                 recurrent_activation='hard_sigmoid',\n                 use_bias=True,\n                 kernel_initializer='glorot_uniform',\n                 recurrent_initializer='orthogonal',\n                 bias_initializer='zeros',\n                 kernel_regularizer=None,\n                 recurrent_regularizer=None,\n                 bias_regularizer=None,\n                 activity_regularizer=None,\n                 kernel_constraint=None,\n                 recurrent_constraint=None,\n                 bias_constraint=None,\n                 dropout=0.,\n                 recurrent_dropout=0.,\n                 **kwargs):\n        super(GRU, self).__init__(**kwargs)\n        self.units = units\n        self.activation = activations.get(activation)\n        self.recurrent_activation = activations.get(recurrent_activation)\n        self.use_bias = use_bias\n\n        self.kernel_initializer = initializers.get(kernel_initializer)\n        self.recurrent_initializer = initializers.get(recurrent_initializer)\n        self.bias_initializer = initializers.get(bias_initializer)\n\n        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n        self.recurrent_regularizer = regularizers.get(recurrent_regularizer)\n        self.bias_regularizer = regularizers.get(bias_regularizer)\n\n        self.kernel_constraint = constraints.get(kernel_constraint)\n        self.recurrent_constraint = constraints.get(recurrent_constraint)\n        self.bias_constraint = constraints.get(bias_constraint)\n\n        self.dropout = min(1., max(0., dropout))\n        self.recurrent_dropout = min(1., max(0., recurrent_dropout))\n\n    def build(self, input_shape):\n        if isinstance(input_shape, list):\n            input_shape = input_shape[0]\n\n        batch_size = input_shape[0] if self.stateful else None\n        self.input_dim = input_shape[2]\n        self.input_spec = InputSpec(shape=(batch_size, None, self.input_dim))\n        self.state_spec = InputSpec(shape=(batch_size, self.units))\n\n        self.states = [None]\n        if self.stateful:\n            self.reset_states()\n\n        self.kernel = self.add_weight((self.input_dim, self.units * 3),\n                                      name='kernel',\n                                      initializer=self.kernel_initializer,\n                                      regularizer=self.kernel_regularizer,\n                                      constraint=self.kernel_constraint)\n        self.recurrent_kernel = self.add_weight(\n            (self.units, self.units * 3),\n            name='recurrent_kernel',\n            initializer=self.recurrent_initializer,\n            regularizer=self.recurrent_regularizer,\n            constraint=self.recurrent_constraint)\n\n        if self.use_bias:\n            self.bias = self.add_weight((self.units * 3,),\n                                        name='bias',\n                                        initializer='zero',\n                                        regularizer=self.bias_regularizer,\n                                        constraint=self.bias_constraint)\n        else:\n            self.bias = None\n\n        self.kernel_z = self.kernel[:, :self.units]\n        self.recurrent_kernel_z = self.recurrent_kernel[:, :self.units]\n        self.kernel_r = self.kernel[:, self.units: self.units * 2]\n        self.recurrent_kernel_r = self.recurrent_kernel[:,\n                                                        self.units:\n                                                        self.units * 2]\n        self.kernel_h = self.kernel[:, self.units * 2:]\n        self.recurrent_kernel_h = self.recurrent_kernel[:, self.units * 2:]\n\n        if self.use_bias:\n            self.bias_z = self.bias[:self.units]\n            self.bias_r = self.bias[self.units: self.units * 2]\n            self.bias_h = self.bias[self.units * 2:]\n        else:\n            self.bias_z = None\n            self.bias_r = None\n            self.bias_h = None\n        self.built = True\n\n    def preprocess_input(self, inputs, training=None):\n        if self.implementation == 0:\n            input_shape = K.int_shape(inputs)\n            input_dim = input_shape[2]\n            timesteps = input_shape[1]\n\n            x_z = _time_distributed_dense(inputs, self.kernel_z, self.bias_z,\n                                          self.dropout, input_dim, self.units,\n                                          timesteps, training=training)\n            x_r = _time_distributed_dense(inputs, self.kernel_r, self.bias_r,\n                                          self.dropout, input_dim, self.units,\n                                          timesteps, training=training)\n            x_h = _time_distributed_dense(inputs, self.kernel_h, self.bias_h,\n                                          self.dropout, input_dim, self.units,\n                                          timesteps, training=training)\n            return K.concatenate([x_z, x_r, x_h], axis=2)\n        else:\n            return inputs\n\n    def get_constants(self, inputs, training=None):\n        constants = []\n        if self.implementation == 0 and 0 < self.dropout < 1:\n            input_shape = K.int_shape(inputs)\n            input_dim = input_shape[-1]\n            ones = K.ones_like(K.reshape(inputs[:, 0, 0], (-1, 1)))\n            ones = K.tile(ones, (1, int(input_dim)))\n\n            def dropped_inputs():\n                return K.dropout(ones, self.dropout)\n\n            dp_mask = [K.in_train_phase(dropped_inputs,\n                                        ones,\n                                        training=training) for _ in range(3)]\n            constants.append(dp_mask)\n        else:\n            constants.append([K.cast_to_floatx(1.) for _ in range(3)])\n\n        if 0 < self.recurrent_dropout < 1:\n            ones = K.ones_like(K.reshape(inputs[:, 0, 0], (-1, 1)))\n            ones = K.tile(ones, (1, self.units))\n\n            def dropped_inputs():\n                return K.dropout(ones, self.recurrent_dropout)\n            rec_dp_mask = [K.in_train_phase(dropped_inputs,\n                                            ones,\n                                            training=training) for _ in range(3)]\n            constants.append(rec_dp_mask)\n        else:\n            constants.append([K.cast_to_floatx(1.) for _ in range(3)])\n        return constants\n\n    def step(self, inputs, states):\n        h_tm1 = states[0]  # previous memory\n        dp_mask = states[1]  # dropout matrices for recurrent units\n        rec_dp_mask = states[2]\n\n        if self.implementation == 1:\n            matrix_x = K.dot(inputs * dp_mask[0], self.kernel)\n            if self.use_bias:\n                matrix_x += self.bias\n            matrix_inner = K.dot(h_tm1 * rec_dp_mask[0],\n                                 self.recurrent_kernel[:, :2 * self.units])\n\n            x_z = matrix_x[:, :self.units]\n            x_r = matrix_x[:, self.units: 2 * self.units]\n            recurrent_z = matrix_inner[:, :self.units]\n            recurrent_r = matrix_inner[:, self.units: 2 * self.units]\n\n            z = self.recurrent_activation(x_z + recurrent_z)\n            r = self.recurrent_activation(x_r + recurrent_r)\n\n            x_h = matrix_x[:, 2 * self.units:]\n            recurrent_h = K.dot(r * h_tm1 * rec_dp_mask[0],\n                                self.recurrent_kernel[:, 2 * self.units:])\n            hh = self.activation(x_h + recurrent_h)\n        else:\n            if self.implementation == 0:\n                x_z = inputs[:, :self.units]\n                x_r = inputs[:, self.units: 2 * self.units]\n                x_h = inputs[:, 2 * self.units:]\n            elif self.implementation == 2:\n                x_z = K.dot(inputs * dp_mask[0], self.kernel_z)\n                x_r = K.dot(inputs * dp_mask[1], self.kernel_r)\n                x_h = K.dot(inputs * dp_mask[2], self.kernel_h)\n                if self.use_bias:\n                    x_z += self.bias_z\n                    x_r += self.bias_r\n                    x_h += self.bias_h\n            else:\n                raise ValueError('Unknown `implementation` mode.')\n            z = self.recurrent_activation(x_z + K.dot(h_tm1 * rec_dp_mask[0],\n                                                      self.recurrent_kernel_z))\n            r = self.recurrent_activation(x_r + K.dot(h_tm1 * rec_dp_mask[1],\n                                                      self.recurrent_kernel_r))\n\n            hh = self.activation(x_h + K.dot(r * h_tm1 * rec_dp_mask[2],\n                                             self.recurrent_kernel_h))\n        h = z * h_tm1 + (1 - z) * hh\n        if 0 < self.dropout + self.recurrent_dropout:\n            h._uses_learning_phase = True\n        return h, [h]\n\n    def get_config(self):\n        config = {'units': self.units,\n                  'activation': activations.serialize(self.activation),\n                  'recurrent_activation': activations.serialize(self.recurrent_activation),\n                  'use_bias': self.use_bias,\n                  'kernel_initializer': initializers.serialize(self.kernel_initializer),\n                  'recurrent_initializer': initializers.serialize(self.recurrent_initializer),\n                  'bias_initializer': initializers.serialize(self.bias_initializer),\n                  'kernel_regularizer': regularizers.serialize(self.kernel_regularizer),\n                  'recurrent_regularizer': regularizers.serialize(self.recurrent_regularizer),\n                  'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n                  'kernel_constraint': constraints.serialize(self.kernel_constraint),\n                  'recurrent_constraint': constraints.serialize(self.recurrent_constraint),\n                  'bias_constraint': constraints.serialize(self.bias_constraint),\n                  'dropout': self.dropout,\n                  'recurrent_dropout': self.recurrent_dropout}\n        base_config = super(GRU, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\n\nclass LSTM(Recurrent):\n    \"\"\"Long-Short Term Memory unit - Hochreiter 1997.\n\n    For a step-by-step description of the algorithm, see\n    [this tutorial](http://deeplearning.net/tutorial/lstm.html).\n\n    # Arguments\n        units: Positive integer, dimensionality of the output space.\n        activation: Activation function to use\n            (see [activations](../activations.md)).\n            If you don't specify anything, no activation is applied\n            (ie. \"linear\" activation: `a(x) = x`).\n        recurrent_activation: Activation function to use\n            for the recurrent step\n            (see [activations](../activations.md)).\n        use_bias: Boolean, whether the layer uses a bias vector.\n        kernel_initializer: Initializer for the `kernel` weights matrix,\n            used for the linear transformation of the inputs.\n            (see [initializers](../initializers.md)).\n        recurrent_initializer: Initializer for the `recurrent_kernel`\n            weights matrix,\n            used for the linear transformation of the recurrent state.\n            (see [initializers](../initializers.md)).\n        bias_initializer: Initializer for the bias vector\n            (see [initializers](../initializers.md)).\n        unit_forget_bias: Boolean.\n            If True, add 1 to the bias of the forget gate at initialization.\n            Use in combination with `bias_initializer=\"zeros\"`.\n            This is recommended in [Jozefowicz et al.](http://www.jmlr.org/proceedings/papers/v37/jozefowicz15.pdf)\n        kernel_regularizer: Regularizer function applied to\n            the `kernel` weights matrix\n            (see [regularizer](../regularizers.md)).\n        recurrent_regularizer: Regularizer function applied to\n            the `recurrent_kernel` weights matrix\n            (see [regularizer](../regularizers.md)).\n        bias_regularizer: Regularizer function applied to the bias vector\n            (see [regularizer](../regularizers.md)).\n        activity_regularizer: Regularizer function applied to\n            the output of the layer (its \"activation\").\n            (see [regularizer](../regularizers.md)).\n        kernel_constraint: Constraint function applied to\n            the `kernel` weights matrix\n            (see [constraints](../constraints.md)).\n        recurrent_constraint: Constraint function applied to\n            the `recurrent_kernel` weights matrix\n            (see [constraints](../constraints.md)).\n        bias_constraint: Constraint function applied to the bias vector\n            (see [constraints](../constraints.md)).\n        dropout: Float between 0 and 1.\n            Fraction of the units to drop for\n            the linear transformation of the inputs.\n        recurrent_dropout: Float between 0 and 1.\n            Fraction of the units to drop for\n            the linear transformation of the recurrent state.\n\n    # References\n        - [Long short-term memory](http://deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf) (original 1997 paper)\n        - [Learning to forget: Continual prediction with LSTM](http://www.mitpressjournals.org/doi/pdf/10.1162/089976600300015015)\n        - [Supervised sequence labeling with recurrent neural networks](http://www.cs.toronto.edu/~graves/preprint.pdf)\n        - [A Theoretically Grounded Application of Dropout in Recurrent Neural Networks](http://arxiv.org/abs/1512.05287)\n    \"\"\"\n\n    def __init__(self, units,\n                 activation='tanh',\n                 recurrent_activation='hard_sigmoid',\n                 use_bias=True,\n                 kernel_initializer='glorot_uniform',\n                 recurrent_initializer='orthogonal',\n                 bias_initializer='zeros',\n                 unit_forget_bias=True,\n                 kernel_regularizer=None,\n                 recurrent_regularizer=None,\n                 bias_regularizer=None,\n                 activity_regularizer=None,\n                 kernel_constraint=None,\n                 recurrent_constraint=None,\n                 bias_constraint=None,\n                 dropout=0.,\n                 recurrent_dropout=0.,\n                 **kwargs):\n        super(LSTM, self).__init__(**kwargs)\n        self.units = units\n        self.activation = activations.get(activation)\n        self.recurrent_activation = activations.get(recurrent_activation)\n        self.use_bias = use_bias\n\n        self.kernel_initializer = initializers.get(kernel_initializer)\n        self.recurrent_initializer = initializers.get(recurrent_initializer)\n        self.bias_initializer = initializers.get(bias_initializer)\n        self.unit_forget_bias = unit_forget_bias\n\n        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n        self.recurrent_regularizer = regularizers.get(recurrent_regularizer)\n        self.bias_regularizer = regularizers.get(bias_regularizer)\n\n        self.kernel_constraint = constraints.get(kernel_constraint)\n        self.recurrent_constraint = constraints.get(recurrent_constraint)\n        self.bias_constraint = constraints.get(bias_constraint)\n\n        self.dropout = min(1., max(0., dropout))\n        self.recurrent_dropout = min(1., max(0., recurrent_dropout))\n\n    def build(self, input_shape):\n        if isinstance(input_shape, list):\n            input_shape = input_shape[0]\n\n        batch_size = input_shape[0] if self.stateful else None\n        self.input_dim = input_shape[2]\n        self.input_spec = InputSpec(shape=(batch_size, None, self.input_dim))\n        self.state_spec = [InputSpec(shape=(batch_size, self.units)),\n                           InputSpec(shape=(batch_size, self.units))]\n\n        self.states = [None, None]\n        if self.stateful:\n            self.reset_states()\n\n        self.kernel = self.add_weight((self.input_dim, self.units * 4),\n                                      name='kernel',\n                                      initializer=self.kernel_initializer,\n                                      regularizer=self.kernel_regularizer,\n                                      constraint=self.kernel_constraint)\n        self.recurrent_kernel = self.add_weight(\n            (self.units, self.units * 4),\n            name='recurrent_kernel',\n            initializer=self.recurrent_initializer,\n            regularizer=self.recurrent_regularizer,\n            constraint=self.recurrent_constraint)\n\n        if self.use_bias:\n            self.bias = self.add_weight((self.units * 4,),\n                                        name='bias',\n                                        initializer=self.bias_initializer,\n                                        regularizer=self.bias_regularizer,\n                                        constraint=self.bias_constraint)\n            if self.unit_forget_bias:\n                self.bias += K.concatenate([K.zeros((self.units,)),\n                                           K.ones((self.units,)),\n                                           K.zeros((self.units * 2,))])\n        else:\n            self.bias = None\n\n        self.kernel_i = self.kernel[:, :self.units]\n        self.recurrent_kernel_i = self.recurrent_kernel[:, :self.units]\n        self.kernel_f = self.kernel[:, self.units: self.units * 2]\n        self.recurrent_kernel_f = self.recurrent_kernel[:, self.units: self.units * 2]\n        self.kernel_c = self.kernel[:, self.units * 2: self.units * 3]\n        self.recurrent_kernel_c = self.recurrent_kernel[:, self.units * 2: self.units * 3]\n        self.kernel_o = self.kernel[:, self.units * 3:]\n        self.recurrent_kernel_o = self.recurrent_kernel[:, self.units * 3:]\n\n        if self.use_bias:\n            self.bias_i = self.bias[:self.units]\n            self.bias_f = self.bias[self.units: self.units * 2]\n            self.bias_c = self.bias[self.units * 2: self.units * 3]\n            self.bias_o = self.bias[self.units * 3:]\n        else:\n            self.bias_i = None\n            self.bias_f = None\n            self.bias_c = None\n            self.bias_o = None\n        self.built = True\n\n    def preprocess_input(self, inputs, training=None):\n        if self.implementation == 0:\n            input_shape = K.int_shape(inputs)\n            input_dim = input_shape[2]\n            timesteps = input_shape[1]\n\n            x_i = _time_distributed_dense(inputs, self.kernel_i, self.bias_i,\n                                          self.dropout, input_dim, self.units,\n                                          timesteps, training=training)\n            x_f = _time_distributed_dense(inputs, self.kernel_f, self.bias_f,\n                                          self.dropout, input_dim, self.units,\n                                          timesteps, training=training)\n            x_c = _time_distributed_dense(inputs, self.kernel_c, self.bias_c,\n                                          self.dropout, input_dim, self.units,\n                                          timesteps, training=training)\n            x_o = _time_distributed_dense(inputs, self.kernel_o, self.bias_o,\n                                          self.dropout, input_dim, self.units,\n                                          timesteps, training=training)\n            return K.concatenate([x_i, x_f, x_c, x_o], axis=2)\n        else:\n            return inputs\n\n    def get_constants(self, inputs, training=None):\n        constants = []\n        if self.implementation == 0 and 0 < self.dropout < 1:\n            input_shape = K.int_shape(inputs)\n            input_dim = input_shape[-1]\n            ones = K.ones_like(K.reshape(inputs[:, 0, 0], (-1, 1)))\n            ones = K.tile(ones, (1, int(input_dim)))\n\n            def dropped_inputs():\n                return K.dropout(ones, self.dropout)\n\n            dp_mask = [K.in_train_phase(dropped_inputs,\n                                        ones,\n                                        training=training) for _ in range(4)]\n            constants.append(dp_mask)\n        else:\n            constants.append([K.cast_to_floatx(1.) for _ in range(4)])\n\n        if 0 < self.recurrent_dropout < 1:\n            ones = K.ones_like(K.reshape(inputs[:, 0, 0], (-1, 1)))\n            ones = K.tile(ones, (1, self.units))\n\n            def dropped_inputs():\n                return K.dropout(ones, self.recurrent_dropout)\n            rec_dp_mask = [K.in_train_phase(dropped_inputs,\n                                            ones,\n                                            training=training) for _ in range(4)]\n            constants.append(rec_dp_mask)\n        else:\n            constants.append([K.cast_to_floatx(1.) for _ in range(4)])\n        return constants\n\n    def step(self, inputs, states):\n        h_tm1 = states[0]\n        c_tm1 = states[1]\n        dp_mask = states[2]\n        rec_dp_mask = states[3]\n\n        if self.implementation == 1:\n            z = K.dot(inputs * dp_mask[0], self.kernel)\n            z += K.dot(h_tm1 * rec_dp_mask[0], self.recurrent_kernel)\n            if self.use_bias:\n                z += self.bias\n\n            z0 = z[:, :self.units]\n            z1 = z[:, self.units: 2 * self.units]\n            z2 = z[:, 2 * self.units: 3 * self.units]\n            z3 = z[:, 3 * self.units:]\n\n            i = self.recurrent_activation(z0)\n            f = self.recurrent_activation(z1)\n            c = f * c_tm1 + i * self.activation(z2)\n            o = self.recurrent_activation(z3)\n        else:\n            if self.implementation == 0:\n                x_i = inputs[:, :self.units]\n                x_f = inputs[:, self.units: 2 * self.units]\n                x_c = inputs[:, 2 * self.units: 3 * self.units]\n                x_o = inputs[:, 3 * self.units:]\n            elif self.implementation == 2:\n                x_i = K.dot(inputs * dp_mask[0], self.kernel_i) + self.bias_i\n                x_f = K.dot(inputs * dp_mask[1], self.kernel_f) + self.bias_f\n                x_c = K.dot(inputs * dp_mask[2], self.kernel_c) + self.bias_c\n                x_o = K.dot(inputs * dp_mask[3], self.kernel_o) + self.bias_o\n            else:\n                raise ValueError('Unknown `implementation` mode.')\n\n            i = self.recurrent_activation(x_i + K.dot(h_tm1 * rec_dp_mask[0],\n                                                      self.recurrent_kernel_i))\n            f = self.recurrent_activation(x_f + K.dot(h_tm1 * rec_dp_mask[1],\n                                                      self.recurrent_kernel_f))\n            c = f * c_tm1 + i * self.activation(x_c + K.dot(h_tm1 * rec_dp_mask[2],\n                                                            self.recurrent_kernel_c))\n            o = self.recurrent_activation(x_o + K.dot(h_tm1 * rec_dp_mask[3],\n                                                      self.recurrent_kernel_o))\n        h = o * self.activation(c)\n        if 0 < self.dropout + self.recurrent_dropout:\n            h._uses_learning_phase = True\n        return h, [h, c]\n\n    def get_config(self):\n        config = {'units': self.units,\n                  'activation': activations.serialize(self.activation),\n                  'recurrent_activation': activations.serialize(self.recurrent_activation),\n                  'use_bias': self.use_bias,\n                  'kernel_initializer': initializers.serialize(self.kernel_initializer),\n                  'recurrent_initializer': initializers.serialize(self.recurrent_initializer),\n                  'bias_initializer': initializers.serialize(self.bias_initializer),\n                  'unit_forget_bias': self.unit_forget_bias,\n                  'kernel_regularizer': regularizers.serialize(self.kernel_regularizer),\n                  'recurrent_regularizer': regularizers.serialize(self.recurrent_regularizer),\n                  'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n                  'kernel_constraint': constraints.serialize(self.kernel_constraint),\n                  'recurrent_constraint': constraints.serialize(self.recurrent_constraint),\n                  'bias_constraint': constraints.serialize(self.bias_constraint),\n                  'dropout': self.dropout,\n                  'recurrent_dropout': self.recurrent_dropout}\n        base_config = super(LSTM, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n",
          "file_patch": "@@ -165,14 +165,10 @@ class Recurrent(Layer):\n         a specific layer, or on your entire model.\n \n     # Note on specifying initial states in RNNs\n-        Most RNN layers can be called with either a single tensor, or a list\n-        of tensors.\n-        If the an RNN layer is called with a single tensor, that tensor is\n-        treated as the input, and the initial states are assigned an\n-        appropriate default value.\n-        If an RNN layer is called with a list of tensors, the first element is\n-        treated as the input, and the remaining tensors are treated as the\n-        initial states.\n+        You can specify the initial state of RNN layers by calling theme with\n+        the keyword argument `initial_state`. The value of `initial_state`\n+        should be a tensor or list of tensors representing the initial state\n+        of the RNN layer.\n     \"\"\"\n \n     def __init__(self, return_sequences=False,\n@@ -188,7 +184,8 @@ class Recurrent(Layer):\n         self.unroll = unroll\n         self.implementation = 0\n         self.supports_masking = True\n-        self.input_spec = None\n+        self.input_spec = InputSpec(ndim=3)\n+        self.state_spec = None\n         self.dropout = 0\n         self.recurrent_dropout = 0\n \n@@ -224,10 +221,58 @@ class Recurrent(Layer):\n     def preprocess_input(self, inputs, training=None):\n         return inputs\n \n+    def __call__(self, inputs, **kwargs):\n+        with K.name_scope(self.name):\n+            # Handle laying building (weight creating, input spec locking,\n+            # state spec locking)\n+            if not self.built:\n+                # Raise exceptions in case the input is not compatible\n+                # with the input_spec specified in the layer constructor.\n+                self.assert_input_compatibility(inputs)\n+\n+                if hasattr(inputs, '_keras_shape'):\n+                    input_shape = inputs._keras_shape\n+                elif hasattr(K, 'int_shape'):\n+                    input_shape = K.int_shape(inputs)\n+                else:\n+                    raise ValueError('You tried to call layer \"' + self.name +\n+                                        '\". This layer has no information'\n+                                        ' about its expected input shape, '\n+                                        'and thus cannot be built. '\n+                                        'You can build it manually via: '\n+                                        '`layer.build(batch_input_shape)`')\n+                self.build(input_shape)\n+                self.built = True\n+\n+        # If initial_state is specified, add it to the inputs and temporarily\n+        # modify the input spec to include the state\n+        if 'initial_state' in kwargs:\n+            # Compute the full input spec, including state\n+            input_spec = self.input_spec\n+            state_spec = self.state_spec\n+            if not isinstance(state_spec, list):\n+                state_spec = [state_spec]\n+            self.input_spec = [input_spec] + state_spec\n+\n+            # Compute the full inputs, including state\n+            initial_state = kwargs.pop('initial_state')\n+            if not isinstance(initial_state, list):\n+                initial_state = [initial_state]\n+            inputs = [inputs] + initial_state\n+\n+            # Perform the call\n+            output = super(Recurrent, self).__call__(inputs, **kwargs)\n+\n+            # Restore original input spec\n+            self.input_spec = input_spec\n+            return output\n+\n+        return super(Recurrent, self).__call__(inputs, **kwargs)\n+\n     def call(self, inputs, mask=None, training=None):\n         # input shape: (nbias_samples, time (padded with zeros), input_dim)\n         # note that the .build() method of subclasses MUST define\n-        # self.input_spec with a complete input shape.\n+        # self.input_spec and self.state_spec with complete input shapes.\n         if isinstance(inputs, list):\n             initial_states = inputs[1:]\n             inputs = inputs[0]\n@@ -277,7 +322,8 @@ class Recurrent(Layer):\n     def reset_states(self):\n         if not self.stateful:\n             raise AttributeError('Layer must be stateful.')\n-        if not self.batch_size:\n+        batch_size = self.input_spec.shape[0]\n+        if not batch_size:\n             raise ValueError('If a RNN is stateful, it needs to know '\n                              'its batch size. Specify the batch size '\n                              'of your input tensors: \\n'\n@@ -290,9 +336,9 @@ class Recurrent(Layer):\n                              '`batch_shape` argument to your Input layer.')\n         if self.states[0] is not None:\n             for state in self.states:\n-                K.set_value(state, np.zeros((self.batch_size, self.units)))\n+                K.set_value(state, np.zeros((batch_size, self.units)))\n         else:\n-            self.states = [K.zeros((self.batch_size, self.units))\n+            self.states = [K.zeros((batch_size, self.units))\n                            for _ in self.states]\n \n     def get_config(self):\n@@ -394,8 +440,10 @@ class SimpleRNN(Recurrent):\n         if isinstance(input_shape, list):\n             input_shape = input_shape[0]\n \n-        self.batch_size = input_shape[0]\n+        batch_size = input_shape[0] if self.stateful else None\n         self.input_dim = input_shape[2]\n+        self.input_spec = InputSpec(shape=(batch_size, None, self.input_dim))\n+        self.state_spec = InputSpec(shape=(batch_size, self.units))\n \n         self.states = [None]\n         if self.stateful:\n@@ -608,8 +656,10 @@ class GRU(Recurrent):\n         if isinstance(input_shape, list):\n             input_shape = input_shape[0]\n \n-        self.batch_size = input_shape[0]\n+        batch_size = input_shape[0] if self.stateful else None\n         self.input_dim = input_shape[2]\n+        self.input_spec = InputSpec(shape=(batch_size, None, self.input_dim))\n+        self.state_spec = InputSpec(shape=(batch_size, self.units))\n \n         self.states = [None]\n         if self.stateful:\n@@ -883,8 +933,11 @@ class LSTM(Recurrent):\n         if isinstance(input_shape, list):\n             input_shape = input_shape[0]\n \n-        self.batch_size = input_shape[0]\n+        batch_size = input_shape[0] if self.stateful else None\n         self.input_dim = input_shape[2]\n+        self.input_spec = InputSpec(shape=(batch_size, None, self.input_dim))\n+        self.state_spec = [InputSpec(shape=(batch_size, self.units)),\n+                           InputSpec(shape=(batch_size, self.units))]\n \n         self.states = [None, None]\n         if self.stateful:\n",
          "files_name_in_blame_commit": [
            "recurrent.py",
            "recurrent_test.py"
          ]
        }
      },
      "e45bce14b7b55e2fef99e1cd18a0a06681e6956b": {
        "commit": {
          "commit_id": "e45bce14b7b55e2fef99e1cd18a0a06681e6956b",
          "commit_message": "Fixed statefulness",
          "commit_author": "Joshua Chin",
          "commit_date": "2017-03-04 16:27:03",
          "commit_parent": "a214f4e64d9e1f09a6ffdcad5e9595d0534e08f0"
        },
        "function": {
          "function_name": "build",
          "function_code_before": "def build(self, input_shape):\n    if isinstance(input_shape, list):\n        input_shape = input_shape[0]\n    if self.stateful:\n        self.reset_states()\n    else:\n        self.states = [None]\n    input_dim = input_shape[2]\n    self.input_dim = input_dim\n    self.kernel = self.add_weight((input_dim, self.units), name='kernel', initializer=self.kernel_initializer, regularizer=self.kernel_regularizer, constraint=self.kernel_constraint)\n    self.recurrent_kernel = self.add_weight((self.units, self.units), name='recurrent_kernel', initializer=self.recurrent_initializer, regularizer=self.recurrent_regularizer, constraint=self.recurrent_constraint)\n    if self.use_bias:\n        self.bias = self.add_weight((self.units,), name='bias', initializer=self.bias_initializer, regularizer=self.bias_regularizer, constraint=self.bias_constraint)\n    else:\n        self.bias = None\n    self.built = True",
          "function_code_after": "def build(self, input_shape):\n    if isinstance(input_shape, list):\n        input_shape = input_shape[0]\n    self.batch_size = input_shape[0]\n    self.input_dim = input_shape[2]\n    self.states = [None]\n    if self.stateful:\n        self.reset_states()\n    self.kernel = self.add_weight((self.input_dim, self.units), name='kernel', initializer=self.kernel_initializer, regularizer=self.kernel_regularizer, constraint=self.kernel_constraint)\n    self.recurrent_kernel = self.add_weight((self.units, self.units), name='recurrent_kernel', initializer=self.recurrent_initializer, regularizer=self.recurrent_regularizer, constraint=self.recurrent_constraint)\n    if self.use_bias:\n        self.bias = self.add_weight((self.units,), name='bias', initializer=self.bias_initializer, regularizer=self.bias_regularizer, constraint=self.bias_constraint)\n    else:\n        self.bias = None\n    self.built = True",
          "function_before_start_line": 372,
          "function_before_end_line": 402,
          "function_after_start_line": 393,
          "function_after_end_line": 423,
          "function_before_token_count": 183,
          "function_after_token_count": 188,
          "functions_name_modified_file": [
            "__init__",
            "compute_mask",
            "build",
            "call",
            "get_constants",
            "_time_distributed_dense",
            "compute_output_shape",
            "step",
            "get_initial_states",
            "reset_states",
            "preprocess_input",
            "get_config"
          ],
          "functions_name_all_files": [
            "__init__",
            "compute_mask",
            "build",
            "call",
            "get_constants",
            "_time_distributed_dense",
            "compute_output_shape",
            "step",
            "get_initial_states",
            "reset_states",
            "preprocess_input",
            "get_config"
          ],
          "functions_name_co_evolved_modified_file": [
            "reset_states"
          ],
          "functions_name_co_evolved_all_files": [
            "reset_states"
          ]
        },
        "file": {
          "file_name": "recurrent.py",
          "file_nloc": 938,
          "file_complexity": 108,
          "file_token_count": 5632,
          "file_before": "# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import\nimport numpy as np\n\nfrom .. import backend as K\nfrom .. import activations\nfrom .. import initializers\nfrom .. import regularizers\nfrom .. import constraints\nfrom ..engine import Layer\nfrom ..engine import InputSpec\n\n\ndef _time_distributed_dense(x, w, b=None, dropout=None,\n                            input_dim=None, output_dim=None,\n                            timesteps=None, training=None):\n    \"\"\"Apply `y . w + b` for every temporal slice y of x.\n\n    # Arguments\n        x: input tensor.\n        w: weight matrix.\n        b: optional bias vector.\n        dropout: wether to apply dropout (same dropout mask\n            for every temporal slice of the input).\n        input_dim: integer; optional dimensionality of the input.\n        output_dim: integer; optional dimensionality of the output.\n        timesteps: integer; optional number of timesteps.\n        training: training phase tensor or boolean.\n\n    # Returns\n        Output tensor.\n    \"\"\"\n    if not input_dim:\n        input_dim = K.shape(x)[2]\n    if not timesteps:\n        timesteps = K.shape(x)[1]\n    if not output_dim:\n        output_dim = K.shape(w)[1]\n\n    if dropout is not None and 0. < dropout < 1.:\n        # apply the same dropout pattern at every timestep\n        ones = K.ones_like(K.reshape(x[:, 0, :], (-1, input_dim)))\n        dropout_matrix = K.dropout(ones, dropout)\n        expanded_dropout_matrix = K.repeat(dropout_matrix, timesteps)\n        x = K.in_train_phase(x * expanded_dropout_matrix, x, training=training)\n\n    # collapse time dimension and batch dimension together\n    x = K.reshape(x, (-1, input_dim))\n    x = K.dot(x, w)\n    if b is not None:\n        x += b\n    # reshape to 3D tensor\n    if K.backend() == 'tensorflow':\n        x = K.reshape(x, K.stack([-1, timesteps, output_dim]))\n        x.set_shape([None, None, output_dim])\n    else:\n        x = K.reshape(x, (-1, timesteps, output_dim))\n    return x\n\n\nclass Recurrent(Layer):\n    \"\"\"Abstract base class for recurrent layers.\n\n    Do not use in a model -- it's not a valid layer!\n    Use its children classes `LSTM`, `GRU` and `SimpleRNN` instead.\n\n    All recurrent layers (`LSTM`, `GRU`, `SimpleRNN`) also\n    follow the specifications of this class and accept\n    the keyword arguments listed below.\n\n    # Example\n\n    ```python\n        # as the first layer in a Sequential model\n        model = Sequential()\n        model.add(LSTM(32, input_shape=(10, 64)))\n        # now model.output_shape == (None, 32)\n        # note: `None` is the batch dimension.\n\n        # the following is identical:\n        model = Sequential()\n        model.add(LSTM(32, input_dim=64, input_length=10))\n\n        # for subsequent layers, not need to specify the input size:\n        model.add(LSTM(16))\n    ```\n\n    # Arguments\n        weights: list of Numpy arrays to set as initial weights.\n            The list should have 3 elements, of shapes:\n            `[(input_dim, output_dim), (output_dim, output_dim), (output_dim,)]`.\n        return_sequences: Boolean. Whether to return the last output\n            in the output sequence, or the full sequence.\n        go_backwards: Boolean (default False).\n            If True, process the input sequence backwards.\n        stateful: Boolean (default False). If True, the last state\n            for each sample at index i in a batch will be used as initial\n            state for the sample of index i in the following batch.\n        unroll: Boolean (default False).\n            If True, the network will be unrolled,\n            else a symbolic loop will be used.\n            Unrolling can speed-up a RNN,\n            although it tends to be more memory-intensive.\n            Unrolling is only suitable for short sequences.\n        implementation: one of {0, 1, or 2}.\n            If set to 0, the RNN will use\n            an implementation that uses fewer, larger matrix products,\n            thus running faster on CPU but consuming more memory.\n            If set to 1, the RNN will use more matrix products,\n            but smaller ones, thus running slower\n            (may actually be faster on GPU) while consuming less memory.\n            If set to 2 (LSTM/GRU only),\n            the RNN will combine the input gate,\n            the forget gate and the output gate into a single matrix,\n            enabling more time-efficient parallelization on the GPU.\n            Note: RNN dropout must be shared for all gates,\n            resulting in a slightly reduced regularization.\n        input_dim: dimensionality of the input (integer).\n            This argument (or alternatively, the keyword argument `input_shape`)\n            is required when using this layer as the first layer in a model.\n        input_length: Length of input sequences, to be specified\n            when it is constant.\n            This argument is required if you are going to connect\n            `Flatten` then `Dense` layers upstream\n            (without it, the shape of the dense outputs cannot be computed).\n            Note that if the recurrent layer is not the first layer\n            in your model, you would need to specify the input length\n            at the level of the first layer\n            (e.g. via the `input_shape` argument)\n\n    # Input shapes\n        3D tensor with shape `(batch_size, timesteps, input_dim)`,\n        (Optional) 2D tensors with shape `(batch_size, output_dim)`.\n\n    # Output shape\n        - if `return_sequences`: 3D tensor with shape\n            `(batch_size, timesteps, units)`.\n        - else, 2D tensor with shape `(batch_size, units)`.\n\n    # Masking\n        This layer supports masking for input data with a variable number\n        of timesteps. To introduce masks to your data,\n        use an [Embedding](embeddings.md) layer with the `mask_zero` parameter\n        set to `True`.\n\n    # Note on using statefulness in RNNs\n        You can set RNN layers to be 'stateful', which means that the states\n        computed for the samples in one batch will be reused as initial states\n        for the samples in the next batch. This assumes a one-to-one mapping\n        between samples in different successive batches.\n\n        To enable statefulness:\n            - specify `stateful=True` in the layer constructor.\n            - specify a fixed batch size for your model, by passing\n                if sequential model:\n                  `batch_input_shape=(...)` to the first layer in your model.\n                else for functional model with 1 or more Input layers:\n                  `batch_shape=(...)` to all the first layers in your model.\n                This is the expected shape of your inputs\n                *including the batch size*.\n                It should be a tuple of integers, e.g. `(32, 10, 100)`.\n            - specify `shuffle=False` when calling fit().\n\n        To reset the states of your model, call `.reset_states()` on either\n        a specific layer, or on your entire model.\n\n    # Note on specifying initial states in RNNs\n        Most RNN layers can be called with either a single tensor, or a list\n        of tensors.\n        If the an RNN layer is called with a single tensor, that tensor is\n        treated as the input, and the initial states are assigned an\n        appropriate default value.\n        If an RNN layer is called with a list of tensors, the first element is\n        treated as the input, and the remaining tensors are treated as the\n        initial states.\n    \"\"\"\n\n    def __init__(self, return_sequences=False,\n                 go_backwards=False,\n                 stateful=False,\n                 unroll=False,\n                 implementation=0,\n                 **kwargs):\n        super(Recurrent, self).__init__(**kwargs)\n        self.return_sequences = return_sequences\n        self.go_backwards = go_backwards\n        self.stateful = stateful\n        self.unroll = unroll\n        self.implementation = 0\n        self.supports_masking = True\n        self.input_spec = None\n        self.dropout = 0\n        self.recurrent_dropout = 0\n\n    def compute_output_shape(self, input_shape):\n        if isinstance(input_shape, list):\n            input_shape = input_shape[0]\n        if self.return_sequences:\n            return (input_shape[0], input_shape[1], self.units)\n        else:\n            return (input_shape[0], self.units)\n\n    def compute_mask(self, inputs, mask):\n        if self.return_sequences:\n            return mask\n        else:\n            return None\n\n    def step(self, inputs, states):\n        raise NotImplementedError\n\n    def get_constants(self, inputs, training=None):\n        return []\n\n    def get_initial_states(self, inputs):\n        # build an all-zero tensor of shape (samples, output_dim)\n        initial_state = K.zeros_like(inputs)  # (samples, timesteps, input_dim)\n        initial_state = K.sum(initial_state, axis=(1, 2))  # (samples,)\n        initial_state = K.expand_dims(initial_state)  # (samples, 1)\n        initial_state = K.tile(initial_state, [1, self.units])  # (samples, output_dim)\n        initial_states = [initial_state for _ in range(len(self.states))]\n        return initial_states\n\n    def preprocess_input(self, inputs, training=None):\n        return inputs\n\n    def call(self, inputs, mask=None, training=None):\n        # input shape: (nbias_samples, time (padded with zeros), input_dim)\n        # note that the .build() method of subclasses MUST define\n        # self.input_spec with a complete input shape.\n        if isinstance(inputs, list):\n            initial_states = inputs[1:]\n            inputs = inputs[0]\n        elif self.stateful:\n            initial_states = self.states\n        else:\n            initial_states = self.get_initial_states(inputs)\n        input_shape = K.int_shape(inputs)\n        if self.unroll and input_shape[1] is None:\n            raise ValueError('Cannot unroll a RNN if the '\n                             'time dimension is undefined. \\n'\n                             '- If using a Sequential model, '\n                             'specify the time dimension by passing '\n                             'an `input_shape` or `batch_input_shape` '\n                             'argument to your first layer. If your '\n                             'first layer is an Embedding, you can '\n                             'also use the `input_length` argument.\\n'\n                             '- If using the functional API, specify '\n                             'the time dimension by passing a `shape` '\n                             'or `batch_shape` argument to your Input layer.')\n        constants = self.get_constants(inputs, training=None)\n        preprocessed_input = self.preprocess_input(inputs, training=None)\n        last_output, outputs, states = K.rnn(self.step,\n                                             preprocessed_input,\n                                             initial_states,\n                                             go_backwards=self.go_backwards,\n                                             mask=mask,\n                                             constants=constants,\n                                             unroll=self.unroll,\n                                             input_length=input_shape[1])\n        if self.stateful:\n            updates = []\n            for i in range(len(states)):\n                updates.append((self.states[i], states[i]))\n            self.add_update(updates, inputs)\n\n        # Properly set learning phase\n        if 0 < self.dropout + self.recurrent_dropout:\n            last_output._uses_learning_phase = True\n            outputs._uses_learning_phase = True\n\n        if self.return_sequences:\n            return outputs\n        else:\n            return last_output\n\n    def get_config(self):\n        config = {'return_sequences': self.return_sequences,\n                  'go_backwards': self.go_backwards,\n                  'stateful': self.stateful,\n                  'unroll': self.unroll,\n                  'implementation': self.implementation}\n        base_config = super(Recurrent, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\n\nclass SimpleRNN(Recurrent):\n    \"\"\"Fully-connected RNN where the output is to be fed back to input.\n\n    # Arguments\n        units: Positive integer, dimensionality of the output space.\n        activation: Activation function to use\n            (see [activations](../activations.md)).\n            If you don't specify anything, no activation is applied\n            (ie. \"linear\" activation: `a(x) = x`).\n        use_bias: Boolean, whether the layer uses a bias vector.\n        kernel_initializer: Initializer for the `kernel` weights matrix,\n            used for the linear transformation of the inputs.\n            (see [initializers](../initializers.md)).\n        recurrent_initializer: Initializer for the `recurrent_kernel`\n            weights matrix,\n            used for the linear transformation of the recurrent state.\n            (see [initializers](../initializers.md)).\n        bias_initializer: Initializer for the bias vector\n            (see [initializers](../initializers.md)).\n        kernel_regularizer: Regularizer function applied to\n            the `kernel` weights matrix\n            (see [regularizer](../regularizers.md)).\n        recurrent_regularizer: Regularizer function applied to\n            the `recurrent_kernel` weights matrix\n            (see [regularizer](../regularizers.md)).\n        bias_regularizer: Regularizer function applied to the bias vector\n            (see [regularizer](../regularizers.md)).\n        activity_regularizer: Regularizer function applied to\n            the output of the layer (its \"activation\").\n            (see [regularizer](../regularizers.md)).\n        kernel_constraint: Constraint function applied to\n            the `kernel` weights matrix\n            (see [constraints](../constraints.md)).\n        recurrent_constraint: Constraint function applied to\n            the `recurrent_kernel` weights matrix\n            (see [constraints](../constraints.md)).\n        bias_constraint: Constraint function applied to the bias vector\n            (see [constraints](../constraints.md)).\n        dropout: Float between 0 and 1.\n            Fraction of the units to drop for\n            the linear transformation of the inputs.\n        recurrent_dropout: Float between 0 and 1.\n            Fraction of the units to drop for\n            the linear transformation of the recurrent state.\n\n    # References\n        - [A Theoretically Grounded Application of Dropout in Recurrent Neural Networks](http://arxiv.org/abs/1512.05287)\n    \"\"\"\n\n    def __init__(self, units,\n                 activation='tanh',\n                 use_bias=True,\n                 kernel_initializer='glorot_uniform',\n                 recurrent_initializer='orthogonal',\n                 bias_initializer='zeros',\n                 kernel_regularizer=None,\n                 recurrent_regularizer=None,\n                 bias_regularizer=None,\n                 activity_regularizer=None,\n                 kernel_constraint=None,\n                 recurrent_constraint=None,\n                 bias_constraint=None,\n                 dropout=0.,\n                 recurrent_dropout=0.,\n                 **kwargs):\n        super(SimpleRNN, self).__init__(**kwargs)\n        self.units = units\n        self.activation = activations.get(activation)\n        self.use_bias = use_bias\n\n        self.kernel_initializer = initializers.get(kernel_initializer)\n        self.recurrent_initializer = initializers.get(recurrent_initializer)\n        self.bias_initializer = initializers.get(bias_initializer)\n\n        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n        self.recurrent_regularizer = regularizers.get(recurrent_regularizer)\n        self.bias_regularizer = regularizers.get(bias_regularizer)\n\n        self.kernel_constraint = constraints.get(kernel_constraint)\n        self.recurrent_constraint = constraints.get(recurrent_constraint)\n        self.bias_constraint = constraints.get(bias_constraint)\n\n        self.dropout = min(1., max(0., dropout))\n        self.recurrent_dropout = min(1., max(0., recurrent_dropout))\n\n    def build(self, input_shape):\n        if isinstance(input_shape, list):\n            input_shape = input_shape[0]\n        if self.stateful:\n            self.reset_states()\n        else:\n            # Initial states: all-zero tensor of shape (output_dim).\n            self.states = [None]\n        input_dim = input_shape[2]\n        self.input_dim = input_dim\n\n        self.kernel = self.add_weight((input_dim, self.units),\n                                      name='kernel',\n                                      initializer=self.kernel_initializer,\n                                      regularizer=self.kernel_regularizer,\n                                      constraint=self.kernel_constraint)\n        self.recurrent_kernel = self.add_weight(\n            (self.units, self.units),\n            name='recurrent_kernel',\n            initializer=self.recurrent_initializer,\n            regularizer=self.recurrent_regularizer,\n            constraint=self.recurrent_constraint)\n        if self.use_bias:\n            self.bias = self.add_weight((self.units,),\n                                        name='bias',\n                                        initializer=self.bias_initializer,\n                                        regularizer=self.bias_regularizer,\n                                        constraint=self.bias_constraint)\n        else:\n            self.bias = None\n        self.built = True\n\n    def reset_states(self):\n        if not self.stateful:\n            raise AttributeError('Layer must be stateful.')\n        input_shape = self.input_spec.shape\n        if not input_shape[0]:\n            raise ValueError('If a RNN is stateful, it needs to know '\n                             'its batch size. Specify the batch size '\n                             'of your input tensors: \\n'\n                             '- If using a Sequential model, '\n                             'specify the batch size by passing '\n                             'a `batch_input_shape` '\n                             'argument to your first layer.\\n'\n                             '- If using the functional API, specify '\n                             'the time dimension by passing a '\n                             '`batch_shape` argument to your Input layer.')\n        if hasattr(self, 'states'):\n            K.set_value(self.states[0],\n                        np.zeros((input_shape[0], self.units)))\n        else:\n            self.states = [K.zeros((input_shape[0], self.units))]\n\n    def preprocess_input(self, inputs, training=None):\n        if self.implementation > 0:\n            return inputs\n        else:\n            input_shape = K.int_shape(inputs)\n            input_dim = input_shape[2]\n            timesteps = input_shape[1]\n            return _time_distributed_dense(inputs,\n                                           self.kernel,\n                                           self.bias,\n                                           self.dropout,\n                                           input_dim,\n                                           self.units,\n                                           timesteps,\n                                           training=training)\n\n    def step(self, inputs, states):\n        if self.implementation == 0:\n            h = inputs\n        else:\n            if 0 < self.dropout < 1:\n                h = K.dot(inputs * states[1], self.kernel)\n            else:\n                h = K.dot(inputs, self.kernel)\n            if self.bias is not None:\n                h += self.bias\n\n        prev_output = states[0]\n        if 0 < self.recurrent_dropout < 1:\n            prev_output *= states[2]\n        output = h + K.dot(prev_output, self.recurrent_kernel)\n        if self.activation is not None:\n            output = self.activation(output)\n\n        # Properly set learning phase on output tensor.\n        if 0 < self.dropout + self.recurrent_dropout:\n            output._uses_learning_phase = True\n        return output, [output]\n\n    def get_constants(self, inputs, training=None):\n        constants = []\n        if self.implementation == 0 and 0 < self.dropout < 1:\n            input_shape = K.int_shape(inputs)\n            input_dim = input_shape[-1]\n            ones = K.ones_like(K.reshape(inputs[:, 0, 0], (-1, 1)))\n            ones = K.tile(ones, (1, int(input_dim)))\n\n            def dropped_inputs():\n                return K.dropout(ones, self.dropout)\n\n            dp_mask = K.in_train_phase(dropped_inputs,\n                                       ones,\n                                       training=training)\n            constants.append(dp_mask)\n        else:\n            constants.append(K.cast_to_floatx(1.))\n\n        if 0 < self.recurrent_dropout < 1:\n            ones = K.ones_like(K.reshape(inputs[:, 0, 0], (-1, 1)))\n            ones = K.tile(ones, (1, self.units))\n\n            def dropped_inputs():\n                return K.dropout(ones, self.recurrent_dropout)\n            rec_dp_mask = K.in_train_phase(dropped_inputs,\n                                           ones,\n                                           training=training)\n            constants.append(rec_dp_mask)\n        else:\n            constants.append(K.cast_to_floatx(1.))\n        return constants\n\n    def get_config(self):\n        config = {'units': self.units,\n                  'activation': activations.serialize(self.activation),\n                  'use_bias': self.use_bias,\n                  'kernel_initializer': initializers.serialize(self.kernel_initializer),\n                  'recurrent_initializer': initializers.serialize(self.recurrent_initializer),\n                  'bias_initializer': initializers.serialize(self.bias_initializer),\n                  'kernel_regularizer': regularizers.serialize(self.kernel_regularizer),\n                  'recurrent_regularizer': regularizers.serialize(self.recurrent_regularizer),\n                  'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n                  'kernel_constraint': constraints.serialize(self.kernel_constraint),\n                  'recurrent_constraint': constraints.serialize(self.recurrent_constraint),\n                  'bias_constraint': constraints.serialize(self.bias_constraint),\n                  'dropout': self.dropout,\n                  'recurrent_dropout': self.recurrent_dropout}\n        base_config = super(SimpleRNN, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\n\nclass GRU(Recurrent):\n    \"\"\"Gated Recurrent Unit - Cho et al. 2014.\n\n    # Arguments\n        units: Positive integer, dimensionality of the output space.\n        activation: Activation function to use\n            (see [activations](../activations.md)).\n            If you don't specify anything, no activation is applied\n            (ie. \"linear\" activation: `a(x) = x`).\n        recurrent_activation: Activation function to use\n            for the recurrent step\n            (see [activations](../activations.md)).\n        use_bias: Boolean, whether the layer uses a bias vector.\n        kernel_initializer: Initializer for the `kernel` weights matrix,\n            used for the linear transformation of the inputs.\n            (see [initializers](../initializers.md)).\n        recurrent_initializer: Initializer for the `recurrent_kernel`\n            weights matrix,\n            used for the linear transformation of the recurrent state.\n            (see [initializers](../initializers.md)).\n        bias_initializer: Initializer for the bias vector\n            (see [initializers](../initializers.md)).\n        kernel_regularizer: Regularizer function applied to\n            the `kernel` weights matrix\n            (see [regularizer](../regularizers.md)).\n        recurrent_regularizer: Regularizer function applied to\n            the `recurrent_kernel` weights matrix\n            (see [regularizer](../regularizers.md)).\n        bias_regularizer: Regularizer function applied to the bias vector\n            (see [regularizer](../regularizers.md)).\n        activity_regularizer: Regularizer function applied to\n            the output of the layer (its \"activation\").\n            (see [regularizer](../regularizers.md)).\n        kernel_constraint: Constraint function applied to\n            the `kernel` weights matrix\n            (see [constraints](../constraints.md)).\n        recurrent_constraint: Constraint function applied to\n            the `recurrent_kernel` weights matrix\n            (see [constraints](../constraints.md)).\n        bias_constraint: Constraint function applied to the bias vector\n            (see [constraints](../constraints.md)).\n        dropout: Float between 0 and 1.\n            Fraction of the units to drop for\n            the linear transformation of the inputs.\n        recurrent_dropout: Float between 0 and 1.\n            Fraction of the units to drop for\n            the linear transformation of the recurrent state.\n\n    # References\n        - [On the Properties of Neural Machine Translation: Encoder-Decoder Approaches](https://arxiv.org/abs/1409.1259)\n        - [Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling](http://arxiv.org/abs/1412.3555v1)\n        - [A Theoretically Grounded Application of Dropout in Recurrent Neural Networks](http://arxiv.org/abs/1512.05287)\n    \"\"\"\n\n    def __init__(self, units,\n                 activation='tanh',\n                 recurrent_activation='hard_sigmoid',\n                 use_bias=True,\n                 kernel_initializer='glorot_uniform',\n                 recurrent_initializer='orthogonal',\n                 bias_initializer='zeros',\n                 kernel_regularizer=None,\n                 recurrent_regularizer=None,\n                 bias_regularizer=None,\n                 activity_regularizer=None,\n                 kernel_constraint=None,\n                 recurrent_constraint=None,\n                 bias_constraint=None,\n                 dropout=0.,\n                 recurrent_dropout=0.,\n                 **kwargs):\n        super(GRU, self).__init__(**kwargs)\n        self.units = units\n        self.activation = activations.get(activation)\n        self.recurrent_activation = activations.get(recurrent_activation)\n        self.use_bias = use_bias\n\n        self.kernel_initializer = initializers.get(kernel_initializer)\n        self.recurrent_initializer = initializers.get(recurrent_initializer)\n        self.bias_initializer = initializers.get(bias_initializer)\n\n        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n        self.recurrent_regularizer = regularizers.get(recurrent_regularizer)\n        self.bias_regularizer = regularizers.get(bias_regularizer)\n\n        self.kernel_constraint = constraints.get(kernel_constraint)\n        self.recurrent_constraint = constraints.get(recurrent_constraint)\n        self.bias_constraint = constraints.get(bias_constraint)\n\n        self.dropout = min(1., max(0., dropout))\n        self.recurrent_dropout = min(1., max(0., recurrent_dropout))\n\n    def build(self, input_shape):\n        if isinstance(input_shape, list):\n            input_shape = input_shape[0]\n        self.input_dim = input_shape[2]\n\n        if self.stateful:\n            self.reset_states()\n        else:\n            # initial states: all-zero tensor of shape (output_dim)\n            self.states = [None]\n\n        self.kernel = self.add_weight((self.input_dim, self.units * 3),\n                                      name='kernel',\n                                      initializer=self.kernel_initializer,\n                                      regularizer=self.kernel_regularizer,\n                                      constraint=self.kernel_constraint)\n        self.recurrent_kernel = self.add_weight(\n            (self.units, self.units * 3),\n            name='recurrent_kernel',\n            initializer=self.recurrent_initializer,\n            regularizer=self.recurrent_regularizer,\n            constraint=self.recurrent_constraint)\n\n        if self.use_bias:\n            self.bias = self.add_weight((self.units * 3,),\n                                        name='bias',\n                                        initializer='zero',\n                                        regularizer=self.bias_regularizer,\n                                        constraint=self.bias_constraint)\n        else:\n            self.bias = None\n\n        self.kernel_z = self.kernel[:, :self.units]\n        self.recurrent_kernel_z = self.recurrent_kernel[:, :self.units]\n        self.kernel_r = self.kernel[:, self.units: self.units * 2]\n        self.recurrent_kernel_r = self.recurrent_kernel[:,\n                                                        self.units:\n                                                        self.units * 2]\n        self.kernel_h = self.kernel[:, self.units * 2:]\n        self.recurrent_kernel_h = self.recurrent_kernel[:, self.units * 2:]\n\n        if self.use_bias:\n            self.bias_z = self.bias[:self.units]\n            self.bias_r = self.bias[self.units: self.units * 2]\n            self.bias_h = self.bias[self.units * 2:]\n        else:\n            self.bias_z = None\n            self.bias_r = None\n            self.bias_h = None\n        self.built = True\n\n    def reset_states(self):\n        if not self.stateful:\n            raise RuntimeError('Layer must be stateful.')\n        input_shape = self.input_spec.shape\n        if not input_shape[0]:\n            raise ValueError('If a RNN is stateful, a complete '\n                             'input_shape must be provided '\n                             '(including batch size).')\n        if hasattr(self, 'states'):\n            K.set_value(self.states[0],\n                        np.zeros((input_shape[0], self.units)))\n        else:\n            self.states = [K.zeros((input_shape[0], self.units))]\n\n    def preprocess_input(self, inputs, training=None):\n        if self.implementation == 0:\n            input_shape = K.int_shape(inputs)\n            input_dim = input_shape[2]\n            timesteps = input_shape[1]\n\n            x_z = _time_distributed_dense(inputs, self.kernel_z, self.bias_z,\n                                          self.dropout, input_dim, self.units,\n                                          timesteps, training=training)\n            x_r = _time_distributed_dense(inputs, self.kernel_r, self.bias_r,\n                                          self.dropout, input_dim, self.units,\n                                          timesteps, training=training)\n            x_h = _time_distributed_dense(inputs, self.kernel_h, self.bias_h,\n                                          self.dropout, input_dim, self.units,\n                                          timesteps, training=training)\n            return K.concatenate([x_z, x_r, x_h], axis=2)\n        else:\n            return inputs\n\n    def get_constants(self, inputs, training=None):\n        constants = []\n        if self.implementation == 0 and 0 < self.dropout < 1:\n            input_shape = K.int_shape(inputs)\n            input_dim = input_shape[-1]\n            ones = K.ones_like(K.reshape(inputs[:, 0, 0], (-1, 1)))\n            ones = K.tile(ones, (1, int(input_dim)))\n\n            def dropped_inputs():\n                return K.dropout(ones, self.dropout)\n\n            dp_mask = [K.in_train_phase(dropped_inputs,\n                                        ones,\n                                        training=training) for _ in range(3)]\n            constants.append(dp_mask)\n        else:\n            constants.append([K.cast_to_floatx(1.) for _ in range(3)])\n\n        if 0 < self.recurrent_dropout < 1:\n            ones = K.ones_like(K.reshape(inputs[:, 0, 0], (-1, 1)))\n            ones = K.tile(ones, (1, self.units))\n\n            def dropped_inputs():\n                return K.dropout(ones, self.recurrent_dropout)\n            rec_dp_mask = [K.in_train_phase(dropped_inputs,\n                                            ones,\n                                            training=training) for _ in range(3)]\n            constants.append(rec_dp_mask)\n        else:\n            constants.append([K.cast_to_floatx(1.) for _ in range(3)])\n        return constants\n\n    def step(self, inputs, states):\n        h_tm1 = states[0]  # previous memory\n        dp_mask = states[1]  # dropout matrices for recurrent units\n        rec_dp_mask = states[2]\n\n        if self.implementation == 1:\n            matrix_x = K.dot(inputs * dp_mask[0], self.kernel)\n            if self.use_bias:\n                matrix_x += self.bias\n            matrix_inner = K.dot(h_tm1 * rec_dp_mask[0],\n                                 self.recurrent_kernel[:, :2 * self.units])\n\n            x_z = matrix_x[:, :self.units]\n            x_r = matrix_x[:, self.units: 2 * self.units]\n            recurrent_z = matrix_inner[:, :self.units]\n            recurrent_r = matrix_inner[:, self.units: 2 * self.units]\n\n            z = self.recurrent_activation(x_z + recurrent_z)\n            r = self.recurrent_activation(x_r + recurrent_r)\n\n            x_h = matrix_x[:, 2 * self.units:]\n            recurrent_h = K.dot(r * h_tm1 * rec_dp_mask[0],\n                                self.recurrent_kernel[:, 2 * self.units:])\n            hh = self.activation(x_h + recurrent_h)\n        else:\n            if self.implementation == 0:\n                x_z = inputs[:, :self.units]\n                x_r = inputs[:, self.units: 2 * self.units]\n                x_h = inputs[:, 2 * self.units:]\n            elif self.implementation == 2:\n                x_z = K.dot(inputs * dp_mask[0], self.kernel_z)\n                x_r = K.dot(inputs * dp_mask[1], self.kernel_r)\n                x_h = K.dot(inputs * dp_mask[2], self.kernel_h)\n                if self.use_bias:\n                    x_z += self.bias_z\n                    x_r += self.bias_r\n                    x_h += self.bias_h\n            else:\n                raise ValueError('Unknown `implementation` mode.')\n            z = self.recurrent_activation(x_z + K.dot(h_tm1 * rec_dp_mask[0],\n                                                      self.recurrent_kernel_z))\n            r = self.recurrent_activation(x_r + K.dot(h_tm1 * rec_dp_mask[1],\n                                                      self.recurrent_kernel_r))\n\n            hh = self.activation(x_h + K.dot(r * h_tm1 * rec_dp_mask[2],\n                                             self.recurrent_kernel_h))\n        h = z * h_tm1 + (1 - z) * hh\n        if 0 < self.dropout + self.recurrent_dropout:\n            h._uses_learning_phase = True\n        return h, [h]\n\n    def get_config(self):\n        config = {'units': self.units,\n                  'activation': activations.serialize(self.activation),\n                  'recurrent_activation': activations.serialize(self.recurrent_activation),\n                  'use_bias': self.use_bias,\n                  'kernel_initializer': initializers.serialize(self.kernel_initializer),\n                  'recurrent_initializer': initializers.serialize(self.recurrent_initializer),\n                  'bias_initializer': initializers.serialize(self.bias_initializer),\n                  'kernel_regularizer': regularizers.serialize(self.kernel_regularizer),\n                  'recurrent_regularizer': regularizers.serialize(self.recurrent_regularizer),\n                  'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n                  'kernel_constraint': constraints.serialize(self.kernel_constraint),\n                  'recurrent_constraint': constraints.serialize(self.recurrent_constraint),\n                  'bias_constraint': constraints.serialize(self.bias_constraint),\n                  'dropout': self.dropout,\n                  'recurrent_dropout': self.recurrent_dropout}\n        base_config = super(GRU, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\n\nclass LSTM(Recurrent):\n    \"\"\"Long-Short Term Memory unit - Hochreiter 1997.\n\n    For a step-by-step description of the algorithm, see\n    [this tutorial](http://deeplearning.net/tutorial/lstm.html).\n\n    # Arguments\n        units: Positive integer, dimensionality of the output space.\n        activation: Activation function to use\n            (see [activations](../activations.md)).\n            If you don't specify anything, no activation is applied\n            (ie. \"linear\" activation: `a(x) = x`).\n        recurrent_activation: Activation function to use\n            for the recurrent step\n            (see [activations](../activations.md)).\n        use_bias: Boolean, whether the layer uses a bias vector.\n        kernel_initializer: Initializer for the `kernel` weights matrix,\n            used for the linear transformation of the inputs.\n            (see [initializers](../initializers.md)).\n        recurrent_initializer: Initializer for the `recurrent_kernel`\n            weights matrix,\n            used for the linear transformation of the recurrent state.\n            (see [initializers](../initializers.md)).\n        bias_initializer: Initializer for the bias vector\n            (see [initializers](../initializers.md)).\n        unit_forget_bias: Boolean.\n            If True, add 1 to the bias of the forget gate at initialization.\n            Use in combination with `bias_initializer=\"zeros\"`.\n            This is recommended in [Jozefowicz et al.](http://www.jmlr.org/proceedings/papers/v37/jozefowicz15.pdf)\n        kernel_regularizer: Regularizer function applied to\n            the `kernel` weights matrix\n            (see [regularizer](../regularizers.md)).\n        recurrent_regularizer: Regularizer function applied to\n            the `recurrent_kernel` weights matrix\n            (see [regularizer](../regularizers.md)).\n        bias_regularizer: Regularizer function applied to the bias vector\n            (see [regularizer](../regularizers.md)).\n        activity_regularizer: Regularizer function applied to\n            the output of the layer (its \"activation\").\n            (see [regularizer](../regularizers.md)).\n        kernel_constraint: Constraint function applied to\n            the `kernel` weights matrix\n            (see [constraints](../constraints.md)).\n        recurrent_constraint: Constraint function applied to\n            the `recurrent_kernel` weights matrix\n            (see [constraints](../constraints.md)).\n        bias_constraint: Constraint function applied to the bias vector\n            (see [constraints](../constraints.md)).\n        dropout: Float between 0 and 1.\n            Fraction of the units to drop for\n            the linear transformation of the inputs.\n        recurrent_dropout: Float between 0 and 1.\n            Fraction of the units to drop for\n            the linear transformation of the recurrent state.\n\n    # References\n        - [Long short-term memory](http://deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf) (original 1997 paper)\n        - [Learning to forget: Continual prediction with LSTM](http://www.mitpressjournals.org/doi/pdf/10.1162/089976600300015015)\n        - [Supervised sequence labeling with recurrent neural networks](http://www.cs.toronto.edu/~graves/preprint.pdf)\n        - [A Theoretically Grounded Application of Dropout in Recurrent Neural Networks](http://arxiv.org/abs/1512.05287)\n    \"\"\"\n\n    def __init__(self, units,\n                 activation='tanh',\n                 recurrent_activation='hard_sigmoid',\n                 use_bias=True,\n                 kernel_initializer='glorot_uniform',\n                 recurrent_initializer='orthogonal',\n                 bias_initializer='zeros',\n                 unit_forget_bias=True,\n                 kernel_regularizer=None,\n                 recurrent_regularizer=None,\n                 bias_regularizer=None,\n                 activity_regularizer=None,\n                 kernel_constraint=None,\n                 recurrent_constraint=None,\n                 bias_constraint=None,\n                 dropout=0.,\n                 recurrent_dropout=0.,\n                 **kwargs):\n        super(LSTM, self).__init__(**kwargs)\n        self.units = units\n        self.activation = activations.get(activation)\n        self.recurrent_activation = activations.get(recurrent_activation)\n        self.use_bias = use_bias\n\n        self.kernel_initializer = initializers.get(kernel_initializer)\n        self.recurrent_initializer = initializers.get(recurrent_initializer)\n        self.bias_initializer = initializers.get(bias_initializer)\n        self.unit_forget_bias = unit_forget_bias\n\n        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n        self.recurrent_regularizer = regularizers.get(recurrent_regularizer)\n        self.bias_regularizer = regularizers.get(bias_regularizer)\n\n        self.kernel_constraint = constraints.get(kernel_constraint)\n        self.recurrent_constraint = constraints.get(recurrent_constraint)\n        self.bias_constraint = constraints.get(bias_constraint)\n\n        self.dropout = min(1., max(0., dropout))\n        self.recurrent_dropout = min(1., max(0., recurrent_dropout))\n\n    def build(self, input_shape):\n        if isinstance(input_shape, list):\n            input_shape = input_shape[0]\n        self.input_dim = input_shape[2]\n\n        if self.stateful:\n            self.reset_states()\n        else:\n            # initial states: 2 all-zero tensors of shape (output_dim)\n            self.states = [None, None]\n\n        self.kernel = self.add_weight((self.input_dim, self.units * 4),\n                                      name='kernel',\n                                      initializer=self.kernel_initializer,\n                                      regularizer=self.kernel_regularizer,\n                                      constraint=self.kernel_constraint)\n        self.recurrent_kernel = self.add_weight(\n            (self.units, self.units * 4),\n            name='recurrent_kernel',\n            initializer=self.recurrent_initializer,\n            regularizer=self.recurrent_regularizer,\n            constraint=self.recurrent_constraint)\n\n        if self.use_bias:\n            self.bias = self.add_weight((self.units * 4,),\n                                        name='bias',\n                                        initializer=self.bias_initializer,\n                                        regularizer=self.bias_regularizer,\n                                        constraint=self.bias_constraint)\n            if self.unit_forget_bias:\n                self.bias += K.concatenate([K.zeros((self.units,)),\n                                           K.ones((self.units,)),\n                                           K.zeros((self.units * 2,))])\n        else:\n            self.bias = None\n\n        self.kernel_i = self.kernel[:, :self.units]\n        self.recurrent_kernel_i = self.recurrent_kernel[:, :self.units]\n        self.kernel_f = self.kernel[:, self.units: self.units * 2]\n        self.recurrent_kernel_f = self.recurrent_kernel[:, self.units: self.units * 2]\n        self.kernel_c = self.kernel[:, self.units * 2: self.units * 3]\n        self.recurrent_kernel_c = self.recurrent_kernel[:, self.units * 2: self.units * 3]\n        self.kernel_o = self.kernel[:, self.units * 3:]\n        self.recurrent_kernel_o = self.recurrent_kernel[:, self.units * 3:]\n\n        if self.use_bias:\n            self.bias_i = self.bias[:self.units]\n            self.bias_f = self.bias[self.units: self.units * 2]\n            self.bias_c = self.bias[self.units * 2: self.units * 3]\n            self.bias_o = self.bias[self.units * 3:]\n        else:\n            self.bias_i = None\n            self.bias_f = None\n            self.bias_c = None\n            self.bias_o = None\n        self.built = True\n\n    def reset_states(self):\n        assert self.stateful, 'Layer must be stateful.'\n        input_shape = self.input_spec.shape\n        if not input_shape[0]:\n            raise ValueError('If a RNN is stateful, a complete '\n                             'input_shape must be provided '\n                             '(including batch size).')\n        if hasattr(self, 'states'):\n            K.set_value(self.states[0],\n                        np.zeros((input_shape[0], self.units)))\n            K.set_value(self.states[1],\n                        np.zeros((input_shape[0], self.units)))\n        else:\n            self.states = [K.zeros((input_shape[0], self.units)),\n                           K.zeros((input_shape[0], self.units))]\n\n    def preprocess_input(self, inputs, training=None):\n        if self.implementation == 0:\n            input_shape = K.int_shape(inputs)\n            input_dim = input_shape[2]\n            timesteps = input_shape[1]\n\n            x_i = _time_distributed_dense(inputs, self.kernel_i, self.bias_i,\n                                          self.dropout, input_dim, self.units,\n                                          timesteps, training=training)\n            x_f = _time_distributed_dense(inputs, self.kernel_f, self.bias_f,\n                                          self.dropout, input_dim, self.units,\n                                          timesteps, training=training)\n            x_c = _time_distributed_dense(inputs, self.kernel_c, self.bias_c,\n                                          self.dropout, input_dim, self.units,\n                                          timesteps, training=training)\n            x_o = _time_distributed_dense(inputs, self.kernel_o, self.bias_o,\n                                          self.dropout, input_dim, self.units,\n                                          timesteps, training=training)\n            return K.concatenate([x_i, x_f, x_c, x_o], axis=2)\n        else:\n            return inputs\n\n    def get_constants(self, inputs, training=None):\n        constants = []\n        if self.implementation == 0 and 0 < self.dropout < 1:\n            input_shape = K.int_shape(inputs)\n            input_dim = input_shape[-1]\n            ones = K.ones_like(K.reshape(inputs[:, 0, 0], (-1, 1)))\n            ones = K.tile(ones, (1, int(input_dim)))\n\n            def dropped_inputs():\n                return K.dropout(ones, self.dropout)\n\n            dp_mask = [K.in_train_phase(dropped_inputs,\n                                        ones,\n                                        training=training) for _ in range(4)]\n            constants.append(dp_mask)\n        else:\n            constants.append([K.cast_to_floatx(1.) for _ in range(4)])\n\n        if 0 < self.recurrent_dropout < 1:\n            ones = K.ones_like(K.reshape(inputs[:, 0, 0], (-1, 1)))\n            ones = K.tile(ones, (1, self.units))\n\n            def dropped_inputs():\n                return K.dropout(ones, self.recurrent_dropout)\n            rec_dp_mask = [K.in_train_phase(dropped_inputs,\n                                            ones,\n                                            training=training) for _ in range(4)]\n            constants.append(rec_dp_mask)\n        else:\n            constants.append([K.cast_to_floatx(1.) for _ in range(4)])\n        return constants\n\n    def step(self, inputs, states):\n        h_tm1 = states[0]\n        c_tm1 = states[1]\n        dp_mask = states[2]\n        rec_dp_mask = states[3]\n\n        if self.implementation == 1:\n            z = K.dot(inputs * dp_mask[0], self.kernel)\n            z += K.dot(h_tm1 * rec_dp_mask[0], self.recurrent_kernel)\n            if self.use_bias:\n                z += self.bias\n\n            z0 = z[:, :self.units]\n            z1 = z[:, self.units: 2 * self.units]\n            z2 = z[:, 2 * self.units: 3 * self.units]\n            z3 = z[:, 3 * self.units:]\n\n            i = self.recurrent_activation(z0)\n            f = self.recurrent_activation(z1)\n            c = f * c_tm1 + i * self.activation(z2)\n            o = self.recurrent_activation(z3)\n        else:\n            if self.implementation == 0:\n                x_i = inputs[:, :self.units]\n                x_f = inputs[:, self.units: 2 * self.units]\n                x_c = inputs[:, 2 * self.units: 3 * self.units]\n                x_o = inputs[:, 3 * self.units:]\n            elif self.implementation == 2:\n                x_i = K.dot(inputs * dp_mask[0], self.kernel_i) + self.bias_i\n                x_f = K.dot(inputs * dp_mask[1], self.kernel_f) + self.bias_f\n                x_c = K.dot(inputs * dp_mask[2], self.kernel_c) + self.bias_c\n                x_o = K.dot(inputs * dp_mask[3], self.kernel_o) + self.bias_o\n            else:\n                raise ValueError('Unknown `implementation` mode.')\n\n            i = self.recurrent_activation(x_i + K.dot(h_tm1 * rec_dp_mask[0],\n                                                      self.recurrent_kernel_i))\n            f = self.recurrent_activation(x_f + K.dot(h_tm1 * rec_dp_mask[1],\n                                                      self.recurrent_kernel_f))\n            c = f * c_tm1 + i * self.activation(x_c + K.dot(h_tm1 * rec_dp_mask[2],\n                                                            self.recurrent_kernel_c))\n            o = self.recurrent_activation(x_o + K.dot(h_tm1 * rec_dp_mask[3],\n                                                      self.recurrent_kernel_o))\n        h = o * self.activation(c)\n        if 0 < self.dropout + self.recurrent_dropout:\n            h._uses_learning_phase = True\n        return h, [h, c]\n\n    def get_config(self):\n        config = {'units': self.units,\n                  'activation': activations.serialize(self.activation),\n                  'recurrent_activation': activations.serialize(self.recurrent_activation),\n                  'use_bias': self.use_bias,\n                  'kernel_initializer': initializers.serialize(self.kernel_initializer),\n                  'recurrent_initializer': initializers.serialize(self.recurrent_initializer),\n                  'bias_initializer': initializers.serialize(self.bias_initializer),\n                  'unit_forget_bias': self.unit_forget_bias,\n                  'kernel_regularizer': regularizers.serialize(self.kernel_regularizer),\n                  'recurrent_regularizer': regularizers.serialize(self.recurrent_regularizer),\n                  'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n                  'kernel_constraint': constraints.serialize(self.kernel_constraint),\n                  'recurrent_constraint': constraints.serialize(self.recurrent_constraint),\n                  'bias_constraint': constraints.serialize(self.bias_constraint),\n                  'dropout': self.dropout,\n                  'recurrent_dropout': self.recurrent_dropout}\n        base_config = super(LSTM, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n",
          "file_after": "# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import\nimport numpy as np\n\nfrom .. import backend as K\nfrom .. import activations\nfrom .. import initializers\nfrom .. import regularizers\nfrom .. import constraints\nfrom ..engine import Layer\nfrom ..engine import InputSpec\n\n\ndef _time_distributed_dense(x, w, b=None, dropout=None,\n                            input_dim=None, output_dim=None,\n                            timesteps=None, training=None):\n    \"\"\"Apply `y . w + b` for every temporal slice y of x.\n\n    # Arguments\n        x: input tensor.\n        w: weight matrix.\n        b: optional bias vector.\n        dropout: wether to apply dropout (same dropout mask\n            for every temporal slice of the input).\n        input_dim: integer; optional dimensionality of the input.\n        output_dim: integer; optional dimensionality of the output.\n        timesteps: integer; optional number of timesteps.\n        training: training phase tensor or boolean.\n\n    # Returns\n        Output tensor.\n    \"\"\"\n    if not input_dim:\n        input_dim = K.shape(x)[2]\n    if not timesteps:\n        timesteps = K.shape(x)[1]\n    if not output_dim:\n        output_dim = K.shape(w)[1]\n\n    if dropout is not None and 0. < dropout < 1.:\n        # apply the same dropout pattern at every timestep\n        ones = K.ones_like(K.reshape(x[:, 0, :], (-1, input_dim)))\n        dropout_matrix = K.dropout(ones, dropout)\n        expanded_dropout_matrix = K.repeat(dropout_matrix, timesteps)\n        x = K.in_train_phase(x * expanded_dropout_matrix, x, training=training)\n\n    # collapse time dimension and batch dimension together\n    x = K.reshape(x, (-1, input_dim))\n    x = K.dot(x, w)\n    if b is not None:\n        x += b\n    # reshape to 3D tensor\n    if K.backend() == 'tensorflow':\n        x = K.reshape(x, K.stack([-1, timesteps, output_dim]))\n        x.set_shape([None, None, output_dim])\n    else:\n        x = K.reshape(x, (-1, timesteps, output_dim))\n    return x\n\n\nclass Recurrent(Layer):\n    \"\"\"Abstract base class for recurrent layers.\n\n    Do not use in a model -- it's not a valid layer!\n    Use its children classes `LSTM`, `GRU` and `SimpleRNN` instead.\n\n    All recurrent layers (`LSTM`, `GRU`, `SimpleRNN`) also\n    follow the specifications of this class and accept\n    the keyword arguments listed below.\n\n    # Example\n\n    ```python\n        # as the first layer in a Sequential model\n        model = Sequential()\n        model.add(LSTM(32, input_shape=(10, 64)))\n        # now model.output_shape == (None, 32)\n        # note: `None` is the batch dimension.\n\n        # the following is identical:\n        model = Sequential()\n        model.add(LSTM(32, input_dim=64, input_length=10))\n\n        # for subsequent layers, not need to specify the input size:\n        model.add(LSTM(16))\n    ```\n\n    # Arguments\n        weights: list of Numpy arrays to set as initial weights.\n            The list should have 3 elements, of shapes:\n            `[(input_dim, output_dim), (output_dim, output_dim), (output_dim,)]`.\n        return_sequences: Boolean. Whether to return the last output\n            in the output sequence, or the full sequence.\n        go_backwards: Boolean (default False).\n            If True, process the input sequence backwards.\n        stateful: Boolean (default False). If True, the last state\n            for each sample at index i in a batch will be used as initial\n            state for the sample of index i in the following batch.\n        unroll: Boolean (default False).\n            If True, the network will be unrolled,\n            else a symbolic loop will be used.\n            Unrolling can speed-up a RNN,\n            although it tends to be more memory-intensive.\n            Unrolling is only suitable for short sequences.\n        implementation: one of {0, 1, or 2}.\n            If set to 0, the RNN will use\n            an implementation that uses fewer, larger matrix products,\n            thus running faster on CPU but consuming more memory.\n            If set to 1, the RNN will use more matrix products,\n            but smaller ones, thus running slower\n            (may actually be faster on GPU) while consuming less memory.\n            If set to 2 (LSTM/GRU only),\n            the RNN will combine the input gate,\n            the forget gate and the output gate into a single matrix,\n            enabling more time-efficient parallelization on the GPU.\n            Note: RNN dropout must be shared for all gates,\n            resulting in a slightly reduced regularization.\n        input_dim: dimensionality of the input (integer).\n            This argument (or alternatively, the keyword argument `input_shape`)\n            is required when using this layer as the first layer in a model.\n        input_length: Length of input sequences, to be specified\n            when it is constant.\n            This argument is required if you are going to connect\n            `Flatten` then `Dense` layers upstream\n            (without it, the shape of the dense outputs cannot be computed).\n            Note that if the recurrent layer is not the first layer\n            in your model, you would need to specify the input length\n            at the level of the first layer\n            (e.g. via the `input_shape` argument)\n\n    # Input shapes\n        3D tensor with shape `(batch_size, timesteps, input_dim)`,\n        (Optional) 2D tensors with shape `(batch_size, output_dim)`.\n\n    # Output shape\n        - if `return_sequences`: 3D tensor with shape\n            `(batch_size, timesteps, units)`.\n        - else, 2D tensor with shape `(batch_size, units)`.\n\n    # Masking\n        This layer supports masking for input data with a variable number\n        of timesteps. To introduce masks to your data,\n        use an [Embedding](embeddings.md) layer with the `mask_zero` parameter\n        set to `True`.\n\n    # Note on using statefulness in RNNs\n        You can set RNN layers to be 'stateful', which means that the states\n        computed for the samples in one batch will be reused as initial states\n        for the samples in the next batch. This assumes a one-to-one mapping\n        between samples in different successive batches.\n\n        To enable statefulness:\n            - specify `stateful=True` in the layer constructor.\n            - specify a fixed batch size for your model, by passing\n                if sequential model:\n                  `batch_input_shape=(...)` to the first layer in your model.\n                else for functional model with 1 or more Input layers:\n                  `batch_shape=(...)` to all the first layers in your model.\n                This is the expected shape of your inputs\n                *including the batch size*.\n                It should be a tuple of integers, e.g. `(32, 10, 100)`.\n            - specify `shuffle=False` when calling fit().\n\n        To reset the states of your model, call `.reset_states()` on either\n        a specific layer, or on your entire model.\n\n    # Note on specifying initial states in RNNs\n        Most RNN layers can be called with either a single tensor, or a list\n        of tensors.\n        If the an RNN layer is called with a single tensor, that tensor is\n        treated as the input, and the initial states are assigned an\n        appropriate default value.\n        If an RNN layer is called with a list of tensors, the first element is\n        treated as the input, and the remaining tensors are treated as the\n        initial states.\n    \"\"\"\n\n    def __init__(self, return_sequences=False,\n                 go_backwards=False,\n                 stateful=False,\n                 unroll=False,\n                 implementation=0,\n                 **kwargs):\n        super(Recurrent, self).__init__(**kwargs)\n        self.return_sequences = return_sequences\n        self.go_backwards = go_backwards\n        self.stateful = stateful\n        self.unroll = unroll\n        self.implementation = 0\n        self.supports_masking = True\n        self.input_spec = None\n        self.dropout = 0\n        self.recurrent_dropout = 0\n\n    def compute_output_shape(self, input_shape):\n        if isinstance(input_shape, list):\n            input_shape = input_shape[0]\n        if self.return_sequences:\n            return (input_shape[0], input_shape[1], self.units)\n        else:\n            return (input_shape[0], self.units)\n\n    def compute_mask(self, inputs, mask):\n        if self.return_sequences:\n            return mask\n        else:\n            return None\n\n    def step(self, inputs, states):\n        raise NotImplementedError\n\n    def get_constants(self, inputs, training=None):\n        return []\n\n    def get_initial_states(self, inputs):\n        # build an all-zero tensor of shape (samples, output_dim)\n        initial_state = K.zeros_like(inputs)  # (samples, timesteps, input_dim)\n        initial_state = K.sum(initial_state, axis=(1, 2))  # (samples,)\n        initial_state = K.expand_dims(initial_state)  # (samples, 1)\n        initial_state = K.tile(initial_state, [1, self.units])  # (samples, output_dim)\n        initial_states = [initial_state for _ in range(len(self.states))]\n        return initial_states\n\n    def preprocess_input(self, inputs, training=None):\n        return inputs\n\n    def call(self, inputs, mask=None, training=None):\n        # input shape: (nbias_samples, time (padded with zeros), input_dim)\n        # note that the .build() method of subclasses MUST define\n        # self.input_spec with a complete input shape.\n        if isinstance(inputs, list):\n            initial_states = inputs[1:]\n            inputs = inputs[0]\n        elif self.stateful:\n            initial_states = self.states\n        else:\n            initial_states = self.get_initial_states(inputs)\n        input_shape = K.int_shape(inputs)\n        if self.unroll and input_shape[1] is None:\n            raise ValueError('Cannot unroll a RNN if the '\n                             'time dimension is undefined. \\n'\n                             '- If using a Sequential model, '\n                             'specify the time dimension by passing '\n                             'an `input_shape` or `batch_input_shape` '\n                             'argument to your first layer. If your '\n                             'first layer is an Embedding, you can '\n                             'also use the `input_length` argument.\\n'\n                             '- If using the functional API, specify '\n                             'the time dimension by passing a `shape` '\n                             'or `batch_shape` argument to your Input layer.')\n        constants = self.get_constants(inputs, training=None)\n        preprocessed_input = self.preprocess_input(inputs, training=None)\n        last_output, outputs, states = K.rnn(self.step,\n                                             preprocessed_input,\n                                             initial_states,\n                                             go_backwards=self.go_backwards,\n                                             mask=mask,\n                                             constants=constants,\n                                             unroll=self.unroll,\n                                             input_length=input_shape[1])\n        if self.stateful:\n            updates = []\n            for i in range(len(states)):\n                updates.append((self.states[i], states[i]))\n            self.add_update(updates, inputs)\n\n        # Properly set learning phase\n        if 0 < self.dropout + self.recurrent_dropout:\n            last_output._uses_learning_phase = True\n            outputs._uses_learning_phase = True\n\n        if self.return_sequences:\n            return outputs\n        else:\n            return last_output\n\n    def reset_states(self):\n        if not self.stateful:\n            raise AttributeError('Layer must be stateful.')\n        if not self.batch_size:\n            raise ValueError('If a RNN is stateful, it needs to know '\n                             'its batch size. Specify the batch size '\n                             'of your input tensors: \\n'\n                             '- If using a Sequential model, '\n                             'specify the batch size by passing '\n                             'a `batch_input_shape` '\n                             'argument to your first layer.\\n'\n                             '- If using the functional API, specify '\n                             'the time dimension by passing a '\n                             '`batch_shape` argument to your Input layer.')\n        if self.states[0] is not None:\n            for state in self.states:\n                K.set_value(state, np.zeros((self.batch_size, self.units)))\n        else:\n            self.states = [K.zeros((self.batch_size, self.units))\n                           for _ in self.states]\n\n    def get_config(self):\n        config = {'return_sequences': self.return_sequences,\n                  'go_backwards': self.go_backwards,\n                  'stateful': self.stateful,\n                  'unroll': self.unroll,\n                  'implementation': self.implementation}\n        base_config = super(Recurrent, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\n\nclass SimpleRNN(Recurrent):\n    \"\"\"Fully-connected RNN where the output is to be fed back to input.\n\n    # Arguments\n        units: Positive integer, dimensionality of the output space.\n        activation: Activation function to use\n            (see [activations](../activations.md)).\n            If you don't specify anything, no activation is applied\n            (ie. \"linear\" activation: `a(x) = x`).\n        use_bias: Boolean, whether the layer uses a bias vector.\n        kernel_initializer: Initializer for the `kernel` weights matrix,\n            used for the linear transformation of the inputs.\n            (see [initializers](../initializers.md)).\n        recurrent_initializer: Initializer for the `recurrent_kernel`\n            weights matrix,\n            used for the linear transformation of the recurrent state.\n            (see [initializers](../initializers.md)).\n        bias_initializer: Initializer for the bias vector\n            (see [initializers](../initializers.md)).\n        kernel_regularizer: Regularizer function applied to\n            the `kernel` weights matrix\n            (see [regularizer](../regularizers.md)).\n        recurrent_regularizer: Regularizer function applied to\n            the `recurrent_kernel` weights matrix\n            (see [regularizer](../regularizers.md)).\n        bias_regularizer: Regularizer function applied to the bias vector\n            (see [regularizer](../regularizers.md)).\n        activity_regularizer: Regularizer function applied to\n            the output of the layer (its \"activation\").\n            (see [regularizer](../regularizers.md)).\n        kernel_constraint: Constraint function applied to\n            the `kernel` weights matrix\n            (see [constraints](../constraints.md)).\n        recurrent_constraint: Constraint function applied to\n            the `recurrent_kernel` weights matrix\n            (see [constraints](../constraints.md)).\n        bias_constraint: Constraint function applied to the bias vector\n            (see [constraints](../constraints.md)).\n        dropout: Float between 0 and 1.\n            Fraction of the units to drop for\n            the linear transformation of the inputs.\n        recurrent_dropout: Float between 0 and 1.\n            Fraction of the units to drop for\n            the linear transformation of the recurrent state.\n\n    # References\n        - [A Theoretically Grounded Application of Dropout in Recurrent Neural Networks](http://arxiv.org/abs/1512.05287)\n    \"\"\"\n\n    def __init__(self, units,\n                 activation='tanh',\n                 use_bias=True,\n                 kernel_initializer='glorot_uniform',\n                 recurrent_initializer='orthogonal',\n                 bias_initializer='zeros',\n                 kernel_regularizer=None,\n                 recurrent_regularizer=None,\n                 bias_regularizer=None,\n                 activity_regularizer=None,\n                 kernel_constraint=None,\n                 recurrent_constraint=None,\n                 bias_constraint=None,\n                 dropout=0.,\n                 recurrent_dropout=0.,\n                 **kwargs):\n        super(SimpleRNN, self).__init__(**kwargs)\n        self.units = units\n        self.activation = activations.get(activation)\n        self.use_bias = use_bias\n\n        self.kernel_initializer = initializers.get(kernel_initializer)\n        self.recurrent_initializer = initializers.get(recurrent_initializer)\n        self.bias_initializer = initializers.get(bias_initializer)\n\n        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n        self.recurrent_regularizer = regularizers.get(recurrent_regularizer)\n        self.bias_regularizer = regularizers.get(bias_regularizer)\n\n        self.kernel_constraint = constraints.get(kernel_constraint)\n        self.recurrent_constraint = constraints.get(recurrent_constraint)\n        self.bias_constraint = constraints.get(bias_constraint)\n\n        self.dropout = min(1., max(0., dropout))\n        self.recurrent_dropout = min(1., max(0., recurrent_dropout))\n\n    def build(self, input_shape):\n        if isinstance(input_shape, list):\n            input_shape = input_shape[0]\n\n        self.batch_size = input_shape[0]\n        self.input_dim = input_shape[2]\n\n        self.states = [None]\n        if self.stateful:\n            self.reset_states()\n\n        self.kernel = self.add_weight((self.input_dim, self.units),\n                                      name='kernel',\n                                      initializer=self.kernel_initializer,\n                                      regularizer=self.kernel_regularizer,\n                                      constraint=self.kernel_constraint)\n        self.recurrent_kernel = self.add_weight(\n            (self.units, self.units),\n            name='recurrent_kernel',\n            initializer=self.recurrent_initializer,\n            regularizer=self.recurrent_regularizer,\n            constraint=self.recurrent_constraint)\n        if self.use_bias:\n            self.bias = self.add_weight((self.units,),\n                                        name='bias',\n                                        initializer=self.bias_initializer,\n                                        regularizer=self.bias_regularizer,\n                                        constraint=self.bias_constraint)\n        else:\n            self.bias = None\n        self.built = True\n\n    def preprocess_input(self, inputs, training=None):\n        if self.implementation > 0:\n            return inputs\n        else:\n            input_shape = K.int_shape(inputs)\n            input_dim = input_shape[2]\n            timesteps = input_shape[1]\n            return _time_distributed_dense(inputs,\n                                           self.kernel,\n                                           self.bias,\n                                           self.dropout,\n                                           input_dim,\n                                           self.units,\n                                           timesteps,\n                                           training=training)\n\n    def step(self, inputs, states):\n        if self.implementation == 0:\n            h = inputs\n        else:\n            if 0 < self.dropout < 1:\n                h = K.dot(inputs * states[1], self.kernel)\n            else:\n                h = K.dot(inputs, self.kernel)\n            if self.bias is not None:\n                h += self.bias\n\n        prev_output = states[0]\n        if 0 < self.recurrent_dropout < 1:\n            prev_output *= states[2]\n        output = h + K.dot(prev_output, self.recurrent_kernel)\n        if self.activation is not None:\n            output = self.activation(output)\n\n        # Properly set learning phase on output tensor.\n        if 0 < self.dropout + self.recurrent_dropout:\n            output._uses_learning_phase = True\n        return output, [output]\n\n    def get_constants(self, inputs, training=None):\n        constants = []\n        if self.implementation == 0 and 0 < self.dropout < 1:\n            input_shape = K.int_shape(inputs)\n            input_dim = input_shape[-1]\n            ones = K.ones_like(K.reshape(inputs[:, 0, 0], (-1, 1)))\n            ones = K.tile(ones, (1, int(input_dim)))\n\n            def dropped_inputs():\n                return K.dropout(ones, self.dropout)\n\n            dp_mask = K.in_train_phase(dropped_inputs,\n                                       ones,\n                                       training=training)\n            constants.append(dp_mask)\n        else:\n            constants.append(K.cast_to_floatx(1.))\n\n        if 0 < self.recurrent_dropout < 1:\n            ones = K.ones_like(K.reshape(inputs[:, 0, 0], (-1, 1)))\n            ones = K.tile(ones, (1, self.units))\n\n            def dropped_inputs():\n                return K.dropout(ones, self.recurrent_dropout)\n            rec_dp_mask = K.in_train_phase(dropped_inputs,\n                                           ones,\n                                           training=training)\n            constants.append(rec_dp_mask)\n        else:\n            constants.append(K.cast_to_floatx(1.))\n        return constants\n\n    def get_config(self):\n        config = {'units': self.units,\n                  'activation': activations.serialize(self.activation),\n                  'use_bias': self.use_bias,\n                  'kernel_initializer': initializers.serialize(self.kernel_initializer),\n                  'recurrent_initializer': initializers.serialize(self.recurrent_initializer),\n                  'bias_initializer': initializers.serialize(self.bias_initializer),\n                  'kernel_regularizer': regularizers.serialize(self.kernel_regularizer),\n                  'recurrent_regularizer': regularizers.serialize(self.recurrent_regularizer),\n                  'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n                  'kernel_constraint': constraints.serialize(self.kernel_constraint),\n                  'recurrent_constraint': constraints.serialize(self.recurrent_constraint),\n                  'bias_constraint': constraints.serialize(self.bias_constraint),\n                  'dropout': self.dropout,\n                  'recurrent_dropout': self.recurrent_dropout}\n        base_config = super(SimpleRNN, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\n\nclass GRU(Recurrent):\n    \"\"\"Gated Recurrent Unit - Cho et al. 2014.\n\n    # Arguments\n        units: Positive integer, dimensionality of the output space.\n        activation: Activation function to use\n            (see [activations](../activations.md)).\n            If you don't specify anything, no activation is applied\n            (ie. \"linear\" activation: `a(x) = x`).\n        recurrent_activation: Activation function to use\n            for the recurrent step\n            (see [activations](../activations.md)).\n        use_bias: Boolean, whether the layer uses a bias vector.\n        kernel_initializer: Initializer for the `kernel` weights matrix,\n            used for the linear transformation of the inputs.\n            (see [initializers](../initializers.md)).\n        recurrent_initializer: Initializer for the `recurrent_kernel`\n            weights matrix,\n            used for the linear transformation of the recurrent state.\n            (see [initializers](../initializers.md)).\n        bias_initializer: Initializer for the bias vector\n            (see [initializers](../initializers.md)).\n        kernel_regularizer: Regularizer function applied to\n            the `kernel` weights matrix\n            (see [regularizer](../regularizers.md)).\n        recurrent_regularizer: Regularizer function applied to\n            the `recurrent_kernel` weights matrix\n            (see [regularizer](../regularizers.md)).\n        bias_regularizer: Regularizer function applied to the bias vector\n            (see [regularizer](../regularizers.md)).\n        activity_regularizer: Regularizer function applied to\n            the output of the layer (its \"activation\").\n            (see [regularizer](../regularizers.md)).\n        kernel_constraint: Constraint function applied to\n            the `kernel` weights matrix\n            (see [constraints](../constraints.md)).\n        recurrent_constraint: Constraint function applied to\n            the `recurrent_kernel` weights matrix\n            (see [constraints](../constraints.md)).\n        bias_constraint: Constraint function applied to the bias vector\n            (see [constraints](../constraints.md)).\n        dropout: Float between 0 and 1.\n            Fraction of the units to drop for\n            the linear transformation of the inputs.\n        recurrent_dropout: Float between 0 and 1.\n            Fraction of the units to drop for\n            the linear transformation of the recurrent state.\n\n    # References\n        - [On the Properties of Neural Machine Translation: Encoder-Decoder Approaches](https://arxiv.org/abs/1409.1259)\n        - [Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling](http://arxiv.org/abs/1412.3555v1)\n        - [A Theoretically Grounded Application of Dropout in Recurrent Neural Networks](http://arxiv.org/abs/1512.05287)\n    \"\"\"\n\n    def __init__(self, units,\n                 activation='tanh',\n                 recurrent_activation='hard_sigmoid',\n                 use_bias=True,\n                 kernel_initializer='glorot_uniform',\n                 recurrent_initializer='orthogonal',\n                 bias_initializer='zeros',\n                 kernel_regularizer=None,\n                 recurrent_regularizer=None,\n                 bias_regularizer=None,\n                 activity_regularizer=None,\n                 kernel_constraint=None,\n                 recurrent_constraint=None,\n                 bias_constraint=None,\n                 dropout=0.,\n                 recurrent_dropout=0.,\n                 **kwargs):\n        super(GRU, self).__init__(**kwargs)\n        self.units = units\n        self.activation = activations.get(activation)\n        self.recurrent_activation = activations.get(recurrent_activation)\n        self.use_bias = use_bias\n\n        self.kernel_initializer = initializers.get(kernel_initializer)\n        self.recurrent_initializer = initializers.get(recurrent_initializer)\n        self.bias_initializer = initializers.get(bias_initializer)\n\n        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n        self.recurrent_regularizer = regularizers.get(recurrent_regularizer)\n        self.bias_regularizer = regularizers.get(bias_regularizer)\n\n        self.kernel_constraint = constraints.get(kernel_constraint)\n        self.recurrent_constraint = constraints.get(recurrent_constraint)\n        self.bias_constraint = constraints.get(bias_constraint)\n\n        self.dropout = min(1., max(0., dropout))\n        self.recurrent_dropout = min(1., max(0., recurrent_dropout))\n\n    def build(self, input_shape):\n        if isinstance(input_shape, list):\n            input_shape = input_shape[0]\n\n        self.batch_size = input_shape[0]\n        self.input_dim = input_shape[2]\n\n        self.states = [None]\n        if self.stateful:\n            self.reset_states()\n\n        self.kernel = self.add_weight((self.input_dim, self.units * 3),\n                                      name='kernel',\n                                      initializer=self.kernel_initializer,\n                                      regularizer=self.kernel_regularizer,\n                                      constraint=self.kernel_constraint)\n        self.recurrent_kernel = self.add_weight(\n            (self.units, self.units * 3),\n            name='recurrent_kernel',\n            initializer=self.recurrent_initializer,\n            regularizer=self.recurrent_regularizer,\n            constraint=self.recurrent_constraint)\n\n        if self.use_bias:\n            self.bias = self.add_weight((self.units * 3,),\n                                        name='bias',\n                                        initializer='zero',\n                                        regularizer=self.bias_regularizer,\n                                        constraint=self.bias_constraint)\n        else:\n            self.bias = None\n\n        self.kernel_z = self.kernel[:, :self.units]\n        self.recurrent_kernel_z = self.recurrent_kernel[:, :self.units]\n        self.kernel_r = self.kernel[:, self.units: self.units * 2]\n        self.recurrent_kernel_r = self.recurrent_kernel[:,\n                                                        self.units:\n                                                        self.units * 2]\n        self.kernel_h = self.kernel[:, self.units * 2:]\n        self.recurrent_kernel_h = self.recurrent_kernel[:, self.units * 2:]\n\n        if self.use_bias:\n            self.bias_z = self.bias[:self.units]\n            self.bias_r = self.bias[self.units: self.units * 2]\n            self.bias_h = self.bias[self.units * 2:]\n        else:\n            self.bias_z = None\n            self.bias_r = None\n            self.bias_h = None\n        self.built = True\n\n    def preprocess_input(self, inputs, training=None):\n        if self.implementation == 0:\n            input_shape = K.int_shape(inputs)\n            input_dim = input_shape[2]\n            timesteps = input_shape[1]\n\n            x_z = _time_distributed_dense(inputs, self.kernel_z, self.bias_z,\n                                          self.dropout, input_dim, self.units,\n                                          timesteps, training=training)\n            x_r = _time_distributed_dense(inputs, self.kernel_r, self.bias_r,\n                                          self.dropout, input_dim, self.units,\n                                          timesteps, training=training)\n            x_h = _time_distributed_dense(inputs, self.kernel_h, self.bias_h,\n                                          self.dropout, input_dim, self.units,\n                                          timesteps, training=training)\n            return K.concatenate([x_z, x_r, x_h], axis=2)\n        else:\n            return inputs\n\n    def get_constants(self, inputs, training=None):\n        constants = []\n        if self.implementation == 0 and 0 < self.dropout < 1:\n            input_shape = K.int_shape(inputs)\n            input_dim = input_shape[-1]\n            ones = K.ones_like(K.reshape(inputs[:, 0, 0], (-1, 1)))\n            ones = K.tile(ones, (1, int(input_dim)))\n\n            def dropped_inputs():\n                return K.dropout(ones, self.dropout)\n\n            dp_mask = [K.in_train_phase(dropped_inputs,\n                                        ones,\n                                        training=training) for _ in range(3)]\n            constants.append(dp_mask)\n        else:\n            constants.append([K.cast_to_floatx(1.) for _ in range(3)])\n\n        if 0 < self.recurrent_dropout < 1:\n            ones = K.ones_like(K.reshape(inputs[:, 0, 0], (-1, 1)))\n            ones = K.tile(ones, (1, self.units))\n\n            def dropped_inputs():\n                return K.dropout(ones, self.recurrent_dropout)\n            rec_dp_mask = [K.in_train_phase(dropped_inputs,\n                                            ones,\n                                            training=training) for _ in range(3)]\n            constants.append(rec_dp_mask)\n        else:\n            constants.append([K.cast_to_floatx(1.) for _ in range(3)])\n        return constants\n\n    def step(self, inputs, states):\n        h_tm1 = states[0]  # previous memory\n        dp_mask = states[1]  # dropout matrices for recurrent units\n        rec_dp_mask = states[2]\n\n        if self.implementation == 1:\n            matrix_x = K.dot(inputs * dp_mask[0], self.kernel)\n            if self.use_bias:\n                matrix_x += self.bias\n            matrix_inner = K.dot(h_tm1 * rec_dp_mask[0],\n                                 self.recurrent_kernel[:, :2 * self.units])\n\n            x_z = matrix_x[:, :self.units]\n            x_r = matrix_x[:, self.units: 2 * self.units]\n            recurrent_z = matrix_inner[:, :self.units]\n            recurrent_r = matrix_inner[:, self.units: 2 * self.units]\n\n            z = self.recurrent_activation(x_z + recurrent_z)\n            r = self.recurrent_activation(x_r + recurrent_r)\n\n            x_h = matrix_x[:, 2 * self.units:]\n            recurrent_h = K.dot(r * h_tm1 * rec_dp_mask[0],\n                                self.recurrent_kernel[:, 2 * self.units:])\n            hh = self.activation(x_h + recurrent_h)\n        else:\n            if self.implementation == 0:\n                x_z = inputs[:, :self.units]\n                x_r = inputs[:, self.units: 2 * self.units]\n                x_h = inputs[:, 2 * self.units:]\n            elif self.implementation == 2:\n                x_z = K.dot(inputs * dp_mask[0], self.kernel_z)\n                x_r = K.dot(inputs * dp_mask[1], self.kernel_r)\n                x_h = K.dot(inputs * dp_mask[2], self.kernel_h)\n                if self.use_bias:\n                    x_z += self.bias_z\n                    x_r += self.bias_r\n                    x_h += self.bias_h\n            else:\n                raise ValueError('Unknown `implementation` mode.')\n            z = self.recurrent_activation(x_z + K.dot(h_tm1 * rec_dp_mask[0],\n                                                      self.recurrent_kernel_z))\n            r = self.recurrent_activation(x_r + K.dot(h_tm1 * rec_dp_mask[1],\n                                                      self.recurrent_kernel_r))\n\n            hh = self.activation(x_h + K.dot(r * h_tm1 * rec_dp_mask[2],\n                                             self.recurrent_kernel_h))\n        h = z * h_tm1 + (1 - z) * hh\n        if 0 < self.dropout + self.recurrent_dropout:\n            h._uses_learning_phase = True\n        return h, [h]\n\n    def get_config(self):\n        config = {'units': self.units,\n                  'activation': activations.serialize(self.activation),\n                  'recurrent_activation': activations.serialize(self.recurrent_activation),\n                  'use_bias': self.use_bias,\n                  'kernel_initializer': initializers.serialize(self.kernel_initializer),\n                  'recurrent_initializer': initializers.serialize(self.recurrent_initializer),\n                  'bias_initializer': initializers.serialize(self.bias_initializer),\n                  'kernel_regularizer': regularizers.serialize(self.kernel_regularizer),\n                  'recurrent_regularizer': regularizers.serialize(self.recurrent_regularizer),\n                  'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n                  'kernel_constraint': constraints.serialize(self.kernel_constraint),\n                  'recurrent_constraint': constraints.serialize(self.recurrent_constraint),\n                  'bias_constraint': constraints.serialize(self.bias_constraint),\n                  'dropout': self.dropout,\n                  'recurrent_dropout': self.recurrent_dropout}\n        base_config = super(GRU, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\n\nclass LSTM(Recurrent):\n    \"\"\"Long-Short Term Memory unit - Hochreiter 1997.\n\n    For a step-by-step description of the algorithm, see\n    [this tutorial](http://deeplearning.net/tutorial/lstm.html).\n\n    # Arguments\n        units: Positive integer, dimensionality of the output space.\n        activation: Activation function to use\n            (see [activations](../activations.md)).\n            If you don't specify anything, no activation is applied\n            (ie. \"linear\" activation: `a(x) = x`).\n        recurrent_activation: Activation function to use\n            for the recurrent step\n            (see [activations](../activations.md)).\n        use_bias: Boolean, whether the layer uses a bias vector.\n        kernel_initializer: Initializer for the `kernel` weights matrix,\n            used for the linear transformation of the inputs.\n            (see [initializers](../initializers.md)).\n        recurrent_initializer: Initializer for the `recurrent_kernel`\n            weights matrix,\n            used for the linear transformation of the recurrent state.\n            (see [initializers](../initializers.md)).\n        bias_initializer: Initializer for the bias vector\n            (see [initializers](../initializers.md)).\n        unit_forget_bias: Boolean.\n            If True, add 1 to the bias of the forget gate at initialization.\n            Use in combination with `bias_initializer=\"zeros\"`.\n            This is recommended in [Jozefowicz et al.](http://www.jmlr.org/proceedings/papers/v37/jozefowicz15.pdf)\n        kernel_regularizer: Regularizer function applied to\n            the `kernel` weights matrix\n            (see [regularizer](../regularizers.md)).\n        recurrent_regularizer: Regularizer function applied to\n            the `recurrent_kernel` weights matrix\n            (see [regularizer](../regularizers.md)).\n        bias_regularizer: Regularizer function applied to the bias vector\n            (see [regularizer](../regularizers.md)).\n        activity_regularizer: Regularizer function applied to\n            the output of the layer (its \"activation\").\n            (see [regularizer](../regularizers.md)).\n        kernel_constraint: Constraint function applied to\n            the `kernel` weights matrix\n            (see [constraints](../constraints.md)).\n        recurrent_constraint: Constraint function applied to\n            the `recurrent_kernel` weights matrix\n            (see [constraints](../constraints.md)).\n        bias_constraint: Constraint function applied to the bias vector\n            (see [constraints](../constraints.md)).\n        dropout: Float between 0 and 1.\n            Fraction of the units to drop for\n            the linear transformation of the inputs.\n        recurrent_dropout: Float between 0 and 1.\n            Fraction of the units to drop for\n            the linear transformation of the recurrent state.\n\n    # References\n        - [Long short-term memory](http://deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf) (original 1997 paper)\n        - [Learning to forget: Continual prediction with LSTM](http://www.mitpressjournals.org/doi/pdf/10.1162/089976600300015015)\n        - [Supervised sequence labeling with recurrent neural networks](http://www.cs.toronto.edu/~graves/preprint.pdf)\n        - [A Theoretically Grounded Application of Dropout in Recurrent Neural Networks](http://arxiv.org/abs/1512.05287)\n    \"\"\"\n\n    def __init__(self, units,\n                 activation='tanh',\n                 recurrent_activation='hard_sigmoid',\n                 use_bias=True,\n                 kernel_initializer='glorot_uniform',\n                 recurrent_initializer='orthogonal',\n                 bias_initializer='zeros',\n                 unit_forget_bias=True,\n                 kernel_regularizer=None,\n                 recurrent_regularizer=None,\n                 bias_regularizer=None,\n                 activity_regularizer=None,\n                 kernel_constraint=None,\n                 recurrent_constraint=None,\n                 bias_constraint=None,\n                 dropout=0.,\n                 recurrent_dropout=0.,\n                 **kwargs):\n        super(LSTM, self).__init__(**kwargs)\n        self.units = units\n        self.activation = activations.get(activation)\n        self.recurrent_activation = activations.get(recurrent_activation)\n        self.use_bias = use_bias\n\n        self.kernel_initializer = initializers.get(kernel_initializer)\n        self.recurrent_initializer = initializers.get(recurrent_initializer)\n        self.bias_initializer = initializers.get(bias_initializer)\n        self.unit_forget_bias = unit_forget_bias\n\n        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n        self.recurrent_regularizer = regularizers.get(recurrent_regularizer)\n        self.bias_regularizer = regularizers.get(bias_regularizer)\n\n        self.kernel_constraint = constraints.get(kernel_constraint)\n        self.recurrent_constraint = constraints.get(recurrent_constraint)\n        self.bias_constraint = constraints.get(bias_constraint)\n\n        self.dropout = min(1., max(0., dropout))\n        self.recurrent_dropout = min(1., max(0., recurrent_dropout))\n\n    def build(self, input_shape):\n        if isinstance(input_shape, list):\n            input_shape = input_shape[0]\n\n        self.batch_size = input_shape[0]\n        self.input_dim = input_shape[2]\n\n        self.states = [None, None]\n        if self.stateful:\n            self.reset_states()\n\n        self.kernel = self.add_weight((self.input_dim, self.units * 4),\n                                      name='kernel',\n                                      initializer=self.kernel_initializer,\n                                      regularizer=self.kernel_regularizer,\n                                      constraint=self.kernel_constraint)\n        self.recurrent_kernel = self.add_weight(\n            (self.units, self.units * 4),\n            name='recurrent_kernel',\n            initializer=self.recurrent_initializer,\n            regularizer=self.recurrent_regularizer,\n            constraint=self.recurrent_constraint)\n\n        if self.use_bias:\n            self.bias = self.add_weight((self.units * 4,),\n                                        name='bias',\n                                        initializer=self.bias_initializer,\n                                        regularizer=self.bias_regularizer,\n                                        constraint=self.bias_constraint)\n            if self.unit_forget_bias:\n                self.bias += K.concatenate([K.zeros((self.units,)),\n                                           K.ones((self.units,)),\n                                           K.zeros((self.units * 2,))])\n        else:\n            self.bias = None\n\n        self.kernel_i = self.kernel[:, :self.units]\n        self.recurrent_kernel_i = self.recurrent_kernel[:, :self.units]\n        self.kernel_f = self.kernel[:, self.units: self.units * 2]\n        self.recurrent_kernel_f = self.recurrent_kernel[:, self.units: self.units * 2]\n        self.kernel_c = self.kernel[:, self.units * 2: self.units * 3]\n        self.recurrent_kernel_c = self.recurrent_kernel[:, self.units * 2: self.units * 3]\n        self.kernel_o = self.kernel[:, self.units * 3:]\n        self.recurrent_kernel_o = self.recurrent_kernel[:, self.units * 3:]\n\n        if self.use_bias:\n            self.bias_i = self.bias[:self.units]\n            self.bias_f = self.bias[self.units: self.units * 2]\n            self.bias_c = self.bias[self.units * 2: self.units * 3]\n            self.bias_o = self.bias[self.units * 3:]\n        else:\n            self.bias_i = None\n            self.bias_f = None\n            self.bias_c = None\n            self.bias_o = None\n        self.built = True\n\n    def preprocess_input(self, inputs, training=None):\n        if self.implementation == 0:\n            input_shape = K.int_shape(inputs)\n            input_dim = input_shape[2]\n            timesteps = input_shape[1]\n\n            x_i = _time_distributed_dense(inputs, self.kernel_i, self.bias_i,\n                                          self.dropout, input_dim, self.units,\n                                          timesteps, training=training)\n            x_f = _time_distributed_dense(inputs, self.kernel_f, self.bias_f,\n                                          self.dropout, input_dim, self.units,\n                                          timesteps, training=training)\n            x_c = _time_distributed_dense(inputs, self.kernel_c, self.bias_c,\n                                          self.dropout, input_dim, self.units,\n                                          timesteps, training=training)\n            x_o = _time_distributed_dense(inputs, self.kernel_o, self.bias_o,\n                                          self.dropout, input_dim, self.units,\n                                          timesteps, training=training)\n            return K.concatenate([x_i, x_f, x_c, x_o], axis=2)\n        else:\n            return inputs\n\n    def get_constants(self, inputs, training=None):\n        constants = []\n        if self.implementation == 0 and 0 < self.dropout < 1:\n            input_shape = K.int_shape(inputs)\n            input_dim = input_shape[-1]\n            ones = K.ones_like(K.reshape(inputs[:, 0, 0], (-1, 1)))\n            ones = K.tile(ones, (1, int(input_dim)))\n\n            def dropped_inputs():\n                return K.dropout(ones, self.dropout)\n\n            dp_mask = [K.in_train_phase(dropped_inputs,\n                                        ones,\n                                        training=training) for _ in range(4)]\n            constants.append(dp_mask)\n        else:\n            constants.append([K.cast_to_floatx(1.) for _ in range(4)])\n\n        if 0 < self.recurrent_dropout < 1:\n            ones = K.ones_like(K.reshape(inputs[:, 0, 0], (-1, 1)))\n            ones = K.tile(ones, (1, self.units))\n\n            def dropped_inputs():\n                return K.dropout(ones, self.recurrent_dropout)\n            rec_dp_mask = [K.in_train_phase(dropped_inputs,\n                                            ones,\n                                            training=training) for _ in range(4)]\n            constants.append(rec_dp_mask)\n        else:\n            constants.append([K.cast_to_floatx(1.) for _ in range(4)])\n        return constants\n\n    def step(self, inputs, states):\n        h_tm1 = states[0]\n        c_tm1 = states[1]\n        dp_mask = states[2]\n        rec_dp_mask = states[3]\n\n        if self.implementation == 1:\n            z = K.dot(inputs * dp_mask[0], self.kernel)\n            z += K.dot(h_tm1 * rec_dp_mask[0], self.recurrent_kernel)\n            if self.use_bias:\n                z += self.bias\n\n            z0 = z[:, :self.units]\n            z1 = z[:, self.units: 2 * self.units]\n            z2 = z[:, 2 * self.units: 3 * self.units]\n            z3 = z[:, 3 * self.units:]\n\n            i = self.recurrent_activation(z0)\n            f = self.recurrent_activation(z1)\n            c = f * c_tm1 + i * self.activation(z2)\n            o = self.recurrent_activation(z3)\n        else:\n            if self.implementation == 0:\n                x_i = inputs[:, :self.units]\n                x_f = inputs[:, self.units: 2 * self.units]\n                x_c = inputs[:, 2 * self.units: 3 * self.units]\n                x_o = inputs[:, 3 * self.units:]\n            elif self.implementation == 2:\n                x_i = K.dot(inputs * dp_mask[0], self.kernel_i) + self.bias_i\n                x_f = K.dot(inputs * dp_mask[1], self.kernel_f) + self.bias_f\n                x_c = K.dot(inputs * dp_mask[2], self.kernel_c) + self.bias_c\n                x_o = K.dot(inputs * dp_mask[3], self.kernel_o) + self.bias_o\n            else:\n                raise ValueError('Unknown `implementation` mode.')\n\n            i = self.recurrent_activation(x_i + K.dot(h_tm1 * rec_dp_mask[0],\n                                                      self.recurrent_kernel_i))\n            f = self.recurrent_activation(x_f + K.dot(h_tm1 * rec_dp_mask[1],\n                                                      self.recurrent_kernel_f))\n            c = f * c_tm1 + i * self.activation(x_c + K.dot(h_tm1 * rec_dp_mask[2],\n                                                            self.recurrent_kernel_c))\n            o = self.recurrent_activation(x_o + K.dot(h_tm1 * rec_dp_mask[3],\n                                                      self.recurrent_kernel_o))\n        h = o * self.activation(c)\n        if 0 < self.dropout + self.recurrent_dropout:\n            h._uses_learning_phase = True\n        return h, [h, c]\n\n    def get_config(self):\n        config = {'units': self.units,\n                  'activation': activations.serialize(self.activation),\n                  'recurrent_activation': activations.serialize(self.recurrent_activation),\n                  'use_bias': self.use_bias,\n                  'kernel_initializer': initializers.serialize(self.kernel_initializer),\n                  'recurrent_initializer': initializers.serialize(self.recurrent_initializer),\n                  'bias_initializer': initializers.serialize(self.bias_initializer),\n                  'unit_forget_bias': self.unit_forget_bias,\n                  'kernel_regularizer': regularizers.serialize(self.kernel_regularizer),\n                  'recurrent_regularizer': regularizers.serialize(self.recurrent_regularizer),\n                  'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n                  'kernel_constraint': constraints.serialize(self.kernel_constraint),\n                  'recurrent_constraint': constraints.serialize(self.recurrent_constraint),\n                  'bias_constraint': constraints.serialize(self.bias_constraint),\n                  'dropout': self.dropout,\n                  'recurrent_dropout': self.recurrent_dropout}\n        base_config = super(LSTM, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n",
          "file_patch": "@@ -274,6 +274,27 @@ class Recurrent(Layer):\n         else:\n             return last_output\n \n+    def reset_states(self):\n+        if not self.stateful:\n+            raise AttributeError('Layer must be stateful.')\n+        if not self.batch_size:\n+            raise ValueError('If a RNN is stateful, it needs to know '\n+                             'its batch size. Specify the batch size '\n+                             'of your input tensors: \\n'\n+                             '- If using a Sequential model, '\n+                             'specify the batch size by passing '\n+                             'a `batch_input_shape` '\n+                             'argument to your first layer.\\n'\n+                             '- If using the functional API, specify '\n+                             'the time dimension by passing a '\n+                             '`batch_shape` argument to your Input layer.')\n+        if self.states[0] is not None:\n+            for state in self.states:\n+                K.set_value(state, np.zeros((self.batch_size, self.units)))\n+        else:\n+            self.states = [K.zeros((self.batch_size, self.units))\n+                           for _ in self.states]\n+\n     def get_config(self):\n         config = {'return_sequences': self.return_sequences,\n                   'go_backwards': self.go_backwards,\n@@ -372,15 +393,15 @@ class SimpleRNN(Recurrent):\n     def build(self, input_shape):\n         if isinstance(input_shape, list):\n             input_shape = input_shape[0]\n+\n+        self.batch_size = input_shape[0]\n+        self.input_dim = input_shape[2]\n+\n+        self.states = [None]\n         if self.stateful:\n             self.reset_states()\n-        else:\n-            # Initial states: all-zero tensor of shape (output_dim).\n-            self.states = [None]\n-        input_dim = input_shape[2]\n-        self.input_dim = input_dim\n \n-        self.kernel = self.add_weight((input_dim, self.units),\n+        self.kernel = self.add_weight((self.input_dim, self.units),\n                                       name='kernel',\n                                       initializer=self.kernel_initializer,\n                                       regularizer=self.kernel_regularizer,\n@@ -401,27 +422,6 @@ class SimpleRNN(Recurrent):\n             self.bias = None\n         self.built = True\n \n-    def reset_states(self):\n-        if not self.stateful:\n-            raise AttributeError('Layer must be stateful.')\n-        input_shape = self.input_spec.shape\n-        if not input_shape[0]:\n-            raise ValueError('If a RNN is stateful, it needs to know '\n-                             'its batch size. Specify the batch size '\n-                             'of your input tensors: \\n'\n-                             '- If using a Sequential model, '\n-                             'specify the batch size by passing '\n-                             'a `batch_input_shape` '\n-                             'argument to your first layer.\\n'\n-                             '- If using the functional API, specify '\n-                             'the time dimension by passing a '\n-                             '`batch_shape` argument to your Input layer.')\n-        if hasattr(self, 'states'):\n-            K.set_value(self.states[0],\n-                        np.zeros((input_shape[0], self.units)))\n-        else:\n-            self.states = [K.zeros((input_shape[0], self.units))]\n-\n     def preprocess_input(self, inputs, training=None):\n         if self.implementation > 0:\n             return inputs\n@@ -607,13 +607,13 @@ class GRU(Recurrent):\n     def build(self, input_shape):\n         if isinstance(input_shape, list):\n             input_shape = input_shape[0]\n+\n+        self.batch_size = input_shape[0]\n         self.input_dim = input_shape[2]\n \n+        self.states = [None]\n         if self.stateful:\n             self.reset_states()\n-        else:\n-            # initial states: all-zero tensor of shape (output_dim)\n-            self.states = [None]\n \n         self.kernel = self.add_weight((self.input_dim, self.units * 3),\n                                       name='kernel',\n@@ -655,20 +655,6 @@ class GRU(Recurrent):\n             self.bias_h = None\n         self.built = True\n \n-    def reset_states(self):\n-        if not self.stateful:\n-            raise RuntimeError('Layer must be stateful.')\n-        input_shape = self.input_spec.shape\n-        if not input_shape[0]:\n-            raise ValueError('If a RNN is stateful, a complete '\n-                             'input_shape must be provided '\n-                             '(including batch size).')\n-        if hasattr(self, 'states'):\n-            K.set_value(self.states[0],\n-                        np.zeros((input_shape[0], self.units)))\n-        else:\n-            self.states = [K.zeros((input_shape[0], self.units))]\n-\n     def preprocess_input(self, inputs, training=None):\n         if self.implementation == 0:\n             input_shape = K.int_shape(inputs)\n@@ -896,13 +882,13 @@ class LSTM(Recurrent):\n     def build(self, input_shape):\n         if isinstance(input_shape, list):\n             input_shape = input_shape[0]\n+\n+        self.batch_size = input_shape[0]\n         self.input_dim = input_shape[2]\n \n+        self.states = [None, None]\n         if self.stateful:\n             self.reset_states()\n-        else:\n-            # initial states: 2 all-zero tensors of shape (output_dim)\n-            self.states = [None, None]\n \n         self.kernel = self.add_weight((self.input_dim, self.units * 4),\n                                       name='kernel',\n@@ -950,22 +936,6 @@ class LSTM(Recurrent):\n             self.bias_o = None\n         self.built = True\n \n-    def reset_states(self):\n-        assert self.stateful, 'Layer must be stateful.'\n-        input_shape = self.input_spec.shape\n-        if not input_shape[0]:\n-            raise ValueError('If a RNN is stateful, a complete '\n-                             'input_shape must be provided '\n-                             '(including batch size).')\n-        if hasattr(self, 'states'):\n-            K.set_value(self.states[0],\n-                        np.zeros((input_shape[0], self.units)))\n-            K.set_value(self.states[1],\n-                        np.zeros((input_shape[0], self.units)))\n-        else:\n-            self.states = [K.zeros((input_shape[0], self.units)),\n-                           K.zeros((input_shape[0], self.units))]\n-\n     def preprocess_input(self, inputs, training=None):\n         if self.implementation == 0:\n             input_shape = K.int_shape(inputs)\n",
          "files_name_in_blame_commit": [
            "recurrent.py"
          ]
        }
      },
      "a214f4e64d9e1f09a6ffdcad5e9595d0534e08f0": {
        "commit": {
          "commit_id": "a214f4e64d9e1f09a6ffdcad5e9595d0534e08f0",
          "commit_message": "specify initial states symbolically",
          "commit_author": "Joshua Chin",
          "commit_date": "2017-03-03 20:14:22",
          "commit_parent": "ada7aa12bfed8193cc4bb80e090dd3b4cf222c05"
        },
        "function": {
          "function_name": "build",
          "function_code_before": "def build(self, input_shape):\n    self.input_spec = InputSpec(shape=input_shape)\n    if self.stateful:\n        self.reset_states()\n    else:\n        self.states = [None]\n    input_dim = input_shape[2]\n    self.input_dim = input_dim\n    self.kernel = self.add_weight((input_dim, self.units), name='kernel', initializer=self.kernel_initializer, regularizer=self.kernel_regularizer, constraint=self.kernel_constraint)\n    self.recurrent_kernel = self.add_weight((self.units, self.units), name='recurrent_kernel', initializer=self.recurrent_initializer, regularizer=self.recurrent_regularizer, constraint=self.recurrent_constraint)\n    if self.use_bias:\n        self.bias = self.add_weight((self.units,), name='bias', initializer=self.bias_initializer, regularizer=self.bias_regularizer, constraint=self.bias_constraint)\n    else:\n        self.bias = None\n    self.built = True",
          "function_code_after": "def build(self, input_shape):\n    if isinstance(input_shape, list):\n        input_shape = input_shape[0]\n    if self.stateful:\n        self.reset_states()\n    else:\n        self.states = [None]\n    input_dim = input_shape[2]\n    self.input_dim = input_dim\n    self.kernel = self.add_weight((input_dim, self.units), name='kernel', initializer=self.kernel_initializer, regularizer=self.kernel_regularizer, constraint=self.kernel_constraint)\n    self.recurrent_kernel = self.add_weight((self.units, self.units), name='recurrent_kernel', initializer=self.recurrent_initializer, regularizer=self.recurrent_regularizer, constraint=self.recurrent_constraint)\n    if self.use_bias:\n        self.bias = self.add_weight((self.units,), name='bias', initializer=self.bias_initializer, regularizer=self.bias_regularizer, constraint=self.bias_constraint)\n    else:\n        self.bias = None\n    self.built = True",
          "function_before_start_line": 356,
          "function_before_end_line": 386,
          "function_after_start_line": 372,
          "function_after_end_line": 402,
          "function_before_token_count": 179,
          "function_after_token_count": 183,
          "functions_name_modified_file": [
            "__init__",
            "compute_mask",
            "build",
            "call",
            "get_constants",
            "_time_distributed_dense",
            "compute_output_shape",
            "step",
            "get_initial_states",
            "reset_states",
            "preprocess_input",
            "get_config"
          ],
          "functions_name_all_files": [
            "__init__",
            "compute_mask",
            "build",
            "call",
            "get_constants",
            "_time_distributed_dense",
            "compute_output_shape",
            "step",
            "get_initial_states",
            "reset_states",
            "preprocess_input",
            "get_config"
          ],
          "functions_name_co_evolved_modified_file": [
            "__init__",
            "call",
            "compute_output_shape"
          ],
          "functions_name_co_evolved_all_files": [
            "__init__",
            "call",
            "compute_output_shape"
          ]
        },
        "file": {
          "file_name": "recurrent.py",
          "file_nloc": 967,
          "file_complexity": 113,
          "file_token_count": 5844,
          "file_before": "# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import\nimport numpy as np\n\nfrom .. import backend as K\nfrom .. import activations\nfrom .. import initializers\nfrom .. import regularizers\nfrom .. import constraints\nfrom ..engine import Layer\nfrom ..engine import InputSpec\n\n\ndef _time_distributed_dense(x, w, b=None, dropout=None,\n                            input_dim=None, output_dim=None,\n                            timesteps=None, training=None):\n    \"\"\"Apply `y . w + b` for every temporal slice y of x.\n\n    # Arguments\n        x: input tensor.\n        w: weight matrix.\n        b: optional bias vector.\n        dropout: wether to apply dropout (same dropout mask\n            for every temporal slice of the input).\n        input_dim: integer; optional dimensionality of the input.\n        output_dim: integer; optional dimensionality of the output.\n        timesteps: integer; optional number of timesteps.\n        training: training phase tensor or boolean.\n\n    # Returns\n        Output tensor.\n    \"\"\"\n    if not input_dim:\n        input_dim = K.shape(x)[2]\n    if not timesteps:\n        timesteps = K.shape(x)[1]\n    if not output_dim:\n        output_dim = K.shape(w)[1]\n\n    if dropout is not None and 0. < dropout < 1.:\n        # apply the same dropout pattern at every timestep\n        ones = K.ones_like(K.reshape(x[:, 0, :], (-1, input_dim)))\n        dropout_matrix = K.dropout(ones, dropout)\n        expanded_dropout_matrix = K.repeat(dropout_matrix, timesteps)\n        x = K.in_train_phase(x * expanded_dropout_matrix, x, training=training)\n\n    # collapse time dimension and batch dimension together\n    x = K.reshape(x, (-1, input_dim))\n    x = K.dot(x, w)\n    if b is not None:\n        x += b\n    # reshape to 3D tensor\n    if K.backend() == 'tensorflow':\n        x = K.reshape(x, K.stack([-1, timesteps, output_dim]))\n        x.set_shape([None, None, output_dim])\n    else:\n        x = K.reshape(x, (-1, timesteps, output_dim))\n    return x\n\n\nclass Recurrent(Layer):\n    \"\"\"Abstract base class for recurrent layers.\n\n    Do not use in a model -- it's not a valid layer!\n    Use its children classes `LSTM`, `GRU` and `SimpleRNN` instead.\n\n    All recurrent layers (`LSTM`, `GRU`, `SimpleRNN`) also\n    follow the specifications of this class and accept\n    the keyword arguments listed below.\n\n    # Example\n\n    ```python\n        # as the first layer in a Sequential model\n        model = Sequential()\n        model.add(LSTM(32, input_shape=(10, 64)))\n        # now model.output_shape == (None, 32)\n        # note: `None` is the batch dimension.\n\n        # the following is identical:\n        model = Sequential()\n        model.add(LSTM(32, input_dim=64, input_length=10))\n\n        # for subsequent layers, not need to specify the input size:\n        model.add(LSTM(16))\n    ```\n\n    # Arguments\n        weights: list of Numpy arrays to set as initial weights.\n            The list should have 3 elements, of shapes:\n            `[(input_dim, output_dim), (output_dim, output_dim), (output_dim,)]`.\n        return_sequences: Boolean. Whether to return the last output\n            in the output sequence, or the full sequence.\n        go_backwards: Boolean (default False).\n            If True, process the input sequence backwards.\n        stateful: Boolean (default False). If True, the last state\n            for each sample at index i in a batch will be used as initial\n            state for the sample of index i in the following batch.\n        unroll: Boolean (default False).\n            If True, the network will be unrolled,\n            else a symbolic loop will be used.\n            Unrolling can speed-up a RNN,\n            although it tends to be more memory-intensive.\n            Unrolling is only suitable for short sequences.\n        implementation: one of {0, 1, or 2}.\n            If set to 0, the RNN will use\n            an implementation that uses fewer, larger matrix products,\n            thus running faster on CPU but consuming more memory.\n            If set to 1, the RNN will use more matrix products,\n            but smaller ones, thus running slower\n            (may actually be faster on GPU) while consuming less memory.\n            If set to 2 (LSTM/GRU only),\n            the RNN will combine the input gate,\n            the forget gate and the output gate into a single matrix,\n            enabling more time-efficient parallelization on the GPU.\n            Note: RNN dropout must be shared for all gates,\n            resulting in a slightly reduced regularization.\n        input_dim: dimensionality of the input (integer).\n            This argument (or alternatively, the keyword argument `input_shape`)\n            is required when using this layer as the first layer in a model.\n        input_length: Length of input sequences, to be specified\n            when it is constant.\n            This argument is required if you are going to connect\n            `Flatten` then `Dense` layers upstream\n            (without it, the shape of the dense outputs cannot be computed).\n            Note that if the recurrent layer is not the first layer\n            in your model, you would need to specify the input length\n            at the level of the first layer\n            (e.g. via the `input_shape` argument)\n\n    # Input shape\n        3D tensor with shape `(batch_size, timesteps, input_dim)`.\n\n    # Output shape\n        - if `return_sequences`: 3D tensor with shape\n            `(batch_size, timesteps, units)`.\n        - else, 2D tensor with shape `(batch_size, units)`.\n\n    # Masking\n        This layer supports masking for input data with a variable number\n        of timesteps. To introduce masks to your data,\n        use an [Embedding](embeddings.md) layer with the `mask_zero` parameter\n        set to `True`.\n\n    # Note on using statefulness in RNNs\n        You can set RNN layers to be 'stateful', which means that the states\n        computed for the samples in one batch will be reused as initial states\n        for the samples in the next batch. This assumes a one-to-one mapping\n        between samples in different successive batches.\n\n        To enable statefulness:\n            - specify `stateful=True` in the layer constructor.\n            - specify a fixed batch size for your model, by passing\n                if sequential model:\n                  `batch_input_shape=(...)` to the first layer in your model.\n                else for functional model with 1 or more Input layers:\n                  `batch_shape=(...)` to all the first layers in your model.\n                This is the expected shape of your inputs\n                *including the batch size*.\n                It should be a tuple of integers, e.g. `(32, 10, 100)`.\n            - specify `shuffle=False` when calling fit().\n\n        To reset the states of your model, call `.reset_states()` on either\n        a specific layer, or on your entire model.\n    \"\"\"\n\n    def __init__(self, return_sequences=False,\n                 go_backwards=False,\n                 stateful=False,\n                 unroll=False,\n                 implementation=0,\n                 **kwargs):\n        super(Recurrent, self).__init__(**kwargs)\n        self.return_sequences = return_sequences\n        self.go_backwards = go_backwards\n        self.stateful = stateful\n        self.unroll = unroll\n        self.implementation = 0\n        self.supports_masking = True\n        self.input_spec = InputSpec(ndim=3)\n        self.dropout = 0\n        self.recurrent_dropout = 0\n\n    def compute_output_shape(self, input_shape):\n        if self.return_sequences:\n            return (input_shape[0], input_shape[1], self.units)\n        else:\n            return (input_shape[0], self.units)\n\n    def compute_mask(self, inputs, mask):\n        if self.return_sequences:\n            return mask\n        else:\n            return None\n\n    def step(self, inputs, states):\n        raise NotImplementedError\n\n    def get_constants(self, inputs, training=None):\n        return []\n\n    def get_initial_states(self, inputs):\n        # build an all-zero tensor of shape (samples, output_dim)\n        initial_state = K.zeros_like(inputs)  # (samples, timesteps, input_dim)\n        initial_state = K.sum(initial_state, axis=(1, 2))  # (samples,)\n        initial_state = K.expand_dims(initial_state)  # (samples, 1)\n        initial_state = K.tile(initial_state, [1, self.units])  # (samples, output_dim)\n        initial_states = [initial_state for _ in range(len(self.states))]\n        return initial_states\n\n    def preprocess_input(self, inputs, training=None):\n        return inputs\n\n    def call(self, inputs, mask=None, training=None):\n        # input shape: (nbias_samples, time (padded with zeros), input_dim)\n        # note that the .build() method of subclasses MUST define\n        # self.input_spec with a complete input shape.\n        input_shape = K.int_shape(inputs)\n        if self.unroll and input_shape[1] is None:\n            raise ValueError('Cannot unroll a RNN if the '\n                             'time dimension is undefined. \\n'\n                             '- If using a Sequential model, '\n                             'specify the time dimension by passing '\n                             'an `input_shape` or `batch_input_shape` '\n                             'argument to your first layer. If your '\n                             'first layer is an Embedding, you can '\n                             'also use the `input_length` argument.\\n'\n                             '- If using the functional API, specify '\n                             'the time dimension by passing a `shape` '\n                             'or `batch_shape` argument to your Input layer.')\n        if self.stateful:\n            initial_states = self.states\n        else:\n            initial_states = self.get_initial_states(inputs)\n        constants = self.get_constants(inputs, training=None)\n        preprocessed_input = self.preprocess_input(inputs, training=None)\n        last_output, outputs, states = K.rnn(self.step,\n                                             preprocessed_input,\n                                             initial_states,\n                                             go_backwards=self.go_backwards,\n                                             mask=mask,\n                                             constants=constants,\n                                             unroll=self.unroll,\n                                             input_length=input_shape[1])\n        if self.stateful:\n            updates = []\n            for i in range(len(states)):\n                updates.append((self.states[i], states[i]))\n            self.add_update(updates, inputs)\n\n        # Properly set learning phase\n        if 0 < self.dropout + self.recurrent_dropout:\n            last_output._uses_learning_phase = True\n            outputs._uses_learning_phase = True\n\n        if self.return_sequences:\n            return outputs\n        else:\n            return last_output\n\n    def get_config(self):\n        config = {'return_sequences': self.return_sequences,\n                  'go_backwards': self.go_backwards,\n                  'stateful': self.stateful,\n                  'unroll': self.unroll,\n                  'implementation': self.implementation}\n        base_config = super(Recurrent, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\n\nclass SimpleRNN(Recurrent):\n    \"\"\"Fully-connected RNN where the output is to be fed back to input.\n\n    # Arguments\n        units: Positive integer, dimensionality of the output space.\n        activation: Activation function to use\n            (see [activations](../activations.md)).\n            If you don't specify anything, no activation is applied\n            (ie. \"linear\" activation: `a(x) = x`).\n        use_bias: Boolean, whether the layer uses a bias vector.\n        kernel_initializer: Initializer for the `kernel` weights matrix,\n            used for the linear transformation of the inputs.\n            (see [initializers](../initializers.md)).\n        recurrent_initializer: Initializer for the `recurrent_kernel`\n            weights matrix,\n            used for the linear transformation of the recurrent state.\n            (see [initializers](../initializers.md)).\n        bias_initializer: Initializer for the bias vector\n            (see [initializers](../initializers.md)).\n        kernel_regularizer: Regularizer function applied to\n            the `kernel` weights matrix\n            (see [regularizer](../regularizers.md)).\n        recurrent_regularizer: Regularizer function applied to\n            the `recurrent_kernel` weights matrix\n            (see [regularizer](../regularizers.md)).\n        bias_regularizer: Regularizer function applied to the bias vector\n            (see [regularizer](../regularizers.md)).\n        activity_regularizer: Regularizer function applied to\n            the output of the layer (its \"activation\").\n            (see [regularizer](../regularizers.md)).\n        kernel_constraint: Constraint function applied to\n            the `kernel` weights matrix\n            (see [constraints](../constraints.md)).\n        recurrent_constraint: Constraint function applied to\n            the `recurrent_kernel` weights matrix\n            (see [constraints](../constraints.md)).\n        bias_constraint: Constraint function applied to the bias vector\n            (see [constraints](../constraints.md)).\n        dropout: Float between 0 and 1.\n            Fraction of the units to drop for\n            the linear transformation of the inputs.\n        recurrent_dropout: Float between 0 and 1.\n            Fraction of the units to drop for\n            the linear transformation of the recurrent state.\n\n    # References\n        - [A Theoretically Grounded Application of Dropout in Recurrent Neural Networks](http://arxiv.org/abs/1512.05287)\n    \"\"\"\n\n    def __init__(self, units,\n                 activation='tanh',\n                 use_bias=True,\n                 kernel_initializer='glorot_uniform',\n                 recurrent_initializer='orthogonal',\n                 bias_initializer='zeros',\n                 kernel_regularizer=None,\n                 recurrent_regularizer=None,\n                 bias_regularizer=None,\n                 activity_regularizer=None,\n                 kernel_constraint=None,\n                 recurrent_constraint=None,\n                 bias_constraint=None,\n                 dropout=0.,\n                 recurrent_dropout=0.,\n                 **kwargs):\n        super(SimpleRNN, self).__init__(**kwargs)\n        self.units = units\n        self.activation = activations.get(activation)\n        self.use_bias = use_bias\n\n        self.kernel_initializer = initializers.get(kernel_initializer)\n        self.recurrent_initializer = initializers.get(recurrent_initializer)\n        self.bias_initializer = initializers.get(bias_initializer)\n\n        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n        self.recurrent_regularizer = regularizers.get(recurrent_regularizer)\n        self.bias_regularizer = regularizers.get(bias_regularizer)\n\n        self.kernel_constraint = constraints.get(kernel_constraint)\n        self.recurrent_constraint = constraints.get(recurrent_constraint)\n        self.bias_constraint = constraints.get(bias_constraint)\n\n        self.dropout = min(1., max(0., dropout))\n        self.recurrent_dropout = min(1., max(0., recurrent_dropout))\n\n    def build(self, input_shape):\n        # TODO: handle variable-length seqs in input_spec\n        self.input_spec = InputSpec(shape=input_shape)\n        if self.stateful:\n            self.reset_states()\n        else:\n            # Initial states: all-zero tensor of shape (output_dim).\n            self.states = [None]\n        input_dim = input_shape[2]\n        self.input_dim = input_dim\n\n        self.kernel = self.add_weight((input_dim, self.units),\n                                      name='kernel',\n                                      initializer=self.kernel_initializer,\n                                      regularizer=self.kernel_regularizer,\n                                      constraint=self.kernel_constraint)\n        self.recurrent_kernel = self.add_weight(\n            (self.units, self.units),\n            name='recurrent_kernel',\n            initializer=self.recurrent_initializer,\n            regularizer=self.recurrent_regularizer,\n            constraint=self.recurrent_constraint)\n        if self.use_bias:\n            self.bias = self.add_weight((self.units,),\n                                        name='bias',\n                                        initializer=self.bias_initializer,\n                                        regularizer=self.bias_regularizer,\n                                        constraint=self.bias_constraint)\n        else:\n            self.bias = None\n        self.built = True\n\n    def reset_states(self):\n        if not self.stateful:\n            raise AttributeError('Layer must be stateful.')\n        input_shape = self.input_spec.shape\n        if not input_shape[0]:\n            raise ValueError('If a RNN is stateful, it needs to know '\n                             'its batch size. Specify the batch size '\n                             'of your input tensors: \\n'\n                             '- If using a Sequential model, '\n                             'specify the batch size by passing '\n                             'a `batch_input_shape` '\n                             'argument to your first layer.\\n'\n                             '- If using the functional API, specify '\n                             'the time dimension by passing a '\n                             '`batch_shape` argument to your Input layer.')\n        if hasattr(self, 'states'):\n            K.set_value(self.states[0],\n                        np.zeros((input_shape[0], self.units)))\n        else:\n            self.states = [K.zeros((input_shape[0], self.units))]\n\n    def preprocess_input(self, inputs, training=None):\n        if self.implementation > 0:\n            return inputs\n        else:\n            input_shape = K.int_shape(inputs)\n            input_dim = input_shape[2]\n            timesteps = input_shape[1]\n            return _time_distributed_dense(inputs,\n                                           self.kernel,\n                                           self.bias,\n                                           self.dropout,\n                                           input_dim,\n                                           self.units,\n                                           timesteps,\n                                           training=training)\n\n    def step(self, inputs, states):\n        if self.implementation == 0:\n            h = inputs\n        else:\n            if 0 < self.dropout < 1:\n                h = K.dot(inputs * states[1], self.kernel)\n            else:\n                h = K.dot(inputs, self.kernel)\n            if self.bias is not None:\n                h += self.bias\n\n        prev_output = states[0]\n        if 0 < self.recurrent_dropout < 1:\n            prev_output *= states[2]\n        output = h + K.dot(prev_output, self.recurrent_kernel)\n        if self.activation is not None:\n            output = self.activation(output)\n\n        # Properly set learning phase on output tensor.\n        if 0 < self.dropout + self.recurrent_dropout:\n            output._uses_learning_phase = True\n        return output, [output]\n\n    def get_constants(self, inputs, training=None):\n        constants = []\n        if self.implementation == 0 and 0 < self.dropout < 1:\n            input_shape = K.int_shape(inputs)\n            input_dim = input_shape[-1]\n            ones = K.ones_like(K.reshape(inputs[:, 0, 0], (-1, 1)))\n            ones = K.tile(ones, (1, int(input_dim)))\n\n            def dropped_inputs():\n                return K.dropout(ones, self.dropout)\n\n            dp_mask = K.in_train_phase(dropped_inputs,\n                                       ones,\n                                       training=training)\n            constants.append(dp_mask)\n        else:\n            constants.append(K.cast_to_floatx(1.))\n\n        if 0 < self.recurrent_dropout < 1:\n            ones = K.ones_like(K.reshape(inputs[:, 0, 0], (-1, 1)))\n            ones = K.tile(ones, (1, self.units))\n\n            def dropped_inputs():\n                return K.dropout(ones, self.recurrent_dropout)\n            rec_dp_mask = K.in_train_phase(dropped_inputs,\n                                           ones,\n                                           training=training)\n            constants.append(rec_dp_mask)\n        else:\n            constants.append(K.cast_to_floatx(1.))\n        return constants\n\n    def get_config(self):\n        config = {'units': self.units,\n                  'activation': activations.serialize(self.activation),\n                  'use_bias': self.use_bias,\n                  'kernel_initializer': initializers.serialize(self.kernel_initializer),\n                  'recurrent_initializer': initializers.serialize(self.recurrent_initializer),\n                  'bias_initializer': initializers.serialize(self.bias_initializer),\n                  'kernel_regularizer': regularizers.serialize(self.kernel_regularizer),\n                  'recurrent_regularizer': regularizers.serialize(self.recurrent_regularizer),\n                  'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n                  'kernel_constraint': constraints.serialize(self.kernel_constraint),\n                  'recurrent_constraint': constraints.serialize(self.recurrent_constraint),\n                  'bias_constraint': constraints.serialize(self.bias_constraint),\n                  'dropout': self.dropout,\n                  'recurrent_dropout': self.recurrent_dropout}\n        base_config = super(SimpleRNN, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\n\nclass GRU(Recurrent):\n    \"\"\"Gated Recurrent Unit - Cho et al. 2014.\n\n    # Arguments\n        units: Positive integer, dimensionality of the output space.\n        activation: Activation function to use\n            (see [activations](../activations.md)).\n            If you don't specify anything, no activation is applied\n            (ie. \"linear\" activation: `a(x) = x`).\n        recurrent_activation: Activation function to use\n            for the recurrent step\n            (see [activations](../activations.md)).\n        use_bias: Boolean, whether the layer uses a bias vector.\n        kernel_initializer: Initializer for the `kernel` weights matrix,\n            used for the linear transformation of the inputs.\n            (see [initializers](../initializers.md)).\n        recurrent_initializer: Initializer for the `recurrent_kernel`\n            weights matrix,\n            used for the linear transformation of the recurrent state.\n            (see [initializers](../initializers.md)).\n        bias_initializer: Initializer for the bias vector\n            (see [initializers](../initializers.md)).\n        kernel_regularizer: Regularizer function applied to\n            the `kernel` weights matrix\n            (see [regularizer](../regularizers.md)).\n        recurrent_regularizer: Regularizer function applied to\n            the `recurrent_kernel` weights matrix\n            (see [regularizer](../regularizers.md)).\n        bias_regularizer: Regularizer function applied to the bias vector\n            (see [regularizer](../regularizers.md)).\n        activity_regularizer: Regularizer function applied to\n            the output of the layer (its \"activation\").\n            (see [regularizer](../regularizers.md)).\n        kernel_constraint: Constraint function applied to\n            the `kernel` weights matrix\n            (see [constraints](../constraints.md)).\n        recurrent_constraint: Constraint function applied to\n            the `recurrent_kernel` weights matrix\n            (see [constraints](../constraints.md)).\n        bias_constraint: Constraint function applied to the bias vector\n            (see [constraints](../constraints.md)).\n        dropout: Float between 0 and 1.\n            Fraction of the units to drop for\n            the linear transformation of the inputs.\n        recurrent_dropout: Float between 0 and 1.\n            Fraction of the units to drop for\n            the linear transformation of the recurrent state.\n\n    # References\n        - [On the Properties of Neural Machine Translation: Encoder-Decoder Approaches](https://arxiv.org/abs/1409.1259)\n        - [Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling](http://arxiv.org/abs/1412.3555v1)\n        - [A Theoretically Grounded Application of Dropout in Recurrent Neural Networks](http://arxiv.org/abs/1512.05287)\n    \"\"\"\n\n    def __init__(self, units,\n                 activation='tanh',\n                 recurrent_activation='hard_sigmoid',\n                 use_bias=True,\n                 kernel_initializer='glorot_uniform',\n                 recurrent_initializer='orthogonal',\n                 bias_initializer='zeros',\n                 kernel_regularizer=None,\n                 recurrent_regularizer=None,\n                 bias_regularizer=None,\n                 activity_regularizer=None,\n                 kernel_constraint=None,\n                 recurrent_constraint=None,\n                 bias_constraint=None,\n                 dropout=0.,\n                 recurrent_dropout=0.,\n                 **kwargs):\n        super(GRU, self).__init__(**kwargs)\n        self.units = units\n        self.activation = activations.get(activation)\n        self.recurrent_activation = activations.get(recurrent_activation)\n        self.use_bias = use_bias\n\n        self.kernel_initializer = initializers.get(kernel_initializer)\n        self.recurrent_initializer = initializers.get(recurrent_initializer)\n        self.bias_initializer = initializers.get(bias_initializer)\n\n        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n        self.recurrent_regularizer = regularizers.get(recurrent_regularizer)\n        self.bias_regularizer = regularizers.get(bias_regularizer)\n\n        self.kernel_constraint = constraints.get(kernel_constraint)\n        self.recurrent_constraint = constraints.get(recurrent_constraint)\n        self.bias_constraint = constraints.get(bias_constraint)\n\n        self.dropout = min(1., max(0., dropout))\n        self.recurrent_dropout = min(1., max(0., recurrent_dropout))\n\n    def build(self, input_shape):\n        # TODO: handle variable-length sequences in input spec.\n        self.input_spec = InputSpec(shape=input_shape)\n        self.input_dim = input_shape[2]\n\n        if self.stateful:\n            self.reset_states()\n        else:\n            # initial states: all-zero tensor of shape (output_dim)\n            self.states = [None]\n\n        self.kernel = self.add_weight((self.input_dim, self.units * 3),\n                                      name='kernel',\n                                      initializer=self.kernel_initializer,\n                                      regularizer=self.kernel_regularizer,\n                                      constraint=self.kernel_constraint)\n        self.recurrent_kernel = self.add_weight(\n            (self.units, self.units * 3),\n            name='recurrent_kernel',\n            initializer=self.recurrent_initializer,\n            regularizer=self.recurrent_regularizer,\n            constraint=self.recurrent_constraint)\n\n        if self.use_bias:\n            self.bias = self.add_weight((self.units * 3,),\n                                        name='bias',\n                                        initializer='zero',\n                                        regularizer=self.bias_regularizer,\n                                        constraint=self.bias_constraint)\n        else:\n            self.bias = None\n\n        self.kernel_z = self.kernel[:, :self.units]\n        self.recurrent_kernel_z = self.recurrent_kernel[:, :self.units]\n        self.kernel_r = self.kernel[:, self.units: self.units * 2]\n        self.recurrent_kernel_r = self.recurrent_kernel[:,\n                                                        self.units:\n                                                        self.units * 2]\n        self.kernel_h = self.kernel[:, self.units * 2:]\n        self.recurrent_kernel_h = self.recurrent_kernel[:, self.units * 2:]\n\n        if self.use_bias:\n            self.bias_z = self.bias[:self.units]\n            self.bias_r = self.bias[self.units: self.units * 2]\n            self.bias_h = self.bias[self.units * 2:]\n        else:\n            self.bias_z = None\n            self.bias_r = None\n            self.bias_h = None\n        self.built = True\n\n    def reset_states(self):\n        if not self.stateful:\n            raise RuntimeError('Layer must be stateful.')\n        input_shape = self.input_spec.shape\n        if not input_shape[0]:\n            raise ValueError('If a RNN is stateful, a complete '\n                             'input_shape must be provided '\n                             '(including batch size).')\n        if hasattr(self, 'states'):\n            K.set_value(self.states[0],\n                        np.zeros((input_shape[0], self.units)))\n        else:\n            self.states = [K.zeros((input_shape[0], self.units))]\n\n    def preprocess_input(self, inputs, training=None):\n        if self.implementation == 0:\n            input_shape = K.int_shape(inputs)\n            input_dim = input_shape[2]\n            timesteps = input_shape[1]\n\n            x_z = _time_distributed_dense(inputs, self.kernel_z, self.bias_z,\n                                          self.dropout, input_dim, self.units,\n                                          timesteps, training=training)\n            x_r = _time_distributed_dense(inputs, self.kernel_r, self.bias_r,\n                                          self.dropout, input_dim, self.units,\n                                          timesteps, training=training)\n            x_h = _time_distributed_dense(inputs, self.kernel_h, self.bias_h,\n                                          self.dropout, input_dim, self.units,\n                                          timesteps, training=training)\n            return K.concatenate([x_z, x_r, x_h], axis=2)\n        else:\n            return inputs\n\n    def get_constants(self, inputs, training=None):\n        constants = []\n        if self.implementation == 0 and 0 < self.dropout < 1:\n            input_shape = K.int_shape(inputs)\n            input_dim = input_shape[-1]\n            ones = K.ones_like(K.reshape(inputs[:, 0, 0], (-1, 1)))\n            ones = K.tile(ones, (1, int(input_dim)))\n\n            def dropped_inputs():\n                return K.dropout(ones, self.dropout)\n\n            dp_mask = [K.in_train_phase(dropped_inputs,\n                                        ones,\n                                        training=training) for _ in range(3)]\n            constants.append(dp_mask)\n        else:\n            constants.append([K.cast_to_floatx(1.) for _ in range(3)])\n\n        if 0 < self.recurrent_dropout < 1:\n            ones = K.ones_like(K.reshape(inputs[:, 0, 0], (-1, 1)))\n            ones = K.tile(ones, (1, self.units))\n\n            def dropped_inputs():\n                return K.dropout(ones, self.recurrent_dropout)\n            rec_dp_mask = [K.in_train_phase(dropped_inputs,\n                                            ones,\n                                            training=training) for _ in range(3)]\n            constants.append(rec_dp_mask)\n        else:\n            constants.append([K.cast_to_floatx(1.) for _ in range(3)])\n        return constants\n\n    def step(self, inputs, states):\n        h_tm1 = states[0]  # previous memory\n        dp_mask = states[1]  # dropout matrices for recurrent units\n        rec_dp_mask = states[2]\n\n        if self.implementation == 1:\n            matrix_x = K.dot(inputs * dp_mask[0], self.kernel)\n            if self.use_bias:\n                matrix_x += self.bias\n            matrix_inner = K.dot(h_tm1 * rec_dp_mask[0],\n                                 self.recurrent_kernel[:, :2 * self.units])\n\n            x_z = matrix_x[:, :self.units]\n            x_r = matrix_x[:, self.units: 2 * self.units]\n            recurrent_z = matrix_inner[:, :self.units]\n            recurrent_r = matrix_inner[:, self.units: 2 * self.units]\n\n            z = self.recurrent_activation(x_z + recurrent_z)\n            r = self.recurrent_activation(x_r + recurrent_r)\n\n            x_h = matrix_x[:, 2 * self.units:]\n            recurrent_h = K.dot(r * h_tm1 * rec_dp_mask[0],\n                                self.recurrent_kernel[:, 2 * self.units:])\n            hh = self.activation(x_h + recurrent_h)\n        else:\n            if self.implementation == 0:\n                x_z = inputs[:, :self.units]\n                x_r = inputs[:, self.units: 2 * self.units]\n                x_h = inputs[:, 2 * self.units:]\n            elif self.implementation == 2:\n                x_z = K.dot(inputs * dp_mask[0], self.kernel_z)\n                x_r = K.dot(inputs * dp_mask[1], self.kernel_r)\n                x_h = K.dot(inputs * dp_mask[2], self.kernel_h)\n                if self.use_bias:\n                    x_z += self.bias_z\n                    x_r += self.bias_r\n                    x_h += self.bias_h\n            else:\n                raise ValueError('Unknown `implementation` mode.')\n            z = self.recurrent_activation(x_z + K.dot(h_tm1 * rec_dp_mask[0],\n                                                      self.recurrent_kernel_z))\n            r = self.recurrent_activation(x_r + K.dot(h_tm1 * rec_dp_mask[1],\n                                                      self.recurrent_kernel_r))\n\n            hh = self.activation(x_h + K.dot(r * h_tm1 * rec_dp_mask[2],\n                                             self.recurrent_kernel_h))\n        h = z * h_tm1 + (1 - z) * hh\n        if 0 < self.dropout + self.recurrent_dropout:\n            h._uses_learning_phase = True\n        return h, [h]\n\n    def get_config(self):\n        config = {'units': self.units,\n                  'activation': activations.serialize(self.activation),\n                  'recurrent_activation': activations.serialize(self.recurrent_activation),\n                  'use_bias': self.use_bias,\n                  'kernel_initializer': initializers.serialize(self.kernel_initializer),\n                  'recurrent_initializer': initializers.serialize(self.recurrent_initializer),\n                  'bias_initializer': initializers.serialize(self.bias_initializer),\n                  'kernel_regularizer': regularizers.serialize(self.kernel_regularizer),\n                  'recurrent_regularizer': regularizers.serialize(self.recurrent_regularizer),\n                  'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n                  'kernel_constraint': constraints.serialize(self.kernel_constraint),\n                  'recurrent_constraint': constraints.serialize(self.recurrent_constraint),\n                  'bias_constraint': constraints.serialize(self.bias_constraint),\n                  'dropout': self.dropout,\n                  'recurrent_dropout': self.recurrent_dropout}\n        base_config = super(GRU, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\n\nclass LSTM(Recurrent):\n    \"\"\"Long-Short Term Memory unit - Hochreiter 1997.\n\n    For a step-by-step description of the algorithm, see\n    [this tutorial](http://deeplearning.net/tutorial/lstm.html).\n\n    # Arguments\n        units: Positive integer, dimensionality of the output space.\n        activation: Activation function to use\n            (see [activations](../activations.md)).\n            If you don't specify anything, no activation is applied\n            (ie. \"linear\" activation: `a(x) = x`).\n        recurrent_activation: Activation function to use\n            for the recurrent step\n            (see [activations](../activations.md)).\n        use_bias: Boolean, whether the layer uses a bias vector.\n        kernel_initializer: Initializer for the `kernel` weights matrix,\n            used for the linear transformation of the inputs.\n            (see [initializers](../initializers.md)).\n        recurrent_initializer: Initializer for the `recurrent_kernel`\n            weights matrix,\n            used for the linear transformation of the recurrent state.\n            (see [initializers](../initializers.md)).\n        bias_initializer: Initializer for the bias vector\n            (see [initializers](../initializers.md)).\n        unit_forget_bias: Boolean.\n            If True, add 1 to the bias of the forget gate at initialization.\n            Use in combination with `bias_initializer=\"zeros\"`.\n            This is recommended in [Jozefowicz et al.](http://www.jmlr.org/proceedings/papers/v37/jozefowicz15.pdf)\n        kernel_regularizer: Regularizer function applied to\n            the `kernel` weights matrix\n            (see [regularizer](../regularizers.md)).\n        recurrent_regularizer: Regularizer function applied to\n            the `recurrent_kernel` weights matrix\n            (see [regularizer](../regularizers.md)).\n        bias_regularizer: Regularizer function applied to the bias vector\n            (see [regularizer](../regularizers.md)).\n        activity_regularizer: Regularizer function applied to\n            the output of the layer (its \"activation\").\n            (see [regularizer](../regularizers.md)).\n        kernel_constraint: Constraint function applied to\n            the `kernel` weights matrix\n            (see [constraints](../constraints.md)).\n        recurrent_constraint: Constraint function applied to\n            the `recurrent_kernel` weights matrix\n            (see [constraints](../constraints.md)).\n        bias_constraint: Constraint function applied to the bias vector\n            (see [constraints](../constraints.md)).\n        dropout: Float between 0 and 1.\n            Fraction of the units to drop for\n            the linear transformation of the inputs.\n        recurrent_dropout: Float between 0 and 1.\n            Fraction of the units to drop for\n            the linear transformation of the recurrent state.\n\n    # References\n        - [Long short-term memory](http://deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf) (original 1997 paper)\n        - [Learning to forget: Continual prediction with LSTM](http://www.mitpressjournals.org/doi/pdf/10.1162/089976600300015015)\n        - [Supervised sequence labeling with recurrent neural networks](http://www.cs.toronto.edu/~graves/preprint.pdf)\n        - [A Theoretically Grounded Application of Dropout in Recurrent Neural Networks](http://arxiv.org/abs/1512.05287)\n    \"\"\"\n\n    def __init__(self, units,\n                 activation='tanh',\n                 recurrent_activation='hard_sigmoid',\n                 use_bias=True,\n                 kernel_initializer='glorot_uniform',\n                 recurrent_initializer='orthogonal',\n                 bias_initializer='zeros',\n                 unit_forget_bias=True,\n                 kernel_regularizer=None,\n                 recurrent_regularizer=None,\n                 bias_regularizer=None,\n                 activity_regularizer=None,\n                 kernel_constraint=None,\n                 recurrent_constraint=None,\n                 bias_constraint=None,\n                 dropout=0.,\n                 recurrent_dropout=0.,\n                 **kwargs):\n        super(LSTM, self).__init__(**kwargs)\n        self.units = units\n        self.activation = activations.get(activation)\n        self.recurrent_activation = activations.get(recurrent_activation)\n        self.use_bias = use_bias\n\n        self.kernel_initializer = initializers.get(kernel_initializer)\n        self.recurrent_initializer = initializers.get(recurrent_initializer)\n        self.bias_initializer = initializers.get(bias_initializer)\n        self.unit_forget_bias = unit_forget_bias\n\n        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n        self.recurrent_regularizer = regularizers.get(recurrent_regularizer)\n        self.bias_regularizer = regularizers.get(bias_regularizer)\n\n        self.kernel_constraint = constraints.get(kernel_constraint)\n        self.recurrent_constraint = constraints.get(recurrent_constraint)\n        self.bias_constraint = constraints.get(bias_constraint)\n\n        self.dropout = min(1., max(0., dropout))\n        self.recurrent_dropout = min(1., max(0., recurrent_dropout))\n\n    def build(self, input_shape):\n        self.input_spec = InputSpec(shape=input_shape)\n        self.input_dim = input_shape[2]\n\n        if self.stateful:\n            self.reset_states()\n        else:\n            # initial states: 2 all-zero tensors of shape (output_dim)\n            self.states = [None, None]\n\n        self.kernel = self.add_weight((self.input_dim, self.units * 4),\n                                      name='kernel',\n                                      initializer=self.kernel_initializer,\n                                      regularizer=self.kernel_regularizer,\n                                      constraint=self.kernel_constraint)\n        self.recurrent_kernel = self.add_weight(\n            (self.units, self.units * 4),\n            name='recurrent_kernel',\n            initializer=self.recurrent_initializer,\n            regularizer=self.recurrent_regularizer,\n            constraint=self.recurrent_constraint)\n\n        if self.use_bias:\n            self.bias = self.add_weight((self.units * 4,),\n                                        name='bias',\n                                        initializer=self.bias_initializer,\n                                        regularizer=self.bias_regularizer,\n                                        constraint=self.bias_constraint)\n            if self.unit_forget_bias:\n                self.bias += K.concatenate([K.zeros((self.units,)),\n                                           K.ones((self.units,)),\n                                           K.zeros((self.units * 2,))])\n        else:\n            self.bias = None\n\n        self.kernel_i = self.kernel[:, :self.units]\n        self.recurrent_kernel_i = self.recurrent_kernel[:, :self.units]\n        self.kernel_f = self.kernel[:, self.units: self.units * 2]\n        self.recurrent_kernel_f = self.recurrent_kernel[:, self.units: self.units * 2]\n        self.kernel_c = self.kernel[:, self.units * 2: self.units * 3]\n        self.recurrent_kernel_c = self.recurrent_kernel[:, self.units * 2: self.units * 3]\n        self.kernel_o = self.kernel[:, self.units * 3:]\n        self.recurrent_kernel_o = self.recurrent_kernel[:, self.units * 3:]\n\n        if self.use_bias:\n            self.bias_i = self.bias[:self.units]\n            self.bias_f = self.bias[self.units: self.units * 2]\n            self.bias_c = self.bias[self.units * 2: self.units * 3]\n            self.bias_o = self.bias[self.units * 3:]\n        else:\n            self.bias_i = None\n            self.bias_f = None\n            self.bias_c = None\n            self.bias_o = None\n        self.built = True\n\n    def reset_states(self):\n        assert self.stateful, 'Layer must be stateful.'\n        input_shape = self.input_spec.shape\n        if not input_shape[0]:\n            raise ValueError('If a RNN is stateful, a complete '\n                             'input_shape must be provided '\n                             '(including batch size).')\n        if hasattr(self, 'states'):\n            K.set_value(self.states[0],\n                        np.zeros((input_shape[0], self.units)))\n            K.set_value(self.states[1],\n                        np.zeros((input_shape[0], self.units)))\n        else:\n            self.states = [K.zeros((input_shape[0], self.units)),\n                           K.zeros((input_shape[0], self.units))]\n\n    def preprocess_input(self, inputs, training=None):\n        if self.implementation == 0:\n            input_shape = K.int_shape(inputs)\n            input_dim = input_shape[2]\n            timesteps = input_shape[1]\n\n            x_i = _time_distributed_dense(inputs, self.kernel_i, self.bias_i,\n                                          self.dropout, input_dim, self.units,\n                                          timesteps, training=training)\n            x_f = _time_distributed_dense(inputs, self.kernel_f, self.bias_f,\n                                          self.dropout, input_dim, self.units,\n                                          timesteps, training=training)\n            x_c = _time_distributed_dense(inputs, self.kernel_c, self.bias_c,\n                                          self.dropout, input_dim, self.units,\n                                          timesteps, training=training)\n            x_o = _time_distributed_dense(inputs, self.kernel_o, self.bias_o,\n                                          self.dropout, input_dim, self.units,\n                                          timesteps, training=training)\n            return K.concatenate([x_i, x_f, x_c, x_o], axis=2)\n        else:\n            return inputs\n\n    def get_constants(self, inputs, training=None):\n        constants = []\n        if self.implementation == 0 and 0 < self.dropout < 1:\n            input_shape = K.int_shape(inputs)\n            input_dim = input_shape[-1]\n            ones = K.ones_like(K.reshape(inputs[:, 0, 0], (-1, 1)))\n            ones = K.tile(ones, (1, int(input_dim)))\n\n            def dropped_inputs():\n                return K.dropout(ones, self.dropout)\n\n            dp_mask = [K.in_train_phase(dropped_inputs,\n                                        ones,\n                                        training=training) for _ in range(4)]\n            constants.append(dp_mask)\n        else:\n            constants.append([K.cast_to_floatx(1.) for _ in range(4)])\n\n        if 0 < self.recurrent_dropout < 1:\n            ones = K.ones_like(K.reshape(inputs[:, 0, 0], (-1, 1)))\n            ones = K.tile(ones, (1, self.units))\n\n            def dropped_inputs():\n                return K.dropout(ones, self.recurrent_dropout)\n            rec_dp_mask = [K.in_train_phase(dropped_inputs,\n                                            ones,\n                                            training=training) for _ in range(4)]\n            constants.append(rec_dp_mask)\n        else:\n            constants.append([K.cast_to_floatx(1.) for _ in range(4)])\n        return constants\n\n    def step(self, inputs, states):\n        h_tm1 = states[0]\n        c_tm1 = states[1]\n        dp_mask = states[2]\n        rec_dp_mask = states[3]\n\n        if self.implementation == 1:\n            z = K.dot(inputs * dp_mask[0], self.kernel)\n            z += K.dot(h_tm1 * rec_dp_mask[0], self.recurrent_kernel)\n            if self.use_bias:\n                z += self.bias\n\n            z0 = z[:, :self.units]\n            z1 = z[:, self.units: 2 * self.units]\n            z2 = z[:, 2 * self.units: 3 * self.units]\n            z3 = z[:, 3 * self.units:]\n\n            i = self.recurrent_activation(z0)\n            f = self.recurrent_activation(z1)\n            c = f * c_tm1 + i * self.activation(z2)\n            o = self.recurrent_activation(z3)\n        else:\n            if self.implementation == 0:\n                x_i = inputs[:, :self.units]\n                x_f = inputs[:, self.units: 2 * self.units]\n                x_c = inputs[:, 2 * self.units: 3 * self.units]\n                x_o = inputs[:, 3 * self.units:]\n            elif self.implementation == 2:\n                x_i = K.dot(inputs * dp_mask[0], self.kernel_i) + self.bias_i\n                x_f = K.dot(inputs * dp_mask[1], self.kernel_f) + self.bias_f\n                x_c = K.dot(inputs * dp_mask[2], self.kernel_c) + self.bias_c\n                x_o = K.dot(inputs * dp_mask[3], self.kernel_o) + self.bias_o\n            else:\n                raise ValueError('Unknown `implementation` mode.')\n\n            i = self.recurrent_activation(x_i + K.dot(h_tm1 * rec_dp_mask[0],\n                                                      self.recurrent_kernel_i))\n            f = self.recurrent_activation(x_f + K.dot(h_tm1 * rec_dp_mask[1],\n                                                      self.recurrent_kernel_f))\n            c = f * c_tm1 + i * self.activation(x_c + K.dot(h_tm1 * rec_dp_mask[2],\n                                                            self.recurrent_kernel_c))\n            o = self.recurrent_activation(x_o + K.dot(h_tm1 * rec_dp_mask[3],\n                                                      self.recurrent_kernel_o))\n        h = o * self.activation(c)\n        if 0 < self.dropout + self.recurrent_dropout:\n            h._uses_learning_phase = True\n        return h, [h, c]\n\n    def get_config(self):\n        config = {'units': self.units,\n                  'activation': activations.serialize(self.activation),\n                  'recurrent_activation': activations.serialize(self.recurrent_activation),\n                  'use_bias': self.use_bias,\n                  'kernel_initializer': initializers.serialize(self.kernel_initializer),\n                  'recurrent_initializer': initializers.serialize(self.recurrent_initializer),\n                  'bias_initializer': initializers.serialize(self.bias_initializer),\n                  'unit_forget_bias': self.unit_forget_bias,\n                  'kernel_regularizer': regularizers.serialize(self.kernel_regularizer),\n                  'recurrent_regularizer': regularizers.serialize(self.recurrent_regularizer),\n                  'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n                  'kernel_constraint': constraints.serialize(self.kernel_constraint),\n                  'recurrent_constraint': constraints.serialize(self.recurrent_constraint),\n                  'bias_constraint': constraints.serialize(self.bias_constraint),\n                  'dropout': self.dropout,\n                  'recurrent_dropout': self.recurrent_dropout}\n        base_config = super(LSTM, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n",
          "file_after": "# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import\nimport numpy as np\n\nfrom .. import backend as K\nfrom .. import activations\nfrom .. import initializers\nfrom .. import regularizers\nfrom .. import constraints\nfrom ..engine import Layer\nfrom ..engine import InputSpec\n\n\ndef _time_distributed_dense(x, w, b=None, dropout=None,\n                            input_dim=None, output_dim=None,\n                            timesteps=None, training=None):\n    \"\"\"Apply `y . w + b` for every temporal slice y of x.\n\n    # Arguments\n        x: input tensor.\n        w: weight matrix.\n        b: optional bias vector.\n        dropout: wether to apply dropout (same dropout mask\n            for every temporal slice of the input).\n        input_dim: integer; optional dimensionality of the input.\n        output_dim: integer; optional dimensionality of the output.\n        timesteps: integer; optional number of timesteps.\n        training: training phase tensor or boolean.\n\n    # Returns\n        Output tensor.\n    \"\"\"\n    if not input_dim:\n        input_dim = K.shape(x)[2]\n    if not timesteps:\n        timesteps = K.shape(x)[1]\n    if not output_dim:\n        output_dim = K.shape(w)[1]\n\n    if dropout is not None and 0. < dropout < 1.:\n        # apply the same dropout pattern at every timestep\n        ones = K.ones_like(K.reshape(x[:, 0, :], (-1, input_dim)))\n        dropout_matrix = K.dropout(ones, dropout)\n        expanded_dropout_matrix = K.repeat(dropout_matrix, timesteps)\n        x = K.in_train_phase(x * expanded_dropout_matrix, x, training=training)\n\n    # collapse time dimension and batch dimension together\n    x = K.reshape(x, (-1, input_dim))\n    x = K.dot(x, w)\n    if b is not None:\n        x += b\n    # reshape to 3D tensor\n    if K.backend() == 'tensorflow':\n        x = K.reshape(x, K.stack([-1, timesteps, output_dim]))\n        x.set_shape([None, None, output_dim])\n    else:\n        x = K.reshape(x, (-1, timesteps, output_dim))\n    return x\n\n\nclass Recurrent(Layer):\n    \"\"\"Abstract base class for recurrent layers.\n\n    Do not use in a model -- it's not a valid layer!\n    Use its children classes `LSTM`, `GRU` and `SimpleRNN` instead.\n\n    All recurrent layers (`LSTM`, `GRU`, `SimpleRNN`) also\n    follow the specifications of this class and accept\n    the keyword arguments listed below.\n\n    # Example\n\n    ```python\n        # as the first layer in a Sequential model\n        model = Sequential()\n        model.add(LSTM(32, input_shape=(10, 64)))\n        # now model.output_shape == (None, 32)\n        # note: `None` is the batch dimension.\n\n        # the following is identical:\n        model = Sequential()\n        model.add(LSTM(32, input_dim=64, input_length=10))\n\n        # for subsequent layers, not need to specify the input size:\n        model.add(LSTM(16))\n    ```\n\n    # Arguments\n        weights: list of Numpy arrays to set as initial weights.\n            The list should have 3 elements, of shapes:\n            `[(input_dim, output_dim), (output_dim, output_dim), (output_dim,)]`.\n        return_sequences: Boolean. Whether to return the last output\n            in the output sequence, or the full sequence.\n        go_backwards: Boolean (default False).\n            If True, process the input sequence backwards.\n        stateful: Boolean (default False). If True, the last state\n            for each sample at index i in a batch will be used as initial\n            state for the sample of index i in the following batch.\n        unroll: Boolean (default False).\n            If True, the network will be unrolled,\n            else a symbolic loop will be used.\n            Unrolling can speed-up a RNN,\n            although it tends to be more memory-intensive.\n            Unrolling is only suitable for short sequences.\n        implementation: one of {0, 1, or 2}.\n            If set to 0, the RNN will use\n            an implementation that uses fewer, larger matrix products,\n            thus running faster on CPU but consuming more memory.\n            If set to 1, the RNN will use more matrix products,\n            but smaller ones, thus running slower\n            (may actually be faster on GPU) while consuming less memory.\n            If set to 2 (LSTM/GRU only),\n            the RNN will combine the input gate,\n            the forget gate and the output gate into a single matrix,\n            enabling more time-efficient parallelization on the GPU.\n            Note: RNN dropout must be shared for all gates,\n            resulting in a slightly reduced regularization.\n        input_dim: dimensionality of the input (integer).\n            This argument (or alternatively, the keyword argument `input_shape`)\n            is required when using this layer as the first layer in a model.\n        input_length: Length of input sequences, to be specified\n            when it is constant.\n            This argument is required if you are going to connect\n            `Flatten` then `Dense` layers upstream\n            (without it, the shape of the dense outputs cannot be computed).\n            Note that if the recurrent layer is not the first layer\n            in your model, you would need to specify the input length\n            at the level of the first layer\n            (e.g. via the `input_shape` argument)\n\n    # Input shapes\n        3D tensor with shape `(batch_size, timesteps, input_dim)`,\n        (Optional) 2D tensors with shape `(batch_size, output_dim)`.\n\n    # Output shape\n        - if `return_sequences`: 3D tensor with shape\n            `(batch_size, timesteps, units)`.\n        - else, 2D tensor with shape `(batch_size, units)`.\n\n    # Masking\n        This layer supports masking for input data with a variable number\n        of timesteps. To introduce masks to your data,\n        use an [Embedding](embeddings.md) layer with the `mask_zero` parameter\n        set to `True`.\n\n    # Note on using statefulness in RNNs\n        You can set RNN layers to be 'stateful', which means that the states\n        computed for the samples in one batch will be reused as initial states\n        for the samples in the next batch. This assumes a one-to-one mapping\n        between samples in different successive batches.\n\n        To enable statefulness:\n            - specify `stateful=True` in the layer constructor.\n            - specify a fixed batch size for your model, by passing\n                if sequential model:\n                  `batch_input_shape=(...)` to the first layer in your model.\n                else for functional model with 1 or more Input layers:\n                  `batch_shape=(...)` to all the first layers in your model.\n                This is the expected shape of your inputs\n                *including the batch size*.\n                It should be a tuple of integers, e.g. `(32, 10, 100)`.\n            - specify `shuffle=False` when calling fit().\n\n        To reset the states of your model, call `.reset_states()` on either\n        a specific layer, or on your entire model.\n\n    # Note on specifying initial states in RNNs\n        Most RNN layers can be called with either a single tensor, or a list\n        of tensors.\n        If the an RNN layer is called with a single tensor, that tensor is\n        treated as the input, and the initial states are assigned an\n        appropriate default value.\n        If an RNN layer is called with a list of tensors, the first element is\n        treated as the input, and the remaining tensors are treated as the\n        initial states.\n    \"\"\"\n\n    def __init__(self, return_sequences=False,\n                 go_backwards=False,\n                 stateful=False,\n                 unroll=False,\n                 implementation=0,\n                 **kwargs):\n        super(Recurrent, self).__init__(**kwargs)\n        self.return_sequences = return_sequences\n        self.go_backwards = go_backwards\n        self.stateful = stateful\n        self.unroll = unroll\n        self.implementation = 0\n        self.supports_masking = True\n        self.input_spec = None\n        self.dropout = 0\n        self.recurrent_dropout = 0\n\n    def compute_output_shape(self, input_shape):\n        if isinstance(input_shape, list):\n            input_shape = input_shape[0]\n        if self.return_sequences:\n            return (input_shape[0], input_shape[1], self.units)\n        else:\n            return (input_shape[0], self.units)\n\n    def compute_mask(self, inputs, mask):\n        if self.return_sequences:\n            return mask\n        else:\n            return None\n\n    def step(self, inputs, states):\n        raise NotImplementedError\n\n    def get_constants(self, inputs, training=None):\n        return []\n\n    def get_initial_states(self, inputs):\n        # build an all-zero tensor of shape (samples, output_dim)\n        initial_state = K.zeros_like(inputs)  # (samples, timesteps, input_dim)\n        initial_state = K.sum(initial_state, axis=(1, 2))  # (samples,)\n        initial_state = K.expand_dims(initial_state)  # (samples, 1)\n        initial_state = K.tile(initial_state, [1, self.units])  # (samples, output_dim)\n        initial_states = [initial_state for _ in range(len(self.states))]\n        return initial_states\n\n    def preprocess_input(self, inputs, training=None):\n        return inputs\n\n    def call(self, inputs, mask=None, training=None):\n        # input shape: (nbias_samples, time (padded with zeros), input_dim)\n        # note that the .build() method of subclasses MUST define\n        # self.input_spec with a complete input shape.\n        if isinstance(inputs, list):\n            initial_states = inputs[1:]\n            inputs = inputs[0]\n        elif self.stateful:\n            initial_states = self.states\n        else:\n            initial_states = self.get_initial_states(inputs)\n        input_shape = K.int_shape(inputs)\n        if self.unroll and input_shape[1] is None:\n            raise ValueError('Cannot unroll a RNN if the '\n                             'time dimension is undefined. \\n'\n                             '- If using a Sequential model, '\n                             'specify the time dimension by passing '\n                             'an `input_shape` or `batch_input_shape` '\n                             'argument to your first layer. If your '\n                             'first layer is an Embedding, you can '\n                             'also use the `input_length` argument.\\n'\n                             '- If using the functional API, specify '\n                             'the time dimension by passing a `shape` '\n                             'or `batch_shape` argument to your Input layer.')\n        constants = self.get_constants(inputs, training=None)\n        preprocessed_input = self.preprocess_input(inputs, training=None)\n        last_output, outputs, states = K.rnn(self.step,\n                                             preprocessed_input,\n                                             initial_states,\n                                             go_backwards=self.go_backwards,\n                                             mask=mask,\n                                             constants=constants,\n                                             unroll=self.unroll,\n                                             input_length=input_shape[1])\n        if self.stateful:\n            updates = []\n            for i in range(len(states)):\n                updates.append((self.states[i], states[i]))\n            self.add_update(updates, inputs)\n\n        # Properly set learning phase\n        if 0 < self.dropout + self.recurrent_dropout:\n            last_output._uses_learning_phase = True\n            outputs._uses_learning_phase = True\n\n        if self.return_sequences:\n            return outputs\n        else:\n            return last_output\n\n    def get_config(self):\n        config = {'return_sequences': self.return_sequences,\n                  'go_backwards': self.go_backwards,\n                  'stateful': self.stateful,\n                  'unroll': self.unroll,\n                  'implementation': self.implementation}\n        base_config = super(Recurrent, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\n\nclass SimpleRNN(Recurrent):\n    \"\"\"Fully-connected RNN where the output is to be fed back to input.\n\n    # Arguments\n        units: Positive integer, dimensionality of the output space.\n        activation: Activation function to use\n            (see [activations](../activations.md)).\n            If you don't specify anything, no activation is applied\n            (ie. \"linear\" activation: `a(x) = x`).\n        use_bias: Boolean, whether the layer uses a bias vector.\n        kernel_initializer: Initializer for the `kernel` weights matrix,\n            used for the linear transformation of the inputs.\n            (see [initializers](../initializers.md)).\n        recurrent_initializer: Initializer for the `recurrent_kernel`\n            weights matrix,\n            used for the linear transformation of the recurrent state.\n            (see [initializers](../initializers.md)).\n        bias_initializer: Initializer for the bias vector\n            (see [initializers](../initializers.md)).\n        kernel_regularizer: Regularizer function applied to\n            the `kernel` weights matrix\n            (see [regularizer](../regularizers.md)).\n        recurrent_regularizer: Regularizer function applied to\n            the `recurrent_kernel` weights matrix\n            (see [regularizer](../regularizers.md)).\n        bias_regularizer: Regularizer function applied to the bias vector\n            (see [regularizer](../regularizers.md)).\n        activity_regularizer: Regularizer function applied to\n            the output of the layer (its \"activation\").\n            (see [regularizer](../regularizers.md)).\n        kernel_constraint: Constraint function applied to\n            the `kernel` weights matrix\n            (see [constraints](../constraints.md)).\n        recurrent_constraint: Constraint function applied to\n            the `recurrent_kernel` weights matrix\n            (see [constraints](../constraints.md)).\n        bias_constraint: Constraint function applied to the bias vector\n            (see [constraints](../constraints.md)).\n        dropout: Float between 0 and 1.\n            Fraction of the units to drop for\n            the linear transformation of the inputs.\n        recurrent_dropout: Float between 0 and 1.\n            Fraction of the units to drop for\n            the linear transformation of the recurrent state.\n\n    # References\n        - [A Theoretically Grounded Application of Dropout in Recurrent Neural Networks](http://arxiv.org/abs/1512.05287)\n    \"\"\"\n\n    def __init__(self, units,\n                 activation='tanh',\n                 use_bias=True,\n                 kernel_initializer='glorot_uniform',\n                 recurrent_initializer='orthogonal',\n                 bias_initializer='zeros',\n                 kernel_regularizer=None,\n                 recurrent_regularizer=None,\n                 bias_regularizer=None,\n                 activity_regularizer=None,\n                 kernel_constraint=None,\n                 recurrent_constraint=None,\n                 bias_constraint=None,\n                 dropout=0.,\n                 recurrent_dropout=0.,\n                 **kwargs):\n        super(SimpleRNN, self).__init__(**kwargs)\n        self.units = units\n        self.activation = activations.get(activation)\n        self.use_bias = use_bias\n\n        self.kernel_initializer = initializers.get(kernel_initializer)\n        self.recurrent_initializer = initializers.get(recurrent_initializer)\n        self.bias_initializer = initializers.get(bias_initializer)\n\n        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n        self.recurrent_regularizer = regularizers.get(recurrent_regularizer)\n        self.bias_regularizer = regularizers.get(bias_regularizer)\n\n        self.kernel_constraint = constraints.get(kernel_constraint)\n        self.recurrent_constraint = constraints.get(recurrent_constraint)\n        self.bias_constraint = constraints.get(bias_constraint)\n\n        self.dropout = min(1., max(0., dropout))\n        self.recurrent_dropout = min(1., max(0., recurrent_dropout))\n\n    def build(self, input_shape):\n        if isinstance(input_shape, list):\n            input_shape = input_shape[0]\n        if self.stateful:\n            self.reset_states()\n        else:\n            # Initial states: all-zero tensor of shape (output_dim).\n            self.states = [None]\n        input_dim = input_shape[2]\n        self.input_dim = input_dim\n\n        self.kernel = self.add_weight((input_dim, self.units),\n                                      name='kernel',\n                                      initializer=self.kernel_initializer,\n                                      regularizer=self.kernel_regularizer,\n                                      constraint=self.kernel_constraint)\n        self.recurrent_kernel = self.add_weight(\n            (self.units, self.units),\n            name='recurrent_kernel',\n            initializer=self.recurrent_initializer,\n            regularizer=self.recurrent_regularizer,\n            constraint=self.recurrent_constraint)\n        if self.use_bias:\n            self.bias = self.add_weight((self.units,),\n                                        name='bias',\n                                        initializer=self.bias_initializer,\n                                        regularizer=self.bias_regularizer,\n                                        constraint=self.bias_constraint)\n        else:\n            self.bias = None\n        self.built = True\n\n    def reset_states(self):\n        if not self.stateful:\n            raise AttributeError('Layer must be stateful.')\n        input_shape = self.input_spec.shape\n        if not input_shape[0]:\n            raise ValueError('If a RNN is stateful, it needs to know '\n                             'its batch size. Specify the batch size '\n                             'of your input tensors: \\n'\n                             '- If using a Sequential model, '\n                             'specify the batch size by passing '\n                             'a `batch_input_shape` '\n                             'argument to your first layer.\\n'\n                             '- If using the functional API, specify '\n                             'the time dimension by passing a '\n                             '`batch_shape` argument to your Input layer.')\n        if hasattr(self, 'states'):\n            K.set_value(self.states[0],\n                        np.zeros((input_shape[0], self.units)))\n        else:\n            self.states = [K.zeros((input_shape[0], self.units))]\n\n    def preprocess_input(self, inputs, training=None):\n        if self.implementation > 0:\n            return inputs\n        else:\n            input_shape = K.int_shape(inputs)\n            input_dim = input_shape[2]\n            timesteps = input_shape[1]\n            return _time_distributed_dense(inputs,\n                                           self.kernel,\n                                           self.bias,\n                                           self.dropout,\n                                           input_dim,\n                                           self.units,\n                                           timesteps,\n                                           training=training)\n\n    def step(self, inputs, states):\n        if self.implementation == 0:\n            h = inputs\n        else:\n            if 0 < self.dropout < 1:\n                h = K.dot(inputs * states[1], self.kernel)\n            else:\n                h = K.dot(inputs, self.kernel)\n            if self.bias is not None:\n                h += self.bias\n\n        prev_output = states[0]\n        if 0 < self.recurrent_dropout < 1:\n            prev_output *= states[2]\n        output = h + K.dot(prev_output, self.recurrent_kernel)\n        if self.activation is not None:\n            output = self.activation(output)\n\n        # Properly set learning phase on output tensor.\n        if 0 < self.dropout + self.recurrent_dropout:\n            output._uses_learning_phase = True\n        return output, [output]\n\n    def get_constants(self, inputs, training=None):\n        constants = []\n        if self.implementation == 0 and 0 < self.dropout < 1:\n            input_shape = K.int_shape(inputs)\n            input_dim = input_shape[-1]\n            ones = K.ones_like(K.reshape(inputs[:, 0, 0], (-1, 1)))\n            ones = K.tile(ones, (1, int(input_dim)))\n\n            def dropped_inputs():\n                return K.dropout(ones, self.dropout)\n\n            dp_mask = K.in_train_phase(dropped_inputs,\n                                       ones,\n                                       training=training)\n            constants.append(dp_mask)\n        else:\n            constants.append(K.cast_to_floatx(1.))\n\n        if 0 < self.recurrent_dropout < 1:\n            ones = K.ones_like(K.reshape(inputs[:, 0, 0], (-1, 1)))\n            ones = K.tile(ones, (1, self.units))\n\n            def dropped_inputs():\n                return K.dropout(ones, self.recurrent_dropout)\n            rec_dp_mask = K.in_train_phase(dropped_inputs,\n                                           ones,\n                                           training=training)\n            constants.append(rec_dp_mask)\n        else:\n            constants.append(K.cast_to_floatx(1.))\n        return constants\n\n    def get_config(self):\n        config = {'units': self.units,\n                  'activation': activations.serialize(self.activation),\n                  'use_bias': self.use_bias,\n                  'kernel_initializer': initializers.serialize(self.kernel_initializer),\n                  'recurrent_initializer': initializers.serialize(self.recurrent_initializer),\n                  'bias_initializer': initializers.serialize(self.bias_initializer),\n                  'kernel_regularizer': regularizers.serialize(self.kernel_regularizer),\n                  'recurrent_regularizer': regularizers.serialize(self.recurrent_regularizer),\n                  'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n                  'kernel_constraint': constraints.serialize(self.kernel_constraint),\n                  'recurrent_constraint': constraints.serialize(self.recurrent_constraint),\n                  'bias_constraint': constraints.serialize(self.bias_constraint),\n                  'dropout': self.dropout,\n                  'recurrent_dropout': self.recurrent_dropout}\n        base_config = super(SimpleRNN, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\n\nclass GRU(Recurrent):\n    \"\"\"Gated Recurrent Unit - Cho et al. 2014.\n\n    # Arguments\n        units: Positive integer, dimensionality of the output space.\n        activation: Activation function to use\n            (see [activations](../activations.md)).\n            If you don't specify anything, no activation is applied\n            (ie. \"linear\" activation: `a(x) = x`).\n        recurrent_activation: Activation function to use\n            for the recurrent step\n            (see [activations](../activations.md)).\n        use_bias: Boolean, whether the layer uses a bias vector.\n        kernel_initializer: Initializer for the `kernel` weights matrix,\n            used for the linear transformation of the inputs.\n            (see [initializers](../initializers.md)).\n        recurrent_initializer: Initializer for the `recurrent_kernel`\n            weights matrix,\n            used for the linear transformation of the recurrent state.\n            (see [initializers](../initializers.md)).\n        bias_initializer: Initializer for the bias vector\n            (see [initializers](../initializers.md)).\n        kernel_regularizer: Regularizer function applied to\n            the `kernel` weights matrix\n            (see [regularizer](../regularizers.md)).\n        recurrent_regularizer: Regularizer function applied to\n            the `recurrent_kernel` weights matrix\n            (see [regularizer](../regularizers.md)).\n        bias_regularizer: Regularizer function applied to the bias vector\n            (see [regularizer](../regularizers.md)).\n        activity_regularizer: Regularizer function applied to\n            the output of the layer (its \"activation\").\n            (see [regularizer](../regularizers.md)).\n        kernel_constraint: Constraint function applied to\n            the `kernel` weights matrix\n            (see [constraints](../constraints.md)).\n        recurrent_constraint: Constraint function applied to\n            the `recurrent_kernel` weights matrix\n            (see [constraints](../constraints.md)).\n        bias_constraint: Constraint function applied to the bias vector\n            (see [constraints](../constraints.md)).\n        dropout: Float between 0 and 1.\n            Fraction of the units to drop for\n            the linear transformation of the inputs.\n        recurrent_dropout: Float between 0 and 1.\n            Fraction of the units to drop for\n            the linear transformation of the recurrent state.\n\n    # References\n        - [On the Properties of Neural Machine Translation: Encoder-Decoder Approaches](https://arxiv.org/abs/1409.1259)\n        - [Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling](http://arxiv.org/abs/1412.3555v1)\n        - [A Theoretically Grounded Application of Dropout in Recurrent Neural Networks](http://arxiv.org/abs/1512.05287)\n    \"\"\"\n\n    def __init__(self, units,\n                 activation='tanh',\n                 recurrent_activation='hard_sigmoid',\n                 use_bias=True,\n                 kernel_initializer='glorot_uniform',\n                 recurrent_initializer='orthogonal',\n                 bias_initializer='zeros',\n                 kernel_regularizer=None,\n                 recurrent_regularizer=None,\n                 bias_regularizer=None,\n                 activity_regularizer=None,\n                 kernel_constraint=None,\n                 recurrent_constraint=None,\n                 bias_constraint=None,\n                 dropout=0.,\n                 recurrent_dropout=0.,\n                 **kwargs):\n        super(GRU, self).__init__(**kwargs)\n        self.units = units\n        self.activation = activations.get(activation)\n        self.recurrent_activation = activations.get(recurrent_activation)\n        self.use_bias = use_bias\n\n        self.kernel_initializer = initializers.get(kernel_initializer)\n        self.recurrent_initializer = initializers.get(recurrent_initializer)\n        self.bias_initializer = initializers.get(bias_initializer)\n\n        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n        self.recurrent_regularizer = regularizers.get(recurrent_regularizer)\n        self.bias_regularizer = regularizers.get(bias_regularizer)\n\n        self.kernel_constraint = constraints.get(kernel_constraint)\n        self.recurrent_constraint = constraints.get(recurrent_constraint)\n        self.bias_constraint = constraints.get(bias_constraint)\n\n        self.dropout = min(1., max(0., dropout))\n        self.recurrent_dropout = min(1., max(0., recurrent_dropout))\n\n    def build(self, input_shape):\n        if isinstance(input_shape, list):\n            input_shape = input_shape[0]\n        self.input_dim = input_shape[2]\n\n        if self.stateful:\n            self.reset_states()\n        else:\n            # initial states: all-zero tensor of shape (output_dim)\n            self.states = [None]\n\n        self.kernel = self.add_weight((self.input_dim, self.units * 3),\n                                      name='kernel',\n                                      initializer=self.kernel_initializer,\n                                      regularizer=self.kernel_regularizer,\n                                      constraint=self.kernel_constraint)\n        self.recurrent_kernel = self.add_weight(\n            (self.units, self.units * 3),\n            name='recurrent_kernel',\n            initializer=self.recurrent_initializer,\n            regularizer=self.recurrent_regularizer,\n            constraint=self.recurrent_constraint)\n\n        if self.use_bias:\n            self.bias = self.add_weight((self.units * 3,),\n                                        name='bias',\n                                        initializer='zero',\n                                        regularizer=self.bias_regularizer,\n                                        constraint=self.bias_constraint)\n        else:\n            self.bias = None\n\n        self.kernel_z = self.kernel[:, :self.units]\n        self.recurrent_kernel_z = self.recurrent_kernel[:, :self.units]\n        self.kernel_r = self.kernel[:, self.units: self.units * 2]\n        self.recurrent_kernel_r = self.recurrent_kernel[:,\n                                                        self.units:\n                                                        self.units * 2]\n        self.kernel_h = self.kernel[:, self.units * 2:]\n        self.recurrent_kernel_h = self.recurrent_kernel[:, self.units * 2:]\n\n        if self.use_bias:\n            self.bias_z = self.bias[:self.units]\n            self.bias_r = self.bias[self.units: self.units * 2]\n            self.bias_h = self.bias[self.units * 2:]\n        else:\n            self.bias_z = None\n            self.bias_r = None\n            self.bias_h = None\n        self.built = True\n\n    def reset_states(self):\n        if not self.stateful:\n            raise RuntimeError('Layer must be stateful.')\n        input_shape = self.input_spec.shape\n        if not input_shape[0]:\n            raise ValueError('If a RNN is stateful, a complete '\n                             'input_shape must be provided '\n                             '(including batch size).')\n        if hasattr(self, 'states'):\n            K.set_value(self.states[0],\n                        np.zeros((input_shape[0], self.units)))\n        else:\n            self.states = [K.zeros((input_shape[0], self.units))]\n\n    def preprocess_input(self, inputs, training=None):\n        if self.implementation == 0:\n            input_shape = K.int_shape(inputs)\n            input_dim = input_shape[2]\n            timesteps = input_shape[1]\n\n            x_z = _time_distributed_dense(inputs, self.kernel_z, self.bias_z,\n                                          self.dropout, input_dim, self.units,\n                                          timesteps, training=training)\n            x_r = _time_distributed_dense(inputs, self.kernel_r, self.bias_r,\n                                          self.dropout, input_dim, self.units,\n                                          timesteps, training=training)\n            x_h = _time_distributed_dense(inputs, self.kernel_h, self.bias_h,\n                                          self.dropout, input_dim, self.units,\n                                          timesteps, training=training)\n            return K.concatenate([x_z, x_r, x_h], axis=2)\n        else:\n            return inputs\n\n    def get_constants(self, inputs, training=None):\n        constants = []\n        if self.implementation == 0 and 0 < self.dropout < 1:\n            input_shape = K.int_shape(inputs)\n            input_dim = input_shape[-1]\n            ones = K.ones_like(K.reshape(inputs[:, 0, 0], (-1, 1)))\n            ones = K.tile(ones, (1, int(input_dim)))\n\n            def dropped_inputs():\n                return K.dropout(ones, self.dropout)\n\n            dp_mask = [K.in_train_phase(dropped_inputs,\n                                        ones,\n                                        training=training) for _ in range(3)]\n            constants.append(dp_mask)\n        else:\n            constants.append([K.cast_to_floatx(1.) for _ in range(3)])\n\n        if 0 < self.recurrent_dropout < 1:\n            ones = K.ones_like(K.reshape(inputs[:, 0, 0], (-1, 1)))\n            ones = K.tile(ones, (1, self.units))\n\n            def dropped_inputs():\n                return K.dropout(ones, self.recurrent_dropout)\n            rec_dp_mask = [K.in_train_phase(dropped_inputs,\n                                            ones,\n                                            training=training) for _ in range(3)]\n            constants.append(rec_dp_mask)\n        else:\n            constants.append([K.cast_to_floatx(1.) for _ in range(3)])\n        return constants\n\n    def step(self, inputs, states):\n        h_tm1 = states[0]  # previous memory\n        dp_mask = states[1]  # dropout matrices for recurrent units\n        rec_dp_mask = states[2]\n\n        if self.implementation == 1:\n            matrix_x = K.dot(inputs * dp_mask[0], self.kernel)\n            if self.use_bias:\n                matrix_x += self.bias\n            matrix_inner = K.dot(h_tm1 * rec_dp_mask[0],\n                                 self.recurrent_kernel[:, :2 * self.units])\n\n            x_z = matrix_x[:, :self.units]\n            x_r = matrix_x[:, self.units: 2 * self.units]\n            recurrent_z = matrix_inner[:, :self.units]\n            recurrent_r = matrix_inner[:, self.units: 2 * self.units]\n\n            z = self.recurrent_activation(x_z + recurrent_z)\n            r = self.recurrent_activation(x_r + recurrent_r)\n\n            x_h = matrix_x[:, 2 * self.units:]\n            recurrent_h = K.dot(r * h_tm1 * rec_dp_mask[0],\n                                self.recurrent_kernel[:, 2 * self.units:])\n            hh = self.activation(x_h + recurrent_h)\n        else:\n            if self.implementation == 0:\n                x_z = inputs[:, :self.units]\n                x_r = inputs[:, self.units: 2 * self.units]\n                x_h = inputs[:, 2 * self.units:]\n            elif self.implementation == 2:\n                x_z = K.dot(inputs * dp_mask[0], self.kernel_z)\n                x_r = K.dot(inputs * dp_mask[1], self.kernel_r)\n                x_h = K.dot(inputs * dp_mask[2], self.kernel_h)\n                if self.use_bias:\n                    x_z += self.bias_z\n                    x_r += self.bias_r\n                    x_h += self.bias_h\n            else:\n                raise ValueError('Unknown `implementation` mode.')\n            z = self.recurrent_activation(x_z + K.dot(h_tm1 * rec_dp_mask[0],\n                                                      self.recurrent_kernel_z))\n            r = self.recurrent_activation(x_r + K.dot(h_tm1 * rec_dp_mask[1],\n                                                      self.recurrent_kernel_r))\n\n            hh = self.activation(x_h + K.dot(r * h_tm1 * rec_dp_mask[2],\n                                             self.recurrent_kernel_h))\n        h = z * h_tm1 + (1 - z) * hh\n        if 0 < self.dropout + self.recurrent_dropout:\n            h._uses_learning_phase = True\n        return h, [h]\n\n    def get_config(self):\n        config = {'units': self.units,\n                  'activation': activations.serialize(self.activation),\n                  'recurrent_activation': activations.serialize(self.recurrent_activation),\n                  'use_bias': self.use_bias,\n                  'kernel_initializer': initializers.serialize(self.kernel_initializer),\n                  'recurrent_initializer': initializers.serialize(self.recurrent_initializer),\n                  'bias_initializer': initializers.serialize(self.bias_initializer),\n                  'kernel_regularizer': regularizers.serialize(self.kernel_regularizer),\n                  'recurrent_regularizer': regularizers.serialize(self.recurrent_regularizer),\n                  'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n                  'kernel_constraint': constraints.serialize(self.kernel_constraint),\n                  'recurrent_constraint': constraints.serialize(self.recurrent_constraint),\n                  'bias_constraint': constraints.serialize(self.bias_constraint),\n                  'dropout': self.dropout,\n                  'recurrent_dropout': self.recurrent_dropout}\n        base_config = super(GRU, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\n\nclass LSTM(Recurrent):\n    \"\"\"Long-Short Term Memory unit - Hochreiter 1997.\n\n    For a step-by-step description of the algorithm, see\n    [this tutorial](http://deeplearning.net/tutorial/lstm.html).\n\n    # Arguments\n        units: Positive integer, dimensionality of the output space.\n        activation: Activation function to use\n            (see [activations](../activations.md)).\n            If you don't specify anything, no activation is applied\n            (ie. \"linear\" activation: `a(x) = x`).\n        recurrent_activation: Activation function to use\n            for the recurrent step\n            (see [activations](../activations.md)).\n        use_bias: Boolean, whether the layer uses a bias vector.\n        kernel_initializer: Initializer for the `kernel` weights matrix,\n            used for the linear transformation of the inputs.\n            (see [initializers](../initializers.md)).\n        recurrent_initializer: Initializer for the `recurrent_kernel`\n            weights matrix,\n            used for the linear transformation of the recurrent state.\n            (see [initializers](../initializers.md)).\n        bias_initializer: Initializer for the bias vector\n            (see [initializers](../initializers.md)).\n        unit_forget_bias: Boolean.\n            If True, add 1 to the bias of the forget gate at initialization.\n            Use in combination with `bias_initializer=\"zeros\"`.\n            This is recommended in [Jozefowicz et al.](http://www.jmlr.org/proceedings/papers/v37/jozefowicz15.pdf)\n        kernel_regularizer: Regularizer function applied to\n            the `kernel` weights matrix\n            (see [regularizer](../regularizers.md)).\n        recurrent_regularizer: Regularizer function applied to\n            the `recurrent_kernel` weights matrix\n            (see [regularizer](../regularizers.md)).\n        bias_regularizer: Regularizer function applied to the bias vector\n            (see [regularizer](../regularizers.md)).\n        activity_regularizer: Regularizer function applied to\n            the output of the layer (its \"activation\").\n            (see [regularizer](../regularizers.md)).\n        kernel_constraint: Constraint function applied to\n            the `kernel` weights matrix\n            (see [constraints](../constraints.md)).\n        recurrent_constraint: Constraint function applied to\n            the `recurrent_kernel` weights matrix\n            (see [constraints](../constraints.md)).\n        bias_constraint: Constraint function applied to the bias vector\n            (see [constraints](../constraints.md)).\n        dropout: Float between 0 and 1.\n            Fraction of the units to drop for\n            the linear transformation of the inputs.\n        recurrent_dropout: Float between 0 and 1.\n            Fraction of the units to drop for\n            the linear transformation of the recurrent state.\n\n    # References\n        - [Long short-term memory](http://deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf) (original 1997 paper)\n        - [Learning to forget: Continual prediction with LSTM](http://www.mitpressjournals.org/doi/pdf/10.1162/089976600300015015)\n        - [Supervised sequence labeling with recurrent neural networks](http://www.cs.toronto.edu/~graves/preprint.pdf)\n        - [A Theoretically Grounded Application of Dropout in Recurrent Neural Networks](http://arxiv.org/abs/1512.05287)\n    \"\"\"\n\n    def __init__(self, units,\n                 activation='tanh',\n                 recurrent_activation='hard_sigmoid',\n                 use_bias=True,\n                 kernel_initializer='glorot_uniform',\n                 recurrent_initializer='orthogonal',\n                 bias_initializer='zeros',\n                 unit_forget_bias=True,\n                 kernel_regularizer=None,\n                 recurrent_regularizer=None,\n                 bias_regularizer=None,\n                 activity_regularizer=None,\n                 kernel_constraint=None,\n                 recurrent_constraint=None,\n                 bias_constraint=None,\n                 dropout=0.,\n                 recurrent_dropout=0.,\n                 **kwargs):\n        super(LSTM, self).__init__(**kwargs)\n        self.units = units\n        self.activation = activations.get(activation)\n        self.recurrent_activation = activations.get(recurrent_activation)\n        self.use_bias = use_bias\n\n        self.kernel_initializer = initializers.get(kernel_initializer)\n        self.recurrent_initializer = initializers.get(recurrent_initializer)\n        self.bias_initializer = initializers.get(bias_initializer)\n        self.unit_forget_bias = unit_forget_bias\n\n        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n        self.recurrent_regularizer = regularizers.get(recurrent_regularizer)\n        self.bias_regularizer = regularizers.get(bias_regularizer)\n\n        self.kernel_constraint = constraints.get(kernel_constraint)\n        self.recurrent_constraint = constraints.get(recurrent_constraint)\n        self.bias_constraint = constraints.get(bias_constraint)\n\n        self.dropout = min(1., max(0., dropout))\n        self.recurrent_dropout = min(1., max(0., recurrent_dropout))\n\n    def build(self, input_shape):\n        if isinstance(input_shape, list):\n            input_shape = input_shape[0]\n        self.input_dim = input_shape[2]\n\n        if self.stateful:\n            self.reset_states()\n        else:\n            # initial states: 2 all-zero tensors of shape (output_dim)\n            self.states = [None, None]\n\n        self.kernel = self.add_weight((self.input_dim, self.units * 4),\n                                      name='kernel',\n                                      initializer=self.kernel_initializer,\n                                      regularizer=self.kernel_regularizer,\n                                      constraint=self.kernel_constraint)\n        self.recurrent_kernel = self.add_weight(\n            (self.units, self.units * 4),\n            name='recurrent_kernel',\n            initializer=self.recurrent_initializer,\n            regularizer=self.recurrent_regularizer,\n            constraint=self.recurrent_constraint)\n\n        if self.use_bias:\n            self.bias = self.add_weight((self.units * 4,),\n                                        name='bias',\n                                        initializer=self.bias_initializer,\n                                        regularizer=self.bias_regularizer,\n                                        constraint=self.bias_constraint)\n            if self.unit_forget_bias:\n                self.bias += K.concatenate([K.zeros((self.units,)),\n                                           K.ones((self.units,)),\n                                           K.zeros((self.units * 2,))])\n        else:\n            self.bias = None\n\n        self.kernel_i = self.kernel[:, :self.units]\n        self.recurrent_kernel_i = self.recurrent_kernel[:, :self.units]\n        self.kernel_f = self.kernel[:, self.units: self.units * 2]\n        self.recurrent_kernel_f = self.recurrent_kernel[:, self.units: self.units * 2]\n        self.kernel_c = self.kernel[:, self.units * 2: self.units * 3]\n        self.recurrent_kernel_c = self.recurrent_kernel[:, self.units * 2: self.units * 3]\n        self.kernel_o = self.kernel[:, self.units * 3:]\n        self.recurrent_kernel_o = self.recurrent_kernel[:, self.units * 3:]\n\n        if self.use_bias:\n            self.bias_i = self.bias[:self.units]\n            self.bias_f = self.bias[self.units: self.units * 2]\n            self.bias_c = self.bias[self.units * 2: self.units * 3]\n            self.bias_o = self.bias[self.units * 3:]\n        else:\n            self.bias_i = None\n            self.bias_f = None\n            self.bias_c = None\n            self.bias_o = None\n        self.built = True\n\n    def reset_states(self):\n        assert self.stateful, 'Layer must be stateful.'\n        input_shape = self.input_spec.shape\n        if not input_shape[0]:\n            raise ValueError('If a RNN is stateful, a complete '\n                             'input_shape must be provided '\n                             '(including batch size).')\n        if hasattr(self, 'states'):\n            K.set_value(self.states[0],\n                        np.zeros((input_shape[0], self.units)))\n            K.set_value(self.states[1],\n                        np.zeros((input_shape[0], self.units)))\n        else:\n            self.states = [K.zeros((input_shape[0], self.units)),\n                           K.zeros((input_shape[0], self.units))]\n\n    def preprocess_input(self, inputs, training=None):\n        if self.implementation == 0:\n            input_shape = K.int_shape(inputs)\n            input_dim = input_shape[2]\n            timesteps = input_shape[1]\n\n            x_i = _time_distributed_dense(inputs, self.kernel_i, self.bias_i,\n                                          self.dropout, input_dim, self.units,\n                                          timesteps, training=training)\n            x_f = _time_distributed_dense(inputs, self.kernel_f, self.bias_f,\n                                          self.dropout, input_dim, self.units,\n                                          timesteps, training=training)\n            x_c = _time_distributed_dense(inputs, self.kernel_c, self.bias_c,\n                                          self.dropout, input_dim, self.units,\n                                          timesteps, training=training)\n            x_o = _time_distributed_dense(inputs, self.kernel_o, self.bias_o,\n                                          self.dropout, input_dim, self.units,\n                                          timesteps, training=training)\n            return K.concatenate([x_i, x_f, x_c, x_o], axis=2)\n        else:\n            return inputs\n\n    def get_constants(self, inputs, training=None):\n        constants = []\n        if self.implementation == 0 and 0 < self.dropout < 1:\n            input_shape = K.int_shape(inputs)\n            input_dim = input_shape[-1]\n            ones = K.ones_like(K.reshape(inputs[:, 0, 0], (-1, 1)))\n            ones = K.tile(ones, (1, int(input_dim)))\n\n            def dropped_inputs():\n                return K.dropout(ones, self.dropout)\n\n            dp_mask = [K.in_train_phase(dropped_inputs,\n                                        ones,\n                                        training=training) for _ in range(4)]\n            constants.append(dp_mask)\n        else:\n            constants.append([K.cast_to_floatx(1.) for _ in range(4)])\n\n        if 0 < self.recurrent_dropout < 1:\n            ones = K.ones_like(K.reshape(inputs[:, 0, 0], (-1, 1)))\n            ones = K.tile(ones, (1, self.units))\n\n            def dropped_inputs():\n                return K.dropout(ones, self.recurrent_dropout)\n            rec_dp_mask = [K.in_train_phase(dropped_inputs,\n                                            ones,\n                                            training=training) for _ in range(4)]\n            constants.append(rec_dp_mask)\n        else:\n            constants.append([K.cast_to_floatx(1.) for _ in range(4)])\n        return constants\n\n    def step(self, inputs, states):\n        h_tm1 = states[0]\n        c_tm1 = states[1]\n        dp_mask = states[2]\n        rec_dp_mask = states[3]\n\n        if self.implementation == 1:\n            z = K.dot(inputs * dp_mask[0], self.kernel)\n            z += K.dot(h_tm1 * rec_dp_mask[0], self.recurrent_kernel)\n            if self.use_bias:\n                z += self.bias\n\n            z0 = z[:, :self.units]\n            z1 = z[:, self.units: 2 * self.units]\n            z2 = z[:, 2 * self.units: 3 * self.units]\n            z3 = z[:, 3 * self.units:]\n\n            i = self.recurrent_activation(z0)\n            f = self.recurrent_activation(z1)\n            c = f * c_tm1 + i * self.activation(z2)\n            o = self.recurrent_activation(z3)\n        else:\n            if self.implementation == 0:\n                x_i = inputs[:, :self.units]\n                x_f = inputs[:, self.units: 2 * self.units]\n                x_c = inputs[:, 2 * self.units: 3 * self.units]\n                x_o = inputs[:, 3 * self.units:]\n            elif self.implementation == 2:\n                x_i = K.dot(inputs * dp_mask[0], self.kernel_i) + self.bias_i\n                x_f = K.dot(inputs * dp_mask[1], self.kernel_f) + self.bias_f\n                x_c = K.dot(inputs * dp_mask[2], self.kernel_c) + self.bias_c\n                x_o = K.dot(inputs * dp_mask[3], self.kernel_o) + self.bias_o\n            else:\n                raise ValueError('Unknown `implementation` mode.')\n\n            i = self.recurrent_activation(x_i + K.dot(h_tm1 * rec_dp_mask[0],\n                                                      self.recurrent_kernel_i))\n            f = self.recurrent_activation(x_f + K.dot(h_tm1 * rec_dp_mask[1],\n                                                      self.recurrent_kernel_f))\n            c = f * c_tm1 + i * self.activation(x_c + K.dot(h_tm1 * rec_dp_mask[2],\n                                                            self.recurrent_kernel_c))\n            o = self.recurrent_activation(x_o + K.dot(h_tm1 * rec_dp_mask[3],\n                                                      self.recurrent_kernel_o))\n        h = o * self.activation(c)\n        if 0 < self.dropout + self.recurrent_dropout:\n            h._uses_learning_phase = True\n        return h, [h, c]\n\n    def get_config(self):\n        config = {'units': self.units,\n                  'activation': activations.serialize(self.activation),\n                  'recurrent_activation': activations.serialize(self.recurrent_activation),\n                  'use_bias': self.use_bias,\n                  'kernel_initializer': initializers.serialize(self.kernel_initializer),\n                  'recurrent_initializer': initializers.serialize(self.recurrent_initializer),\n                  'bias_initializer': initializers.serialize(self.bias_initializer),\n                  'unit_forget_bias': self.unit_forget_bias,\n                  'kernel_regularizer': regularizers.serialize(self.kernel_regularizer),\n                  'recurrent_regularizer': regularizers.serialize(self.recurrent_regularizer),\n                  'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n                  'kernel_constraint': constraints.serialize(self.kernel_constraint),\n                  'recurrent_constraint': constraints.serialize(self.recurrent_constraint),\n                  'bias_constraint': constraints.serialize(self.bias_constraint),\n                  'dropout': self.dropout,\n                  'recurrent_dropout': self.recurrent_dropout}\n        base_config = super(LSTM, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n",
          "file_patch": "@@ -128,8 +128,9 @@ class Recurrent(Layer):\n             at the level of the first layer\n             (e.g. via the `input_shape` argument)\n \n-    # Input shape\n-        3D tensor with shape `(batch_size, timesteps, input_dim)`.\n+    # Input shapes\n+        3D tensor with shape `(batch_size, timesteps, input_dim)`,\n+        (Optional) 2D tensors with shape `(batch_size, output_dim)`.\n \n     # Output shape\n         - if `return_sequences`: 3D tensor with shape\n@@ -162,6 +163,16 @@ class Recurrent(Layer):\n \n         To reset the states of your model, call `.reset_states()` on either\n         a specific layer, or on your entire model.\n+\n+    # Note on specifying initial states in RNNs\n+        Most RNN layers can be called with either a single tensor, or a list\n+        of tensors.\n+        If the an RNN layer is called with a single tensor, that tensor is\n+        treated as the input, and the initial states are assigned an\n+        appropriate default value.\n+        If an RNN layer is called with a list of tensors, the first element is\n+        treated as the input, and the remaining tensors are treated as the\n+        initial states.\n     \"\"\"\n \n     def __init__(self, return_sequences=False,\n@@ -177,11 +188,13 @@ class Recurrent(Layer):\n         self.unroll = unroll\n         self.implementation = 0\n         self.supports_masking = True\n-        self.input_spec = InputSpec(ndim=3)\n+        self.input_spec = None\n         self.dropout = 0\n         self.recurrent_dropout = 0\n \n     def compute_output_shape(self, input_shape):\n+        if isinstance(input_shape, list):\n+            input_shape = input_shape[0]\n         if self.return_sequences:\n             return (input_shape[0], input_shape[1], self.units)\n         else:\n@@ -215,6 +228,13 @@ class Recurrent(Layer):\n         # input shape: (nbias_samples, time (padded with zeros), input_dim)\n         # note that the .build() method of subclasses MUST define\n         # self.input_spec with a complete input shape.\n+        if isinstance(inputs, list):\n+            initial_states = inputs[1:]\n+            inputs = inputs[0]\n+        elif self.stateful:\n+            initial_states = self.states\n+        else:\n+            initial_states = self.get_initial_states(inputs)\n         input_shape = K.int_shape(inputs)\n         if self.unroll and input_shape[1] is None:\n             raise ValueError('Cannot unroll a RNN if the '\n@@ -228,10 +248,6 @@ class Recurrent(Layer):\n                              '- If using the functional API, specify '\n                              'the time dimension by passing a `shape` '\n                              'or `batch_shape` argument to your Input layer.')\n-        if self.stateful:\n-            initial_states = self.states\n-        else:\n-            initial_states = self.get_initial_states(inputs)\n         constants = self.get_constants(inputs, training=None)\n         preprocessed_input = self.preprocess_input(inputs, training=None)\n         last_output, outputs, states = K.rnn(self.step,\n@@ -354,8 +370,8 @@ class SimpleRNN(Recurrent):\n         self.recurrent_dropout = min(1., max(0., recurrent_dropout))\n \n     def build(self, input_shape):\n-        # TODO: handle variable-length seqs in input_spec\n-        self.input_spec = InputSpec(shape=input_shape)\n+        if isinstance(input_shape, list):\n+            input_shape = input_shape[0]\n         if self.stateful:\n             self.reset_states()\n         else:\n@@ -589,8 +605,8 @@ class GRU(Recurrent):\n         self.recurrent_dropout = min(1., max(0., recurrent_dropout))\n \n     def build(self, input_shape):\n-        # TODO: handle variable-length sequences in input spec.\n-        self.input_spec = InputSpec(shape=input_shape)\n+        if isinstance(input_shape, list):\n+            input_shape = input_shape[0]\n         self.input_dim = input_shape[2]\n \n         if self.stateful:\n@@ -878,7 +894,8 @@ class LSTM(Recurrent):\n         self.recurrent_dropout = min(1., max(0., recurrent_dropout))\n \n     def build(self, input_shape):\n-        self.input_spec = InputSpec(shape=input_shape)\n+        if isinstance(input_shape, list):\n+            input_shape = input_shape[0]\n         self.input_dim = input_shape[2]\n \n         if self.stateful:\n",
          "files_name_in_blame_commit": [
            "recurrent.py"
          ]
        }
      },
      "d663fda862df1c831e7f93f1e3feb2e189a1b9ef": {
        "commit": {
          "commit_id": "d663fda862df1c831e7f93f1e3feb2e189a1b9ef",
          "commit_message": "Fix up layers unit tests with TF.",
          "commit_author": "Francois Chollet",
          "commit_date": "2017-02-16 15:26:58",
          "commit_parent": "36ac91f0576b2295df1f3f8b23c305d69698a0ff"
        },
        "function": {
          "function_name": "build",
          "function_code_before": "def build(self, input_shape):\n    self.input_spec = InputSpec(shape=input_shape)\n    if self.stateful:\n        self.reset_states()\n    else:\n        self.states = [None]\n    input_dim = input_shape[2]\n    self.input_dim = input_dim\n    self.kernel = self.add_weight((input_dim, self.units), name='kernel', initializer=self.kernel_initializer, regularizer=self.kernel_regularizer, constraint=self.kernel_constraint)\n    self.recurrent_kernel = self.add_weight((self.units, self.units), name='recurrent_kernel', initializer=self.recurrent_initializer, regularizer=self.recurrent_regularizer, constraint=self.recurrent_constraint)\n    if self.use_bias:\n        self.bias = self.add_weight((self.units,), name='bias', initializer=self.bias_initializer, regularizer=self.bias_regularizer, constraint=self.bias_constraint)\n    else:\n        self.bias = None",
          "function_code_after": "def build(self, input_shape):\n    self.input_spec = InputSpec(shape=input_shape)\n    if self.stateful:\n        self.reset_states()\n    else:\n        self.states = [None]\n    input_dim = input_shape[2]\n    self.input_dim = input_dim\n    self.kernel = self.add_weight((input_dim, self.units), name='kernel', initializer=self.kernel_initializer, regularizer=self.kernel_regularizer, constraint=self.kernel_constraint)\n    self.recurrent_kernel = self.add_weight((self.units, self.units), name='recurrent_kernel', initializer=self.recurrent_initializer, regularizer=self.recurrent_regularizer, constraint=self.recurrent_constraint)\n    if self.use_bias:\n        self.bias = self.add_weight((self.units,), name='bias', initializer=self.bias_initializer, regularizer=self.bias_regularizer, constraint=self.bias_constraint)\n    else:\n        self.bias = None\n    self.built = True",
          "function_before_start_line": 348,
          "function_before_end_line": 377,
          "function_after_start_line": 355,
          "function_after_end_line": 385,
          "function_before_token_count": 174,
          "function_after_token_count": 179,
          "functions_name_modified_file": [
            "__init__",
            "compute_mask",
            "build",
            "call",
            "get_constants",
            "_time_distributed_dense",
            "compute_output_shape",
            "step",
            "get_initial_states",
            "reset_states",
            "preprocess_input",
            "get_config"
          ],
          "functions_name_all_files": [
            "test_maxpooling_3d",
            "test_embedding",
            "test_GaussianNoise",
            "output",
            "save_weights",
            "test_elu",
            "test_loss_masking",
            "run_internal_graph",
            "get_layer",
            "test_maxpooling_1d",
            "test_zero_padding_2d",
            "deserialize",
            "normalize_data_format",
            "test_dropout",
            "get_input_shape_at",
            "_updated_config",
            "test_TimeDistributed",
            "test_from_config",
            "step",
            "get_weights",
            "stateful",
            "test_maxpooling_2d",
            "input_conv",
            "test_averagepooling_2d",
            "get_test_data",
            "_get_noise_shape",
            "test_prelu",
            "test_averagepooling_1d",
            "test_regularizer",
            "VGG19",
            "_time_distributed_dense",
            "test_return_sequences",
            "test_recurrent_convolutional",
            "deconv_length",
            "__init__",
            "compute_mask",
            "get_output_shape_for",
            "get_input_at",
            "get_constants",
            "compute_output_shape",
            "test_statefulness",
            "save",
            "test_globalpooling_2d",
            "test_conv2d_transpose",
            "test_globalpooling_1d",
            "test_zero_padding_1d",
            "trainable_weights",
            "get_output_shape_at",
            "test_flatten",
            "test_repeat_vector",
            "test_globalpooling_3d",
            "losses",
            "test_prelu_share",
            "test_separable_conv_2d",
            "test_averagepooling_3d",
            "get_config",
            "_to_snake_case",
            "_fix_unknown_dimension",
            "test_masking_layer",
            "conv_output_length",
            "test_cropping_3d",
            "test_implementation_mode",
            "normalize_padding",
            "test_lambda",
            "test_permute",
            "save_weights_to_hdf5_group",
            "test_regularizers",
            "build",
            "call",
            "test_image_classification",
            "updates",
            "get_losses_for",
            "test_reshape",
            "add_weight",
            "test_dynamic_behavior",
            "keras_test",
            "output_shape",
            "add_update",
            "test_thresholded_relu",
            "test_upsampling_1d",
            "basic_batchnorm_test",
            "test_batchnorm_mode_twice",
            "test_cropping_1d",
            "test_shared_batchnorm",
            "test_masking",
            "add_loss",
            "reset_states",
            "_to_list",
            "count_params",
            "test_activity_regularization",
            "test_upsampling_3d",
            "test_leaky_relu",
            "set_weights",
            "_pooling_function",
            "get_output_mask_at",
            "input",
            "test_zero_padding_3d",
            "load_weights",
            "test_batchnorm_correctness",
            "get_source_inputs",
            "preprocess_input",
            "_get_node_attribute_at_index",
            "serialize",
            "non_trainable_weights",
            "test_GaussianDropout",
            "input_mask",
            "input_shape",
            "constraints",
            "__call__",
            "_collect_input_shape",
            "test_upsampling_2d",
            "test_dense",
            "_add_inbound_node",
            "state_updates",
            "get_input_mask_at",
            "normalize_tuple",
            "weights",
            "test_Bidirectional",
            "test_locallyconnected_2d",
            "_arguments_validation",
            "to_yaml",
            "conv_input_length",
            "get_updates_for",
            "test_convolution_3d",
            "test_batchnorm_convnet",
            "output_mask",
            "test_activation",
            "rnn_test",
            "layer_test",
            "load_weights_from_hdf5_group_by_name",
            "Input",
            "get_initial_states",
            "test_convolution_2d",
            "get",
            "test_cropping_2d",
            "to_json",
            "input_spec",
            "test_conv_1d",
            "get_output_at",
            "uses_learning_phase",
            "load_weights_from_hdf5_group",
            "summary",
            "_is_all_none",
            "reccurent_conv",
            "_object_list_uid",
            "from_config",
            "_collect_previous_mask",
            "convert_kernel",
            "assert_input_compatibility",
            "test_locallyconnected_1d"
          ],
          "functions_name_co_evolved_modified_file": [
            "__init__",
            "call",
            "reset_states",
            "step",
            "get_config"
          ],
          "functions_name_co_evolved_all_files": [
            "test_maxpooling_3d",
            "test_embedding",
            "test_GaussianNoise",
            "test_merge_mask_3d",
            "test_batchnorm_mode_0_or_2",
            "test_elu",
            "test_loss_masking",
            "run_internal_graph",
            "test_maxpooling_1d",
            "test_zero_padding_2d",
            "deserialize",
            "test_dropout",
            "test_TimeDistributed",
            "test_from_config",
            "step",
            "test_merge_mask_2d",
            "test_maxpooling_2d",
            "input_conv",
            "test_averagepooling_2d",
            "test_regularizer",
            "test_prelu",
            "test_averagepooling_1d",
            "test_atrous_conv_2d",
            "VGG19",
            "test_return_sequences",
            "test_recurrent_convolutional",
            "test_merge",
            "__init__",
            "compute_mask",
            "get_output_shape_for",
            "get_constants",
            "compute_output_shape",
            "test_statefulness",
            "test_timedistributeddense",
            "test_globalpooling_2d",
            "test_zero_padding_1d",
            "test_globalpooling_1d",
            "test_conv2d_transpose",
            "test_flatten",
            "test_repeat_vector",
            "test_globalpooling_3d",
            "test_prelu_share",
            "test_separable_conv_2d",
            "test_averagepooling_3d",
            "get_config",
            "test_masking_layer",
            "test_cropping_3d",
            "test_implementation_mode",
            "normalize_padding",
            "test_lambda",
            "test_permute",
            "test_regularizers",
            "call",
            "test_image_classification",
            "test_reshape",
            "test_dynamic_behavior",
            "test_parametric_softplus_share",
            "test_thresholded_relu",
            "test_upsampling_1d",
            "basic_batchnorm_test",
            "test_batchnorm_mode_twice",
            "test_srelu",
            "test_cropping_1d",
            "test_shared_batchnorm",
            "test_masking",
            "test_deconvolution_2d",
            "reset_states",
            "test_activity_regularization",
            "test_maxout_dense",
            "test_batchnorm_mode_1",
            "test_upsampling_3d",
            "test_highway",
            "test_leaky_relu",
            "test_zero_padding_3d",
            "test_batchnorm_correctness",
            "test_batchnorm_mode_0_or_2_twice",
            "test_GaussianDropout",
            "constraints",
            "test_upsampling_2d",
            "test_dense",
            "_add_inbound_node",
            "test_srelu_share",
            "test_locallyconnected_2d",
            "_arguments_validation",
            "test_convolution_3d",
            "test_batchnorm_convnet",
            "test_parametric_softplus",
            "test_batchnorm_mode_0_convnet",
            "test_activation",
            "rnn_test",
            "layer_test",
            "test_convolution_2d",
            "test_cropping_2d",
            "test_conv_1d",
            "_collect_previous_mask",
            "from_config",
            "test_locallyconnected_1d"
          ]
        },
        "file": {
          "file_name": "recurrent.py",
          "file_nloc": 947,
          "file_complexity": 108,
          "file_token_count": 5803,
          "file_before": "# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import\nimport numpy as np\n\nfrom .. import backend as K\nfrom .. import activations\nfrom .. import initializers\nfrom .. import regularizers\nfrom .. import constraints\nfrom ..engine import Layer\nfrom ..engine import InputSpec\n\n\ndef _time_distributed_dense(x, w, b=None, dropout=None,\n                            input_dim=None, output_dim=None,\n                            timesteps=None, training=None):\n    \"\"\"Apply `y . w + b` for every temporal slice y of x.\n\n    # Arguments\n        x: input tensor.\n        w: weight matrix.\n        b: optional bias vector.\n        dropout: wether to apply dropout (same dropout mask\n            for every temporal slice of the input).\n        input_dim: integer; optional dimensionality of the input.\n        output_dim: integer; optional dimensionality of the output.\n        timesteps: integer; optional number of timesteps.\n        training: training phase tensor or boolean.\n\n    # Returns\n        Output tensor.\n    \"\"\"\n    if not input_dim:\n        input_dim = K.shape(x)[2]\n    if not timesteps:\n        timesteps = K.shape(x)[1]\n    if not output_dim:\n        output_dim = K.shape(w)[1]\n\n    if dropout is not None and 0. < dropout < 1.:\n        # apply the same dropout pattern at every timestep\n        ones = K.ones_like(K.reshape(x[:, 0, :], (-1, input_dim)))\n        dropout_matrix = K.dropout(ones, dropout)\n        expanded_dropout_matrix = K.repeat(dropout_matrix, timesteps)\n        x = K.in_train_phase(x * expanded_dropout_matrix, x, training=training)\n\n    # collapse time dimension and batch dimension together\n    x = K.reshape(x, (-1, input_dim))\n    x = K.dot(x, w)\n    if b is not None:\n        x += b\n    # reshape to 3D tensor\n    if K.backend() == 'tensorflow':\n        x = K.reshape(x, K.stack([-1, timesteps, output_dim]))\n        x.set_shape([None, None, output_dim])\n    else:\n        x = K.reshape(x, (-1, timesteps, output_dim))\n    return x\n\n\nclass Recurrent(Layer):\n    \"\"\"Abstract base class for recurrent layers.\n    Do not use in a model -- it's not a valid layer!\n    Use its children classes `LSTM`, `GRU` and `SimpleRNN` instead.\n\n    All recurrent layers (`LSTM`, `GRU`, `SimpleRNN`) also\n    follow the specifications of this class and accept\n    the keyword arguments listed below.\n\n    # Example\n\n    ```python\n        # as the first layer in a Sequential model\n        model = Sequential()\n        model.add(LSTM(32, input_shape=(10, 64)))\n        # now model.output_shape == (None, 32)\n        # note: `None` is the batch dimension.\n\n        # the following is identical:\n        model = Sequential()\n        model.add(LSTM(32, input_dim=64, input_length=10))\n\n        # for subsequent layers, not need to specify the input size:\n        model.add(LSTM(16))\n    ```\n\n    # Arguments\n        weights: list of Numpy arrays to set as initial weights.\n            The list should have 3 elements, of shapes:\n            `[(input_dim, output_dim), (output_dim, output_dim), (output_dim,)]`.\n        return_sequences: Boolean. Whether to return the last output\n            in the output sequence, or the full sequence.\n        go_backwards: Boolean (default False).\n            If True, process the input sequence backwards.\n        stateful: Boolean (default False). If True, the last state\n            for each sample at index i in a batch will be used as initial\n            state for the sample of index i in the following batch.\n        unroll: Boolean (default False).\n            If True, the network will be unrolled,\n            else a symbolic loop will be used.\n            Unrolling can speed-up a RNN,\n            although it tends to be more memory-intensive.\n            Unrolling is only suitable for short sequences.\n        implementation: one of {0, 1, or 2}.\n            If set to 0, the RNN will use\n            an implementation that uses fewer, larger matrix products,\n            thus running faster on CPU but consuming more memory.\n            If set to 1, the RNN will use more matrix products,\n            but smaller ones, thus running slower\n            (may actually be faster on GPU) while consuming less memory.\n            If set to 2 (LSTM/GRU only),\n            the RNN will combine the input gate,\n            the forget gate and the output gate into a single matrix,\n            enabling more time-efficient parallelization on the GPU.\n            Note: RNN dropout must be shared for all gates,\n            resulting in a slightly reduced regularization.\n        input_dim: dimensionality of the input (integer).\n            This argument (or alternatively, the keyword argument `input_shape`)\n            is required when using this layer as the first layer in a model.\n        input_length: Length of input sequences, to be specified\n            when it is constant.\n            This argument is required if you are going to connect\n            `Flatten` then `Dense` layers upstream\n            (without it, the shape of the dense outputs cannot be computed).\n            Note that if the recurrent layer is not the first layer\n            in your model, you would need to specify the input length\n            at the level of the first layer\n            (e.g. via the `input_shape` argument)\n\n    # Input shape\n        3D tensor with shape `(batch_size, timesteps, input_dim)`.\n\n    # Output shape\n        - if `return_sequences`: 3D tensor with shape\n            `(batch_size, timesteps, units)`.\n        - else, 2D tensor with shape `(batch_size, units)`.\n\n    # Masking\n        This layer supports masking for input data with a variable number\n        of timesteps. To introduce masks to your data,\n        use an [Embedding](embeddings.md) layer with the `mask_zero` parameter\n        set to `True`.\n\n    # Note on using statefulness in RNNs\n        You can set RNN layers to be 'stateful', which means that the states\n        computed for the samples in one batch will be reused as initial states\n        for the samples in the next batch. This assumes a one-to-one mapping\n        between samples in different successive batches.\n\n        To enable statefulness:\n            - specify `stateful=True` in the layer constructor.\n            - specify a fixed batch size for your model, by passing\n                if sequential model:\n                  `batch_input_shape=(...)` to the first layer in your model.\n                else for functional model with 1 or more Input layers:\n                  `batch_shape=(...)` to all the first layers in your model.\n                This is the expected shape of your inputs\n                *including the batch size*.\n                It should be a tuple of integers, e.g. `(32, 10, 100)`.\n            - specify `shuffle=False` when calling fit().\n\n        To reset the states of your model, call `.reset_states()` on either\n        a specific layer, or on your entire model.\n    \"\"\"\n\n    def __init__(self, return_sequences=False,\n                 go_backwards=False,\n                 stateful=False,\n                 unroll=False,\n                 implementation=0,\n                 **kwargs):\n        super(Recurrent, self).__init__(**kwargs)\n        self.return_sequences = return_sequences\n        self.go_backwards = go_backwards\n        self.stateful = stateful\n        self.unroll = unroll\n        self.implementation = 0\n        self.supports_masking = True\n        self.input_spec = InputSpec(ndim=3)\n\n    def compute_output_shape(self, input_shape):\n        if self.return_sequences:\n            return (input_shape[0], input_shape[1], self.units)\n        else:\n            return (input_shape[0], self.units)\n\n    def compute_mask(self, inputs, mask):\n        if self.return_sequences:\n            return mask\n        else:\n            return None\n\n    def step(self, inputs, states):\n        raise NotImplementedError\n\n    def get_constants(self, inputs, training=None):\n        return []\n\n    def get_initial_states(self, inputs):\n        # build an all-zero tensor of shape (samples, output_dim)\n        initial_state = K.zeros_like(inputs)  # (samples, timesteps, input_dim)\n        initial_state = K.sum(initial_state, axis=(1, 2))  # (samples,)\n        initial_state = K.expand_dims(initial_state)  # (samples, 1)\n        initial_state = K.tile(initial_state, [1, self.units])  # (samples, output_dim)\n        initial_states = [initial_state for _ in range(len(self.states))]\n        return initial_states\n\n    def preprocess_input(self, inputs, training=None):\n        return inputs\n\n    def call(self, inputs, mask=None, training=None):\n        # input shape: (nbias_samples, time (padded with zeros), input_dim)\n        # note that the .build() method of subclasses MUST define\n        # self.input_spec with a complete input shape.\n        input_shape = K.int_shape(inputs)\n        if self.unroll and input_shape[1] is None:\n            raise ValueError('Cannot unroll a RNN if the '\n                             'time dimension is undefined. \\n'\n                             '- If using a Sequential model, '\n                             'specify the time dimension by passing '\n                             'an `input_shape` or `batch_input_shape` '\n                             'argument to your first layer. If your '\n                             'first layer is an Embedding, you can '\n                             'also use the `input_length` argument.\\n'\n                             '- If using the functional API, specify '\n                             'the time dimension by passing a `shape` '\n                             'or `batch_shape` argument to your Input layer.')\n        if self.stateful:\n            initial_states = self.states\n        else:\n            initial_states = self.get_initial_states(inputs)\n        constants = self.get_constants(inputs, training=None)\n        preprocessed_input = self.preprocess_input(inputs, training=None)\n        last_output, outputs, states = K.rnn(self.step,\n                                             preprocessed_input,\n                                             initial_states,\n                                             go_backwards=self.go_backwards,\n                                             mask=mask,\n                                             constants=constants,\n                                             unroll=self.unroll,\n                                             input_length=input_shape[1])\n        if self.stateful:\n            updates = []\n            for i in range(len(states)):\n                updates.append((self.states[i], states[i]))\n            self.add_update(updates, inputs)\n\n        if self.return_sequences:\n            return outputs\n        else:\n            return last_output\n\n    def get_config(self):\n        config = {'return_sequences': self.return_sequences,\n                  'go_backwards': self.go_backwards,\n                  'stateful': self.stateful,\n                  'unroll': self.unroll,\n                  'implementation': self.implementation}\n        base_config = super(Recurrent, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\n\nclass SimpleRNN(Recurrent):\n    \"\"\"Fully-connected RNN where the output is to be fed back to input.\n\n    # Arguments\n        units: Positive integer, dimensionality of the output space.\n        activation: Activation function to use\n            (see [activations](../activations.md)).\n            If you don't specify anything, no activation is applied\n            (ie. \"linear\" activation: `a(x) = x`).\n        use_bias: Boolean, whether the layer uses a bias vector.\n        kernel_initializer: Initializer for the `kernel` weights matrix,\n            used for the linear transformation of the inputs.\n            (see [initializers](../initializers.md)).\n        recurrent_initializer: Initializer for the `recurrent_kernel`\n            weights matrix,\n            used for the linear transformation of the recurrent state.\n            (see [initializers](../initializers.md)).\n        bias_initializer: Initializer for the bias vector\n            (see [initializers](../initializers.md)).\n        kernel_regularizer: Regularizer function applied to\n            the `kernel` weights matrix\n            (see [regularizer](../regularizers.md)).\n        recurrent_regularizer: Regularizer function applied to\n            the `recurrent_kernel` weights matrix\n            (see [regularizer](../regularizers.md)).\n        bias_regularizer: Regularizer function applied to the bias vector\n            (see [regularizer](../regularizers.md)).\n        activity_regularizer: Regularizer function applied to\n            the output of the layer (its \"activation\").\n            (see [regularizer](../regularizers.md)).\n        kernel_constraint: Constraint function applied to\n            the `kernel` weights matrix\n            (see [constraints](../constraints.md)).\n        recurrent_constraint: Constraint function applied to\n            the `recurrent_kernel` weights matrix\n            (see [constraints](../constraints.md)).\n        bias_constraint: Constraint function applied to the bias vector\n            (see [constraints](../constraints.md)).\n        dropout: Float between 0 and 1.\n            Fraction of the units to drop for\n            the linear transformation of the inputs.\n        recurrent_dropout: Float between 0 and 1.\n            Fraction of the units to drop for\n            the linear transformation of the recurrent state.\n\n    # References\n        - [A Theoretically Grounded Application of Dropout in Recurrent Neural Networks](http://arxiv.org/abs/1512.05287)\n    \"\"\"\n\n    def __init__(self, units,\n                 activation='tanh',\n                 use_bias=True,\n                 kernel_initializer='glorot_uniform',\n                 recurrent_initializer='orthogonal',\n                 bias_initializer='zeros',\n                 kernel_regularizer=None,\n                 recurrent_regularizer=None,\n                 bias_regularizer=None,\n                 activity_regularizer=None,\n                 kernel_constraint=None,\n                 recurrent_constraint=None,\n                 bias_constraint=None,\n                 dropout=0.,\n                 recurrent_dropout=0.,\n                 **kwargs):\n        super(SimpleRNN, self).__init__(**kwargs)\n        self.units = units\n        self.activation = activations.get(activation)\n        self.use_bias = use_bias\n\n        self.kernel_initializer = initializers.get(kernel_initializer)\n        self.recurrent_initializer = initializers.get(recurrent_initializer)\n        self.bias_initializer = initializers.get(bias_initializer)\n\n        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n        self.recurrent_regularizer = regularizers.get(recurrent_regularizer)\n        self.bias_regularizer = regularizers.get(bias_regularizer)\n\n        self.kernel_constraint = constraints.get(kernel_constraint)\n        self.recurrent_constraint = constraints.get(recurrent_constraint)\n        self.bias_constraint = constraints.get(bias_constraint)\n\n        self.dropout = min(1., max(0., dropout))\n        self.recurrent_dropout = min(1., max(0., recurrent_dropout))\n\n    def build(self, input_shape):\n        # TODO: handle variable-length seqs in input_spec\n        self.input_spec = InputSpec(shape=input_shape)\n        if self.stateful:\n            self.reset_states()\n        else:\n            # Initial states: all-zero tensor of shape (output_dim).\n            self.states = [None]\n        input_dim = input_shape[2]\n        self.input_dim = input_dim\n\n        self.kernel = self.add_weight((input_dim, self.units),\n                                      name='kernel',\n                                      initializer=self.kernel_initializer,\n                                      regularizer=self.kernel_regularizer,\n                                      constraint=self.kernel_constraint)\n        self.recurrent_kernel = self.add_weight(\n            (self.units, self.units),\n            name='recurrent_kernel',\n            initializer=self.recurrent_initializer,\n            regularizer=self.recurrent_regularizer,\n            constraint=self.recurrent_constraint)\n        if self.use_bias:\n            self.bias = self.add_weight((self.units,),\n                                        name='bias',\n                                        initializer=self.bias_initializer,\n                                        regularizer=self.bias_regularizer,\n                                        constraint=self.bias_constraint)\n        else:\n            self.bias = None\n\n    def reset_states(self):\n        if not self.stateful:\n            raise AttributeError('Layer must be stateful.')\n        input_shape = self.input_spec[0].shape\n        if not input_shape[0]:\n            raise ValueError('If a RNN is stateful, it needs to know '\n                             'its batch size. Specify the batch size '\n                             'of your input tensors: \\n'\n                             '- If using a Sequential model, '\n                             'specify the batch size by passing '\n                             'a `batch_input_shape` '\n                             'argument to your first layer.\\n'\n                             '- If using the functional API, specify '\n                             'the time dimension by passing a '\n                             '`batch_shape` argument to your Input layer.')\n        if hasattr(self, 'states'):\n            K.set_value(self.states[0],\n                        np.zeros((input_shape[0], self.units)))\n        else:\n            self.states = [K.zeros((input_shape[0], self.units))]\n\n    def preprocess_input(self, inputs, training=None):\n        if self.implementation > 0:\n            return inputs\n        else:\n            input_shape = K.int_shape(inputs)\n            input_dim = input_shape[2]\n            timesteps = input_shape[1]\n            return _time_distributed_dense(inputs,\n                                           self.kernel,\n                                           self.bias,\n                                           self.dropout,\n                                           input_dim,\n                                           self.units,\n                                           timesteps,\n                                           training=training)\n\n    def step(self, inputs, states):\n        if self.implementation == 0:\n            h = inputs\n        else:\n            if 0 < self.dropout < 1:\n                h = K.dot(inputs * states[1], self.kernel)\n            else:\n                h = K.dot(inputs, self.kernel)\n            if self.bias is not None:\n                h += self.bias\n\n        prev_output = states[0]\n        if 0 < self.recurrent_dropout < 1:\n            prev_output *= states[2]\n        output = h + K.dot(prev_output, self.recurrent_kernel)\n        if self.activation is not None:\n            output = self.activation(output)\n\n        # Properly set learning phase on output tensor.\n        if 0 < self.dropout + self.recurrent_dropout:\n            uses_learning_phase = False\n            if getattr(states[0], '_uses_learning_phase', False):\n                uses_learning_phase = True\n            if getattr(states[1], '_uses_learning_phase', False):\n                uses_learning_phase = True\n            output._uses_learning_phase = uses_learning_phase\n        return output, [output]\n\n    def get_constants(self, inputs, training=None):\n        constants = []\n        if self.implementation == 0 and 0 < self.dropout < 1:\n            input_shape = K.int_shape(inputs)\n            input_dim = input_shape[-1]\n            ones = K.ones_like(K.reshape(inputs[:, 0, 0], (-1, 1)))\n            ones = K.tile(ones, (1, int(input_dim)))\n\n            def dropped_inputs():\n                return K.dropout(ones, self.dropout)\n\n            dp_mask = K.in_train_phase(dropped_inputs,\n                                       ones,\n                                       training=training)\n            constants.append(dp_mask)\n        else:\n            constants.append(K.cast_to_floatx(1.))\n\n        if 0 < self.recurrent_dropout < 1:\n            ones = K.ones_like(K.reshape(inputs[:, 0, 0], (-1, 1)))\n            ones = K.tile(ones, (1, self.units))\n\n            def dropped_inputs():\n                return K.dropout(ones, self.recurrent_dropout)\n            rec_dp_mask = K.in_train_phase(dropped_inputs,\n                                           ones,\n                                           training=training)\n            constants.append(rec_dp_mask)\n        else:\n            constants.append(K.cast_to_floatx(1.))\n        return constants\n\n    def get_config(self):\n        config = {'units': self.units,\n                  'activation': activations.serialize(self.activation),\n                  'use_bias': self.use_bias,\n                  'kernel_initializer': initializers.serialize(self.kernel_initializer),\n                  'recurrent_initializer': initializers.serialize(self.recurrent_initializer),\n                  'bias_initializer': initializers.serialize(self.bias_initializer),\n                  'kernel_regularizer': regularizers.serialize(kernel_regularizer),\n                  'recurrent_regularizer': regularizers.serialize(recurrent_regularizer),\n                  'bias_regularizer': regularizers.serialize(bias_regularizer),\n                  'kernel_constraint': constraints.serialize(self.kernel_constraint),\n                  'recurrent_constraint': constraints.serialize(self.recurrent_constraint),\n                  'bias_constraint': constraints.serialize(self.bias_constraint),\n                  'dropout': self.dropout,\n                  'recurrent_dropout': self.recurrent_dropout}\n        base_config = super(SimpleRNN, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\n\nclass GRU(Recurrent):\n    \"\"\"Gated Recurrent Unit - Cho et al. 2014.\n\n    # Arguments\n        units: Positive integer, dimensionality of the output space.\n        activation: Activation function to use\n            (see [activations](../activations.md)).\n            If you don't specify anything, no activation is applied\n            (ie. \"linear\" activation: `a(x) = x`).\n        recurrent_activation: Activation function to use\n            for the recurrent step\n            (see [activations](../activations.md)).\n        use_bias: Boolean, whether the layer uses a bias vector.\n        kernel_initializer: Initializer for the `kernel` weights matrix,\n            used for the linear transformation of the inputs.\n            (see [initializers](../initializers.md)).\n        recurrent_initializer: Initializer for the `recurrent_kernel`\n            weights matrix,\n            used for the linear transformation of the recurrent state.\n            (see [initializers](../initializers.md)).\n        bias_initializer: Initializer for the bias vector\n            (see [initializers](../initializers.md)).\n        kernel_regularizer: Regularizer function applied to\n            the `kernel` weights matrix\n            (see [regularizer](../regularizers.md)).\n        recurrent_regularizer: Regularizer function applied to\n            the `recurrent_kernel` weights matrix\n            (see [regularizer](../regularizers.md)).\n        bias_regularizer: Regularizer function applied to the bias vector\n            (see [regularizer](../regularizers.md)).\n        activity_regularizer: Regularizer function applied to\n            the output of the layer (its \"activation\").\n            (see [regularizer](../regularizers.md)).\n        kernel_constraint: Constraint function applied to\n            the `kernel` weights matrix\n            (see [constraints](../constraints.md)).\n        recurrent_constraint: Constraint function applied to\n            the `recurrent_kernel` weights matrix\n            (see [constraints](../constraints.md)).\n        bias_constraint: Constraint function applied to the bias vector\n            (see [constraints](../constraints.md)).\n        dropout: Float between 0 and 1.\n            Fraction of the units to drop for\n            the linear transformation of the inputs.\n        recurrent_dropout: Float between 0 and 1.\n            Fraction of the units to drop for\n            the linear transformation of the recurrent state.\n\n    # References\n        - [On the Properties of Neural Machine Translation: Encoder-Decoder Approaches](https://arxiv.org/abs/1409.1259)\n        - [Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling](http://arxiv.org/abs/1412.3555v1)\n        - [A Theoretically Grounded Application of Dropout in Recurrent Neural Networks](http://arxiv.org/abs/1512.05287)\n    \"\"\"\n\n    def __init__(self, units,\n                 activation='tanh',\n                 recurrent_activation='hard_sigmoid',\n                 use_bias=True,\n                 kernel_initializer='glorot_uniform',\n                 recurrent_initializer='orthogonal',\n                 bias_initializer='zeros',\n                 kernel_regularizer=None,\n                 recurrent_regularizer=None,\n                 bias_regularizer=None,\n                 activity_regularizer=None,\n                 kernel_constraint=None,\n                 recurrent_constraint=None,\n                 bias_constraint=None,\n                 dropout=0.,\n                 recurrent_dropout=0.,\n                 **kwargs):\n        super(GRU, self).__init__(**kwargs)\n        self.units = units\n        self.activation = activations.get(activation)\n        self.recurrent_activation = activations.get(recurrent_activation)\n        self.use_bias = use_bias\n\n        self.kernel_initializer = initializers.get(kernel_initializer)\n        self.recurrent_initializer = initializers.get(recurrent_initializer)\n        self.bias_initializer = initializers.get(bias_initializer)\n\n        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n        self.recurrent_regularizer = regularizers.get(recurrent_regularizer)\n        self.bias_regularizer = regularizers.get(bias_regularizer)\n\n        self.kernel_constraint = constraints.get(kernel_constraint)\n        self.recurrent_constraint = constraints.get(recurrent_constraint)\n        self.bias_constraint = constraints.get(bias_constraint)\n\n        self.dropout = min(1., max(0., dropout))\n        self.recurrent_dropout = min(1., max(0., recurrent_dropout))\n\n    def build(self, input_shape):\n        # TODO: handle variable-length sequences in input spec.\n        self.input_spec = InputSpec(shape=input_shape)\n        self.input_dim = input_shape[2]\n\n        if self.stateful:\n            self.reset_states()\n        else:\n            # initial states: all-zero tensor of shape (output_dim)\n            self.states = [None]\n\n        self.kernel = self.add_weight((self.input_dim, self.units * 3),\n                                      name='kernel',\n                                      initializer=self.kernel_initializer,\n                                      regularizer=self.kernel_regularizer,\n                                      constraint=self.kernel_constraint)\n        self.recurrent_kernel = self.add_weight(\n            (self.units, self.units * 3),\n            name='recurrent_kernel',\n            initializer=self.recurrent_initializer,\n            regularizer=self.recurrent_regularizer,\n            constraint=self.recurrent_constraint)\n\n        if self.use_bias:\n            self.bias = self.add_weight((self.units * 3,),\n                                        name='bias',\n                                        initializer='zero',\n                                        regularizer=self.bias_regularizer,\n                                        constraint=self.bias_constraint)\n        else:\n            self.bias = None\n\n        self.kernel_z = self.kernel[:, :self.units]\n        self.recurrent_kernel_z = self.recurrent_kernel[:, :self.units]\n        self.kernel_r = self.kernel[:, self.units: self.units * 2]\n        self.recurrent_kernel_r = self.recurrent_kernel[:,\n                                                        self.units:\n                                                        self.units * 2]\n        self.kernel_h = self.kernel[:, self.units * 2:]\n        self.recurrent_kernel_h = self.recurrent_kernel[:, self.units * 2:]\n\n        if self.use_bias:\n            self.bias_z = self.bias[:self.units]\n            self.bias_r = self.bias[self.units: self.units * 2]\n            self.bias_h = self.bias[self.units * 2:]\n        else:\n            self.bias_z = None\n            self.bias_r = None\n            self.bias_h = None\n\n    def reset_states(self):\n        if not self.stateful:\n            raise RuntimeError('Layer must be stateful.')\n        input_shape = self.input_spec[0].shape\n        if not input_shape[0]:\n            raise ValueError('If a RNN is stateful, a complete '\n                             'input_shape must be provided '\n                             '(including batch size).')\n        if hasattr(self, 'states'):\n            K.set_value(self.states[0],\n                        np.zeros((input_shape[0], self.units)))\n        else:\n            self.states = [K.zeros((input_shape[0], self.units))]\n\n    def preprocess_input(self, inputs, training=None):\n        if self.implementation == 0:\n            input_shape = K.int_shape(inputs)\n            input_dim = input_shape[2]\n            timesteps = input_shape[1]\n\n            x_z = _time_distributed_dense(inputs, self.kernel_z, self.bias_z,\n                                          self.dropout, input_dim, self.units,\n                                          timesteps, training=training)\n            x_r = _time_distributed_dense(inputs, self.kernel_r, self.bias_r,\n                                          self.dropout, input_dim, self.units,\n                                          timesteps, training=training)\n            x_h = _time_distributed_dense(inputs, self.kernel_h, self.bias_h,\n                                          self.dropout, input_dim, self.units,\n                                          timesteps, training=training)\n            return K.concatenate([x_z, x_r, x_h], axis=2)\n        else:\n            return inputs\n\n    def get_constants(self, inputs, training=None):\n        constants = []\n        if self.implementation == 0 and 0 < self.dropout < 1:\n            input_shape = K.int_shape(inputs)\n            input_dim = input_shape[-1]\n            ones = K.ones_like(K.reshape(inputs[:, 0, 0], (-1, 1)))\n            ones = K.tile(ones, (1, int(input_dim)))\n\n            def dropped_inputs():\n                return K.dropout(ones, self.dropout)\n\n            dp_mask = [K.in_train_phase(dropped_inputs,\n                                        ones,\n                                        training=training) for _ in range(3)]\n            constants.append(dp_mask)\n        else:\n            constants.append([K.cast_to_floatx(1.) for _ in range(3)])\n\n        if 0 < self.recurrent_dropout < 1:\n            ones = K.ones_like(K.reshape(inputs[:, 0, 0], (-1, 1)))\n            ones = K.tile(ones, (1, self.units))\n\n            def dropped_inputs():\n                return K.dropout(ones, self.recurrent_dropout)\n            rec_dp_mask = [K.in_train_phase(dropped_inputs,\n                                            ones,\n                                            training=training) for _ in range(3)]\n            constants.append(rec_dp_mask)\n        else:\n            constants.append([K.cast_to_floatx(1.) for _ in range(3)])\n        return constants\n\n    def step(self, inputs, states):\n        h_tm1 = states[0]  # previous memory\n        dp_mask = states[1]  # dropout matrices for recurrent units\n        rec_dp_mask = states[2]\n\n        if self.implementation == 1:\n            matrix_x = K.dot(inputs * dp_mask[0], self.kernel)\n            if self.use_bias:\n                matrix_x += self.bias\n            matrix_inner = K.dot(h_tm1 * rec_dp_mask[0],\n                                 self.recurrent_kernel[:, :2 * self.units])\n\n            x_z = matrix_x[:, :self.units]\n            x_r = matrix_x[:, self.units: 2 * self.units]\n            recurrent_z = matrix_inner[:, :self.units]\n            recurrent_r = matrix_inner[:, self.units: 2 * self.units]\n\n            z = self.recurrent_activation(x_z + recurrent_z)\n            r = self.recurrent_activation(x_r + recurrent_r)\n\n            x_h = matrix_x[:, 2 * self.units:]\n            recurrent_h = K.dot(r * h_tm1 * rec_dp_mask[0],\n                            self.recurrent_kernel[:, 2 * self.units:])\n            hh = self.activation(x_h + recurrent_h)\n        else:\n            if self.implementation == 0:\n                x_z = inputs[:, :self.units]\n                x_r = inputs[:, self.units: 2 * self.units]\n                x_h = inputs[:, 2 * self.units:]\n            elif self.implementation == 2:\n                x_z = K.dot(inputs * dp_mask[0], self.kernel_z)\n                x_r = K.dot(inputs * dp_mask[1], self.kernel_r)\n                x_h = K.dot(inputs * dp_mask[2], self.kernel_h)\n                if self.use_bias:\n                    x_z += self.bias_z\n                    x_r += self.bias_r\n                    x_h += self.bias_h\n            else:\n                raise ValueError('Unknown `implementation` mode.')\n            z = self.recurrent_activation(x_z + K.dot(h_tm1 * rec_dp_mask[0],\n                                                      self.recurrent_kernel_z))\n            r = self.recurrent_activation(x_r + K.dot(h_tm1 * rec_dp_mask[1],\n                                                      self.recurrent_kernel_r))\n\n            hh = self.activation(x_h + K.dot(r * h_tm1 * rec_dp_mask[2],\n                                             self.recurrent_kernel_h))\n        h = z * h_tm1 + (1 - z) * hh\n        return h, [h]\n\n    def get_config(self):\n        config = {'units': self.units,\n                  'activation': activations.serialize(self.activation),\n                  'recurrent_activation': activations.serialize(self.recurrent_activation),\n                  'use_bias': self.use_bias,\n                  'kernel_initializer': initializers.serialize(self.kernel_initializer),\n                  'recurrent_initializer': initializers.serialize(self.recurrent_initializer),\n                  'bias_initializer': initializers.serialize(self.bias_initializer),\n                  'kernel_regularizer': regularizers.serialize(self.kernel_regularizer),\n                  'recurrent_regularizer': regularizers.serialize(self.recurrent_regularizer),\n                  'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n                  'kernel_constraint': constraints.serialize(self.kernel_constraint),\n                  'recurrent_constraint': constraints.serialize(self.recurrent_constraint),\n                  'bias_constraint': constraints.serialize(self.bias_constraint),\n                  'dropout': self.dropout,\n                  'recurrent_dropout': self.recurrent_dropout}\n        base_config = super(GRU, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\n\nclass LSTM(Recurrent):\n    \"\"\"Long-Short Term Memory unit - Hochreiter 1997.\n\n    For a step-by-step description of the algorithm, see\n    [this tutorial](http://deeplearning.net/tutorial/lstm.html).\n\n    # Arguments\n        units: Positive integer, dimensionality of the output space.\n        activation: Activation function to use\n            (see [activations](../activations.md)).\n            If you don't specify anything, no activation is applied\n            (ie. \"linear\" activation: `a(x) = x`).\n        recurrent_activation: Activation function to use\n            for the recurrent step\n            (see [activations](../activations.md)).\n        use_bias: Boolean, whether the layer uses a bias vector.\n        kernel_initializer: Initializer for the `kernel` weights matrix,\n            used for the linear transformation of the inputs.\n            (see [initializers](../initializers.md)).\n        recurrent_initializer: Initializer for the `recurrent_kernel`\n            weights matrix,\n            used for the linear transformation of the recurrent state.\n            (see [initializers](../initializers.md)).\n        bias_initializer: Initializer for the bias vector\n            (see [initializers](../initializers.md)).\n        unit_forget_bias: Boolean.\n            If True, add 1 to the bias of the forget gate at initialization.\n            Use in combination with `bias_initializer=\"zeros\"`.\n            This is recommended in [Jozefowicz et al.](http://www.jmlr.org/proceedings/papers/v37/jozefowicz15.pdf)\n        kernel_regularizer: Regularizer function applied to\n            the `kernel` weights matrix\n            (see [regularizer](../regularizers.md)).\n        recurrent_regularizer: Regularizer function applied to\n            the `recurrent_kernel` weights matrix\n            (see [regularizer](../regularizers.md)).\n        bias_regularizer: Regularizer function applied to the bias vector\n            (see [regularizer](../regularizers.md)).\n        activity_regularizer: Regularizer function applied to\n            the output of the layer (its \"activation\").\n            (see [regularizer](../regularizers.md)).\n        kernel_constraint: Constraint function applied to\n            the `kernel` weights matrix\n            (see [constraints](../constraints.md)).\n        recurrent_constraint: Constraint function applied to\n            the `recurrent_kernel` weights matrix\n            (see [constraints](../constraints.md)).\n        bias_constraint: Constraint function applied to the bias vector\n            (see [constraints](../constraints.md)).\n        dropout: Float between 0 and 1.\n            Fraction of the units to drop for\n            the linear transformation of the inputs.\n        recurrent_dropout: Float between 0 and 1.\n            Fraction of the units to drop for\n            the linear transformation of the recurrent state.\n\n    # References\n        - [Long short-term memory](http://deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf) (original 1997 paper)\n        - [Learning to forget: Continual prediction with LSTM](http://www.mitpressjournals.org/doi/pdf/10.1162/089976600300015015)\n        - [Supervised sequence labeling with recurrent neural networks](http://www.cs.toronto.edu/~graves/preprint.pdf)\n        - [A Theoretically Grounded Application of Dropout in Recurrent Neural Networks](http://arxiv.org/abs/1512.05287)\n    \"\"\"\n\n    def __init__(self, units,\n                 activation='tanh',\n                 recurrent_activation='hard_sigmoid',\n                 use_bias=True,\n                 kernel_initializer='glorot_uniform',\n                 recurrent_initializer='orthogonal',\n                 bias_initializer='zeros',\n                 unit_forget_bias=True,\n                 kernel_regularizer=None,\n                 recurrent_regularizer=None,\n                 bias_regularizer=None,\n                 activity_regularizer=None,\n                 kernel_constraint=None,\n                 recurrent_constraint=None,\n                 bias_constraint=None,\n                 dropout=0.,\n                 recurrent_dropout=0.,\n                 **kwargs):\n        super(LSTM, self).__init__(**kwargs)\n        self.units = units\n        self.activation = activations.get(activation)\n        self.recurrent_activation = activations.get(recurrent_activation)\n        self.use_bias = use_bias\n\n        self.kernel_initializer = initializers.get(kernel_initializer)\n        self.recurrent_initializer = initializers.get(recurrent_initializer)\n        self.bias_initializer = initializers.get(bias_initializer)\n        self.unit_forget_bias = unit_forget_bias\n\n        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n        self.recurrent_regularizer = regularizers.get(recurrent_regularizer)\n        self.bias_regularizer = regularizers.get(bias_regularizer)\n\n        self.kernel_constraint = constraints.get(kernel_constraint)\n        self.recurrent_constraint = constraints.get(recurrent_constraint)\n        self.bias_constraint = constraints.get(bias_constraint)\n\n        self.dropout = min(1., max(0., dropout))\n        self.recurrent_dropout = min(1., max(0., recurrent_dropout))\n\n    def build(self, input_shape):\n        self.input_spec = [InputSpec(shape=input_shape)]\n        self.input_dim = input_shape[2]\n\n        if self.stateful:\n            self.reset_states()\n        else:\n            # initial states: 2 all-zero tensors of shape (output_dim)\n            self.states = [None, None]\n\n        self.kernel = self.add_weight((self.input_dim, self.units * 4),\n                                      name='kernel',\n                                      initializer=self.kernel_initializer,\n                                      regularizer=self.kernel_regularizer,\n                                      constraint=self.kernel_constraint)\n        self.recurrent_kernel = self.add_weight(\n            (self.units, self.units * 4),\n            name='recurrent_kernel',\n            initializer=self.recurrent_initializer,\n            regularizer=self.recurrent_regularizer,\n            constraint=self.recurrent_constraint)\n\n        if self.use_bias:\n            self.bias = self.add_weight((self.units * 4,),\n                                        name='bias',\n                                        initializer=self.bias_initializer,\n                                        regularizer=self.bias_regularizer,\n                                        constraint=self.bias_constraint)\n            if self.unit_forget_bias:\n                self.bias += K.concatenate([K.zeros((self.units,)),\n                                           K.ones((self.units,)),\n                                           K.zeros((self.units * 2,))])\n        else:\n            self.bias = None\n\n        self.kernel_i = self.kernel[:, :self.units]\n        self.recurrent_kernel_i = self.recurrent_kernel[:, :self.units]\n        self.kernel_f = self.kernel[:, self.units: self.units * 2]\n        self.recurrent_kernel_f = self.recurrent_kernel[:, self.units: self.units * 2]\n        self.kernel_c = self.kernel[:, self.units * 2: self.units * 3]\n        self.recurrent_kernel_c = self.recurrent_kernel[:, self.units * 2: self.units * 3]\n        self.kernel_o = self.kernel[:, self.units * 3:]\n        self.recurrent_kernel_o = self.recurrent_kernel[:, self.units * 3:]\n\n        if self.use_bias:\n            self.bias_i = self.bias[:self.units]\n            self.bias_f = self.bias[self.units: self.units * 2]\n            self.bias_c = self.bias[self.units * 2: self.units * 3]\n            self.bias_o = self.bias[self.units * 3:]\n        else:\n            self.bias_i = None\n            self.bias_f = None\n            self.bias_c = None\n            self.bias_o = None\n\n    def reset_states(self):\n        assert self.stateful, 'Layer must be stateful.'\n        input_shape = self.input_spec[0].shape\n        if not input_shape[0]:\n            raise ValueError('If a RNN is stateful, a complete ' +\n                             'input_shape must be provided '\n                             '(including batch size).')\n        if hasattr(self, 'states'):\n            K.set_value(self.states[0],\n                        np.zeros((input_shape[0], self.units)))\n            K.set_value(self.states[1],\n                        np.zeros((input_shape[0], self.units)))\n        else:\n            self.states = [K.zeros((input_shape[0], self.units)),\n                           K.zeros((input_shape[0], self.units))]\n\n    def preprocess_input(self, inputs, training=None):\n        if self.implementation == 0:\n            input_shape = K.int_shape(inputs)\n            input_dim = input_shape[2]\n            timesteps = input_shape[1]\n\n            x_i = _time_distributed_dense(inputs, self.kernel_i, self.bias_i,\n                                          self.dropout, input_dim, self.units,\n                                          timesteps, training=training)\n            x_f = _time_distributed_dense(inputs, self.kernel_f, self.bias_f,\n                                          self.dropout, input_dim, self.units,\n                                          timesteps, training=training)\n            x_c = _time_distributed_dense(inputs, self.kernel_c, self.bias_c,\n                                          self.dropout, input_dim, self.units,\n                                          timesteps, training=training)\n            x_o = _time_distributed_dense(inputs, self.kernel_o, self.bias_o,\n                                          self.dropout, input_dim, self.units,\n                                          timesteps, training=training)\n            return K.concatenate([x_i, x_f, x_c, x_o], axis=2)\n        else:\n            return inputs\n\n    def get_constants(self, inputs, training=None):\n        constants = []\n        if self.implementation == 0 and 0 < self.dropout < 1:\n            input_shape = K.int_shape(inputs)\n            input_dim = input_shape[-1]\n            ones = K.ones_like(K.reshape(inputs[:, 0, 0], (-1, 1)))\n            ones = K.tile(ones, (1, int(input_dim)))\n\n            def dropped_inputs():\n                return K.dropout(ones, self.dropout)\n\n            dp_mask = [K.in_train_phase(dropped_inputs,\n                                        ones,\n                                        training=training) for _ in range(4)]\n            constants.append(dp_mask)\n        else:\n            constants.append([K.cast_to_floatx(1.) for _ in range(4)])\n\n        if 0 < self.recurrent_dropout < 1:\n            ones = K.ones_like(K.reshape(inputs[:, 0, 0], (-1, 1)))\n            ones = K.tile(ones, (1, self.units))\n\n            def dropped_inputs():\n                return K.dropout(ones, self.recurrent_dropout)\n            rec_dp_mask = [K.in_train_phase(dropped_inputs,\n                                            ones,\n                                            training=training) for _ in range(4)]\n            constants.append(rec_dp_mask)\n        else:\n            constants.append([K.cast_to_floatx(1.) for _ in range(4)])\n        return constants\n\n    def step(self, inputs, states):\n        h_tm1 = states[0]\n        c_tm1 = states[1]\n        dp_mask = states[2]\n        rec_dp_mask = states[3]\n\n        if self.implementation == 1:\n            z = K.dot(inputs * dp_mask[0], self.kernel)\n            z += K.dot(h_tm1 * rec_dp_mask[0], self.recurrent_kernel)\n            if self.use_bias:\n                z += self.bias\n\n            z0 = z[:, :self.units]\n            z1 = z[:, self.units: 2 * self.units]\n            z2 = z[:, 2 * self.units: 3 * self.units]\n            z3 = z[:, 3 * self.units:]\n\n            i = self.recurrent_activation(z0)\n            f = self.recurrent_activation(z1)\n            c = f * c_tm1 + i * self.activation(z2)\n            o = self.recurrent_activation(z3)\n        else:\n            if self.implementation == 0:\n                x_i = inputs[:, :self.units]\n                x_f = inputs[:, self.units: 2 * self.units]\n                x_c = inputs[:, 2 * self.units: 3 * self.units]\n                x_o = inputs[:, 3 * self.units:]\n            elif self.implementation == 2:\n                x_i = K.dot(inputs * dp_mask[0], self.kernel_i) + self.bias_i\n                x_f = K.dot(inputs * dp_mask[1], self.kernel_f) + self.bias_f\n                x_c = K.dot(inputs * dp_mask[2], self.kernel_c) + self.bias_c\n                x_o = K.dot(inputs * dp_mask[3], self.kernel_o) + self.bias_o\n            else:\n                raise ValueError('Unknown `implementation` mode.')\n\n            i = self.recurrent_activation(x_i + K.dot(h_tm1 * rec_dp_mask[0],\n                                                      self.recurrent_kernel_i))\n            f = self.recurrent_activation(x_f + K.dot(h_tm1 * rec_dp_mask[1],\n                                                      self.recurrent_kernel_f))\n            c = f * c_tm1 + i * self.activation(x_c + K.dot(h_tm1 * rec_dp_mask[2],\n                                                            self.recurrent_kernel_c))\n            o = self.recurrent_activation(x_o + K.dot(h_tm1 * rec_dp_mask[3],\n                                                      self.recurrent_kernel_o))\n        h = o * self.activation(c)\n        return h, [h, c]\n\n    def get_config(self):\n        config = {'units': self.units,\n                  'activation': activations.serialize(self.activation),\n                  'recurrent_activation': activations.serialize(self.recurrent_activation),\n                  'use_bias': self.use_bias,\n                  'kernel_initializer': initializers.serialize(self.kernel_initializer),\n                  'recurrent_initializer': initializers.serialize(self.recurrent_initializer),\n                  'bias_initializer': initializers.serialize(self.bias_initializer),\n                  'unit_forget_bias': self.unit_forget_bias,\n                  'kernel_regularizer': regularizers.serialize(kernel_regularizer),\n                  'recurrent_regularizer': regularizers.serialize(recurrent_regularizer),\n                  'bias_regularizer': regularizers.serialize(bias_regularizer),\n                  'kernel_constraint': constraints.serialize(self.kernel_constraint),\n                  'recurrent_constraint': constraints.serialize(self.recurrent_constraint),\n                  'bias_constraint': constraints.serialize(self.bias_constraint),\n                  'dropout': self.dropout,\n                  'recurrent_dropout': self.recurrent_dropout}\n        base_config = super(LSTM, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n",
          "file_after": "# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import\nimport numpy as np\n\nfrom .. import backend as K\nfrom .. import activations\nfrom .. import initializers\nfrom .. import regularizers\nfrom .. import constraints\nfrom ..engine import Layer\nfrom ..engine import InputSpec\n\n\ndef _time_distributed_dense(x, w, b=None, dropout=None,\n                            input_dim=None, output_dim=None,\n                            timesteps=None, training=None):\n    \"\"\"Apply `y . w + b` for every temporal slice y of x.\n\n    # Arguments\n        x: input tensor.\n        w: weight matrix.\n        b: optional bias vector.\n        dropout: wether to apply dropout (same dropout mask\n            for every temporal slice of the input).\n        input_dim: integer; optional dimensionality of the input.\n        output_dim: integer; optional dimensionality of the output.\n        timesteps: integer; optional number of timesteps.\n        training: training phase tensor or boolean.\n\n    # Returns\n        Output tensor.\n    \"\"\"\n    if not input_dim:\n        input_dim = K.shape(x)[2]\n    if not timesteps:\n        timesteps = K.shape(x)[1]\n    if not output_dim:\n        output_dim = K.shape(w)[1]\n\n    if dropout is not None and 0. < dropout < 1.:\n        # apply the same dropout pattern at every timestep\n        ones = K.ones_like(K.reshape(x[:, 0, :], (-1, input_dim)))\n        dropout_matrix = K.dropout(ones, dropout)\n        expanded_dropout_matrix = K.repeat(dropout_matrix, timesteps)\n        x = K.in_train_phase(x * expanded_dropout_matrix, x, training=training)\n\n    # collapse time dimension and batch dimension together\n    x = K.reshape(x, (-1, input_dim))\n    x = K.dot(x, w)\n    if b is not None:\n        x += b\n    # reshape to 3D tensor\n    if K.backend() == 'tensorflow':\n        x = K.reshape(x, K.stack([-1, timesteps, output_dim]))\n        x.set_shape([None, None, output_dim])\n    else:\n        x = K.reshape(x, (-1, timesteps, output_dim))\n    return x\n\n\nclass Recurrent(Layer):\n    \"\"\"Abstract base class for recurrent layers.\n    Do not use in a model -- it's not a valid layer!\n    Use its children classes `LSTM`, `GRU` and `SimpleRNN` instead.\n\n    All recurrent layers (`LSTM`, `GRU`, `SimpleRNN`) also\n    follow the specifications of this class and accept\n    the keyword arguments listed below.\n\n    # Example\n\n    ```python\n        # as the first layer in a Sequential model\n        model = Sequential()\n        model.add(LSTM(32, input_shape=(10, 64)))\n        # now model.output_shape == (None, 32)\n        # note: `None` is the batch dimension.\n\n        # the following is identical:\n        model = Sequential()\n        model.add(LSTM(32, input_dim=64, input_length=10))\n\n        # for subsequent layers, not need to specify the input size:\n        model.add(LSTM(16))\n    ```\n\n    # Arguments\n        weights: list of Numpy arrays to set as initial weights.\n            The list should have 3 elements, of shapes:\n            `[(input_dim, output_dim), (output_dim, output_dim), (output_dim,)]`.\n        return_sequences: Boolean. Whether to return the last output\n            in the output sequence, or the full sequence.\n        go_backwards: Boolean (default False).\n            If True, process the input sequence backwards.\n        stateful: Boolean (default False). If True, the last state\n            for each sample at index i in a batch will be used as initial\n            state for the sample of index i in the following batch.\n        unroll: Boolean (default False).\n            If True, the network will be unrolled,\n            else a symbolic loop will be used.\n            Unrolling can speed-up a RNN,\n            although it tends to be more memory-intensive.\n            Unrolling is only suitable for short sequences.\n        implementation: one of {0, 1, or 2}.\n            If set to 0, the RNN will use\n            an implementation that uses fewer, larger matrix products,\n            thus running faster on CPU but consuming more memory.\n            If set to 1, the RNN will use more matrix products,\n            but smaller ones, thus running slower\n            (may actually be faster on GPU) while consuming less memory.\n            If set to 2 (LSTM/GRU only),\n            the RNN will combine the input gate,\n            the forget gate and the output gate into a single matrix,\n            enabling more time-efficient parallelization on the GPU.\n            Note: RNN dropout must be shared for all gates,\n            resulting in a slightly reduced regularization.\n        input_dim: dimensionality of the input (integer).\n            This argument (or alternatively, the keyword argument `input_shape`)\n            is required when using this layer as the first layer in a model.\n        input_length: Length of input sequences, to be specified\n            when it is constant.\n            This argument is required if you are going to connect\n            `Flatten` then `Dense` layers upstream\n            (without it, the shape of the dense outputs cannot be computed).\n            Note that if the recurrent layer is not the first layer\n            in your model, you would need to specify the input length\n            at the level of the first layer\n            (e.g. via the `input_shape` argument)\n\n    # Input shape\n        3D tensor with shape `(batch_size, timesteps, input_dim)`.\n\n    # Output shape\n        - if `return_sequences`: 3D tensor with shape\n            `(batch_size, timesteps, units)`.\n        - else, 2D tensor with shape `(batch_size, units)`.\n\n    # Masking\n        This layer supports masking for input data with a variable number\n        of timesteps. To introduce masks to your data,\n        use an [Embedding](embeddings.md) layer with the `mask_zero` parameter\n        set to `True`.\n\n    # Note on using statefulness in RNNs\n        You can set RNN layers to be 'stateful', which means that the states\n        computed for the samples in one batch will be reused as initial states\n        for the samples in the next batch. This assumes a one-to-one mapping\n        between samples in different successive batches.\n\n        To enable statefulness:\n            - specify `stateful=True` in the layer constructor.\n            - specify a fixed batch size for your model, by passing\n                if sequential model:\n                  `batch_input_shape=(...)` to the first layer in your model.\n                else for functional model with 1 or more Input layers:\n                  `batch_shape=(...)` to all the first layers in your model.\n                This is the expected shape of your inputs\n                *including the batch size*.\n                It should be a tuple of integers, e.g. `(32, 10, 100)`.\n            - specify `shuffle=False` when calling fit().\n\n        To reset the states of your model, call `.reset_states()` on either\n        a specific layer, or on your entire model.\n    \"\"\"\n\n    def __init__(self, return_sequences=False,\n                 go_backwards=False,\n                 stateful=False,\n                 unroll=False,\n                 implementation=0,\n                 **kwargs):\n        super(Recurrent, self).__init__(**kwargs)\n        self.return_sequences = return_sequences\n        self.go_backwards = go_backwards\n        self.stateful = stateful\n        self.unroll = unroll\n        self.implementation = 0\n        self.supports_masking = True\n        self.input_spec = InputSpec(ndim=3)\n        self.dropout = 0\n        self.recurrent_dropout = 0\n\n    def compute_output_shape(self, input_shape):\n        if self.return_sequences:\n            return (input_shape[0], input_shape[1], self.units)\n        else:\n            return (input_shape[0], self.units)\n\n    def compute_mask(self, inputs, mask):\n        if self.return_sequences:\n            return mask\n        else:\n            return None\n\n    def step(self, inputs, states):\n        raise NotImplementedError\n\n    def get_constants(self, inputs, training=None):\n        return []\n\n    def get_initial_states(self, inputs):\n        # build an all-zero tensor of shape (samples, output_dim)\n        initial_state = K.zeros_like(inputs)  # (samples, timesteps, input_dim)\n        initial_state = K.sum(initial_state, axis=(1, 2))  # (samples,)\n        initial_state = K.expand_dims(initial_state)  # (samples, 1)\n        initial_state = K.tile(initial_state, [1, self.units])  # (samples, output_dim)\n        initial_states = [initial_state for _ in range(len(self.states))]\n        return initial_states\n\n    def preprocess_input(self, inputs, training=None):\n        return inputs\n\n    def call(self, inputs, mask=None, training=None):\n        # input shape: (nbias_samples, time (padded with zeros), input_dim)\n        # note that the .build() method of subclasses MUST define\n        # self.input_spec with a complete input shape.\n        input_shape = K.int_shape(inputs)\n        if self.unroll and input_shape[1] is None:\n            raise ValueError('Cannot unroll a RNN if the '\n                             'time dimension is undefined. \\n'\n                             '- If using a Sequential model, '\n                             'specify the time dimension by passing '\n                             'an `input_shape` or `batch_input_shape` '\n                             'argument to your first layer. If your '\n                             'first layer is an Embedding, you can '\n                             'also use the `input_length` argument.\\n'\n                             '- If using the functional API, specify '\n                             'the time dimension by passing a `shape` '\n                             'or `batch_shape` argument to your Input layer.')\n        if self.stateful:\n            initial_states = self.states\n        else:\n            initial_states = self.get_initial_states(inputs)\n        constants = self.get_constants(inputs, training=None)\n        preprocessed_input = self.preprocess_input(inputs, training=None)\n        last_output, outputs, states = K.rnn(self.step,\n                                             preprocessed_input,\n                                             initial_states,\n                                             go_backwards=self.go_backwards,\n                                             mask=mask,\n                                             constants=constants,\n                                             unroll=self.unroll,\n                                             input_length=input_shape[1])\n        if self.stateful:\n            updates = []\n            for i in range(len(states)):\n                updates.append((self.states[i], states[i]))\n            self.add_update(updates, inputs)\n\n        # Properly set learning phase\n        if 0 < self.dropout + self.recurrent_dropout:\n            last_output._uses_learning_phase = True\n            outputs._uses_learning_phase = True\n\n        if self.return_sequences:\n            return outputs\n        else:\n            return last_output\n\n    def get_config(self):\n        config = {'return_sequences': self.return_sequences,\n                  'go_backwards': self.go_backwards,\n                  'stateful': self.stateful,\n                  'unroll': self.unroll,\n                  'implementation': self.implementation}\n        base_config = super(Recurrent, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\n\nclass SimpleRNN(Recurrent):\n    \"\"\"Fully-connected RNN where the output is to be fed back to input.\n\n    # Arguments\n        units: Positive integer, dimensionality of the output space.\n        activation: Activation function to use\n            (see [activations](../activations.md)).\n            If you don't specify anything, no activation is applied\n            (ie. \"linear\" activation: `a(x) = x`).\n        use_bias: Boolean, whether the layer uses a bias vector.\n        kernel_initializer: Initializer for the `kernel` weights matrix,\n            used for the linear transformation of the inputs.\n            (see [initializers](../initializers.md)).\n        recurrent_initializer: Initializer for the `recurrent_kernel`\n            weights matrix,\n            used for the linear transformation of the recurrent state.\n            (see [initializers](../initializers.md)).\n        bias_initializer: Initializer for the bias vector\n            (see [initializers](../initializers.md)).\n        kernel_regularizer: Regularizer function applied to\n            the `kernel` weights matrix\n            (see [regularizer](../regularizers.md)).\n        recurrent_regularizer: Regularizer function applied to\n            the `recurrent_kernel` weights matrix\n            (see [regularizer](../regularizers.md)).\n        bias_regularizer: Regularizer function applied to the bias vector\n            (see [regularizer](../regularizers.md)).\n        activity_regularizer: Regularizer function applied to\n            the output of the layer (its \"activation\").\n            (see [regularizer](../regularizers.md)).\n        kernel_constraint: Constraint function applied to\n            the `kernel` weights matrix\n            (see [constraints](../constraints.md)).\n        recurrent_constraint: Constraint function applied to\n            the `recurrent_kernel` weights matrix\n            (see [constraints](../constraints.md)).\n        bias_constraint: Constraint function applied to the bias vector\n            (see [constraints](../constraints.md)).\n        dropout: Float between 0 and 1.\n            Fraction of the units to drop for\n            the linear transformation of the inputs.\n        recurrent_dropout: Float between 0 and 1.\n            Fraction of the units to drop for\n            the linear transformation of the recurrent state.\n\n    # References\n        - [A Theoretically Grounded Application of Dropout in Recurrent Neural Networks](http://arxiv.org/abs/1512.05287)\n    \"\"\"\n\n    def __init__(self, units,\n                 activation='tanh',\n                 use_bias=True,\n                 kernel_initializer='glorot_uniform',\n                 recurrent_initializer='orthogonal',\n                 bias_initializer='zeros',\n                 kernel_regularizer=None,\n                 recurrent_regularizer=None,\n                 bias_regularizer=None,\n                 activity_regularizer=None,\n                 kernel_constraint=None,\n                 recurrent_constraint=None,\n                 bias_constraint=None,\n                 dropout=0.,\n                 recurrent_dropout=0.,\n                 **kwargs):\n        super(SimpleRNN, self).__init__(**kwargs)\n        self.units = units\n        self.activation = activations.get(activation)\n        self.use_bias = use_bias\n\n        self.kernel_initializer = initializers.get(kernel_initializer)\n        self.recurrent_initializer = initializers.get(recurrent_initializer)\n        self.bias_initializer = initializers.get(bias_initializer)\n\n        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n        self.recurrent_regularizer = regularizers.get(recurrent_regularizer)\n        self.bias_regularizer = regularizers.get(bias_regularizer)\n\n        self.kernel_constraint = constraints.get(kernel_constraint)\n        self.recurrent_constraint = constraints.get(recurrent_constraint)\n        self.bias_constraint = constraints.get(bias_constraint)\n\n        self.dropout = min(1., max(0., dropout))\n        self.recurrent_dropout = min(1., max(0., recurrent_dropout))\n\n    def build(self, input_shape):\n        # TODO: handle variable-length seqs in input_spec\n        self.input_spec = InputSpec(shape=input_shape)\n        if self.stateful:\n            self.reset_states()\n        else:\n            # Initial states: all-zero tensor of shape (output_dim).\n            self.states = [None]\n        input_dim = input_shape[2]\n        self.input_dim = input_dim\n\n        self.kernel = self.add_weight((input_dim, self.units),\n                                      name='kernel',\n                                      initializer=self.kernel_initializer,\n                                      regularizer=self.kernel_regularizer,\n                                      constraint=self.kernel_constraint)\n        self.recurrent_kernel = self.add_weight(\n            (self.units, self.units),\n            name='recurrent_kernel',\n            initializer=self.recurrent_initializer,\n            regularizer=self.recurrent_regularizer,\n            constraint=self.recurrent_constraint)\n        if self.use_bias:\n            self.bias = self.add_weight((self.units,),\n                                        name='bias',\n                                        initializer=self.bias_initializer,\n                                        regularizer=self.bias_regularizer,\n                                        constraint=self.bias_constraint)\n        else:\n            self.bias = None\n        self.built = True\n\n    def reset_states(self):\n        if not self.stateful:\n            raise AttributeError('Layer must be stateful.')\n        input_shape = self.input_spec.shape\n        if not input_shape[0]:\n            raise ValueError('If a RNN is stateful, it needs to know '\n                             'its batch size. Specify the batch size '\n                             'of your input tensors: \\n'\n                             '- If using a Sequential model, '\n                             'specify the batch size by passing '\n                             'a `batch_input_shape` '\n                             'argument to your first layer.\\n'\n                             '- If using the functional API, specify '\n                             'the time dimension by passing a '\n                             '`batch_shape` argument to your Input layer.')\n        if hasattr(self, 'states'):\n            K.set_value(self.states[0],\n                        np.zeros((input_shape[0], self.units)))\n        else:\n            self.states = [K.zeros((input_shape[0], self.units))]\n\n    def preprocess_input(self, inputs, training=None):\n        if self.implementation > 0:\n            return inputs\n        else:\n            input_shape = K.int_shape(inputs)\n            input_dim = input_shape[2]\n            timesteps = input_shape[1]\n            return _time_distributed_dense(inputs,\n                                           self.kernel,\n                                           self.bias,\n                                           self.dropout,\n                                           input_dim,\n                                           self.units,\n                                           timesteps,\n                                           training=training)\n\n    def step(self, inputs, states):\n        if self.implementation == 0:\n            h = inputs\n        else:\n            if 0 < self.dropout < 1:\n                h = K.dot(inputs * states[1], self.kernel)\n            else:\n                h = K.dot(inputs, self.kernel)\n            if self.bias is not None:\n                h += self.bias\n\n        prev_output = states[0]\n        if 0 < self.recurrent_dropout < 1:\n            prev_output *= states[2]\n        output = h + K.dot(prev_output, self.recurrent_kernel)\n        if self.activation is not None:\n            output = self.activation(output)\n\n        # Properly set learning phase on output tensor.\n        if 0 < self.dropout + self.recurrent_dropout:\n            output._uses_learning_phase = True\n        return output, [output]\n\n    def get_constants(self, inputs, training=None):\n        constants = []\n        if self.implementation == 0 and 0 < self.dropout < 1:\n            input_shape = K.int_shape(inputs)\n            input_dim = input_shape[-1]\n            ones = K.ones_like(K.reshape(inputs[:, 0, 0], (-1, 1)))\n            ones = K.tile(ones, (1, int(input_dim)))\n\n            def dropped_inputs():\n                return K.dropout(ones, self.dropout)\n\n            dp_mask = K.in_train_phase(dropped_inputs,\n                                       ones,\n                                       training=training)\n            constants.append(dp_mask)\n        else:\n            constants.append(K.cast_to_floatx(1.))\n\n        if 0 < self.recurrent_dropout < 1:\n            ones = K.ones_like(K.reshape(inputs[:, 0, 0], (-1, 1)))\n            ones = K.tile(ones, (1, self.units))\n\n            def dropped_inputs():\n                return K.dropout(ones, self.recurrent_dropout)\n            rec_dp_mask = K.in_train_phase(dropped_inputs,\n                                           ones,\n                                           training=training)\n            constants.append(rec_dp_mask)\n        else:\n            constants.append(K.cast_to_floatx(1.))\n        return constants\n\n    def get_config(self):\n        config = {'units': self.units,\n                  'activation': activations.serialize(self.activation),\n                  'use_bias': self.use_bias,\n                  'kernel_initializer': initializers.serialize(self.kernel_initializer),\n                  'recurrent_initializer': initializers.serialize(self.recurrent_initializer),\n                  'bias_initializer': initializers.serialize(self.bias_initializer),\n                  'kernel_regularizer': regularizers.serialize(self.kernel_regularizer),\n                  'recurrent_regularizer': regularizers.serialize(self.recurrent_regularizer),\n                  'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n                  'kernel_constraint': constraints.serialize(self.kernel_constraint),\n                  'recurrent_constraint': constraints.serialize(self.recurrent_constraint),\n                  'bias_constraint': constraints.serialize(self.bias_constraint),\n                  'dropout': self.dropout,\n                  'recurrent_dropout': self.recurrent_dropout}\n        base_config = super(SimpleRNN, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\n\nclass GRU(Recurrent):\n    \"\"\"Gated Recurrent Unit - Cho et al. 2014.\n\n    # Arguments\n        units: Positive integer, dimensionality of the output space.\n        activation: Activation function to use\n            (see [activations](../activations.md)).\n            If you don't specify anything, no activation is applied\n            (ie. \"linear\" activation: `a(x) = x`).\n        recurrent_activation: Activation function to use\n            for the recurrent step\n            (see [activations](../activations.md)).\n        use_bias: Boolean, whether the layer uses a bias vector.\n        kernel_initializer: Initializer for the `kernel` weights matrix,\n            used for the linear transformation of the inputs.\n            (see [initializers](../initializers.md)).\n        recurrent_initializer: Initializer for the `recurrent_kernel`\n            weights matrix,\n            used for the linear transformation of the recurrent state.\n            (see [initializers](../initializers.md)).\n        bias_initializer: Initializer for the bias vector\n            (see [initializers](../initializers.md)).\n        kernel_regularizer: Regularizer function applied to\n            the `kernel` weights matrix\n            (see [regularizer](../regularizers.md)).\n        recurrent_regularizer: Regularizer function applied to\n            the `recurrent_kernel` weights matrix\n            (see [regularizer](../regularizers.md)).\n        bias_regularizer: Regularizer function applied to the bias vector\n            (see [regularizer](../regularizers.md)).\n        activity_regularizer: Regularizer function applied to\n            the output of the layer (its \"activation\").\n            (see [regularizer](../regularizers.md)).\n        kernel_constraint: Constraint function applied to\n            the `kernel` weights matrix\n            (see [constraints](../constraints.md)).\n        recurrent_constraint: Constraint function applied to\n            the `recurrent_kernel` weights matrix\n            (see [constraints](../constraints.md)).\n        bias_constraint: Constraint function applied to the bias vector\n            (see [constraints](../constraints.md)).\n        dropout: Float between 0 and 1.\n            Fraction of the units to drop for\n            the linear transformation of the inputs.\n        recurrent_dropout: Float between 0 and 1.\n            Fraction of the units to drop for\n            the linear transformation of the recurrent state.\n\n    # References\n        - [On the Properties of Neural Machine Translation: Encoder-Decoder Approaches](https://arxiv.org/abs/1409.1259)\n        - [Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling](http://arxiv.org/abs/1412.3555v1)\n        - [A Theoretically Grounded Application of Dropout in Recurrent Neural Networks](http://arxiv.org/abs/1512.05287)\n    \"\"\"\n\n    def __init__(self, units,\n                 activation='tanh',\n                 recurrent_activation='hard_sigmoid',\n                 use_bias=True,\n                 kernel_initializer='glorot_uniform',\n                 recurrent_initializer='orthogonal',\n                 bias_initializer='zeros',\n                 kernel_regularizer=None,\n                 recurrent_regularizer=None,\n                 bias_regularizer=None,\n                 activity_regularizer=None,\n                 kernel_constraint=None,\n                 recurrent_constraint=None,\n                 bias_constraint=None,\n                 dropout=0.,\n                 recurrent_dropout=0.,\n                 **kwargs):\n        super(GRU, self).__init__(**kwargs)\n        self.units = units\n        self.activation = activations.get(activation)\n        self.recurrent_activation = activations.get(recurrent_activation)\n        self.use_bias = use_bias\n\n        self.kernel_initializer = initializers.get(kernel_initializer)\n        self.recurrent_initializer = initializers.get(recurrent_initializer)\n        self.bias_initializer = initializers.get(bias_initializer)\n\n        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n        self.recurrent_regularizer = regularizers.get(recurrent_regularizer)\n        self.bias_regularizer = regularizers.get(bias_regularizer)\n\n        self.kernel_constraint = constraints.get(kernel_constraint)\n        self.recurrent_constraint = constraints.get(recurrent_constraint)\n        self.bias_constraint = constraints.get(bias_constraint)\n\n        self.dropout = min(1., max(0., dropout))\n        self.recurrent_dropout = min(1., max(0., recurrent_dropout))\n\n    def build(self, input_shape):\n        # TODO: handle variable-length sequences in input spec.\n        self.input_spec = InputSpec(shape=input_shape)\n        self.input_dim = input_shape[2]\n\n        if self.stateful:\n            self.reset_states()\n        else:\n            # initial states: all-zero tensor of shape (output_dim)\n            self.states = [None]\n\n        self.kernel = self.add_weight((self.input_dim, self.units * 3),\n                                      name='kernel',\n                                      initializer=self.kernel_initializer,\n                                      regularizer=self.kernel_regularizer,\n                                      constraint=self.kernel_constraint)\n        self.recurrent_kernel = self.add_weight(\n            (self.units, self.units * 3),\n            name='recurrent_kernel',\n            initializer=self.recurrent_initializer,\n            regularizer=self.recurrent_regularizer,\n            constraint=self.recurrent_constraint)\n\n        if self.use_bias:\n            self.bias = self.add_weight((self.units * 3,),\n                                        name='bias',\n                                        initializer='zero',\n                                        regularizer=self.bias_regularizer,\n                                        constraint=self.bias_constraint)\n        else:\n            self.bias = None\n\n        self.kernel_z = self.kernel[:, :self.units]\n        self.recurrent_kernel_z = self.recurrent_kernel[:, :self.units]\n        self.kernel_r = self.kernel[:, self.units: self.units * 2]\n        self.recurrent_kernel_r = self.recurrent_kernel[:,\n                                                        self.units:\n                                                        self.units * 2]\n        self.kernel_h = self.kernel[:, self.units * 2:]\n        self.recurrent_kernel_h = self.recurrent_kernel[:, self.units * 2:]\n\n        if self.use_bias:\n            self.bias_z = self.bias[:self.units]\n            self.bias_r = self.bias[self.units: self.units * 2]\n            self.bias_h = self.bias[self.units * 2:]\n        else:\n            self.bias_z = None\n            self.bias_r = None\n            self.bias_h = None\n        self.built = True\n\n    def reset_states(self):\n        if not self.stateful:\n            raise RuntimeError('Layer must be stateful.')\n        input_shape = self.input_spec.shape\n        if not input_shape[0]:\n            raise ValueError('If a RNN is stateful, a complete '\n                             'input_shape must be provided '\n                             '(including batch size).')\n        if hasattr(self, 'states'):\n            K.set_value(self.states[0],\n                        np.zeros((input_shape[0], self.units)))\n        else:\n            self.states = [K.zeros((input_shape[0], self.units))]\n\n    def preprocess_input(self, inputs, training=None):\n        if self.implementation == 0:\n            input_shape = K.int_shape(inputs)\n            input_dim = input_shape[2]\n            timesteps = input_shape[1]\n\n            x_z = _time_distributed_dense(inputs, self.kernel_z, self.bias_z,\n                                          self.dropout, input_dim, self.units,\n                                          timesteps, training=training)\n            x_r = _time_distributed_dense(inputs, self.kernel_r, self.bias_r,\n                                          self.dropout, input_dim, self.units,\n                                          timesteps, training=training)\n            x_h = _time_distributed_dense(inputs, self.kernel_h, self.bias_h,\n                                          self.dropout, input_dim, self.units,\n                                          timesteps, training=training)\n            return K.concatenate([x_z, x_r, x_h], axis=2)\n        else:\n            return inputs\n\n    def get_constants(self, inputs, training=None):\n        constants = []\n        if self.implementation == 0 and 0 < self.dropout < 1:\n            input_shape = K.int_shape(inputs)\n            input_dim = input_shape[-1]\n            ones = K.ones_like(K.reshape(inputs[:, 0, 0], (-1, 1)))\n            ones = K.tile(ones, (1, int(input_dim)))\n\n            def dropped_inputs():\n                return K.dropout(ones, self.dropout)\n\n            dp_mask = [K.in_train_phase(dropped_inputs,\n                                        ones,\n                                        training=training) for _ in range(3)]\n            constants.append(dp_mask)\n        else:\n            constants.append([K.cast_to_floatx(1.) for _ in range(3)])\n\n        if 0 < self.recurrent_dropout < 1:\n            ones = K.ones_like(K.reshape(inputs[:, 0, 0], (-1, 1)))\n            ones = K.tile(ones, (1, self.units))\n\n            def dropped_inputs():\n                return K.dropout(ones, self.recurrent_dropout)\n            rec_dp_mask = [K.in_train_phase(dropped_inputs,\n                                            ones,\n                                            training=training) for _ in range(3)]\n            constants.append(rec_dp_mask)\n        else:\n            constants.append([K.cast_to_floatx(1.) for _ in range(3)])\n        return constants\n\n    def step(self, inputs, states):\n        h_tm1 = states[0]  # previous memory\n        dp_mask = states[1]  # dropout matrices for recurrent units\n        rec_dp_mask = states[2]\n\n        if self.implementation == 1:\n            matrix_x = K.dot(inputs * dp_mask[0], self.kernel)\n            if self.use_bias:\n                matrix_x += self.bias\n            matrix_inner = K.dot(h_tm1 * rec_dp_mask[0],\n                                 self.recurrent_kernel[:, :2 * self.units])\n\n            x_z = matrix_x[:, :self.units]\n            x_r = matrix_x[:, self.units: 2 * self.units]\n            recurrent_z = matrix_inner[:, :self.units]\n            recurrent_r = matrix_inner[:, self.units: 2 * self.units]\n\n            z = self.recurrent_activation(x_z + recurrent_z)\n            r = self.recurrent_activation(x_r + recurrent_r)\n\n            x_h = matrix_x[:, 2 * self.units:]\n            recurrent_h = K.dot(r * h_tm1 * rec_dp_mask[0],\n                                self.recurrent_kernel[:, 2 * self.units:])\n            hh = self.activation(x_h + recurrent_h)\n        else:\n            if self.implementation == 0:\n                x_z = inputs[:, :self.units]\n                x_r = inputs[:, self.units: 2 * self.units]\n                x_h = inputs[:, 2 * self.units:]\n            elif self.implementation == 2:\n                x_z = K.dot(inputs * dp_mask[0], self.kernel_z)\n                x_r = K.dot(inputs * dp_mask[1], self.kernel_r)\n                x_h = K.dot(inputs * dp_mask[2], self.kernel_h)\n                if self.use_bias:\n                    x_z += self.bias_z\n                    x_r += self.bias_r\n                    x_h += self.bias_h\n            else:\n                raise ValueError('Unknown `implementation` mode.')\n            z = self.recurrent_activation(x_z + K.dot(h_tm1 * rec_dp_mask[0],\n                                                      self.recurrent_kernel_z))\n            r = self.recurrent_activation(x_r + K.dot(h_tm1 * rec_dp_mask[1],\n                                                      self.recurrent_kernel_r))\n\n            hh = self.activation(x_h + K.dot(r * h_tm1 * rec_dp_mask[2],\n                                             self.recurrent_kernel_h))\n        h = z * h_tm1 + (1 - z) * hh\n        if 0 < self.dropout + self.recurrent_dropout:\n            h._uses_learning_phase = True\n        return h, [h]\n\n    def get_config(self):\n        config = {'units': self.units,\n                  'activation': activations.serialize(self.activation),\n                  'recurrent_activation': activations.serialize(self.recurrent_activation),\n                  'use_bias': self.use_bias,\n                  'kernel_initializer': initializers.serialize(self.kernel_initializer),\n                  'recurrent_initializer': initializers.serialize(self.recurrent_initializer),\n                  'bias_initializer': initializers.serialize(self.bias_initializer),\n                  'kernel_regularizer': regularizers.serialize(self.kernel_regularizer),\n                  'recurrent_regularizer': regularizers.serialize(self.recurrent_regularizer),\n                  'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n                  'kernel_constraint': constraints.serialize(self.kernel_constraint),\n                  'recurrent_constraint': constraints.serialize(self.recurrent_constraint),\n                  'bias_constraint': constraints.serialize(self.bias_constraint),\n                  'dropout': self.dropout,\n                  'recurrent_dropout': self.recurrent_dropout}\n        base_config = super(GRU, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\n\nclass LSTM(Recurrent):\n    \"\"\"Long-Short Term Memory unit - Hochreiter 1997.\n\n    For a step-by-step description of the algorithm, see\n    [this tutorial](http://deeplearning.net/tutorial/lstm.html).\n\n    # Arguments\n        units: Positive integer, dimensionality of the output space.\n        activation: Activation function to use\n            (see [activations](../activations.md)).\n            If you don't specify anything, no activation is applied\n            (ie. \"linear\" activation: `a(x) = x`).\n        recurrent_activation: Activation function to use\n            for the recurrent step\n            (see [activations](../activations.md)).\n        use_bias: Boolean, whether the layer uses a bias vector.\n        kernel_initializer: Initializer for the `kernel` weights matrix,\n            used for the linear transformation of the inputs.\n            (see [initializers](../initializers.md)).\n        recurrent_initializer: Initializer for the `recurrent_kernel`\n            weights matrix,\n            used for the linear transformation of the recurrent state.\n            (see [initializers](../initializers.md)).\n        bias_initializer: Initializer for the bias vector\n            (see [initializers](../initializers.md)).\n        unit_forget_bias: Boolean.\n            If True, add 1 to the bias of the forget gate at initialization.\n            Use in combination with `bias_initializer=\"zeros\"`.\n            This is recommended in [Jozefowicz et al.](http://www.jmlr.org/proceedings/papers/v37/jozefowicz15.pdf)\n        kernel_regularizer: Regularizer function applied to\n            the `kernel` weights matrix\n            (see [regularizer](../regularizers.md)).\n        recurrent_regularizer: Regularizer function applied to\n            the `recurrent_kernel` weights matrix\n            (see [regularizer](../regularizers.md)).\n        bias_regularizer: Regularizer function applied to the bias vector\n            (see [regularizer](../regularizers.md)).\n        activity_regularizer: Regularizer function applied to\n            the output of the layer (its \"activation\").\n            (see [regularizer](../regularizers.md)).\n        kernel_constraint: Constraint function applied to\n            the `kernel` weights matrix\n            (see [constraints](../constraints.md)).\n        recurrent_constraint: Constraint function applied to\n            the `recurrent_kernel` weights matrix\n            (see [constraints](../constraints.md)).\n        bias_constraint: Constraint function applied to the bias vector\n            (see [constraints](../constraints.md)).\n        dropout: Float between 0 and 1.\n            Fraction of the units to drop for\n            the linear transformation of the inputs.\n        recurrent_dropout: Float between 0 and 1.\n            Fraction of the units to drop for\n            the linear transformation of the recurrent state.\n\n    # References\n        - [Long short-term memory](http://deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf) (original 1997 paper)\n        - [Learning to forget: Continual prediction with LSTM](http://www.mitpressjournals.org/doi/pdf/10.1162/089976600300015015)\n        - [Supervised sequence labeling with recurrent neural networks](http://www.cs.toronto.edu/~graves/preprint.pdf)\n        - [A Theoretically Grounded Application of Dropout in Recurrent Neural Networks](http://arxiv.org/abs/1512.05287)\n    \"\"\"\n\n    def __init__(self, units,\n                 activation='tanh',\n                 recurrent_activation='hard_sigmoid',\n                 use_bias=True,\n                 kernel_initializer='glorot_uniform',\n                 recurrent_initializer='orthogonal',\n                 bias_initializer='zeros',\n                 unit_forget_bias=True,\n                 kernel_regularizer=None,\n                 recurrent_regularizer=None,\n                 bias_regularizer=None,\n                 activity_regularizer=None,\n                 kernel_constraint=None,\n                 recurrent_constraint=None,\n                 bias_constraint=None,\n                 dropout=0.,\n                 recurrent_dropout=0.,\n                 **kwargs):\n        super(LSTM, self).__init__(**kwargs)\n        self.units = units\n        self.activation = activations.get(activation)\n        self.recurrent_activation = activations.get(recurrent_activation)\n        self.use_bias = use_bias\n\n        self.kernel_initializer = initializers.get(kernel_initializer)\n        self.recurrent_initializer = initializers.get(recurrent_initializer)\n        self.bias_initializer = initializers.get(bias_initializer)\n        self.unit_forget_bias = unit_forget_bias\n\n        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n        self.recurrent_regularizer = regularizers.get(recurrent_regularizer)\n        self.bias_regularizer = regularizers.get(bias_regularizer)\n\n        self.kernel_constraint = constraints.get(kernel_constraint)\n        self.recurrent_constraint = constraints.get(recurrent_constraint)\n        self.bias_constraint = constraints.get(bias_constraint)\n\n        self.dropout = min(1., max(0., dropout))\n        self.recurrent_dropout = min(1., max(0., recurrent_dropout))\n\n    def build(self, input_shape):\n        self.input_spec = InputSpec(shape=input_shape)\n        self.input_dim = input_shape[2]\n\n        if self.stateful:\n            self.reset_states()\n        else:\n            # initial states: 2 all-zero tensors of shape (output_dim)\n            self.states = [None, None]\n\n        self.kernel = self.add_weight((self.input_dim, self.units * 4),\n                                      name='kernel',\n                                      initializer=self.kernel_initializer,\n                                      regularizer=self.kernel_regularizer,\n                                      constraint=self.kernel_constraint)\n        self.recurrent_kernel = self.add_weight(\n            (self.units, self.units * 4),\n            name='recurrent_kernel',\n            initializer=self.recurrent_initializer,\n            regularizer=self.recurrent_regularizer,\n            constraint=self.recurrent_constraint)\n\n        if self.use_bias:\n            self.bias = self.add_weight((self.units * 4,),\n                                        name='bias',\n                                        initializer=self.bias_initializer,\n                                        regularizer=self.bias_regularizer,\n                                        constraint=self.bias_constraint)\n            if self.unit_forget_bias:\n                self.bias += K.concatenate([K.zeros((self.units,)),\n                                           K.ones((self.units,)),\n                                           K.zeros((self.units * 2,))])\n        else:\n            self.bias = None\n\n        self.kernel_i = self.kernel[:, :self.units]\n        self.recurrent_kernel_i = self.recurrent_kernel[:, :self.units]\n        self.kernel_f = self.kernel[:, self.units: self.units * 2]\n        self.recurrent_kernel_f = self.recurrent_kernel[:, self.units: self.units * 2]\n        self.kernel_c = self.kernel[:, self.units * 2: self.units * 3]\n        self.recurrent_kernel_c = self.recurrent_kernel[:, self.units * 2: self.units * 3]\n        self.kernel_o = self.kernel[:, self.units * 3:]\n        self.recurrent_kernel_o = self.recurrent_kernel[:, self.units * 3:]\n\n        if self.use_bias:\n            self.bias_i = self.bias[:self.units]\n            self.bias_f = self.bias[self.units: self.units * 2]\n            self.bias_c = self.bias[self.units * 2: self.units * 3]\n            self.bias_o = self.bias[self.units * 3:]\n        else:\n            self.bias_i = None\n            self.bias_f = None\n            self.bias_c = None\n            self.bias_o = None\n        self.built = True\n\n    def reset_states(self):\n        assert self.stateful, 'Layer must be stateful.'\n        input_shape = self.input_spec.shape\n        if not input_shape[0]:\n            raise ValueError('If a RNN is stateful, a complete ' +\n                             'input_shape must be provided '\n                             '(including batch size).')\n        if hasattr(self, 'states'):\n            K.set_value(self.states[0],\n                        np.zeros((input_shape[0], self.units)))\n            K.set_value(self.states[1],\n                        np.zeros((input_shape[0], self.units)))\n        else:\n            self.states = [K.zeros((input_shape[0], self.units)),\n                           K.zeros((input_shape[0], self.units))]\n\n    def preprocess_input(self, inputs, training=None):\n        if self.implementation == 0:\n            input_shape = K.int_shape(inputs)\n            input_dim = input_shape[2]\n            timesteps = input_shape[1]\n\n            x_i = _time_distributed_dense(inputs, self.kernel_i, self.bias_i,\n                                          self.dropout, input_dim, self.units,\n                                          timesteps, training=training)\n            x_f = _time_distributed_dense(inputs, self.kernel_f, self.bias_f,\n                                          self.dropout, input_dim, self.units,\n                                          timesteps, training=training)\n            x_c = _time_distributed_dense(inputs, self.kernel_c, self.bias_c,\n                                          self.dropout, input_dim, self.units,\n                                          timesteps, training=training)\n            x_o = _time_distributed_dense(inputs, self.kernel_o, self.bias_o,\n                                          self.dropout, input_dim, self.units,\n                                          timesteps, training=training)\n            return K.concatenate([x_i, x_f, x_c, x_o], axis=2)\n        else:\n            return inputs\n\n    def get_constants(self, inputs, training=None):\n        constants = []\n        if self.implementation == 0 and 0 < self.dropout < 1:\n            input_shape = K.int_shape(inputs)\n            input_dim = input_shape[-1]\n            ones = K.ones_like(K.reshape(inputs[:, 0, 0], (-1, 1)))\n            ones = K.tile(ones, (1, int(input_dim)))\n\n            def dropped_inputs():\n                return K.dropout(ones, self.dropout)\n\n            dp_mask = [K.in_train_phase(dropped_inputs,\n                                        ones,\n                                        training=training) for _ in range(4)]\n            constants.append(dp_mask)\n        else:\n            constants.append([K.cast_to_floatx(1.) for _ in range(4)])\n\n        if 0 < self.recurrent_dropout < 1:\n            ones = K.ones_like(K.reshape(inputs[:, 0, 0], (-1, 1)))\n            ones = K.tile(ones, (1, self.units))\n\n            def dropped_inputs():\n                return K.dropout(ones, self.recurrent_dropout)\n            rec_dp_mask = [K.in_train_phase(dropped_inputs,\n                                            ones,\n                                            training=training) for _ in range(4)]\n            constants.append(rec_dp_mask)\n        else:\n            constants.append([K.cast_to_floatx(1.) for _ in range(4)])\n        return constants\n\n    def step(self, inputs, states):\n        h_tm1 = states[0]\n        c_tm1 = states[1]\n        dp_mask = states[2]\n        rec_dp_mask = states[3]\n\n        if self.implementation == 1:\n            z = K.dot(inputs * dp_mask[0], self.kernel)\n            z += K.dot(h_tm1 * rec_dp_mask[0], self.recurrent_kernel)\n            if self.use_bias:\n                z += self.bias\n\n            z0 = z[:, :self.units]\n            z1 = z[:, self.units: 2 * self.units]\n            z2 = z[:, 2 * self.units: 3 * self.units]\n            z3 = z[:, 3 * self.units:]\n\n            i = self.recurrent_activation(z0)\n            f = self.recurrent_activation(z1)\n            c = f * c_tm1 + i * self.activation(z2)\n            o = self.recurrent_activation(z3)\n        else:\n            if self.implementation == 0:\n                x_i = inputs[:, :self.units]\n                x_f = inputs[:, self.units: 2 * self.units]\n                x_c = inputs[:, 2 * self.units: 3 * self.units]\n                x_o = inputs[:, 3 * self.units:]\n            elif self.implementation == 2:\n                x_i = K.dot(inputs * dp_mask[0], self.kernel_i) + self.bias_i\n                x_f = K.dot(inputs * dp_mask[1], self.kernel_f) + self.bias_f\n                x_c = K.dot(inputs * dp_mask[2], self.kernel_c) + self.bias_c\n                x_o = K.dot(inputs * dp_mask[3], self.kernel_o) + self.bias_o\n            else:\n                raise ValueError('Unknown `implementation` mode.')\n\n            i = self.recurrent_activation(x_i + K.dot(h_tm1 * rec_dp_mask[0],\n                                                      self.recurrent_kernel_i))\n            f = self.recurrent_activation(x_f + K.dot(h_tm1 * rec_dp_mask[1],\n                                                      self.recurrent_kernel_f))\n            c = f * c_tm1 + i * self.activation(x_c + K.dot(h_tm1 * rec_dp_mask[2],\n                                                            self.recurrent_kernel_c))\n            o = self.recurrent_activation(x_o + K.dot(h_tm1 * rec_dp_mask[3],\n                                                      self.recurrent_kernel_o))\n        h = o * self.activation(c)\n        if 0 < self.dropout + self.recurrent_dropout:\n            h._uses_learning_phase = True\n        return h, [h, c]\n\n    def get_config(self):\n        config = {'units': self.units,\n                  'activation': activations.serialize(self.activation),\n                  'recurrent_activation': activations.serialize(self.recurrent_activation),\n                  'use_bias': self.use_bias,\n                  'kernel_initializer': initializers.serialize(self.kernel_initializer),\n                  'recurrent_initializer': initializers.serialize(self.recurrent_initializer),\n                  'bias_initializer': initializers.serialize(self.bias_initializer),\n                  'unit_forget_bias': self.unit_forget_bias,\n                  'kernel_regularizer': regularizers.serialize(self.kernel_regularizer),\n                  'recurrent_regularizer': regularizers.serialize(self.recurrent_regularizer),\n                  'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n                  'kernel_constraint': constraints.serialize(self.kernel_constraint),\n                  'recurrent_constraint': constraints.serialize(self.recurrent_constraint),\n                  'bias_constraint': constraints.serialize(self.bias_constraint),\n                  'dropout': self.dropout,\n                  'recurrent_dropout': self.recurrent_dropout}\n        base_config = super(LSTM, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n",
          "file_patch": "@@ -177,6 +177,8 @@ class Recurrent(Layer):\n         self.implementation = 0\n         self.supports_masking = True\n         self.input_spec = InputSpec(ndim=3)\n+        self.dropout = 0\n+        self.recurrent_dropout = 0\n \n     def compute_output_shape(self, input_shape):\n         if self.return_sequences:\n@@ -245,6 +247,11 @@ class Recurrent(Layer):\n                 updates.append((self.states[i], states[i]))\n             self.add_update(updates, inputs)\n \n+        # Properly set learning phase\n+        if 0 < self.dropout + self.recurrent_dropout:\n+            last_output._uses_learning_phase = True\n+            outputs._uses_learning_phase = True\n+\n         if self.return_sequences:\n             return outputs\n         else:\n@@ -375,11 +382,12 @@ class SimpleRNN(Recurrent):\n                                         constraint=self.bias_constraint)\n         else:\n             self.bias = None\n+        self.built = True\n \n     def reset_states(self):\n         if not self.stateful:\n             raise AttributeError('Layer must be stateful.')\n-        input_shape = self.input_spec[0].shape\n+        input_shape = self.input_spec.shape\n         if not input_shape[0]:\n             raise ValueError('If a RNN is stateful, it needs to know '\n                              'its batch size. Specify the batch size '\n@@ -433,12 +441,7 @@ class SimpleRNN(Recurrent):\n \n         # Properly set learning phase on output tensor.\n         if 0 < self.dropout + self.recurrent_dropout:\n-            uses_learning_phase = False\n-            if getattr(states[0], '_uses_learning_phase', False):\n-                uses_learning_phase = True\n-            if getattr(states[1], '_uses_learning_phase', False):\n-                uses_learning_phase = True\n-            output._uses_learning_phase = uses_learning_phase\n+            output._uses_learning_phase = True\n         return output, [output]\n \n     def get_constants(self, inputs, training=None):\n@@ -480,9 +483,9 @@ class SimpleRNN(Recurrent):\n                   'kernel_initializer': initializers.serialize(self.kernel_initializer),\n                   'recurrent_initializer': initializers.serialize(self.recurrent_initializer),\n                   'bias_initializer': initializers.serialize(self.bias_initializer),\n-                  'kernel_regularizer': regularizers.serialize(kernel_regularizer),\n-                  'recurrent_regularizer': regularizers.serialize(recurrent_regularizer),\n-                  'bias_regularizer': regularizers.serialize(bias_regularizer),\n+                  'kernel_regularizer': regularizers.serialize(self.kernel_regularizer),\n+                  'recurrent_regularizer': regularizers.serialize(self.recurrent_regularizer),\n+                  'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n                   'kernel_constraint': constraints.serialize(self.kernel_constraint),\n                   'recurrent_constraint': constraints.serialize(self.recurrent_constraint),\n                   'bias_constraint': constraints.serialize(self.bias_constraint),\n@@ -633,11 +636,12 @@ class GRU(Recurrent):\n             self.bias_z = None\n             self.bias_r = None\n             self.bias_h = None\n+        self.built = True\n \n     def reset_states(self):\n         if not self.stateful:\n             raise RuntimeError('Layer must be stateful.')\n-        input_shape = self.input_spec[0].shape\n+        input_shape = self.input_spec.shape\n         if not input_shape[0]:\n             raise ValueError('If a RNN is stateful, a complete '\n                              'input_shape must be provided '\n@@ -721,7 +725,7 @@ class GRU(Recurrent):\n \n             x_h = matrix_x[:, 2 * self.units:]\n             recurrent_h = K.dot(r * h_tm1 * rec_dp_mask[0],\n-                            self.recurrent_kernel[:, 2 * self.units:])\n+                                self.recurrent_kernel[:, 2 * self.units:])\n             hh = self.activation(x_h + recurrent_h)\n         else:\n             if self.implementation == 0:\n@@ -746,6 +750,8 @@ class GRU(Recurrent):\n             hh = self.activation(x_h + K.dot(r * h_tm1 * rec_dp_mask[2],\n                                              self.recurrent_kernel_h))\n         h = z * h_tm1 + (1 - z) * hh\n+        if 0 < self.dropout + self.recurrent_dropout:\n+            h._uses_learning_phase = True\n         return h, [h]\n \n     def get_config(self):\n@@ -871,7 +877,7 @@ class LSTM(Recurrent):\n         self.recurrent_dropout = min(1., max(0., recurrent_dropout))\n \n     def build(self, input_shape):\n-        self.input_spec = [InputSpec(shape=input_shape)]\n+        self.input_spec = InputSpec(shape=input_shape)\n         self.input_dim = input_shape[2]\n \n         if self.stateful:\n@@ -924,10 +930,11 @@ class LSTM(Recurrent):\n             self.bias_f = None\n             self.bias_c = None\n             self.bias_o = None\n+        self.built = True\n \n     def reset_states(self):\n         assert self.stateful, 'Layer must be stateful.'\n-        input_shape = self.input_spec[0].shape\n+        input_shape = self.input_spec.shape\n         if not input_shape[0]:\n             raise ValueError('If a RNN is stateful, a complete ' +\n                              'input_shape must be provided '\n@@ -1039,6 +1046,8 @@ class LSTM(Recurrent):\n             o = self.recurrent_activation(x_o + K.dot(h_tm1 * rec_dp_mask[3],\n                                                       self.recurrent_kernel_o))\n         h = o * self.activation(c)\n+        if 0 < self.dropout + self.recurrent_dropout:\n+            h._uses_learning_phase = True\n         return h, [h, c]\n \n     def get_config(self):\n@@ -1050,9 +1059,9 @@ class LSTM(Recurrent):\n                   'recurrent_initializer': initializers.serialize(self.recurrent_initializer),\n                   'bias_initializer': initializers.serialize(self.bias_initializer),\n                   'unit_forget_bias': self.unit_forget_bias,\n-                  'kernel_regularizer': regularizers.serialize(kernel_regularizer),\n-                  'recurrent_regularizer': regularizers.serialize(recurrent_regularizer),\n-                  'bias_regularizer': regularizers.serialize(bias_regularizer),\n+                  'kernel_regularizer': regularizers.serialize(self.kernel_regularizer),\n+                  'recurrent_regularizer': regularizers.serialize(self.recurrent_regularizer),\n+                  'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n                   'kernel_constraint': constraints.serialize(self.kernel_constraint),\n                   'recurrent_constraint': constraints.serialize(self.recurrent_constraint),\n                   'bias_constraint': constraints.serialize(self.bias_constraint),\n",
          "files_name_in_blame_commit": [
            "embeddings.py",
            "vgg19.py",
            "convolutional_recurrent.py",
            "core_test.py",
            "test_local.py",
            "noise.py",
            "topology.py",
            "__init__.py",
            "recurrent.py",
            "local.py",
            "core.py",
            "test_recurrent.py",
            "conv_utils.py",
            "embeddings_test.py",
            "test_convolutional.py",
            "convolutional_test.py",
            "advanced_activations_test.py",
            "pooling.py",
            "test_image_data_tasks.py",
            "normalization.py",
            "normalization_test.py",
            "test_utils.py",
            "wrappers.py",
            "local_test.py",
            "noise_test.py",
            "constraints.py",
            "test_advanced_activations.py",
            "test_core.py",
            "convolutional.py",
            "test_normalization.py",
            "layers.py",
            "advanced_activations.py",
            "wrappers_test.py",
            "test_loss_masking.py",
            "test_convolutional_recurrent.py"
          ]
        }
      },
      "03a7eb89e27b70f2ca0ac932ef4bace7569d6fab": {
        "commit": {
          "commit_id": "03a7eb89e27b70f2ca0ac932ef4bace7569d6fab",
          "commit_message": "Advance API switch (BN; RNNs).",
          "commit_author": "Francois Chollet",
          "commit_date": "2017-02-13 16:55:19",
          "commit_parent": "a856451243d0ac7758a19f5b16392b4c416eebf4"
        },
        "function": {
          "function_name": "build",
          "function_code_before": "def build(self, input_shape):\n    self.input_spec = [InputSpec(shape=input_shape)]\n    if self.stateful:\n        self.reset_states()\n    else:\n        self.states = [None]\n    input_dim = input_shape[2]\n    self.input_dim = input_dim\n    self.W = self.add_weight((input_dim, self.output_dim), initializer=self.init, name='{}_W'.format(self.name), regularizer=self.W_regularizer)\n    self.U = self.add_weight((self.output_dim, self.output_dim), initializer=self.inner_init, name='{}_U'.format(self.name), regularizer=self.U_regularizer)\n    self.b = self.add_weight((self.output_dim,), initializer='zero', name='{}_b'.format(self.name), regularizer=self.b_regularizer)\n    if self.initial_weights is not None:\n        self.set_weights(self.initial_weights)\n        del self.initial_weights\n    self.built = True",
          "function_code_after": "def build(self, input_shape):\n    self.input_spec = InputSpec(shape=input_shape)\n    if self.stateful:\n        self.reset_states()\n    else:\n        self.states = [None]\n    input_dim = input_shape[2]\n    self.input_dim = input_dim\n    self.kernel = self.add_weight((input_dim, self.units), name='kernel', initializer=self.kernel_initializer, regularizer=self.kernel_regularizer, constraint=self.kernel_constraint)\n    self.recurrent_kernel = self.add_weight((self.units, self.units), name='recurrent_kernel', initializer=self.recurrent_initializer, regularizer=self.recurrent_regularizer, constraint=self.recurrent_constraint)\n    if self.use_bias:\n        self.bias = self.add_weight((self.units,), name='bias', initializer=self.bias_initializer, regularizer=self.bias_regularizer, constraint=self.bias_constraint)\n    else:\n        self.bias = None",
          "function_before_start_line": 316,
          "function_before_end_line": 342,
          "function_after_start_line": 353,
          "function_after_end_line": 382,
          "function_before_token_count": 190,
          "function_after_token_count": 174,
          "functions_name_modified_file": [
            "__init__",
            "compute_mask",
            "get_output_shape_for",
            "build",
            "call",
            "get_constants",
            "_time_distributed_dense",
            "step",
            "reset_states",
            "get_initial_states",
            "preprocess_input",
            "get_config"
          ],
          "functions_name_all_files": [
            "_assert_sparse_module",
            "pool3d",
            "output",
            "ctc_update_log_p",
            "save_weights",
            "run_internal_graph",
            "dropout",
            "get_layer",
            "resize_volumes",
            "reverse",
            "_assert_has_capability",
            "tanh",
            "evaluate",
            "get_input_shape_at",
            "_updated_config",
            "ctc_cost",
            "map_fn",
            "batch_normalization",
            "standardize_weights",
            "step",
            "get_weights",
            "in_top_k",
            "stateful",
            "_preprocess_conv3d_input",
            "rnn",
            "stack",
            "eval",
            "_get_noise_shape",
            "ones",
            "cos",
            "get_variable_shape",
            "foldl",
            "dtype",
            "_time_distributed_dense",
            "equal",
            "hard_sigmoid",
            "_preprocess_conv2d_input",
            "__init__",
            "compute_mask",
            "sqrt",
            "random_binomial",
            "spatial_2d_padding",
            "get_output_shape_for",
            "weighted_objective",
            "evaluate_generator",
            "get_input_at",
            "shape",
            "get_constants",
            "expand_dims",
            "categorical_crossentropy",
            "conv2d",
            "collect_metrics",
            "stop",
            "_preprocess_conv3d_kernel",
            "save",
            "batch_set_value",
            "check_loss_and_target_compatibility",
            "cast",
            "greater",
            "int_shape",
            "compile",
            "batch_dot",
            "trainable_weights",
            "moving_average_update",
            "pattern_broadcast",
            "get_output_shape_at",
            "concatenate",
            "reshape",
            "constant",
            "check_array_lengths",
            "lesser_equal",
            "standardize_sample_or_class_weights",
            "fit",
            "losses",
            "_make_predict_function",
            "test_on_batch",
            "asymmetric_spatial_2d_padding",
            "get_config",
            "set_learning_phase",
            "_to_snake_case",
            "lesser",
            "bias_add",
            "eye",
            "print_tensor",
            "truncated_normal",
            "softplus",
            "ctc_path_probs",
            "_fix_unknown_dimension",
            "standardize_input_data",
            "predict",
            "standardize_sample_weights",
            "get_uid",
            "std",
            "spatial_3d_padding",
            "save_weights_to_hdf5_group",
            "build",
            "call",
            "in_train_phase",
            "permute_dimensions",
            "updates",
            "make_batches",
            "get_losses_for",
            "l2_normalize",
            "normalize_batch_in_training",
            "add_weight",
            "_test_loop",
            "fit_generator",
            "ones_like",
            "output_shape",
            "sigmoid",
            "prod",
            "add_update",
            "random_normal_variable",
            "_preprocess_conv2d_kernel",
            "sign",
            "random_uniform",
            "standardize_class_weights",
            "conv3d",
            "_make_train_function",
            "update_add",
            "pool2d",
            "repeat_elements",
            "_predict_loop",
            "asymmetric_temporal_padding",
            "log",
            "separable_conv2d",
            "ctc_interleave_blanks",
            "stop_gradient",
            "add_loss",
            "ctc_batch_cost",
            "reset_states",
            "sin",
            "switch",
            "relu",
            "_to_list",
            "count_params",
            "var",
            "any",
            "_preprocess_conv3d_filter_shape",
            "batch_get_value",
            "function",
            "update_sub",
            "_old_batch_normalization",
            "set_weights",
            "random_normal",
            "zeros_like",
            "get_output_mask_at",
            "input",
            "_pooling_function",
            "predict_generator",
            "softsign",
            "load_weights",
            "_fit_loop",
            "binary_crossentropy",
            "get_source_inputs",
            "preprocess_input",
            "softmax",
            "batch_flatten",
            "_get_node_attribute_at_index",
            "_standardize_user_data",
            "elu",
            "non_trainable_weights",
            "square",
            "sparse_categorical_crossentropy",
            "input_mask",
            "_preprocess_padding",
            "squeeze",
            "input_shape",
            "constraints",
            "_preprocess_conv2d_image_shape",
            "__call__",
            "_collect_input_shape",
            "learning_phase",
            "placeholder",
            "abs",
            "all",
            "mean",
            "clip",
            "conv2d_transpose",
            "dot",
            "_add_inbound_node",
            "variable",
            "in_test_phase",
            "exp",
            "state_updates",
            "one_hot",
            "get_input_mask_at",
            "_make_test_function",
            "_postprocess_conv2d_output",
            "resize_images",
            "max",
            "sum",
            "weights",
            "predict_on_batch",
            "_old_normalize_batch_in_training",
            "slice_X",
            "start",
            "train_on_batch",
            "minimum",
            "foldr",
            "_preprocess_conv3d_volume_shape",
            "argmax",
            "reset_uids",
            "zeros",
            "set_value",
            "to_yaml",
            "to_dense",
            "tile",
            "pow",
            "flatten",
            "_preprocess_conv2d_filter_shape",
            "get_updates_for",
            "argmin",
            "conv1d",
            "round",
            "output_mask",
            "random_uniform_variable",
            "temporal_padding",
            "repeat",
            "Input",
            "load_weights_from_hdf5_group_by_name",
            "get_initial_states",
            "get_value",
            "is_sparse",
            "to_json",
            "input_spec",
            "transpose",
            "get_output_at",
            "ndim",
            "uses_learning_phase",
            "load_weights_from_hdf5_group",
            "summary",
            "is_running",
            "_is_all_none",
            "gradients",
            "update",
            "_object_list_uid",
            "from_config",
            "_collect_previous_mask",
            "batch_shuffle",
            "_postprocess_conv3d_output",
            "assert_input_compatibility",
            "arange",
            "min",
            "greater_equal",
            "not_equal",
            "ctc_create_skip_idxs",
            "gather",
            "maximum",
            "is_explicit_shape"
          ],
          "functions_name_co_evolved_modified_file": [
            "__init__",
            "compute_mask",
            "get_output_shape_for",
            "time_distributed_dense",
            "call",
            "get_constants",
            "reset_states",
            "step",
            "_time_distributed_dense",
            "get_initial_states",
            "preprocess_input",
            "get_config"
          ],
          "functions_name_co_evolved_all_files": [
            "__init__",
            "get_config",
            "compute_mask",
            "get_output_shape_for",
            "time_distributed_dense",
            "call",
            "get_constants",
            "preprocess_input",
            "assert_input_compatibility",
            "_fix_unknown_dimension",
            "reset_states",
            "standardize_input_data",
            "step",
            "_time_distributed_dense",
            "get_initial_states",
            "_get_noise_shape",
            "int_shape"
          ]
        },
        "file": {
          "file_name": "recurrent.py",
          "file_nloc": 938,
          "file_complexity": 107,
          "file_token_count": 5737,
          "file_before": "# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import\nimport numpy as np\n\nfrom .. import backend as K\nfrom .. import activations\nfrom .. import initializers\nfrom .. import regularizers\nfrom ..engine import Layer\nfrom ..engine import InputSpec\n\n\ndef time_distributed_dense(x, w, b=None, dropout=None,\n                           input_dim=None, output_dim=None, timesteps=None):\n    \"\"\"Apply `y . w + b` for every temporal slice y of x.\n\n    # Arguments\n        x: input tensor.\n        w: weight matrix.\n        b: optional bias vector.\n        dropout: wether to apply dropout (same dropout mask\n            for every temporal slice of the input).\n        input_dim: integer; optional dimensionality of the input.\n        output_dim: integer; optional dimensionality of the output.\n        timesteps: integer; optional number of timesteps.\n\n    # Returns\n        Output tensor.\n    \"\"\"\n    if not input_dim:\n        input_dim = K.shape(x)[2]\n    if not timesteps:\n        timesteps = K.shape(x)[1]\n    if not output_dim:\n        output_dim = K.shape(w)[1]\n\n    if dropout is not None and 0. < dropout < 1.:\n        # apply the same dropout pattern at every timestep\n        ones = K.ones_like(K.reshape(x[:, 0, :], (-1, input_dim)))\n        dropout_matrix = K.dropout(ones, dropout)\n        expanded_dropout_matrix = K.repeat(dropout_matrix, timesteps)\n        x = K.in_train_phase(x * expanded_dropout_matrix, x)\n\n    # collapse time dimension and batch dimension together\n    x = K.reshape(x, (-1, input_dim))\n    x = K.dot(x, w)\n    if b:\n        x += b\n    # reshape to 3D tensor\n    if K.backend() == 'tensorflow':\n        x = K.reshape(x, K.stack([-1, timesteps, output_dim]))\n        x.set_shape([None, None, output_dim])\n    else:\n        x = K.reshape(x, (-1, timesteps, output_dim))\n    return x\n\n\nclass Recurrent(Layer):\n    \"\"\"Abstract base class for recurrent layers.\n    Do not use in a model -- it's not a valid layer!\n    Use its children classes `LSTM`, `GRU` and `SimpleRNN` instead.\n\n    All recurrent layers (`LSTM`, `GRU`, `SimpleRNN`) also\n    follow the specifications of this class and accept\n    the keyword arguments listed below.\n\n    # Example\n\n    ```python\n        # as the first layer in a Sequential model\n        model = Sequential()\n        model.add(LSTM(32, input_shape=(10, 64)))\n        # now model.output_shape == (None, 32)\n        # note: `None` is the batch dimension.\n\n        # the following is identical:\n        model = Sequential()\n        model.add(LSTM(32, input_dim=64, input_length=10))\n\n        # for subsequent layers, not need to specify the input size:\n        model.add(LSTM(16))\n    ```\n\n    # Arguments\n        weights: list of Numpy arrays to set as initial weights.\n            The list should have 3 elements, of shapes:\n            `[(input_dim, output_dim), (output_dim, output_dim), (output_dim,)]`.\n        return_sequences: Boolean. Whether to return the last output\n            in the output sequence, or the full sequence.\n        go_backwards: Boolean (default False).\n            If True, process the input sequence backwards.\n        stateful: Boolean (default False). If True, the last state\n            for each sample at index i in a batch will be used as initial\n            state for the sample of index i in the following batch.\n        unroll: Boolean (default False). If True, the network will be unrolled,\n            else a symbolic loop will be used. When using TensorFlow, the network\n            is always unrolled, so this argument does not do anything.\n            Unrolling can speed-up a RNN, although it tends to be more memory-intensive.\n            Unrolling is only suitable for short sequences.\n        consume_less: one of \"cpu\", \"mem\", or \"gpu\" (LSTM/GRU only).\n            If set to \"cpu\", the RNN will use\n            an implementation that uses fewer, larger matrix products,\n            thus running faster on CPU but consuming more memory.\n            If set to \"mem\", the RNN will use more matrix products,\n            but smaller ones, thus running slower (may actually be faster on GPU)\n            while consuming less memory.\n            If set to \"gpu\" (LSTM/GRU only), the RNN will combine the input gate,\n            the forget gate and the output gate into a single matrix,\n            enabling more time-efficient parallelization on the GPU. Note: RNN\n            dropout must be shared for all gates, resulting in a slightly\n            reduced regularization.\n        input_dim: dimensionality of the input (integer).\n            This argument (or alternatively, the keyword argument `input_shape`)\n            is required when using this layer as the first layer in a model.\n        input_length: Length of input sequences, to be specified\n            when it is constant.\n            This argument is required if you are going to connect\n            `Flatten` then `Dense` layers upstream\n            (without it, the shape of the dense outputs cannot be computed).\n            Note that if the recurrent layer is not the first layer\n            in your model, you would need to specify the input length\n            at the level of the first layer\n            (e.g. via the `input_shape` argument)\n\n    # Input shape\n        3D tensor with shape `(nb_samples, timesteps, input_dim)`.\n\n    # Output shape\n        - if `return_sequences`: 3D tensor with shape\n            `(nb_samples, timesteps, output_dim)`.\n        - else, 2D tensor with shape `(nb_samples, output_dim)`.\n\n    # Masking\n        This layer supports masking for input data with a variable number\n        of timesteps. To introduce masks to your data,\n        use an [Embedding](embeddings.md) layer with the `mask_zero` parameter\n        set to `True`.\n\n    # Note on performance\n        You are likely to see better performance with RNNs in Theano compared\n        to TensorFlow. Additionally, when using TensorFlow, it is often\n        preferable to set `unroll=True` for better performance.\n\n    # Note on using statefulness in RNNs\n        You can set RNN layers to be 'stateful', which means that the states\n        computed for the samples in one batch will be reused as initial states\n        for the samples in the next batch. This assumes a one-to-one mapping\n        between samples in different successive batches.\n\n        To enable statefulness:\n            - specify `stateful=True` in the layer constructor.\n            - specify a fixed batch size for your model, by passing\n                if sequential model:\n                  `batch_input_shape=(...)` to the first layer in your model.\n                else for functional model with 1 or more Input layers:\n                  `batch_shape=(...)` to all the first layers in your model.\n                This is the expected shape of your inputs *including the batch size*.\n                It should be a tuple of integers, e.g. `(32, 10, 100)`.\n            - specify `shuffle=False` when calling fit().\n\n        To reset the states of your model, call `.reset_states()` on either\n        a specific layer, or on your entire model.\n    \"\"\"\n\n    def __init__(self, weights=None,\n                 return_sequences=False, go_backwards=False, stateful=False,\n                 unroll=False, consume_less='cpu',\n                 input_dim=None, input_length=None, **kwargs):\n        self.input_dim = input_dim\n        self.input_length = input_length\n        if self.input_dim:\n            kwargs['input_shape'] = (self.input_length, self.input_dim)\n        super(Recurrent, self).__init__(**kwargs)\n        self.return_sequences = return_sequences\n        self.initial_weights = weights\n        self.go_backwards = go_backwards\n        self.stateful = stateful\n        self.unroll = unroll\n        self.consume_less = consume_less\n\n        self.supports_masking = True\n        self.input_spec = [InputSpec(ndim=3)]\n\n    def get_output_shape_for(self, input_shape):\n        if self.return_sequences:\n            return (input_shape[0], input_shape[1], self.output_dim)\n        else:\n            return (input_shape[0], self.output_dim)\n\n    def compute_mask(self, input, mask):\n        if self.return_sequences:\n            return mask\n        else:\n            return None\n\n    def step(self, x, states):\n        raise NotImplementedError\n\n    def get_constants(self, x):\n        return []\n\n    def get_initial_states(self, x):\n        # build an all-zero tensor of shape (samples, output_dim)\n        initial_state = K.zeros_like(x)  # (samples, timesteps, input_dim)\n        initial_state = K.sum(initial_state, axis=(1, 2))  # (samples,)\n        initial_state = K.expand_dims(initial_state)  # (samples, 1)\n        initial_state = K.tile(initial_state, [1, self.output_dim])  # (samples, output_dim)\n        initial_states = [initial_state for _ in range(len(self.states))]\n        return initial_states\n\n    def preprocess_input(self, x):\n        return x\n\n    def call(self, x, mask=None):\n        # input shape: (nb_samples, time (padded with zeros), input_dim)\n        # note that the .build() method of subclasses MUST define\n        # self.input_spec with a complete input shape.\n        input_shape = K.int_shape(x)\n        if self.unroll and input_shape[1] is None:\n            raise ValueError('Cannot unroll a RNN if the '\n                             'time dimension is undefined. \\n'\n                             '- If using a Sequential model, '\n                             'specify the time dimension by passing '\n                             'an `input_shape` or `batch_input_shape` '\n                             'argument to your first layer. If your '\n                             'first layer is an Embedding, you can '\n                             'also use the `input_length` argument.\\n'\n                             '- If using the functional API, specify '\n                             'the time dimension by passing a `shape` '\n                             'or `batch_shape` argument to your Input layer.')\n        if self.stateful:\n            initial_states = self.states\n        else:\n            initial_states = self.get_initial_states(x)\n        constants = self.get_constants(x)\n        preprocessed_input = self.preprocess_input(x)\n\n        last_output, outputs, states = K.rnn(self.step, preprocessed_input,\n                                             initial_states,\n                                             go_backwards=self.go_backwards,\n                                             mask=mask,\n                                             constants=constants,\n                                             unroll=self.unroll,\n                                             input_length=input_shape[1])\n        if self.stateful:\n            updates = []\n            for i in range(len(states)):\n                updates.append((self.states[i], states[i]))\n            self.add_update(updates, x)\n\n        if self.return_sequences:\n            return outputs\n        else:\n            return last_output\n\n    def get_config(self):\n        config = {'return_sequences': self.return_sequences,\n                  'go_backwards': self.go_backwards,\n                  'stateful': self.stateful,\n                  'unroll': self.unroll,\n                  'consume_less': self.consume_less}\n        if self.stateful and self.input_spec[0].shape:\n            config['batch_input_shape'] = self.input_spec[0].shape\n        else:\n            config['input_dim'] = self.input_dim\n            config['input_length'] = self.input_length\n\n        base_config = super(Recurrent, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\n\nclass SimpleRNN(Recurrent):\n    \"\"\"Fully-connected RNN where the output is to be fed back to input.\n\n    # Arguments\n        output_dim: dimension of the internal projections and the final output.\n        init: weight initializer function.\n            Can be the name of an existing function (str),\n            or a Theano function (see: [initializers](../initializers.md)).\n        inner_init: initializer function of the inner cells.\n        activation: activation function.\n            Can be the name of an existing function (str),\n            or a Theano function (see: [activations](../activations.md)).\n        W_regularizer: instance of [WeightRegularizer](../regularizers.md)\n            (eg. L1 or L2 regularization), applied to the input weights matrices.\n        U_regularizer: instance of [WeightRegularizer](../regularizers.md)\n            (eg. L1 or L2 regularization), applied to the recurrent weights matrices.\n        b_regularizer: instance of [WeightRegularizer](../regularizers.md),\n            applied to the bias.\n        dropout_W: float between 0 and 1. Fraction of the input units to drop for input gates.\n        dropout_U: float between 0 and 1. Fraction of the input units to drop for recurrent connections.\n\n    # References\n        - [A Theoretically Grounded Application of Dropout in Recurrent Neural Networks](http://arxiv.org/abs/1512.05287)\n    \"\"\"\n\n    def __init__(self, output_dim,\n                 init='glorot_uniform', inner_init='orthogonal',\n                 activation='tanh',\n                 W_regularizer=None, U_regularizer=None, b_regularizer=None,\n                 dropout_W=0., dropout_U=0., **kwargs):\n        super(SimpleRNN, self).__init__(**kwargs)\n        self.output_dim = output_dim\n        self.init = initializers.get(init)\n        self.inner_init = initializers.get(inner_init)\n        self.activation = activations.get(activation)\n        self.W_regularizer = regularizers.get(W_regularizer)\n        self.U_regularizer = regularizers.get(U_regularizer)\n        self.b_regularizer = regularizers.get(b_regularizer)\n        self.dropout_W = dropout_W\n        self.dropout_U = dropout_U\n\n        if self.dropout_W or self.dropout_U:\n            self.uses_learning_phase = True\n\n    def build(self, input_shape):\n        self.input_spec = [InputSpec(shape=input_shape)]\n        if self.stateful:\n            self.reset_states()\n        else:\n            # initial states: all-zero tensor of shape (output_dim)\n            self.states = [None]\n        input_dim = input_shape[2]\n        self.input_dim = input_dim\n\n        self.W = self.add_weight((input_dim, self.output_dim),\n                                 initializer=self.init,\n                                 name='{}_W'.format(self.name),\n                                 regularizer=self.W_regularizer)\n        self.U = self.add_weight((self.output_dim, self.output_dim),\n                                 initializer=self.inner_init,\n                                 name='{}_U'.format(self.name),\n                                 regularizer=self.U_regularizer)\n        self.b = self.add_weight((self.output_dim,),\n                                 initializer='zero',\n                                 name='{}_b'.format(self.name),\n                                 regularizer=self.b_regularizer)\n\n        if self.initial_weights is not None:\n            self.set_weights(self.initial_weights)\n            del self.initial_weights\n        self.built = True\n\n    def reset_states(self):\n        assert self.stateful, 'Layer must be stateful.'\n        input_shape = self.input_spec[0].shape\n        if not input_shape[0]:\n            raise ValueError('If a RNN is stateful, it needs to know '\n                             'its batch size. Specify the batch size '\n                             'of your input tensors: \\n'\n                             '- If using a Sequential model, '\n                             'specify the batch size by passing '\n                             'a `batch_input_shape` '\n                             'argument to your first layer.\\n'\n                             '- If using the functional API, specify '\n                             'the time dimension by passing a '\n                             '`batch_shape` argument to your Input layer.')\n        if hasattr(self, 'states'):\n            K.set_value(self.states[0],\n                        np.zeros((input_shape[0], self.output_dim)))\n        else:\n            self.states = [K.zeros((input_shape[0], self.output_dim))]\n\n    def preprocess_input(self, x):\n        if self.consume_less == 'cpu':\n            input_shape = K.int_shape(x)\n            input_dim = input_shape[2]\n            timesteps = input_shape[1]\n            return time_distributed_dense(x, self.W, self.b, self.dropout_W,\n                                          input_dim, self.output_dim,\n                                          timesteps)\n        else:\n            return x\n\n    def step(self, x, states):\n        prev_output = states[0]\n        B_U = states[1]\n        B_W = states[2]\n\n        if self.consume_less == 'cpu':\n            h = x\n        else:\n            h = K.dot(x * B_W, self.W) + self.b\n\n        output = self.activation(h + K.dot(prev_output * B_U, self.U))\n        return output, [output]\n\n    def get_constants(self, x):\n        constants = []\n        if 0 < self.dropout_U < 1:\n            ones = K.ones_like(K.reshape(x[:, 0, 0], (-1, 1)))\n            ones = K.tile(ones, (1, self.output_dim))\n            B_U = K.in_train_phase(K.dropout(ones, self.dropout_U), ones)\n            constants.append(B_U)\n        else:\n            constants.append(K.cast_to_floatx(1.))\n        if self.consume_less == 'cpu' and 0 < self.dropout_W < 1:\n            input_shape = K.int_shape(x)\n            input_dim = input_shape[-1]\n            ones = K.ones_like(K.reshape(x[:, 0, 0], (-1, 1)))\n            ones = K.tile(ones, (1, int(input_dim)))\n            B_W = K.in_train_phase(K.dropout(ones, self.dropout_W), ones)\n            constants.append(B_W)\n        else:\n            constants.append(K.cast_to_floatx(1.))\n        return constants\n\n    def get_config(self):\n        config = {'output_dim': self.output_dim,\n                  'init': initializers.get_config(self.init),\n                  'inner_init': initializers.get_config(self.inner_init),\n                  'activation': self.activation.__name__,\n                  'W_regularizer': self.W_regularizer.get_config() if self.W_regularizer else None,\n                  'U_regularizer': self.U_regularizer.get_config() if self.U_regularizer else None,\n                  'b_regularizer': self.b_regularizer.get_config() if self.b_regularizer else None,\n                  'dropout_W': self.dropout_W,\n                  'dropout_U': self.dropout_U}\n        base_config = super(SimpleRNN, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\n\nclass GRU(Recurrent):\n    \"\"\"Gated Recurrent Unit - Cho et al. 2014.\n\n    # Arguments\n        output_dim: dimension of the internal projections and the final output.\n        init: weight initializer function.\n            Can be the name of an existing function (str),\n            or a Theano function (see: [initializers](../initializers.md)).\n        inner_init: initializer function of the inner cells.\n        activation: activation function.\n            Can be the name of an existing function (str),\n            or a Theano function (see: [activations](../activations.md)).\n        inner_activation: activation function for the inner cells.\n        W_regularizer: instance of [WeightRegularizer](../regularizers.md)\n            (eg. L1 or L2 regularization), applied to the input weights matrices.\n        U_regularizer: instance of [WeightRegularizer](../regularizers.md)\n            (eg. L1 or L2 regularization), applied to the recurrent weights matrices.\n        b_regularizer: instance of [WeightRegularizer](../regularizers.md),\n            applied to the bias.\n        dropout_W: float between 0 and 1. Fraction of the input units to drop for input gates.\n        dropout_U: float between 0 and 1. Fraction of the input units to drop for recurrent connections.\n\n    # References\n        - [On the Properties of Neural Machine Translation: Encoder-Decoder Approaches](https://arxiv.org/abs/1409.1259)\n        - [Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling](http://arxiv.org/abs/1412.3555v1)\n        - [A Theoretically Grounded Application of Dropout in Recurrent Neural Networks](http://arxiv.org/abs/1512.05287)\n    \"\"\"\n\n    def __init__(self, output_dim,\n                 init='glorot_uniform', inner_init='orthogonal',\n                 activation='tanh', inner_activation='hard_sigmoid',\n                 W_regularizer=None, U_regularizer=None, b_regularizer=None,\n                 dropout_W=0., dropout_U=0., **kwargs):\n        super(GRU, self).__init__(**kwargs)\n        self.output_dim = output_dim\n        self.init = initializers.get(init)\n        self.inner_init = initializers.get(inner_init)\n        self.activation = activations.get(activation)\n        self.inner_activation = activations.get(inner_activation)\n        self.W_regularizer = regularizers.get(W_regularizer)\n        self.U_regularizer = regularizers.get(U_regularizer)\n        self.b_regularizer = regularizers.get(b_regularizer)\n        self.dropout_W = dropout_W\n        self.dropout_U = dropout_U\n\n        if self.dropout_W or self.dropout_U:\n            self.uses_learning_phase = True\n\n    def build(self, input_shape):\n        self.input_spec = [InputSpec(shape=input_shape)]\n        self.input_dim = input_shape[2]\n\n        if self.stateful:\n            self.reset_states()\n        else:\n            # initial states: all-zero tensor of shape (output_dim)\n            self.states = [None]\n\n        if self.consume_less == 'gpu':\n            self.W = self.add_weight((self.input_dim, 3 * self.output_dim),\n                                     initializer=self.init,\n                                     name='{}_W'.format(self.name),\n                                     regularizer=self.W_regularizer)\n            self.U = self.add_weight((self.output_dim, 3 * self.output_dim),\n                                     initializer=self.inner_init,\n                                     name='{}_U'.format(self.name),\n                                     regularizer=self.U_regularizer)\n            self.b = self.add_weight((self.output_dim * 3,),\n                                     initializer='zero',\n                                     name='{}_b'.format(self.name),\n                                     regularizer=self.b_regularizer)\n        else:\n            self.W_z = self.add_weight((self.input_dim, self.output_dim),\n                                       initializer=self.init,\n                                       name='{}_W_z'.format(self.name),\n                                       regularizer=self.W_regularizer)\n            self.U_z = self.add_weight((self.output_dim, self.output_dim),\n                                       initializer=self.init,\n                                       name='{}_U_z'.format(self.name),\n                                       regularizer=self.W_regularizer)\n            self.b_z = self.add_weight((self.output_dim,),\n                                       initializer='zero',\n                                       name='{}_b_z'.format(self.name),\n                                       regularizer=self.b_regularizer)\n            self.W_r = self.add_weight((self.input_dim, self.output_dim),\n                                       initializer=self.init,\n                                       name='{}_W_r'.format(self.name),\n                                       regularizer=self.W_regularizer)\n            self.U_r = self.add_weight((self.output_dim, self.output_dim),\n                                       initializer=self.init,\n                                       name='{}_U_r'.format(self.name),\n                                       regularizer=self.W_regularizer)\n            self.b_r = self.add_weight((self.output_dim,),\n                                       initializer='zero',\n                                       name='{}_b_r'.format(self.name),\n                                       regularizer=self.b_regularizer)\n            self.W_h = self.add_weight((self.input_dim, self.output_dim),\n                                       initializer=self.init,\n                                       name='{}_W_h'.format(self.name),\n                                       regularizer=self.W_regularizer)\n            self.U_h = self.add_weight((self.output_dim, self.output_dim),\n                                       initializer=self.init,\n                                       name='{}_U_h'.format(self.name),\n                                       regularizer=self.W_regularizer)\n            self.b_h = self.add_weight((self.output_dim,),\n                                       initializer='zero',\n                                       name='{}_b_h'.format(self.name),\n                                       regularizer=self.b_regularizer)\n            self.W = K.concatenate([self.W_z, self.W_r, self.W_h])\n            self.U = K.concatenate([self.U_z, self.U_r, self.U_h])\n            self.b = K.concatenate([self.b_z, self.b_r, self.b_h])\n\n        if self.initial_weights is not None:\n            self.set_weights(self.initial_weights)\n            del self.initial_weights\n        self.built = True\n\n    def reset_states(self):\n        assert self.stateful, 'Layer must be stateful.'\n        input_shape = self.input_spec[0].shape\n        if not input_shape[0]:\n            raise ValueError('If a RNN is stateful, a complete '\n                             'input_shape must be provided '\n                             '(including batch size).')\n        if hasattr(self, 'states'):\n            K.set_value(self.states[0],\n                        np.zeros((input_shape[0], self.output_dim)))\n        else:\n            self.states = [K.zeros((input_shape[0], self.output_dim))]\n\n    def preprocess_input(self, x):\n        if self.consume_less == 'cpu':\n            input_shape = K.int_shape(x)\n            input_dim = input_shape[2]\n            timesteps = input_shape[1]\n\n            x_z = time_distributed_dense(x, self.W_z, self.b_z, self.dropout_W,\n                                         input_dim, self.output_dim, timesteps)\n            x_r = time_distributed_dense(x, self.W_r, self.b_r, self.dropout_W,\n                                         input_dim, self.output_dim, timesteps)\n            x_h = time_distributed_dense(x, self.W_h, self.b_h, self.dropout_W,\n                                         input_dim, self.output_dim, timesteps)\n            return K.concatenate([x_z, x_r, x_h], axis=2)\n        else:\n            return x\n\n    def step(self, x, states):\n        h_tm1 = states[0]  # previous memory\n        B_U = states[1]  # dropout matrices for recurrent units\n        B_W = states[2]\n\n        if self.consume_less == 'gpu':\n\n            matrix_x = K.dot(x * B_W[0], self.W) + self.b\n            matrix_inner = K.dot(h_tm1 * B_U[0], self.U[:, :2 * self.output_dim])\n\n            x_z = matrix_x[:, :self.output_dim]\n            x_r = matrix_x[:, self.output_dim: 2 * self.output_dim]\n            inner_z = matrix_inner[:, :self.output_dim]\n            inner_r = matrix_inner[:, self.output_dim: 2 * self.output_dim]\n\n            z = self.inner_activation(x_z + inner_z)\n            r = self.inner_activation(x_r + inner_r)\n\n            x_h = matrix_x[:, 2 * self.output_dim:]\n            inner_h = K.dot(r * h_tm1 * B_U[0], self.U[:, 2 * self.output_dim:])\n            hh = self.activation(x_h + inner_h)\n        else:\n            if self.consume_less == 'cpu':\n                x_z = x[:, :self.output_dim]\n                x_r = x[:, self.output_dim: 2 * self.output_dim]\n                x_h = x[:, 2 * self.output_dim:]\n            elif self.consume_less == 'mem':\n                x_z = K.dot(x * B_W[0], self.W_z) + self.b_z\n                x_r = K.dot(x * B_W[1], self.W_r) + self.b_r\n                x_h = K.dot(x * B_W[2], self.W_h) + self.b_h\n            else:\n                raise ValueError('Unknown `consume_less` mode.')\n            z = self.inner_activation(x_z + K.dot(h_tm1 * B_U[0], self.U_z))\n            r = self.inner_activation(x_r + K.dot(h_tm1 * B_U[1], self.U_r))\n\n            hh = self.activation(x_h + K.dot(r * h_tm1 * B_U[2], self.U_h))\n        h = z * h_tm1 + (1 - z) * hh\n        return h, [h]\n\n    def get_constants(self, x):\n        constants = []\n        if 0 < self.dropout_U < 1:\n            ones = K.ones_like(K.reshape(x[:, 0, 0], (-1, 1)))\n            ones = K.tile(ones, (1, self.output_dim))\n            B_U = [K.in_train_phase(K.dropout(ones, self.dropout_U), ones) for _ in range(3)]\n            constants.append(B_U)\n        else:\n            constants.append([K.cast_to_floatx(1.) for _ in range(3)])\n\n        if 0 < self.dropout_W < 1:\n            input_shape = K.int_shape(x)\n            input_dim = input_shape[-1]\n            ones = K.ones_like(K.reshape(x[:, 0, 0], (-1, 1)))\n            ones = K.tile(ones, (1, int(input_dim)))\n            B_W = [K.in_train_phase(K.dropout(ones, self.dropout_W), ones) for _ in range(3)]\n            constants.append(B_W)\n        else:\n            constants.append([K.cast_to_floatx(1.) for _ in range(3)])\n        return constants\n\n    def get_config(self):\n        config = {'output_dim': self.output_dim,\n                  'init': initializers.get_config(self.init),\n                  'inner_init': initializers.get_config(self.inner_init),\n                  'activation': self.activation.__name__,\n                  'inner_activation': self.inner_activation.__name__,\n                  'W_regularizer': self.W_regularizer.get_config() if self.W_regularizer else None,\n                  'U_regularizer': self.U_regularizer.get_config() if self.U_regularizer else None,\n                  'b_regularizer': self.b_regularizer.get_config() if self.b_regularizer else None,\n                  'dropout_W': self.dropout_W,\n                  'dropout_U': self.dropout_U}\n        base_config = super(GRU, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\n\nclass LSTM(Recurrent):\n    \"\"\"Long-Short Term Memory unit - Hochreiter 1997.\n\n    For a step-by-step description of the algorithm, see\n    [this tutorial](http://deeplearning.net/tutorial/lstm.html).\n\n    # Arguments\n        output_dim: dimension of the internal projections and the final output.\n        init: weight initializer function.\n            Can be the name of an existing function (str),\n            or a Theano function (see: [initializers](../initializers.md)).\n        inner_init: initializer function of the inner cells.\n        forget_bias_init: initializer function for the bias of the forget gate.\n            [Jozefowicz et al.](http://www.jmlr.org/proceedings/papers/v37/jozefowicz15.pdf)\n            recommend initializing with ones.\n        activation: activation function.\n            Can be the name of an existing function (str),\n            or a Theano function (see: [activations](../activations.md)).\n        inner_activation: activation function for the inner cells.\n        W_regularizer: instance of [WeightRegularizer](../regularizers.md)\n            (eg. L1 or L2 regularization), applied to the input weights matrices.\n        U_regularizer: instance of [WeightRegularizer](../regularizers.md)\n            (eg. L1 or L2 regularization), applied to the recurrent weights matrices.\n        b_regularizer: instance of [WeightRegularizer](../regularizers.md),\n            applied to the bias.\n        dropout_W: float between 0 and 1. Fraction of the input units to drop for input gates.\n        dropout_U: float between 0 and 1. Fraction of the input units to drop for recurrent connections.\n\n    # References\n        - [Long short-term memory](http://deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf) (original 1997 paper)\n        - [Learning to forget: Continual prediction with LSTM](http://www.mitpressjournals.org/doi/pdf/10.1162/089976600300015015)\n        - [Supervised sequence labeling with recurrent neural networks](http://www.cs.toronto.edu/~graves/preprint.pdf)\n        - [A Theoretically Grounded Application of Dropout in Recurrent Neural Networks](http://arxiv.org/abs/1512.05287)\n    \"\"\"\n\n    def __init__(self, output_dim,\n                 init='glorot_uniform', inner_init='orthogonal',\n                 forget_bias_init='one', activation='tanh',\n                 inner_activation='hard_sigmoid',\n                 W_regularizer=None, U_regularizer=None, b_regularizer=None,\n                 dropout_W=0., dropout_U=0., **kwargs):\n        super(LSTM, self).__init__(**kwargs)\n        self.output_dim = output_dim\n        self.init = initializers.get(init)\n        self.inner_init = initializers.get(inner_init)\n        self.forget_bias_init = initializers.get(forget_bias_init)\n        self.activation = activations.get(activation)\n        self.inner_activation = activations.get(inner_activation)\n        self.W_regularizer = regularizers.get(W_regularizer)\n        self.U_regularizer = regularizers.get(U_regularizer)\n        self.b_regularizer = regularizers.get(b_regularizer)\n        self.dropout_W = dropout_W\n        self.dropout_U = dropout_U\n\n        if self.dropout_W or self.dropout_U:\n            self.uses_learning_phase = True\n\n    def build(self, input_shape):\n        self.input_spec = [InputSpec(shape=input_shape)]\n        self.input_dim = input_shape[2]\n\n        if self.stateful:\n            self.reset_states()\n        else:\n            # initial states: 2 all-zero tensors of shape (output_dim)\n            self.states = [None, None]\n\n        if self.consume_less == 'gpu':\n            self.W = self.add_weight((self.input_dim, 4 * self.output_dim),\n                                     initializer=self.init,\n                                     name='{}_W'.format(self.name),\n                                     regularizer=self.W_regularizer)\n            self.U = self.add_weight((self.output_dim, 4 * self.output_dim),\n                                     initializer=self.inner_init,\n                                     name='{}_U'.format(self.name),\n                                     regularizer=self.U_regularizer)\n\n            def b_reg(shape, name=None):\n                return K.variable(np.hstack((np.zeros(self.output_dim),\n                                             K.get_value(self.forget_bias_init((self.output_dim,))),\n                                             np.zeros(self.output_dim),\n                                             np.zeros(self.output_dim))),\n                                  name='{}_b'.format(self.name))\n            self.b = self.add_weight((self.output_dim * 4,),\n                                     initializer=b_reg,\n                                     name='{}_b'.format(self.name),\n                                     regularizer=self.b_regularizer)\n        else:\n            self.W_i = self.add_weight((self.input_dim, self.output_dim),\n                                       initializer=self.init,\n                                       name='{}_W_i'.format(self.name),\n                                       regularizer=self.W_regularizer)\n            self.U_i = self.add_weight((self.output_dim, self.output_dim),\n                                       initializer=self.init,\n                                       name='{}_U_i'.format(self.name),\n                                       regularizer=self.W_regularizer)\n            self.b_i = self.add_weight((self.output_dim,),\n                                       initializer='zero',\n                                       name='{}_b_i'.format(self.name),\n                                       regularizer=self.b_regularizer)\n            self.W_f = self.add_weight((self.input_dim, self.output_dim),\n                                       initializer=self.init,\n                                       name='{}_W_f'.format(self.name),\n                                       regularizer=self.W_regularizer)\n            self.U_f = self.add_weight((self.output_dim, self.output_dim),\n                                       initializer=self.init,\n                                       name='{}_U_f'.format(self.name),\n                                       regularizer=self.W_regularizer)\n            self.b_f = self.add_weight((self.output_dim,),\n                                       initializer=self.forget_bias_init,\n                                       name='{}_b_f'.format(self.name),\n                                       regularizer=self.b_regularizer)\n            self.W_c = self.add_weight((self.input_dim, self.output_dim),\n                                       initializer=self.init,\n                                       name='{}_W_c'.format(self.name),\n                                       regularizer=self.W_regularizer)\n            self.U_c = self.add_weight((self.output_dim, self.output_dim),\n                                       initializer=self.init,\n                                       name='{}_U_c'.format(self.name),\n                                       regularizer=self.W_regularizer)\n            self.b_c = self.add_weight((self.output_dim,),\n                                       initializer='zero',\n                                       name='{}_b_c'.format(self.name),\n                                       regularizer=self.b_regularizer)\n            self.W_o = self.add_weight((self.input_dim, self.output_dim),\n                                       initializer=self.init,\n                                       name='{}_W_o'.format(self.name),\n                                       regularizer=self.W_regularizer)\n            self.U_o = self.add_weight((self.output_dim, self.output_dim),\n                                       initializer=self.init,\n                                       name='{}_U_o'.format(self.name),\n                                       regularizer=self.W_regularizer)\n            self.b_o = self.add_weight((self.output_dim,),\n                                       initializer='zero',\n                                       name='{}_b_o'.format(self.name),\n                                       regularizer=self.b_regularizer)\n\n            self.trainable_weights = [self.W_i, self.U_i, self.b_i,\n                                      self.W_c, self.U_c, self.b_c,\n                                      self.W_f, self.U_f, self.b_f,\n                                      self.W_o, self.U_o, self.b_o]\n            self.W = K.concatenate([self.W_i, self.W_f, self.W_c, self.W_o])\n            self.U = K.concatenate([self.U_i, self.U_f, self.U_c, self.U_o])\n            self.b = K.concatenate([self.b_i, self.b_f, self.b_c, self.b_o])\n\n        if self.initial_weights is not None:\n            self.set_weights(self.initial_weights)\n            del self.initial_weights\n        self.built = True\n\n    def reset_states(self):\n        assert self.stateful, 'Layer must be stateful.'\n        input_shape = self.input_spec[0].shape\n        if not input_shape[0]:\n            raise ValueError('If a RNN is stateful, a complete ' +\n                             'input_shape must be provided (including batch size).')\n        if hasattr(self, 'states'):\n            K.set_value(self.states[0],\n                        np.zeros((input_shape[0], self.output_dim)))\n            K.set_value(self.states[1],\n                        np.zeros((input_shape[0], self.output_dim)))\n        else:\n            self.states = [K.zeros((input_shape[0], self.output_dim)),\n                           K.zeros((input_shape[0], self.output_dim))]\n\n    def preprocess_input(self, x):\n        if self.consume_less == 'cpu':\n            if 0 < self.dropout_W < 1:\n                dropout = self.dropout_W\n            else:\n                dropout = 0\n            input_shape = K.int_shape(x)\n            input_dim = input_shape[2]\n            timesteps = input_shape[1]\n\n            x_i = time_distributed_dense(x, self.W_i, self.b_i, dropout,\n                                         input_dim, self.output_dim, timesteps)\n            x_f = time_distributed_dense(x, self.W_f, self.b_f, dropout,\n                                         input_dim, self.output_dim, timesteps)\n            x_c = time_distributed_dense(x, self.W_c, self.b_c, dropout,\n                                         input_dim, self.output_dim, timesteps)\n            x_o = time_distributed_dense(x, self.W_o, self.b_o, dropout,\n                                         input_dim, self.output_dim, timesteps)\n            return K.concatenate([x_i, x_f, x_c, x_o], axis=2)\n        else:\n            return x\n\n    def step(self, x, states):\n        h_tm1 = states[0]\n        c_tm1 = states[1]\n        B_U = states[2]\n        B_W = states[3]\n\n        if self.consume_less == 'gpu':\n            z = K.dot(x * B_W[0], self.W) + K.dot(h_tm1 * B_U[0], self.U) + self.b\n\n            z0 = z[:, :self.output_dim]\n            z1 = z[:, self.output_dim: 2 * self.output_dim]\n            z2 = z[:, 2 * self.output_dim: 3 * self.output_dim]\n            z3 = z[:, 3 * self.output_dim:]\n\n            i = self.inner_activation(z0)\n            f = self.inner_activation(z1)\n            c = f * c_tm1 + i * self.activation(z2)\n            o = self.inner_activation(z3)\n        else:\n            if self.consume_less == 'cpu':\n                x_i = x[:, :self.output_dim]\n                x_f = x[:, self.output_dim: 2 * self.output_dim]\n                x_c = x[:, 2 * self.output_dim: 3 * self.output_dim]\n                x_o = x[:, 3 * self.output_dim:]\n            elif self.consume_less == 'mem':\n                x_i = K.dot(x * B_W[0], self.W_i) + self.b_i\n                x_f = K.dot(x * B_W[1], self.W_f) + self.b_f\n                x_c = K.dot(x * B_W[2], self.W_c) + self.b_c\n                x_o = K.dot(x * B_W[3], self.W_o) + self.b_o\n            else:\n                raise ValueError('Unknown `consume_less` mode.')\n\n            i = self.inner_activation(x_i + K.dot(h_tm1 * B_U[0], self.U_i))\n            f = self.inner_activation(x_f + K.dot(h_tm1 * B_U[1], self.U_f))\n            c = f * c_tm1 + i * self.activation(x_c + K.dot(h_tm1 * B_U[2], self.U_c))\n            o = self.inner_activation(x_o + K.dot(h_tm1 * B_U[3], self.U_o))\n\n        h = o * self.activation(c)\n        return h, [h, c]\n\n    def get_constants(self, x):\n        constants = []\n        if 0 < self.dropout_U < 1:\n            ones = K.ones_like(K.reshape(x[:, 0, 0], (-1, 1)))\n            ones = K.tile(ones, (1, self.output_dim))\n            B_U = [K.in_train_phase(K.dropout(ones, self.dropout_U), ones) for _ in range(4)]\n            constants.append(B_U)\n        else:\n            constants.append([K.cast_to_floatx(1.) for _ in range(4)])\n\n        if 0 < self.dropout_W < 1:\n            input_shape = K.int_shape(x)\n            input_dim = input_shape[-1]\n            ones = K.ones_like(K.reshape(x[:, 0, 0], (-1, 1)))\n            ones = K.tile(ones, (1, int(input_dim)))\n            B_W = [K.in_train_phase(K.dropout(ones, self.dropout_W), ones) for _ in range(4)]\n            constants.append(B_W)\n        else:\n            constants.append([K.cast_to_floatx(1.) for _ in range(4)])\n        return constants\n\n    def get_config(self):\n        config = {'output_dim': self.output_dim,\n                  'init': initializers.get_config(self.init),\n                  'inner_init': initializers.get_config(self.inner_init),\n                  'forget_bias_init': initializers.get_config(self.forget_bias_init),\n                  'activation': self.activation.__name__,\n                  'inner_activation': self.inner_activation.__name__,\n                  'W_regularizer': self.W_regularizer.get_config() if self.W_regularizer else None,\n                  'U_regularizer': self.U_regularizer.get_config() if self.U_regularizer else None,\n                  'b_regularizer': self.b_regularizer.get_config() if self.b_regularizer else None,\n                  'dropout_W': self.dropout_W,\n                  'dropout_U': self.dropout_U}\n        base_config = super(LSTM, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n",
          "file_after": "# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import\nimport numpy as np\n\nfrom .. import backend as K\nfrom .. import activations\nfrom .. import initializers\nfrom .. import regularizers\nfrom .. import constraints\nfrom ..engine import Layer\nfrom ..engine import InputSpec\n\n\ndef _time_distributed_dense(x, w, b=None, dropout=None,\n                            input_dim=None, output_dim=None,\n                            timesteps=None, training=None):\n    \"\"\"Apply `y . w + b` for every temporal slice y of x.\n\n    # Arguments\n        x: input tensor.\n        w: weight matrix.\n        b: optional bias vector.\n        dropout: wether to apply dropout (same dropout mask\n            for every temporal slice of the input).\n        input_dim: integer; optional dimensionality of the input.\n        output_dim: integer; optional dimensionality of the output.\n        timesteps: integer; optional number of timesteps.\n        training: training phase tensor or boolean.\n\n    # Returns\n        Output tensor.\n    \"\"\"\n    if not input_dim:\n        input_dim = K.shape(x)[2]\n    if not timesteps:\n        timesteps = K.shape(x)[1]\n    if not output_dim:\n        output_dim = K.shape(w)[1]\n\n    if dropout is not None and 0. < dropout < 1.:\n        # apply the same dropout pattern at every timestep\n        ones = K.ones_like(K.reshape(x[:, 0, :], (-1, input_dim)))\n        dropout_matrix = K.dropout(ones, dropout)\n        expanded_dropout_matrix = K.repeat(dropout_matrix, timesteps)\n        x = K.in_train_phase(x * expanded_dropout_matrix, x, training=training)\n\n    # collapse time dimension and batch dimension together\n    x = K.reshape(x, (-1, input_dim))\n    x = K.dot(x, w)\n    if b is not None:\n        x += b\n    # reshape to 3D tensor\n    if K.backend() == 'tensorflow':\n        x = K.reshape(x, K.stack([-1, timesteps, output_dim]))\n        x.set_shape([None, None, output_dim])\n    else:\n        x = K.reshape(x, (-1, timesteps, output_dim))\n    return x\n\n\nclass Recurrent(Layer):\n    \"\"\"Abstract base class for recurrent layers.\n    Do not use in a model -- it's not a valid layer!\n    Use its children classes `LSTM`, `GRU` and `SimpleRNN` instead.\n\n    All recurrent layers (`LSTM`, `GRU`, `SimpleRNN`) also\n    follow the specifications of this class and accept\n    the keyword arguments listed below.\n\n    # Example\n\n    ```python\n        # as the first layer in a Sequential model\n        model = Sequential()\n        model.add(LSTM(32, input_shape=(10, 64)))\n        # now model.output_shape == (None, 32)\n        # note: `None` is the batch dimension.\n\n        # the following is identical:\n        model = Sequential()\n        model.add(LSTM(32, input_dim=64, input_length=10))\n\n        # for subsequent layers, not need to specify the input size:\n        model.add(LSTM(16))\n    ```\n\n    # Arguments\n        weights: list of Numpy arrays to set as initial weights.\n            The list should have 3 elements, of shapes:\n            `[(input_dim, output_dim), (output_dim, output_dim), (output_dim,)]`.\n        return_sequences: Boolean. Whether to return the last output\n            in the output sequence, or the full sequence.\n        go_backwards: Boolean (default False).\n            If True, process the input sequence backwards.\n        stateful: Boolean (default False). If True, the last state\n            for each sample at index i in a batch will be used as initial\n            state for the sample of index i in the following batch.\n        unroll: Boolean (default False).\n            If True, the network will be unrolled,\n            else a symbolic loop will be used.\n            Unrolling can speed-up a RNN,\n            although it tends to be more memory-intensive.\n            Unrolling is only suitable for short sequences.\n        implementation: one of {0, 1, or 2}.\n            If set to 0, the RNN will use\n            an implementation that uses fewer, larger matrix products,\n            thus running faster on CPU but consuming more memory.\n            If set to 1, the RNN will use more matrix products,\n            but smaller ones, thus running slower\n            (may actually be faster on GPU) while consuming less memory.\n            If set to 2 (LSTM/GRU only),\n            the RNN will combine the input gate,\n            the forget gate and the output gate into a single matrix,\n            enabling more time-efficient parallelization on the GPU.\n            Note: RNN dropout must be shared for all gates,\n            resulting in a slightly reduced regularization.\n        input_dim: dimensionality of the input (integer).\n            This argument (or alternatively, the keyword argument `input_shape`)\n            is required when using this layer as the first layer in a model.\n        input_length: Length of input sequences, to be specified\n            when it is constant.\n            This argument is required if you are going to connect\n            `Flatten` then `Dense` layers upstream\n            (without it, the shape of the dense outputs cannot be computed).\n            Note that if the recurrent layer is not the first layer\n            in your model, you would need to specify the input length\n            at the level of the first layer\n            (e.g. via the `input_shape` argument)\n\n    # Input shape\n        3D tensor with shape `(batch_size, timesteps, input_dim)`.\n\n    # Output shape\n        - if `return_sequences`: 3D tensor with shape\n            `(batch_size, timesteps, units)`.\n        - else, 2D tensor with shape `(batch_size, units)`.\n\n    # Masking\n        This layer supports masking for input data with a variable number\n        of timesteps. To introduce masks to your data,\n        use an [Embedding](embeddings.md) layer with the `mask_zero` parameter\n        set to `True`.\n\n    # Note on performance\n        You are likely to see better performance with RNNs in Theano compared\n        to TensorFlow. Additionally, when using TensorFlow, it is often\n        preferable to set `unroll=True` for better performance.\n\n    # Note on using statefulness in RNNs\n        You can set RNN layers to be 'stateful', which means that the states\n        computed for the samples in one batch will be reused as initial states\n        for the samples in the next batch. This assumes a one-to-one mapping\n        between samples in different successive batches.\n\n        To enable statefulness:\n            - specify `stateful=True` in the layer constructor.\n            - specify a fixed batch size for your model, by passing\n                if sequential model:\n                  `batch_input_shape=(...)` to the first layer in your model.\n                else for functional model with 1 or more Input layers:\n                  `batch_shape=(...)` to all the first layers in your model.\n                This is the expected shape of your inputs\n                *including the batch size*.\n                It should be a tuple of integers, e.g. `(32, 10, 100)`.\n            - specify `shuffle=False` when calling fit().\n\n        To reset the states of your model, call `.reset_states()` on either\n        a specific layer, or on your entire model.\n    \"\"\"\n\n    def __init__(self, return_sequences=False,\n                 go_backwards=False,\n                 stateful=False,\n                 unroll=False,\n                 implementation=0,\n                 **kwargs):\n        super(Recurrent, self).__init__(**kwargs)\n        self.return_sequences = return_sequences\n        self.go_backwards = go_backwards\n        self.stateful = stateful\n        self.unroll = unroll\n        self.implementation = 0\n        self.supports_masking = True\n        self.input_spec = InputSpec(ndim=3)\n\n    def get_output_shape_for(self, input_shape):\n        if self.return_sequences:\n            return (input_shape[0], input_shape[1], self.units)\n        else:\n            return (input_shape[0], self.units)\n\n    def compute_mask(self, inputs, mask):\n        if self.return_sequences:\n            return mask\n        else:\n            return None\n\n    def step(self, inputs, states):\n        raise NotImplementedError\n\n    def get_constants(self, inputs, training=None):\n        return []\n\n    def get_initial_states(self, inputs):\n        # build an all-zero tensor of shape (samples, output_dim)\n        initial_state = K.zeros_like(inputs)  # (samples, timesteps, input_dim)\n        initial_state = K.sum(initial_state, axis=(1, 2))  # (samples,)\n        initial_state = K.expand_dims(initial_state)  # (samples, 1)\n        initial_state = K.tile(initial_state, [1, self.units])  # (samples, output_dim)\n        initial_states = [initial_state for _ in range(len(self.states))]\n        return initial_states\n\n    def preprocess_input(self, inputs, training=None):\n        return inputs\n\n    def call(self, inputs, mask=None, training=None):\n        # input shape: (nbias_samples, time (padded with zeros), input_dim)\n        # note that the .build() method of subclasses MUST define\n        # self.input_spec with a complete input shape.\n        input_shape = K.int_shape(inputs)\n        if self.unroll and input_shape[1] is None:\n            raise ValueError('Cannot unroll a RNN if the '\n                             'time dimension is undefined. \\n'\n                             '- If using a Sequential model, '\n                             'specify the time dimension by passing '\n                             'an `input_shape` or `batch_input_shape` '\n                             'argument to your first layer. If your '\n                             'first layer is an Embedding, you can '\n                             'also use the `input_length` argument.\\n'\n                             '- If using the functional API, specify '\n                             'the time dimension by passing a `shape` '\n                             'or `batch_shape` argument to your Input layer.')\n        if self.stateful:\n            initial_states = self.states\n        else:\n            initial_states = self.get_initial_states(inputs)\n        constants = self.get_constants(inputs, training=None)\n        preprocessed_input = self.preprocess_input(inputs, training=None)\n        last_output, outputs, states = K.rnn(self.step,\n                                             preprocessed_input,\n                                             initial_states,\n                                             go_backwards=self.go_backwards,\n                                             mask=mask,\n                                             constants=constants,\n                                             unroll=self.unroll,\n                                             input_length=input_shape[1])\n        if self.stateful:\n            updates = []\n            for i in range(len(states)):\n                updates.append((self.states[i], states[i]))\n            self.add_update(updates, inputs)\n\n        if self.return_sequences:\n            return outputs\n        else:\n            return last_output\n\n    def get_config(self):\n        config = {'return_sequences': self.return_sequences,\n                  'go_backwards': self.go_backwards,\n                  'stateful': self.stateful,\n                  'unroll': self.unroll,\n                  'implementation': self.implementation}\n        base_config = super(Recurrent, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\n\nclass SimpleRNN(Recurrent):\n    \"\"\"Fully-connected RNN where the output is to be fed back to input.\n\n    # Arguments\n        units: Positive integer, dimensionality of the output space.\n        activation: Activation function to use\n            (see [activations](../activations.md)).\n            If you don't specify anything, no activation is applied\n            (ie. \"linear\" activation: `a(x) = x`).\n        use_bias: Boolean, whether the layer uses a bias vector.\n        kernel_initializer: Initializer for the `kernel` weights matrix,\n            used for the linear transformation of the inputs.\n            (see [initializers](../initializers.md)).\n        recurrent_initializer: Initializer for the `recurrent_kernel`\n            weights matrix,\n            used for the linear transformation of the recurrent state.\n            (see [initializers](../initializers.md)).\n        bias_initializer: Initializer for the bias vector\n            (see [initializers](../initializers.md)).\n        kernel_regularizer: Regularizer function applied to\n            the `kernel` weights matrix\n            (see [regularizer](../regularizers.md)).\n        recurrent_regularizer: Regularizer function applied to\n            the `recurrent_kernel` weights matrix\n            (see [regularizer](../regularizers.md)).\n        bias_regularizer: Regularizer function applied to the bias vector\n            (see [regularizer](../regularizers.md)).\n        activity_regularizer: Regularizer function applied to\n            the output of the layer (its \"activation\").\n            (see [regularizer](../regularizers.md)).\n        kernel_constraint: Constraint function applied to\n            the `kernel` weights matrix\n            (see [constraints](../constraints.md)).\n        recurrent_constraint: Constraint function applied to\n            the `recurrent_kernel` weights matrix\n            (see [constraints](../constraints.md)).\n        bias_constraint: Constraint function applied to the bias vector\n            (see [constraints](../constraints.md)).\n        dropout: Float between 0 and 1.\n            Fraction of the units to drop for\n            the linear transformation of the inputs.\n        recurrent_dropout: Float between 0 and 1.\n            Fraction of the units to drop for\n            the linear transformation of the recurrent state.\n\n    # References\n        - [A Theoretically Grounded Application of Dropout in Recurrent Neural Networks](http://arxiv.org/abs/1512.05287)\n    \"\"\"\n\n    def __init__(self, units,\n                 activation='tanh',\n                 use_bias=True,\n                 kernel_initializer='glorot_uniform',\n                 recurrent_initializer='orthogonal',\n                 bias_initializer='zeros',\n                 kernel_regularizer=None,\n                 recurrent_regularizer=None,\n                 bias_regularizer=None,\n                 activity_regularizer=None,\n                 kernel_constraint=None,\n                 recurrent_constraint=None,\n                 bias_constraint=None,\n                 dropout=0.,\n                 recurrent_dropout=0.,\n                 **kwargs):\n        super(SimpleRNN, self).__init__(**kwargs)\n        self.units = units\n        self.activation = activations.get(activation)\n        self.use_bias = use_bias\n\n        self.kernel_initializer = initializers.get(kernel_initializer)\n        self.recurrent_initializer = initializers.get(recurrent_initializer)\n        self.bias_initializer = initializers.get(bias_initializer)\n\n        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n        self.recurrent_regularizer = regularizers.get(recurrent_regularizer)\n        self.bias_regularizer = regularizers.get(bias_regularizer)\n\n        self.kernel_constraint = constraints.get(kernel_constraint)\n        self.recurrent_constraint = constraints.get(recurrent_constraint)\n        self.bias_constraint = constraints.get(bias_constraint)\n\n        self.dropout = min(1., max(0., dropout))\n        self.recurrent_dropout = min(1., max(0., recurrent_dropout))\n\n    def build(self, input_shape):\n        # TODO: handle variable-length seqs in input_spec\n        self.input_spec = InputSpec(shape=input_shape)\n        if self.stateful:\n            self.reset_states()\n        else:\n            # Initial states: all-zero tensor of shape (output_dim).\n            self.states = [None]\n        input_dim = input_shape[2]\n        self.input_dim = input_dim\n\n        self.kernel = self.add_weight((input_dim, self.units),\n                                      name='kernel',\n                                      initializer=self.kernel_initializer,\n                                      regularizer=self.kernel_regularizer,\n                                      constraint=self.kernel_constraint)\n        self.recurrent_kernel = self.add_weight(\n            (self.units, self.units),\n            name='recurrent_kernel',\n            initializer=self.recurrent_initializer,\n            regularizer=self.recurrent_regularizer,\n            constraint=self.recurrent_constraint)\n        if self.use_bias:\n            self.bias = self.add_weight((self.units,),\n                                        name='bias',\n                                        initializer=self.bias_initializer,\n                                        regularizer=self.bias_regularizer,\n                                        constraint=self.bias_constraint)\n        else:\n            self.bias = None\n\n    def reset_states(self):\n        if not self.stateful:\n            raise AttributeError('Layer must be stateful.')\n        input_shape = self.input_spec[0].shape\n        if not input_shape[0]:\n            raise ValueError('If a RNN is stateful, it needs to know '\n                             'its batch size. Specify the batch size '\n                             'of your input tensors: \\n'\n                             '- If using a Sequential model, '\n                             'specify the batch size by passing '\n                             'a `batch_input_shape` '\n                             'argument to your first layer.\\n'\n                             '- If using the functional API, specify '\n                             'the time dimension by passing a '\n                             '`batch_shape` argument to your Input layer.')\n        if hasattr(self, 'states'):\n            K.set_value(self.states[0],\n                        np.zeros((input_shape[0], self.units)))\n        else:\n            self.states = [K.zeros((input_shape[0], self.units))]\n\n    def preprocess_input(self, inputs, training=None):\n        if self.implementation > 0:\n            return inputs\n        else:\n            input_shape = K.int_shape(inputs)\n            input_dim = input_shape[2]\n            timesteps = input_shape[1]\n            return _time_distributed_dense(inputs,\n                                           self.kernel,\n                                           self.bias,\n                                           self.dropout,\n                                           input_dim,\n                                           self.units,\n                                           timesteps,\n                                           training=training)\n\n    def step(self, inputs, states):\n        if self.implementation == 0:\n            h = inputs\n        else:\n            if 0 < self.dropout < 1:\n                h = K.dot(inputs * states[1], self.kernel)\n            else:\n                h = K.dot(inputs, self.kernel)\n            if self.bias is not None:\n                h += self.bias\n\n        prev_output = states[0]\n        if 0 < self.recurrent_dropout < 1:\n            prev_output *= states[2]\n        output = h + K.dot(prev_output, self.recurrent_kernel)\n        if self.activation is not None:\n            output = self.activation(output)\n\n        # Properly set learning phase on output tensor.\n        if 0 < self.dropout + self.recurrent_dropout:\n            uses_learning_phase = False\n            if getattr(states[0], '_uses_learning_phase', False):\n                uses_learning_phase = True\n            if getattr(states[1], '_uses_learning_phase', False):\n                uses_learning_phase = True\n            output._uses_learning_phase = uses_learning_phase\n        return output, [output]\n\n    def get_constants(self, inputs, training=None):\n        constants = []\n        if self.implementation == 0 and 0 < self.dropout < 1:\n            input_shape = K.int_shape(inputs)\n            input_dim = input_shape[-1]\n            ones = K.ones_like(K.reshape(inputs[:, 0, 0], (-1, 1)))\n            ones = K.tile(ones, (1, int(input_dim)))\n\n            def dropped_inputs():\n                return K.dropout(ones, self.dropout)\n\n            dp_mask = K.in_train_phase(dropped_inputs,\n                                       ones,\n                                       training=training)\n            constants.append(dp_mask)\n        else:\n            constants.append(K.cast_to_floatx(1.))\n\n        if 0 < self.recurrent_dropout < 1:\n            ones = K.ones_like(K.reshape(inputs[:, 0, 0], (-1, 1)))\n            ones = K.tile(ones, (1, self.units))\n\n            def dropped_inputs():\n                return K.dropout(ones, self.recurrent_dropout)\n            rec_dp_mask = K.in_train_phase(dropped_inputs,\n                                           ones,\n                                           training=training)\n            constants.append(rec_dp_mask)\n        else:\n            constants.append(K.cast_to_floatx(1.))\n        return constants\n\n    def get_config(self):\n        config = {'units': self.units,\n                  'activation': activations.serialize(self.activation),\n                  'use_bias': self.use_bias,\n                  'kernel_initializer': initializers.serialize(self.kernel_initializer),\n                  'recurrent_initializer': initializers.serialize(self.recurrent_initializer),\n                  'bias_initializer': initializers.serialize(self.bias_initializer),\n                  'kernel_regularizer': regularizers.serialize(kernel_regularizer),\n                  'recurrent_regularizer': regularizers.serialize(recurrent_regularizer),\n                  'bias_regularizer': regularizers.serialize(bias_regularizer),\n                  'kernel_constraint': constraints.serialize(self.kernel_constraint),\n                  'recurrent_constraint': constraints.serialize(self.recurrent_constraint),\n                  'bias_constraint': constraints.serialize(self.bias_constraint),\n                  'dropout': self.dropout,\n                  'recurrent_dropout': self.recurrent_dropout}\n        base_config = super(SimpleRNN, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\n\nclass GRU(Recurrent):\n    \"\"\"Gated Recurrent Unit - Cho et al. 2014.\n\n    # Arguments\n        units: Positive integer, dimensionality of the output space.\n        activation: Activation function to use\n            (see [activations](../activations.md)).\n            If you don't specify anything, no activation is applied\n            (ie. \"linear\" activation: `a(x) = x`).\n        use_bias: Boolean, whether the layer uses a bias vector.\n        kernel_initializer: Initializer for the `kernel` weights matrix,\n            used for the linear transformation of the inputs.\n            (see [initializers](../initializers.md)).\n        recurrent_initializer: Initializer for the `recurrent_kernel`\n            weights matrix,\n            used for the linear transformation of the recurrent state.\n            (see [initializers](../initializers.md)).\n        bias_initializer: Initializer for the bias vector\n            (see [initializers](../initializers.md)).\n        kernel_regularizer: Regularizer function applied to\n            the `kernel` weights matrix\n            (see [regularizer](../regularizers.md)).\n        recurrent_regularizer: Regularizer function applied to\n            the `recurrent_kernel` weights matrix\n            (see [regularizer](../regularizers.md)).\n        bias_regularizer: Regularizer function applied to the bias vector\n            (see [regularizer](../regularizers.md)).\n        activity_regularizer: Regularizer function applied to\n            the output of the layer (its \"activation\").\n            (see [regularizer](../regularizers.md)).\n        kernel_constraint: Constraint function applied to\n            the `kernel` weights matrix\n            (see [constraints](../constraints.md)).\n        recurrent_constraint: Constraint function applied to\n            the `recurrent_kernel` weights matrix\n            (see [constraints](../constraints.md)).\n        bias_constraint: Constraint function applied to the bias vector\n            (see [constraints](../constraints.md)).\n        dropout: Float between 0 and 1.\n            Fraction of the units to drop for\n            the linear transformation of the inputs.\n        recurrent_dropout: Float between 0 and 1.\n            Fraction of the units to drop for\n            the linear transformation of the recurrent state.\n\n    # References\n        - [On the Properties of Neural Machine Translation: Encoder-Decoder Approaches](https://arxiv.org/abs/1409.1259)\n        - [Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling](http://arxiv.org/abs/1412.3555v1)\n        - [A Theoretically Grounded Application of Dropout in Recurrent Neural Networks](http://arxiv.org/abs/1512.05287)\n    \"\"\"\n\n    def __init__(self, units,\n                 activation='tanh',\n                 recurrent_activation='hard_sigmoid',\n                 use_bias=True,\n                 kernel_initializer='glorot_uniform',\n                 recurrent_initializer='orthogonal',\n                 bias_initializer='zeros',\n                 kernel_regularizer=None,\n                 recurrent_regularizer=None,\n                 bias_regularizer=None,\n                 activity_regularizer=None,\n                 kernel_constraint=None,\n                 recurrent_constraint=None,\n                 bias_constraint=None,\n                 dropout=0.,\n                 recurrent_dropout=0.,\n                 **kwargs):\n        super(GRU, self).__init__(**kwargs)\n        self.units = units\n        self.activation = activations.get(activation)\n        self.recurrent_activation = activations.get(recurrent_activation)\n        self.use_bias = use_bias\n\n        self.kernel_initializer = initializers.get(kernel_initializer)\n        self.recurrent_initializer = initializers.get(recurrent_initializer)\n        self.bias_initializer = initializers.get(bias_initializer)\n\n        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n        self.recurrent_regularizer = regularizers.get(recurrent_regularizer)\n        self.bias_regularizer = regularizers.get(bias_regularizer)\n\n        self.kernel_constraint = constraints.get(kernel_constraint)\n        self.recurrent_constraint = constraints.get(recurrent_constraint)\n        self.bias_constraint = constraints.get(bias_constraint)\n\n        self.dropout = min(1., max(0., dropout))\n        self.recurrent_dropout = min(1., max(0., recurrent_dropout))\n\n    def build(self, input_shape):\n        # TODO: handle variable-length sequences in input spec.\n        self.input_spec = InputSpec(shape=input_shape)\n        self.input_dim = input_shape[2]\n\n        if self.stateful:\n            self.reset_states()\n        else:\n            # initial states: all-zero tensor of shape (output_dim)\n            self.states = [None]\n\n        self.kernel = self.add_weight((self.input_dim, self.units * 3),\n                                      name='kernel',\n                                      initializer=self.kernel_initializer,\n                                      regularizer=self.kernel_regularizer,\n                                      constraint=self.kernel_constraint)\n        self.recurrent_kernel = self.add_weight(\n            (self.units, self.units * 3),\n            name='recurrent_kernel',\n            initializer=self.recurrent_initializer,\n            regularizer=self.recurrent_regularizer,\n            constraint=self.recurrent_constraint)\n\n        if self.use_bias:\n            self.bias = self.add_weight((self.units * 3,),\n                                        name='bias',\n                                        initializer='zero',\n                                        regularizer=self.bias_regularizer,\n                                        constraint=self.bias_constraint)\n        else:\n            self.bias = None\n\n        self.kernel_z = self.kernel[:, :self.units]\n        self.recurrent_kernel_z = self.recurrent_kernel[:, :self.units]\n        self.kernel_r = self.kernel[:, self.units: self.units * 2]\n        self.recurrent_kernel_r = self.recurrent_kernel[:,\n                                                        self.units:\n                                                        self.units * 2]\n        self.kernel_h = self.kernel[:, self.units * 2:]\n        self.recurrent_kernel_h = self.recurrent_kernel[:, self.units * 2:]\n\n        if self.use_bias:\n            self.bias_z = self.bias[:self.units]\n            self.bias_r = self.bias[self.units: self.units * 2]\n            self.bias_h = self.bias[self.units * 2:]\n        else:\n            self.bias_z = None\n            self.bias_r = None\n            self.bias_h = None\n\n    def reset_states(self):\n        if not self.stateful:\n            raise RuntimeError('Layer must be stateful.')\n        input_shape = self.input_spec[0].shape\n        if not input_shape[0]:\n            raise ValueError('If a RNN is stateful, a complete '\n                             'input_shape must be provided '\n                             '(including batch size).')\n        if hasattr(self, 'states'):\n            K.set_value(self.states[0],\n                        np.zeros((input_shape[0], self.units)))\n        else:\n            self.states = [K.zeros((input_shape[0], self.units))]\n\n    def preprocess_input(self, inputs, training=None):\n        if self.implementation == 0:\n            input_shape = K.int_shape(inputs)\n            input_dim = input_shape[2]\n            timesteps = input_shape[1]\n\n            x_z = _time_distributed_dense(inputs, self.kernel_z, self.bias_z,\n                                          self.dropout, input_dim, self.units,\n                                          timesteps, training=training)\n            x_r = _time_distributed_dense(inputs, self.kernel_r, self.bias_r,\n                                          self.dropout, input_dim, self.units,\n                                          timesteps, training=training)\n            x_h = _time_distributed_dense(inputs, self.kernel_h, self.bias_h,\n                                          self.dropout, input_dim, self.units,\n                                          timesteps, training=training)\n            return K.concatenate([x_z, x_r, x_h], axis=2)\n        else:\n            return inputs\n\n    def get_constants(self, inputs, training=None):\n        constants = []\n        if self.implementation == 0 and 0 < self.dropout < 1:\n            input_shape = K.int_shape(inputs)\n            input_dim = input_shape[-1]\n            ones = K.ones_like(K.reshape(inputs[:, 0, 0], (-1, 1)))\n            ones = K.tile(ones, (1, int(input_dim)))\n\n            def dropped_inputs():\n                return K.dropout(ones, self.dropout)\n\n            dp_mask = [K.in_train_phase(dropped_inputs,\n                                        ones,\n                                        training=training) for _ in range(3)]\n            constants.append(dp_mask)\n        else:\n            constants.append([K.cast_to_floatx(1.) for _ in range(3)])\n\n        if 0 < self.recurrent_dropout < 1:\n            ones = K.ones_like(K.reshape(inputs[:, 0, 0], (-1, 1)))\n            ones = K.tile(ones, (1, self.units))\n\n            def dropped_inputs():\n                return K.dropout(ones, self.recurrent_dropout)\n            rec_dp_mask = [K.in_train_phase(dropped_inputs,\n                                            ones,\n                                            training=training) for _ in range(3)]\n            constants.append(rec_dp_mask)\n        else:\n            constants.append([K.cast_to_floatx(1.) for _ in range(3)])\n        return constants\n\n    def step(self, inputs, states):\n        h_tm1 = states[0]  # previous memory\n        dp_mask = states[1]  # dropout matrices for recurrent units\n        rec_dp_mask = states[2]\n\n        if self.implementation == 1:\n            matrix_x = K.dot(inputs * dp_mask[0], self.kernel)\n            if self.use_bias:\n                matrix_x += self.bias\n            matrix_inner = K.dot(h_tm1 * rec_dp_mask[0],\n                                 self.recurrent_kernel[:, :2 * self.units])\n\n            x_z = matrix_x[:, :self.units]\n            x_r = matrix_x[:, self.units: 2 * self.units]\n            inner_z = matrix_inner[:, :self.units]\n            inner_r = matrix_inner[:, self.units: 2 * self.units]\n\n            z = self.recurrent_activation(x_z + inner_z)\n            r = self.recurrent_activation(x_r + inner_r)\n\n            x_h = matrix_x[:, 2 * self.units:]\n            inner_h = K.dot(r * h_tm1 * rec_dp_mask[0],\n                            self.recurrent_kernel[:, 2 * self.units:])\n            hh = self.activation(x_h + inner_h)\n        else:\n            if self.implementation == 0:\n                x_z = inputs[:, :self.units]\n                x_r = inputs[:, self.units: 2 * self.units]\n                x_h = inputs[:, 2 * self.units:]\n            elif self.implementation == 2:\n                x_z = K.dot(inputs * dp_mask[0], self.kernel_z)\n                x_r = K.dot(inputs * dp_mask[1], self.kernel_r)\n                x_h = K.dot(inputs * dp_mask[2], self.kernel_h)\n                if self.use_bias:\n                    x_z += self.bias_z\n                    x_r += self.bias_r\n                    x_h += self.bias_h\n            else:\n                raise ValueError('Unknown `implementation` mode.')\n            z = self.recurrent_activation(x_z + K.dot(h_tm1 * rec_dp_mask[0],\n                                                      self.recurrent_kernel_z))\n            r = self.recurrent_activation(x_r + K.dot(h_tm1 * rec_dp_mask[1],\n                                                      self.recurrent_kernel_r))\n\n            hh = self.activation(x_h + K.dot(r * h_tm1 * rec_dp_mask[2],\n                                             self.recurrent_kernel_h))\n        h = z * h_tm1 + (1 - z) * hh\n        return h, [h]\n\n    def get_config(self):\n        config = {'units': self.units,\n                  'activation': activations.serialize(self.activation),\n                  'recurrent_activation': activations.serialize(self.recurrent_activation),\n                  'use_bias': self.use_bias,\n                  'kernel_initializer': initializers.serialize(self.kernel_initializer),\n                  'recurrent_initializer': initializers.serialize(self.recurrent_initializer),\n                  'bias_initializer': initializers.serialize(self.bias_initializer),\n                  'kernel_regularizer': regularizers.serialize(kernel_regularizer),\n                  'recurrent_regularizer': regularizers.serialize(recurrent_regularizer),\n                  'bias_regularizer': regularizers.serialize(bias_regularizer),\n                  'kernel_constraint': constraints.serialize(self.kernel_constraint),\n                  'recurrent_constraint': constraints.serialize(self.recurrent_constraint),\n                  'bias_constraint': constraints.serialize(self.bias_constraint),\n                  'dropout': self.dropout,\n                  'recurrent_dropout': self.recurrent_dropout}\n        base_config = super(GRU, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\n\nclass LSTM(Recurrent):\n    \"\"\"Long-Short Term Memory unit - Hochreiter 1997.\n\n    For a step-by-step description of the algorithm, see\n    [this tutorial](http://deeplearning.net/tutorial/lstm.html).\n\n    # Arguments\n        units: Positive integer, dimensionality of the output space.\n        activation: Activation function to use\n            (see [activations](../activations.md)).\n            If you don't specify anything, no activation is applied\n            (ie. \"linear\" activation: `a(x) = x`).\n        use_bias: Boolean, whether the layer uses a bias vector.\n        kernel_initializer: Initializer for the `kernel` weights matrix,\n            used for the linear transformation of the inputs.\n            (see [initializers](../initializers.md)).\n        recurrent_initializer: Initializer for the `recurrent_kernel`\n            weights matrix,\n            used for the linear transformation of the recurrent state.\n            (see [initializers](../initializers.md)).\n        bias_initializer: Initializer for the bias vector\n            (see [initializers](../initializers.md)).\n        unit_forget_bias: Boolean.\n            If True, add 1 to the bias of the forget gate at initialization.\n            Use in combination with `bias_initializer=\"zeros\"`.\n            This is recommended in [Jozefowicz et al.](http://www.jmlr.org/proceedings/papers/v37/jozefowicz15.pdf)\n        kernel_regularizer: Regularizer function applied to\n            the `kernel` weights matrix\n            (see [regularizer](../regularizers.md)).\n        recurrent_regularizer: Regularizer function applied to\n            the `recurrent_kernel` weights matrix\n            (see [regularizer](../regularizers.md)).\n        bias_regularizer: Regularizer function applied to the bias vector\n            (see [regularizer](../regularizers.md)).\n        activity_regularizer: Regularizer function applied to\n            the output of the layer (its \"activation\").\n            (see [regularizer](../regularizers.md)).\n        kernel_constraint: Constraint function applied to\n            the `kernel` weights matrix\n            (see [constraints](../constraints.md)).\n        recurrent_constraint: Constraint function applied to\n            the `recurrent_kernel` weights matrix\n            (see [constraints](../constraints.md)).\n        bias_constraint: Constraint function applied to the bias vector\n            (see [constraints](../constraints.md)).\n        dropout: Float between 0 and 1.\n            Fraction of the units to drop for\n            the linear transformation of the inputs.\n        recurrent_dropout: Float between 0 and 1.\n            Fraction of the units to drop for\n            the linear transformation of the recurrent state.\n\n    # References\n        - [Long short-term memory](http://deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf) (original 1997 paper)\n        - [Learning to forget: Continual prediction with LSTM](http://www.mitpressjournals.org/doi/pdf/10.1162/089976600300015015)\n        - [Supervised sequence labeling with recurrent neural networks](http://www.cs.toronto.edu/~graves/preprint.pdf)\n        - [A Theoretically Grounded Application of Dropout in Recurrent Neural Networks](http://arxiv.org/abs/1512.05287)\n    \"\"\"\n\n    def __init__(self, units,\n                 activation='tanh',\n                 recurrent_activation='hard_sigmoid',\n                 use_bias=True,\n                 kernel_initializer='glorot_uniform',\n                 recurrent_initializer='orthogonal',\n                 bias_initializer='zeros',\n                 unit_forget_bias=True,\n                 kernel_regularizer=None,\n                 recurrent_regularizer=None,\n                 bias_regularizer=None,\n                 activity_regularizer=None,\n                 kernel_constraint=None,\n                 recurrent_constraint=None,\n                 bias_constraint=None,\n                 dropout=0.,\n                 recurrent_dropout=0.,\n                 **kwargs):\n        super(LSTM, self).__init__(**kwargs)\n        self.units = units\n        self.activation = activations.get(activation)\n        self.recurrent_activation = activations.get(recurrent_activation)\n        self.use_bias = use_bias\n\n        self.kernel_initializer = initializers.get(kernel_initializer)\n        self.recurrent_initializer = initializers.get(recurrent_initializer)\n        self.bias_initializer = initializers.get(bias_initializer)\n        self.unit_forget_bias = unit_forget_bias\n\n        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n        self.recurrent_regularizer = regularizers.get(recurrent_regularizer)\n        self.bias_regularizer = regularizers.get(bias_regularizer)\n\n        self.kernel_constraint = constraints.get(kernel_constraint)\n        self.recurrent_constraint = constraints.get(recurrent_constraint)\n        self.bias_constraint = constraints.get(bias_constraint)\n\n        self.dropout = min(1., max(0., dropout))\n        self.recurrent_dropout = min(1., max(0., recurrent_dropout))\n\n    def build(self, input_shape):\n        self.input_spec = [InputSpec(shape=input_shape)]\n        self.input_dim = input_shape[2]\n\n        if self.stateful:\n            self.reset_states()\n        else:\n            # initial states: 2 all-zero tensors of shape (output_dim)\n            self.states = [None, None]\n\n        self.kernel = self.add_weight((self.input_dim, self.units * 4),\n                                      name='kernel',\n                                      initializer=self.kernel_initializer,\n                                      regularizer=self.kernel_regularizer,\n                                      constraint=self.kernel_constraint)\n        self.recurrent_kernel = self.add_weight(\n            (self.units, self.units * 4),\n            name='recurrent_kernel',\n            initializer=self.recurrent_initializer,\n            regularizer=self.recurrent_regularizer,\n            constraint=self.recurrent_constraint)\n\n        if self.use_bias:\n            self.bias = self.add_weight((self.units * 4,),\n                                        name='bias',\n                                        initializer=self.bias_initializer,\n                                        regularizer=self.bias_regularizer,\n                                        constraint=self.bias_constraint)\n            if self.unit_forget_bias:\n                self.bias += K.reshape(K.ones((self.units,)),\n                                       (1, self.units, 1, 1))\n        else:\n            self.bias = None\n\n        self.kernel_i = self.kernel[:, :self.units]\n        self.recurrent_kernel_i = self.recurrent_kernel[:, :self.units]\n        self.kernel_f = self.kernel[:, self.units: self.units * 2]\n        self.recurrent_kernel_f = self.recurrent_kernel[:, self.units: self.units * 2]\n        self.kernel_c = self.kernel[:, self.units * 2: self.units * 3]\n        self.recurrent_kernel_c = self.recurrent_kernel[:, self.units * 2: self.units * 3]\n        self.kernel_o = self.kernel[:, self.units * 3:]\n        self.recurrent_kernel_o = self.recurrent_kernel[:, self.units * 3:]\n\n        if self.use_bias:\n            self.bias_i = self.bias[:self.units]\n            self.bias_f = self.bias[self.units: self.units * 2]\n            self.bias_c = self.bias[self.units * 2: self.units * 3]\n            self.bias_o = self.bias[self.units * 3:]\n        else:\n            self.bias_i = None\n            self.bias_f = None\n            self.bias_c = None\n            self.bias_o = None\n\n    def reset_states(self):\n        assert self.stateful, 'Layer must be stateful.'\n        input_shape = self.input_spec[0].shape\n        if not input_shape[0]:\n            raise ValueError('If a RNN is stateful, a complete ' +\n                             'input_shape must be provided '\n                             '(including batch size).')\n        if hasattr(self, 'states'):\n            K.set_value(self.states[0],\n                        np.zeros((input_shape[0], self.units)))\n            K.set_value(self.states[1],\n                        np.zeros((input_shape[0], self.units)))\n        else:\n            self.states = [K.zeros((input_shape[0], self.units)),\n                           K.zeros((input_shape[0], self.units))]\n\n    def preprocess_input(self, inputs, training=None):\n        if self.implementation == 0:\n            input_shape = K.int_shape(inputs)\n            input_dim = input_shape[2]\n            timesteps = input_shape[1]\n\n            x_i = _time_distributed_dense(inputs, self.kernel_i, self.bias_i,\n                                          self.dropout, input_dim, self.units,\n                                          timesteps, training=training)\n            x_f = _time_distributed_dense(inputs, self.kernel_f, self.bias_f,\n                                          self.dropout, input_dim, self.units,\n                                          timesteps, training=training)\n            x_c = _time_distributed_dense(inputs, self.kernel_c, self.bias_c,\n                                          self.dropout, input_dim, self.units,\n                                          timesteps, training=training)\n            x_o = _time_distributed_dense(inputs, self.kernel_o, self.bias_o,\n                                          self.dropout, input_dim, self.units,\n                                          timesteps, training=training)\n            return K.concatenate([x_i, x_f, x_c, x_o], axis=2)\n        else:\n            return inputs\n\n    def get_constants(self, inputs, training=None):\n        constants = []\n        if self.implementation == 0 and 0 < self.dropout < 1:\n            input_shape = K.int_shape(inputs)\n            input_dim = input_shape[-1]\n            ones = K.ones_like(K.reshape(inputs[:, 0, 0], (-1, 1)))\n            ones = K.tile(ones, (1, int(input_dim)))\n\n            def dropped_inputs():\n                return K.dropout(ones, self.dropout)\n\n            dp_mask = [K.in_train_phase(dropped_inputs,\n                                        ones,\n                                        training=training) for _ in range(4)]\n            constants.append(dp_mask)\n        else:\n            constants.append([K.cast_to_floatx(1.) for _ in range(4)])\n\n        if 0 < self.recurrent_dropout < 1:\n            ones = K.ones_like(K.reshape(inputs[:, 0, 0], (-1, 1)))\n            ones = K.tile(ones, (1, self.units))\n\n            def dropped_inputs():\n                return K.dropout(ones, self.recurrent_dropout)\n            rec_dp_mask = [K.in_train_phase(dropped_inputs,\n                                            ones,\n                                            training=training) for _ in range(4)]\n            constants.append(rec_dp_mask)\n        else:\n            constants.append([K.cast_to_floatx(1.) for _ in range(4)])\n        return constants\n\n    def step(self, inputs, states):\n        h_tm1 = states[0]\n        c_tm1 = states[1]\n        dp_mask = states[2]\n        rec_dp_mask = states[3]\n\n        if self.implementation == 1:\n            z = K.dot(inputs * dp_mask[0], self.kernel)\n            z += K.dot(h_tm1 * rec_dp_mask[0], self.recurrent_kernel)\n            if self.use_bias:\n                z += self.bias\n\n            z0 = z[:, :self.units]\n            z1 = z[:, self.units: 2 * self.units]\n            z2 = z[:, 2 * self.units: 3 * self.units]\n            z3 = z[:, 3 * self.units:]\n\n            i = self.inner_activation(z0)\n            f = self.inner_activation(z1)\n            c = f * c_tm1 + i * self.activation(z2)\n            o = self.inner_activation(z3)\n        else:\n            if self.implementation == 0:\n                x_i = inputs[:, :self.units]\n                x_f = inputs[:, self.units: 2 * self.units]\n                x_c = inputs[:, 2 * self.units: 3 * self.units]\n                x_o = inputs[:, 3 * self.units:]\n            elif self.implementation == 2:\n                x_i = K.dot(inputs * dp_mask[0], self.kernel_i) + self.bias_i\n                x_f = K.dot(inputs * dp_mask[1], self.kernel_f) + self.bias_f\n                x_c = K.dot(inputs * dp_mask[2], self.kernel_c) + self.bias_c\n                x_o = K.dot(inputs * dp_mask[3], self.kernel_o) + self.bias_o\n            else:\n                raise ValueError('Unknown `implementation` mode.')\n\n            i = self.inner_activation(x_i + K.dot(h_tm1 * rec_dp_mask[0],\n                                                  self.recurrent_kernel_i))\n            f = self.inner_activation(x_f + K.dot(h_tm1 * rec_dp_mask[1],\n                                                  self.recurrent_kernel_f))\n            c = f * c_tm1 + i * self.activation(x_c + K.dot(h_tm1 * rec_dp_mask[2],\n                                                self.recurrent_kernel_c))\n            o = self.inner_activation(x_o + K.dot(h_tm1 * rec_dp_mask[3],\n                                                  self.recurrent_kernel_o))\n        h = o * self.activation(c)\n        return h, [h, c]\n\n    def get_config(self):\n        config = {'units': self.units,\n                  'activation': activations.serialize(self.activation),\n                  'recurrent_activation': activations.serialize(self.recurrent_activation),\n                  'use_bias': self.use_bias,\n                  'kernel_initializer': initializers.serialize(self.kernel_initializer),\n                  'recurrent_initializer': initializers.serialize(self.recurrent_initializer),\n                  'bias_initializer': initializers.serialize(self.bias_initializer),\n                  'unit_forget_bias': self.unit_forget_bias,\n                  'kernel_regularizer': regularizers.serialize(kernel_regularizer),\n                  'recurrent_regularizer': regularizers.serialize(recurrent_regularizer),\n                  'bias_regularizer': regularizers.serialize(bias_regularizer),\n                  'kernel_constraint': constraints.serialize(self.kernel_constraint),\n                  'recurrent_constraint': constraints.serialize(self.recurrent_constraint),\n                  'bias_constraint': constraints.serialize(self.bias_constraint),\n                  'dropout': self.dropout,\n                  'recurrent_dropout': self.recurrent_dropout}\n        base_config = super(LSTM, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n",
          "file_patch": "@@ -6,12 +6,14 @@ from .. import backend as K\n from .. import activations\n from .. import initializers\n from .. import regularizers\n+from .. import constraints\n from ..engine import Layer\n from ..engine import InputSpec\n \n \n-def time_distributed_dense(x, w, b=None, dropout=None,\n-                           input_dim=None, output_dim=None, timesteps=None):\n+def _time_distributed_dense(x, w, b=None, dropout=None,\n+                            input_dim=None, output_dim=None,\n+                            timesteps=None, training=None):\n     \"\"\"Apply `y . w + b` for every temporal slice y of x.\n \n     # Arguments\n@@ -23,6 +25,7 @@ def time_distributed_dense(x, w, b=None, dropout=None,\n         input_dim: integer; optional dimensionality of the input.\n         output_dim: integer; optional dimensionality of the output.\n         timesteps: integer; optional number of timesteps.\n+        training: training phase tensor or boolean.\n \n     # Returns\n         Output tensor.\n@@ -39,12 +42,12 @@ def time_distributed_dense(x, w, b=None, dropout=None,\n         ones = K.ones_like(K.reshape(x[:, 0, :], (-1, input_dim)))\n         dropout_matrix = K.dropout(ones, dropout)\n         expanded_dropout_matrix = K.repeat(dropout_matrix, timesteps)\n-        x = K.in_train_phase(x * expanded_dropout_matrix, x)\n+        x = K.in_train_phase(x * expanded_dropout_matrix, x, training=training)\n \n     # collapse time dimension and batch dimension together\n     x = K.reshape(x, (-1, input_dim))\n     x = K.dot(x, w)\n-    if b:\n+    if b is not None:\n         x += b\n     # reshape to 3D tensor\n     if K.backend() == 'tensorflow':\n@@ -92,23 +95,25 @@ class Recurrent(Layer):\n         stateful: Boolean (default False). If True, the last state\n             for each sample at index i in a batch will be used as initial\n             state for the sample of index i in the following batch.\n-        unroll: Boolean (default False). If True, the network will be unrolled,\n-            else a symbolic loop will be used. When using TensorFlow, the network\n-            is always unrolled, so this argument does not do anything.\n-            Unrolling can speed-up a RNN, although it tends to be more memory-intensive.\n+        unroll: Boolean (default False).\n+            If True, the network will be unrolled,\n+            else a symbolic loop will be used.\n+            Unrolling can speed-up a RNN,\n+            although it tends to be more memory-intensive.\n             Unrolling is only suitable for short sequences.\n-        consume_less: one of \"cpu\", \"mem\", or \"gpu\" (LSTM/GRU only).\n-            If set to \"cpu\", the RNN will use\n+        implementation: one of {0, 1, or 2}.\n+            If set to 0, the RNN will use\n             an implementation that uses fewer, larger matrix products,\n             thus running faster on CPU but consuming more memory.\n-            If set to \"mem\", the RNN will use more matrix products,\n-            but smaller ones, thus running slower (may actually be faster on GPU)\n-            while consuming less memory.\n-            If set to \"gpu\" (LSTM/GRU only), the RNN will combine the input gate,\n+            If set to 1, the RNN will use more matrix products,\n+            but smaller ones, thus running slower\n+            (may actually be faster on GPU) while consuming less memory.\n+            If set to 2 (LSTM/GRU only),\n+            the RNN will combine the input gate,\n             the forget gate and the output gate into a single matrix,\n-            enabling more time-efficient parallelization on the GPU. Note: RNN\n-            dropout must be shared for all gates, resulting in a slightly\n-            reduced regularization.\n+            enabling more time-efficient parallelization on the GPU.\n+            Note: RNN dropout must be shared for all gates,\n+            resulting in a slightly reduced regularization.\n         input_dim: dimensionality of the input (integer).\n             This argument (or alternatively, the keyword argument `input_shape`)\n             is required when using this layer as the first layer in a model.\n@@ -123,12 +128,12 @@ class Recurrent(Layer):\n             (e.g. via the `input_shape` argument)\n \n     # Input shape\n-        3D tensor with shape `(nb_samples, timesteps, input_dim)`.\n+        3D tensor with shape `(batch_size, timesteps, input_dim)`.\n \n     # Output shape\n         - if `return_sequences`: 3D tensor with shape\n-            `(nb_samples, timesteps, output_dim)`.\n-        - else, 2D tensor with shape `(nb_samples, output_dim)`.\n+            `(batch_size, timesteps, units)`.\n+        - else, 2D tensor with shape `(batch_size, units)`.\n \n     # Masking\n         This layer supports masking for input data with a variable number\n@@ -154,7 +159,8 @@ class Recurrent(Layer):\n                   `batch_input_shape=(...)` to the first layer in your model.\n                 else for functional model with 1 or more Input layers:\n                   `batch_shape=(...)` to all the first layers in your model.\n-                This is the expected shape of your inputs *including the batch size*.\n+                This is the expected shape of your inputs\n+                *including the batch size*.\n                 It should be a tuple of integers, e.g. `(32, 10, 100)`.\n             - specify `shuffle=False` when calling fit().\n \n@@ -162,60 +168,56 @@ class Recurrent(Layer):\n         a specific layer, or on your entire model.\n     \"\"\"\n \n-    def __init__(self, weights=None,\n-                 return_sequences=False, go_backwards=False, stateful=False,\n-                 unroll=False, consume_less='cpu',\n-                 input_dim=None, input_length=None, **kwargs):\n-        self.input_dim = input_dim\n-        self.input_length = input_length\n-        if self.input_dim:\n-            kwargs['input_shape'] = (self.input_length, self.input_dim)\n+    def __init__(self, return_sequences=False,\n+                 go_backwards=False,\n+                 stateful=False,\n+                 unroll=False,\n+                 implementation=0,\n+                 **kwargs):\n         super(Recurrent, self).__init__(**kwargs)\n         self.return_sequences = return_sequences\n-        self.initial_weights = weights\n         self.go_backwards = go_backwards\n         self.stateful = stateful\n         self.unroll = unroll\n-        self.consume_less = consume_less\n-\n+        self.implementation = 0\n         self.supports_masking = True\n-        self.input_spec = [InputSpec(ndim=3)]\n+        self.input_spec = InputSpec(ndim=3)\n \n     def get_output_shape_for(self, input_shape):\n         if self.return_sequences:\n-            return (input_shape[0], input_shape[1], self.output_dim)\n+            return (input_shape[0], input_shape[1], self.units)\n         else:\n-            return (input_shape[0], self.output_dim)\n+            return (input_shape[0], self.units)\n \n-    def compute_mask(self, input, mask):\n+    def compute_mask(self, inputs, mask):\n         if self.return_sequences:\n             return mask\n         else:\n             return None\n \n-    def step(self, x, states):\n+    def step(self, inputs, states):\n         raise NotImplementedError\n \n-    def get_constants(self, x):\n+    def get_constants(self, inputs, training=None):\n         return []\n \n-    def get_initial_states(self, x):\n+    def get_initial_states(self, inputs):\n         # build an all-zero tensor of shape (samples, output_dim)\n-        initial_state = K.zeros_like(x)  # (samples, timesteps, input_dim)\n+        initial_state = K.zeros_like(inputs)  # (samples, timesteps, input_dim)\n         initial_state = K.sum(initial_state, axis=(1, 2))  # (samples,)\n         initial_state = K.expand_dims(initial_state)  # (samples, 1)\n-        initial_state = K.tile(initial_state, [1, self.output_dim])  # (samples, output_dim)\n+        initial_state = K.tile(initial_state, [1, self.units])  # (samples, output_dim)\n         initial_states = [initial_state for _ in range(len(self.states))]\n         return initial_states\n \n-    def preprocess_input(self, x):\n-        return x\n+    def preprocess_input(self, inputs, training=None):\n+        return inputs\n \n-    def call(self, x, mask=None):\n-        # input shape: (nb_samples, time (padded with zeros), input_dim)\n+    def call(self, inputs, mask=None, training=None):\n+        # input shape: (nbias_samples, time (padded with zeros), input_dim)\n         # note that the .build() method of subclasses MUST define\n         # self.input_spec with a complete input shape.\n-        input_shape = K.int_shape(x)\n+        input_shape = K.int_shape(inputs)\n         if self.unroll and input_shape[1] is None:\n             raise ValueError('Cannot unroll a RNN if the '\n                              'time dimension is undefined. \\n'\n@@ -231,11 +233,11 @@ class Recurrent(Layer):\n         if self.stateful:\n             initial_states = self.states\n         else:\n-            initial_states = self.get_initial_states(x)\n-        constants = self.get_constants(x)\n-        preprocessed_input = self.preprocess_input(x)\n-\n-        last_output, outputs, states = K.rnn(self.step, preprocessed_input,\n+            initial_states = self.get_initial_states(inputs)\n+        constants = self.get_constants(inputs, training=None)\n+        preprocessed_input = self.preprocess_input(inputs, training=None)\n+        last_output, outputs, states = K.rnn(self.step,\n+                                             preprocessed_input,\n                                              initial_states,\n                                              go_backwards=self.go_backwards,\n                                              mask=mask,\n@@ -246,7 +248,7 @@ class Recurrent(Layer):\n             updates = []\n             for i in range(len(states)):\n                 updates.append((self.states[i], states[i]))\n-            self.add_update(updates, x)\n+            self.add_update(updates, inputs)\n \n         if self.return_sequences:\n             return outputs\n@@ -258,13 +260,7 @@ class Recurrent(Layer):\n                   'go_backwards': self.go_backwards,\n                   'stateful': self.stateful,\n                   'unroll': self.unroll,\n-                  'consume_less': self.consume_less}\n-        if self.stateful and self.input_spec[0].shape:\n-            config['batch_input_shape'] = self.input_spec[0].shape\n-        else:\n-            config['input_dim'] = self.input_dim\n-            config['input_length'] = self.input_length\n-\n+                  'implementation': self.implementation}\n         base_config = super(Recurrent, self).get_config()\n         return dict(list(base_config.items()) + list(config.items()))\n \n@@ -273,76 +269,121 @@ class SimpleRNN(Recurrent):\n     \"\"\"Fully-connected RNN where the output is to be fed back to input.\n \n     # Arguments\n-        output_dim: dimension of the internal projections and the final output.\n-        init: weight initializer function.\n-            Can be the name of an existing function (str),\n-            or a Theano function (see: [initializers](../initializers.md)).\n-        inner_init: initializer function of the inner cells.\n-        activation: activation function.\n-            Can be the name of an existing function (str),\n-            or a Theano function (see: [activations](../activations.md)).\n-        W_regularizer: instance of [WeightRegularizer](../regularizers.md)\n-            (eg. L1 or L2 regularization), applied to the input weights matrices.\n-        U_regularizer: instance of [WeightRegularizer](../regularizers.md)\n-            (eg. L1 or L2 regularization), applied to the recurrent weights matrices.\n-        b_regularizer: instance of [WeightRegularizer](../regularizers.md),\n-            applied to the bias.\n-        dropout_W: float between 0 and 1. Fraction of the input units to drop for input gates.\n-        dropout_U: float between 0 and 1. Fraction of the input units to drop for recurrent connections.\n+        units: Positive integer, dimensionality of the output space.\n+        activation: Activation function to use\n+            (see [activations](../activations.md)).\n+            If you don't specify anything, no activation is applied\n+            (ie. \"linear\" activation: `a(x) = x`).\n+        use_bias: Boolean, whether the layer uses a bias vector.\n+        kernel_initializer: Initializer for the `kernel` weights matrix,\n+            used for the linear transformation of the inputs.\n+            (see [initializers](../initializers.md)).\n+        recurrent_initializer: Initializer for the `recurrent_kernel`\n+            weights matrix,\n+            used for the linear transformation of the recurrent state.\n+            (see [initializers](../initializers.md)).\n+        bias_initializer: Initializer for the bias vector\n+            (see [initializers](../initializers.md)).\n+        kernel_regularizer: Regularizer function applied to\n+            the `kernel` weights matrix\n+            (see [regularizer](../regularizers.md)).\n+        recurrent_regularizer: Regularizer function applied to\n+            the `recurrent_kernel` weights matrix\n+            (see [regularizer](../regularizers.md)).\n+        bias_regularizer: Regularizer function applied to the bias vector\n+            (see [regularizer](../regularizers.md)).\n+        activity_regularizer: Regularizer function applied to\n+            the output of the layer (its \"activation\").\n+            (see [regularizer](../regularizers.md)).\n+        kernel_constraint: Constraint function applied to\n+            the `kernel` weights matrix\n+            (see [constraints](../constraints.md)).\n+        recurrent_constraint: Constraint function applied to\n+            the `recurrent_kernel` weights matrix\n+            (see [constraints](../constraints.md)).\n+        bias_constraint: Constraint function applied to the bias vector\n+            (see [constraints](../constraints.md)).\n+        dropout: Float between 0 and 1.\n+            Fraction of the units to drop for\n+            the linear transformation of the inputs.\n+        recurrent_dropout: Float between 0 and 1.\n+            Fraction of the units to drop for\n+            the linear transformation of the recurrent state.\n \n     # References\n         - [A Theoretically Grounded Application of Dropout in Recurrent Neural Networks](http://arxiv.org/abs/1512.05287)\n     \"\"\"\n \n-    def __init__(self, output_dim,\n-                 init='glorot_uniform', inner_init='orthogonal',\n+    def __init__(self, units,\n                  activation='tanh',\n-                 W_regularizer=None, U_regularizer=None, b_regularizer=None,\n-                 dropout_W=0., dropout_U=0., **kwargs):\n+                 use_bias=True,\n+                 kernel_initializer='glorot_uniform',\n+                 recurrent_initializer='orthogonal',\n+                 bias_initializer='zeros',\n+                 kernel_regularizer=None,\n+                 recurrent_regularizer=None,\n+                 bias_regularizer=None,\n+                 activity_regularizer=None,\n+                 kernel_constraint=None,\n+                 recurrent_constraint=None,\n+                 bias_constraint=None,\n+                 dropout=0.,\n+                 recurrent_dropout=0.,\n+                 **kwargs):\n         super(SimpleRNN, self).__init__(**kwargs)\n-        self.output_dim = output_dim\n-        self.init = initializers.get(init)\n-        self.inner_init = initializers.get(inner_init)\n+        self.units = units\n         self.activation = activations.get(activation)\n-        self.W_regularizer = regularizers.get(W_regularizer)\n-        self.U_regularizer = regularizers.get(U_regularizer)\n-        self.b_regularizer = regularizers.get(b_regularizer)\n-        self.dropout_W = dropout_W\n-        self.dropout_U = dropout_U\n+        self.use_bias = use_bias\n+\n+        self.kernel_initializer = initializers.get(kernel_initializer)\n+        self.recurrent_initializer = initializers.get(recurrent_initializer)\n+        self.bias_initializer = initializers.get(bias_initializer)\n \n-        if self.dropout_W or self.dropout_U:\n-            self.uses_learning_phase = True\n+        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n+        self.recurrent_regularizer = regularizers.get(recurrent_regularizer)\n+        self.bias_regularizer = regularizers.get(bias_regularizer)\n+\n+        self.kernel_constraint = constraints.get(kernel_constraint)\n+        self.recurrent_constraint = constraints.get(recurrent_constraint)\n+        self.bias_constraint = constraints.get(bias_constraint)\n+\n+        self.dropout = min(1., max(0., dropout))\n+        self.recurrent_dropout = min(1., max(0., recurrent_dropout))\n \n     def build(self, input_shape):\n-        self.input_spec = [InputSpec(shape=input_shape)]\n+        # TODO: handle variable-length seqs in input_spec\n+        self.input_spec = InputSpec(shape=input_shape)\n         if self.stateful:\n             self.reset_states()\n         else:\n-            # initial states: all-zero tensor of shape (output_dim)\n+            # Initial states: all-zero tensor of shape (output_dim).\n             self.states = [None]\n         input_dim = input_shape[2]\n         self.input_dim = input_dim\n \n-        self.W = self.add_weight((input_dim, self.output_dim),\n-                                 initializer=self.init,\n-                                 name='{}_W'.format(self.name),\n-                                 regularizer=self.W_regularizer)\n-        self.U = self.add_weight((self.output_dim, self.output_dim),\n-                                 initializer=self.inner_init,\n-                                 name='{}_U'.format(self.name),\n-                                 regularizer=self.U_regularizer)\n-        self.b = self.add_weight((self.output_dim,),\n-                                 initializer='zero',\n-                                 name='{}_b'.format(self.name),\n-                                 regularizer=self.b_regularizer)\n-\n-        if self.initial_weights is not None:\n-            self.set_weights(self.initial_weights)\n-            del self.initial_weights\n-        self.built = True\n+        self.kernel = self.add_weight((input_dim, self.units),\n+                                      name='kernel',\n+                                      initializer=self.kernel_initializer,\n+                                      regularizer=self.kernel_regularizer,\n+                                      constraint=self.kernel_constraint)\n+        self.recurrent_kernel = self.add_weight(\n+            (self.units, self.units),\n+            name='recurrent_kernel',\n+            initializer=self.recurrent_initializer,\n+            regularizer=self.recurrent_regularizer,\n+            constraint=self.recurrent_constraint)\n+        if self.use_bias:\n+            self.bias = self.add_weight((self.units,),\n+                                        name='bias',\n+                                        initializer=self.bias_initializer,\n+                                        regularizer=self.bias_regularizer,\n+                                        constraint=self.bias_constraint)\n+        else:\n+            self.bias = None\n \n     def reset_states(self):\n-        assert self.stateful, 'Layer must be stateful.'\n+        if not self.stateful:\n+            raise AttributeError('Layer must be stateful.')\n         input_shape = self.input_spec[0].shape\n         if not input_shape[0]:\n             raise ValueError('If a RNN is stateful, it needs to know '\n@@ -357,64 +398,101 @@ class SimpleRNN(Recurrent):\n                              '`batch_shape` argument to your Input layer.')\n         if hasattr(self, 'states'):\n             K.set_value(self.states[0],\n-                        np.zeros((input_shape[0], self.output_dim)))\n+                        np.zeros((input_shape[0], self.units)))\n         else:\n-            self.states = [K.zeros((input_shape[0], self.output_dim))]\n+            self.states = [K.zeros((input_shape[0], self.units))]\n \n-    def preprocess_input(self, x):\n-        if self.consume_less == 'cpu':\n-            input_shape = K.int_shape(x)\n+    def preprocess_input(self, inputs, training=None):\n+        if self.implementation > 0:\n+            return inputs\n+        else:\n+            input_shape = K.int_shape(inputs)\n             input_dim = input_shape[2]\n             timesteps = input_shape[1]\n-            return time_distributed_dense(x, self.W, self.b, self.dropout_W,\n-                                          input_dim, self.output_dim,\n-                                          timesteps)\n+            return _time_distributed_dense(inputs,\n+                                           self.kernel,\n+                                           self.bias,\n+                                           self.dropout,\n+                                           input_dim,\n+                                           self.units,\n+                                           timesteps,\n+                                           training=training)\n+\n+    def step(self, inputs, states):\n+        if self.implementation == 0:\n+            h = inputs\n         else:\n-            return x\n+            if 0 < self.dropout < 1:\n+                h = K.dot(inputs * states[1], self.kernel)\n+            else:\n+                h = K.dot(inputs, self.kernel)\n+            if self.bias is not None:\n+                h += self.bias\n \n-    def step(self, x, states):\n         prev_output = states[0]\n-        B_U = states[1]\n-        B_W = states[2]\n-\n-        if self.consume_less == 'cpu':\n-            h = x\n-        else:\n-            h = K.dot(x * B_W, self.W) + self.b\n-\n-        output = self.activation(h + K.dot(prev_output * B_U, self.U))\n+        if 0 < self.recurrent_dropout < 1:\n+            prev_output *= states[2]\n+        output = h + K.dot(prev_output, self.recurrent_kernel)\n+        if self.activation is not None:\n+            output = self.activation(output)\n+\n+        # Properly set learning phase on output tensor.\n+        if 0 < self.dropout + self.recurrent_dropout:\n+            uses_learning_phase = False\n+            if getattr(states[0], '_uses_learning_phase', False):\n+                uses_learning_phase = True\n+            if getattr(states[1], '_uses_learning_phase', False):\n+                uses_learning_phase = True\n+            output._uses_learning_phase = uses_learning_phase\n         return output, [output]\n \n-    def get_constants(self, x):\n+    def get_constants(self, inputs, training=None):\n         constants = []\n-        if 0 < self.dropout_U < 1:\n-            ones = K.ones_like(K.reshape(x[:, 0, 0], (-1, 1)))\n-            ones = K.tile(ones, (1, self.output_dim))\n-            B_U = K.in_train_phase(K.dropout(ones, self.dropout_U), ones)\n-            constants.append(B_U)\n-        else:\n-            constants.append(K.cast_to_floatx(1.))\n-        if self.consume_less == 'cpu' and 0 < self.dropout_W < 1:\n-            input_shape = K.int_shape(x)\n+        if self.implementation == 0 and 0 < self.dropout < 1:\n+            input_shape = K.int_shape(inputs)\n             input_dim = input_shape[-1]\n-            ones = K.ones_like(K.reshape(x[:, 0, 0], (-1, 1)))\n+            ones = K.ones_like(K.reshape(inputs[:, 0, 0], (-1, 1)))\n             ones = K.tile(ones, (1, int(input_dim)))\n-            B_W = K.in_train_phase(K.dropout(ones, self.dropout_W), ones)\n-            constants.append(B_W)\n+\n+            def dropped_inputs():\n+                return K.dropout(ones, self.dropout)\n+\n+            dp_mask = K.in_train_phase(dropped_inputs,\n+                                       ones,\n+                                       training=training)\n+            constants.append(dp_mask)\n+        else:\n+            constants.append(K.cast_to_floatx(1.))\n+\n+        if 0 < self.recurrent_dropout < 1:\n+            ones = K.ones_like(K.reshape(inputs[:, 0, 0], (-1, 1)))\n+            ones = K.tile(ones, (1, self.units))\n+\n+            def dropped_inputs():\n+                return K.dropout(ones, self.recurrent_dropout)\n+            rec_dp_mask = K.in_train_phase(dropped_inputs,\n+                                           ones,\n+                                           training=training)\n+            constants.append(rec_dp_mask)\n         else:\n             constants.append(K.cast_to_floatx(1.))\n         return constants\n \n     def get_config(self):\n-        config = {'output_dim': self.output_dim,\n-                  'init': initializers.get_config(self.init),\n-                  'inner_init': initializers.get_config(self.inner_init),\n-                  'activation': self.activation.__name__,\n-                  'W_regularizer': self.W_regularizer.get_config() if self.W_regularizer else None,\n-                  'U_regularizer': self.U_regularizer.get_config() if self.U_regularizer else None,\n-                  'b_regularizer': self.b_regularizer.get_config() if self.b_regularizer else None,\n-                  'dropout_W': self.dropout_W,\n-                  'dropout_U': self.dropout_U}\n+        config = {'units': self.units,\n+                  'activation': activations.serialize(self.activation),\n+                  'use_bias': self.use_bias,\n+                  'kernel_initializer': initializers.serialize(self.kernel_initializer),\n+                  'recurrent_initializer': initializers.serialize(self.recurrent_initializer),\n+                  'bias_initializer': initializers.serialize(self.bias_initializer),\n+                  'kernel_regularizer': regularizers.serialize(kernel_regularizer),\n+                  'recurrent_regularizer': regularizers.serialize(recurrent_regularizer),\n+                  'bias_regularizer': regularizers.serialize(bias_regularizer),\n+                  'kernel_constraint': constraints.serialize(self.kernel_constraint),\n+                  'recurrent_constraint': constraints.serialize(self.recurrent_constraint),\n+                  'bias_constraint': constraints.serialize(self.bias_constraint),\n+                  'dropout': self.dropout,\n+                  'recurrent_dropout': self.recurrent_dropout}\n         base_config = super(SimpleRNN, self).get_config()\n         return dict(list(base_config.items()) + list(config.items()))\n \n@@ -423,23 +501,46 @@ class GRU(Recurrent):\n     \"\"\"Gated Recurrent Unit - Cho et al. 2014.\n \n     # Arguments\n-        output_dim: dimension of the internal projections and the final output.\n-        init: weight initializer function.\n-            Can be the name of an existing function (str),\n-            or a Theano function (see: [initializers](../initializers.md)).\n-        inner_init: initializer function of the inner cells.\n-        activation: activation function.\n-            Can be the name of an existing function (str),\n-            or a Theano function (see: [activations](../activations.md)).\n-        inner_activation: activation function for the inner cells.\n-        W_regularizer: instance of [WeightRegularizer](../regularizers.md)\n-            (eg. L1 or L2 regularization), applied to the input weights matrices.\n-        U_regularizer: instance of [WeightRegularizer](../regularizers.md)\n-            (eg. L1 or L2 regularization), applied to the recurrent weights matrices.\n-        b_regularizer: instance of [WeightRegularizer](../regularizers.md),\n-            applied to the bias.\n-        dropout_W: float between 0 and 1. Fraction of the input units to drop for input gates.\n-        dropout_U: float between 0 and 1. Fraction of the input units to drop for recurrent connections.\n+        units: Positive integer, dimensionality of the output space.\n+        activation: Activation function to use\n+            (see [activations](../activations.md)).\n+            If you don't specify anything, no activation is applied\n+            (ie. \"linear\" activation: `a(x) = x`).\n+        use_bias: Boolean, whether the layer uses a bias vector.\n+        kernel_initializer: Initializer for the `kernel` weights matrix,\n+            used for the linear transformation of the inputs.\n+            (see [initializers](../initializers.md)).\n+        recurrent_initializer: Initializer for the `recurrent_kernel`\n+            weights matrix,\n+            used for the linear transformation of the recurrent state.\n+            (see [initializers](../initializers.md)).\n+        bias_initializer: Initializer for the bias vector\n+            (see [initializers](../initializers.md)).\n+        kernel_regularizer: Regularizer function applied to\n+            the `kernel` weights matrix\n+            (see [regularizer](../regularizers.md)).\n+        recurrent_regularizer: Regularizer function applied to\n+            the `recurrent_kernel` weights matrix\n+            (see [regularizer](../regularizers.md)).\n+        bias_regularizer: Regularizer function applied to the bias vector\n+            (see [regularizer](../regularizers.md)).\n+        activity_regularizer: Regularizer function applied to\n+            the output of the layer (its \"activation\").\n+            (see [regularizer](../regularizers.md)).\n+        kernel_constraint: Constraint function applied to\n+            the `kernel` weights matrix\n+            (see [constraints](../constraints.md)).\n+        recurrent_constraint: Constraint function applied to\n+            the `recurrent_kernel` weights matrix\n+            (see [constraints](../constraints.md)).\n+        bias_constraint: Constraint function applied to the bias vector\n+            (see [constraints](../constraints.md)).\n+        dropout: Float between 0 and 1.\n+            Fraction of the units to drop for\n+            the linear transformation of the inputs.\n+        recurrent_dropout: Float between 0 and 1.\n+            Fraction of the units to drop for\n+            the linear transformation of the recurrent state.\n \n     # References\n         - [On the Properties of Neural Machine Translation: Encoder-Decoder Approaches](https://arxiv.org/abs/1409.1259)\n@@ -447,28 +548,47 @@ class GRU(Recurrent):\n         - [A Theoretically Grounded Application of Dropout in Recurrent Neural Networks](http://arxiv.org/abs/1512.05287)\n     \"\"\"\n \n-    def __init__(self, output_dim,\n-                 init='glorot_uniform', inner_init='orthogonal',\n-                 activation='tanh', inner_activation='hard_sigmoid',\n-                 W_regularizer=None, U_regularizer=None, b_regularizer=None,\n-                 dropout_W=0., dropout_U=0., **kwargs):\n+    def __init__(self, units,\n+                 activation='tanh',\n+                 recurrent_activation='hard_sigmoid',\n+                 use_bias=True,\n+                 kernel_initializer='glorot_uniform',\n+                 recurrent_initializer='orthogonal',\n+                 bias_initializer='zeros',\n+                 kernel_regularizer=None,\n+                 recurrent_regularizer=None,\n+                 bias_regularizer=None,\n+                 activity_regularizer=None,\n+                 kernel_constraint=None,\n+                 recurrent_constraint=None,\n+                 bias_constraint=None,\n+                 dropout=0.,\n+                 recurrent_dropout=0.,\n+                 **kwargs):\n         super(GRU, self).__init__(**kwargs)\n-        self.output_dim = output_dim\n-        self.init = initializers.get(init)\n-        self.inner_init = initializers.get(inner_init)\n+        self.units = units\n         self.activation = activations.get(activation)\n-        self.inner_activation = activations.get(inner_activation)\n-        self.W_regularizer = regularizers.get(W_regularizer)\n-        self.U_regularizer = regularizers.get(U_regularizer)\n-        self.b_regularizer = regularizers.get(b_regularizer)\n-        self.dropout_W = dropout_W\n-        self.dropout_U = dropout_U\n+        self.recurrent_activation = activations.get(recurrent_activation)\n+        self.use_bias = use_bias\n+\n+        self.kernel_initializer = initializers.get(kernel_initializer)\n+        self.recurrent_initializer = initializers.get(recurrent_initializer)\n+        self.bias_initializer = initializers.get(bias_initializer)\n+\n+        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n+        self.recurrent_regularizer = regularizers.get(recurrent_regularizer)\n+        self.bias_regularizer = regularizers.get(bias_regularizer)\n \n-        if self.dropout_W or self.dropout_U:\n-            self.uses_learning_phase = True\n+        self.kernel_constraint = constraints.get(kernel_constraint)\n+        self.recurrent_constraint = constraints.get(recurrent_constraint)\n+        self.bias_constraint = constraints.get(bias_constraint)\n+\n+        self.dropout = min(1., max(0., dropout))\n+        self.recurrent_dropout = min(1., max(0., recurrent_dropout))\n \n     def build(self, input_shape):\n-        self.input_spec = [InputSpec(shape=input_shape)]\n+        # TODO: handle variable-length sequences in input spec.\n+        self.input_spec = InputSpec(shape=input_shape)\n         self.input_dim = input_shape[2]\n \n         if self.stateful:\n@@ -477,67 +597,48 @@ class GRU(Recurrent):\n             # initial states: all-zero tensor of shape (output_dim)\n             self.states = [None]\n \n-        if self.consume_less == 'gpu':\n-            self.W = self.add_weight((self.input_dim, 3 * self.output_dim),\n-                                     initializer=self.init,\n-                                     name='{}_W'.format(self.name),\n-                                     regularizer=self.W_regularizer)\n-            self.U = self.add_weight((self.output_dim, 3 * self.output_dim),\n-                                     initializer=self.inner_init,\n-                                     name='{}_U'.format(self.name),\n-                                     regularizer=self.U_regularizer)\n-            self.b = self.add_weight((self.output_dim * 3,),\n-                                     initializer='zero',\n-                                     name='{}_b'.format(self.name),\n-                                     regularizer=self.b_regularizer)\n+        self.kernel = self.add_weight((self.input_dim, self.units * 3),\n+                                      name='kernel',\n+                                      initializer=self.kernel_initializer,\n+                                      regularizer=self.kernel_regularizer,\n+                                      constraint=self.kernel_constraint)\n+        self.recurrent_kernel = self.add_weight(\n+            (self.units, self.units * 3),\n+            name='recurrent_kernel',\n+            initializer=self.recurrent_initializer,\n+            regularizer=self.recurrent_regularizer,\n+            constraint=self.recurrent_constraint)\n+\n+        if self.use_bias:\n+            self.bias = self.add_weight((self.units * 3,),\n+                                        name='bias',\n+                                        initializer='zero',\n+                                        regularizer=self.bias_regularizer,\n+                                        constraint=self.bias_constraint)\n+        else:\n+            self.bias = None\n+\n+        self.kernel_z = self.kernel[:, :self.units]\n+        self.recurrent_kernel_z = self.recurrent_kernel[:, :self.units]\n+        self.kernel_r = self.kernel[:, self.units: self.units * 2]\n+        self.recurrent_kernel_r = self.recurrent_kernel[:,\n+                                                        self.units:\n+                                                        self.units * 2]\n+        self.kernel_h = self.kernel[:, self.units * 2:]\n+        self.recurrent_kernel_h = self.recurrent_kernel[:, self.units * 2:]\n+\n+        if self.use_bias:\n+            self.bias_z = self.bias[:self.units]\n+            self.bias_r = self.bias[self.units: self.units * 2]\n+            self.bias_h = self.bias[self.units * 2:]\n         else:\n-            self.W_z = self.add_weight((self.input_dim, self.output_dim),\n-                                       initializer=self.init,\n-                                       name='{}_W_z'.format(self.name),\n-                                       regularizer=self.W_regularizer)\n-            self.U_z = self.add_weight((self.output_dim, self.output_dim),\n-                                       initializer=self.init,\n-                                       name='{}_U_z'.format(self.name),\n-                                       regularizer=self.W_regularizer)\n-            self.b_z = self.add_weight((self.output_dim,),\n-                                       initializer='zero',\n-                                       name='{}_b_z'.format(self.name),\n-                                       regularizer=self.b_regularizer)\n-            self.W_r = self.add_weight((self.input_dim, self.output_dim),\n-                                       initializer=self.init,\n-                                       name='{}_W_r'.format(self.name),\n-                                       regularizer=self.W_regularizer)\n-            self.U_r = self.add_weight((self.output_dim, self.output_dim),\n-                                       initializer=self.init,\n-                                       name='{}_U_r'.format(self.name),\n-                                       regularizer=self.W_regularizer)\n-            self.b_r = self.add_weight((self.output_dim,),\n-                                       initializer='zero',\n-                                       name='{}_b_r'.format(self.name),\n-                                       regularizer=self.b_regularizer)\n-            self.W_h = self.add_weight((self.input_dim, self.output_dim),\n-                                       initializer=self.init,\n-                                       name='{}_W_h'.format(self.name),\n-                                       regularizer=self.W_regularizer)\n-            self.U_h = self.add_weight((self.output_dim, self.output_dim),\n-                                       initializer=self.init,\n-                                       name='{}_U_h'.format(self.name),\n-                                       regularizer=self.W_regularizer)\n-            self.b_h = self.add_weight((self.output_dim,),\n-                                       initializer='zero',\n-                                       name='{}_b_h'.format(self.name),\n-                                       regularizer=self.b_regularizer)\n-            self.W = K.concatenate([self.W_z, self.W_r, self.W_h])\n-            self.U = K.concatenate([self.U_z, self.U_r, self.U_h])\n-            self.b = K.concatenate([self.b_z, self.b_r, self.b_h])\n-\n-        if self.initial_weights is not None:\n-            self.set_weights(self.initial_weights)\n-            del self.initial_weights\n-        self.built = True\n+            self.bias_z = None\n+            self.bias_r = None\n+            self.bias_h = None\n \n     def reset_states(self):\n-        assert self.stateful, 'Layer must be stateful.'\n+        if not self.stateful:\n+            raise RuntimeError('Layer must be stateful.')\n         input_shape = self.input_spec[0].shape\n         if not input_shape[0]:\n             raise ValueError('If a RNN is stateful, a complete '\n@@ -545,97 +646,126 @@ class GRU(Recurrent):\n                              '(including batch size).')\n         if hasattr(self, 'states'):\n             K.set_value(self.states[0],\n-                        np.zeros((input_shape[0], self.output_dim)))\n+                        np.zeros((input_shape[0], self.units)))\n         else:\n-            self.states = [K.zeros((input_shape[0], self.output_dim))]\n+            self.states = [K.zeros((input_shape[0], self.units))]\n \n-    def preprocess_input(self, x):\n-        if self.consume_less == 'cpu':\n-            input_shape = K.int_shape(x)\n+    def preprocess_input(self, inputs, training=None):\n+        if self.implementation == 0:\n+            input_shape = K.int_shape(inputs)\n             input_dim = input_shape[2]\n             timesteps = input_shape[1]\n \n-            x_z = time_distributed_dense(x, self.W_z, self.b_z, self.dropout_W,\n-                                         input_dim, self.output_dim, timesteps)\n-            x_r = time_distributed_dense(x, self.W_r, self.b_r, self.dropout_W,\n-                                         input_dim, self.output_dim, timesteps)\n-            x_h = time_distributed_dense(x, self.W_h, self.b_h, self.dropout_W,\n-                                         input_dim, self.output_dim, timesteps)\n+            x_z = _time_distributed_dense(inputs, self.kernel_z, self.bias_z,\n+                                          self.dropout, input_dim, self.units,\n+                                          timesteps, training=training)\n+            x_r = _time_distributed_dense(inputs, self.kernel_r, self.bias_r,\n+                                          self.dropout, input_dim, self.units,\n+                                          timesteps, training=training)\n+            x_h = _time_distributed_dense(inputs, self.kernel_h, self.bias_h,\n+                                          self.dropout, input_dim, self.units,\n+                                          timesteps, training=training)\n             return K.concatenate([x_z, x_r, x_h], axis=2)\n         else:\n-            return x\n+            return inputs\n \n-    def step(self, x, states):\n-        h_tm1 = states[0]  # previous memory\n-        B_U = states[1]  # dropout matrices for recurrent units\n-        B_W = states[2]\n+    def get_constants(self, inputs, training=None):\n+        constants = []\n+        if self.implementation == 0 and 0 < self.dropout < 1:\n+            input_shape = K.int_shape(inputs)\n+            input_dim = input_shape[-1]\n+            ones = K.ones_like(K.reshape(inputs[:, 0, 0], (-1, 1)))\n+            ones = K.tile(ones, (1, int(input_dim)))\n \n-        if self.consume_less == 'gpu':\n+            def dropped_inputs():\n+                return K.dropout(ones, self.dropout)\n \n-            matrix_x = K.dot(x * B_W[0], self.W) + self.b\n-            matrix_inner = K.dot(h_tm1 * B_U[0], self.U[:, :2 * self.output_dim])\n+            dp_mask = [K.in_train_phase(dropped_inputs,\n+                                        ones,\n+                                        training=training) for _ in range(3)]\n+            constants.append(dp_mask)\n+        else:\n+            constants.append([K.cast_to_floatx(1.) for _ in range(3)])\n \n-            x_z = matrix_x[:, :self.output_dim]\n-            x_r = matrix_x[:, self.output_dim: 2 * self.output_dim]\n-            inner_z = matrix_inner[:, :self.output_dim]\n-            inner_r = matrix_inner[:, self.output_dim: 2 * self.output_dim]\n+        if 0 < self.recurrent_dropout < 1:\n+            ones = K.ones_like(K.reshape(inputs[:, 0, 0], (-1, 1)))\n+            ones = K.tile(ones, (1, self.units))\n \n-            z = self.inner_activation(x_z + inner_z)\n-            r = self.inner_activation(x_r + inner_r)\n+            def dropped_inputs():\n+                return K.dropout(ones, self.recurrent_dropout)\n+            rec_dp_mask = [K.in_train_phase(dropped_inputs,\n+                                            ones,\n+                                            training=training) for _ in range(3)]\n+            constants.append(rec_dp_mask)\n+        else:\n+            constants.append([K.cast_to_floatx(1.) for _ in range(3)])\n+        return constants\n \n-            x_h = matrix_x[:, 2 * self.output_dim:]\n-            inner_h = K.dot(r * h_tm1 * B_U[0], self.U[:, 2 * self.output_dim:])\n+    def step(self, inputs, states):\n+        h_tm1 = states[0]  # previous memory\n+        dp_mask = states[1]  # dropout matrices for recurrent units\n+        rec_dp_mask = states[2]\n+\n+        if self.implementation == 1:\n+            matrix_x = K.dot(inputs * dp_mask[0], self.kernel)\n+            if self.use_bias:\n+                matrix_x += self.bias\n+            matrix_inner = K.dot(h_tm1 * rec_dp_mask[0],\n+                                 self.recurrent_kernel[:, :2 * self.units])\n+\n+            x_z = matrix_x[:, :self.units]\n+            x_r = matrix_x[:, self.units: 2 * self.units]\n+            inner_z = matrix_inner[:, :self.units]\n+            inner_r = matrix_inner[:, self.units: 2 * self.units]\n+\n+            z = self.recurrent_activation(x_z + inner_z)\n+            r = self.recurrent_activation(x_r + inner_r)\n+\n+            x_h = matrix_x[:, 2 * self.units:]\n+            inner_h = K.dot(r * h_tm1 * rec_dp_mask[0],\n+                            self.recurrent_kernel[:, 2 * self.units:])\n             hh = self.activation(x_h + inner_h)\n         else:\n-            if self.consume_less == 'cpu':\n-                x_z = x[:, :self.output_dim]\n-                x_r = x[:, self.output_dim: 2 * self.output_dim]\n-                x_h = x[:, 2 * self.output_dim:]\n-            elif self.consume_less == 'mem':\n-                x_z = K.dot(x * B_W[0], self.W_z) + self.b_z\n-                x_r = K.dot(x * B_W[1], self.W_r) + self.b_r\n-                x_h = K.dot(x * B_W[2], self.W_h) + self.b_h\n+            if self.implementation == 0:\n+                x_z = inputs[:, :self.units]\n+                x_r = inputs[:, self.units: 2 * self.units]\n+                x_h = inputs[:, 2 * self.units:]\n+            elif self.implementation == 2:\n+                x_z = K.dot(inputs * dp_mask[0], self.kernel_z)\n+                x_r = K.dot(inputs * dp_mask[1], self.kernel_r)\n+                x_h = K.dot(inputs * dp_mask[2], self.kernel_h)\n+                if self.use_bias:\n+                    x_z += self.bias_z\n+                    x_r += self.bias_r\n+                    x_h += self.bias_h\n             else:\n-                raise ValueError('Unknown `consume_less` mode.')\n-            z = self.inner_activation(x_z + K.dot(h_tm1 * B_U[0], self.U_z))\n-            r = self.inner_activation(x_r + K.dot(h_tm1 * B_U[1], self.U_r))\n-\n-            hh = self.activation(x_h + K.dot(r * h_tm1 * B_U[2], self.U_h))\n+                raise ValueError('Unknown `implementation` mode.')\n+            z = self.recurrent_activation(x_z + K.dot(h_tm1 * rec_dp_mask[0],\n+                                                      self.recurrent_kernel_z))\n+            r = self.recurrent_activation(x_r + K.dot(h_tm1 * rec_dp_mask[1],\n+                                                      self.recurrent_kernel_r))\n+\n+            hh = self.activation(x_h + K.dot(r * h_tm1 * rec_dp_mask[2],\n+                                             self.recurrent_kernel_h))\n         h = z * h_tm1 + (1 - z) * hh\n         return h, [h]\n \n-    def get_constants(self, x):\n-        constants = []\n-        if 0 < self.dropout_U < 1:\n-            ones = K.ones_like(K.reshape(x[:, 0, 0], (-1, 1)))\n-            ones = K.tile(ones, (1, self.output_dim))\n-            B_U = [K.in_train_phase(K.dropout(ones, self.dropout_U), ones) for _ in range(3)]\n-            constants.append(B_U)\n-        else:\n-            constants.append([K.cast_to_floatx(1.) for _ in range(3)])\n-\n-        if 0 < self.dropout_W < 1:\n-            input_shape = K.int_shape(x)\n-            input_dim = input_shape[-1]\n-            ones = K.ones_like(K.reshape(x[:, 0, 0], (-1, 1)))\n-            ones = K.tile(ones, (1, int(input_dim)))\n-            B_W = [K.in_train_phase(K.dropout(ones, self.dropout_W), ones) for _ in range(3)]\n-            constants.append(B_W)\n-        else:\n-            constants.append([K.cast_to_floatx(1.) for _ in range(3)])\n-        return constants\n-\n     def get_config(self):\n-        config = {'output_dim': self.output_dim,\n-                  'init': initializers.get_config(self.init),\n-                  'inner_init': initializers.get_config(self.inner_init),\n-                  'activation': self.activation.__name__,\n-                  'inner_activation': self.inner_activation.__name__,\n-                  'W_regularizer': self.W_regularizer.get_config() if self.W_regularizer else None,\n-                  'U_regularizer': self.U_regularizer.get_config() if self.U_regularizer else None,\n-                  'b_regularizer': self.b_regularizer.get_config() if self.b_regularizer else None,\n-                  'dropout_W': self.dropout_W,\n-                  'dropout_U': self.dropout_U}\n+        config = {'units': self.units,\n+                  'activation': activations.serialize(self.activation),\n+                  'recurrent_activation': activations.serialize(self.recurrent_activation),\n+                  'use_bias': self.use_bias,\n+                  'kernel_initializer': initializers.serialize(self.kernel_initializer),\n+                  'recurrent_initializer': initializers.serialize(self.recurrent_initializer),\n+                  'bias_initializer': initializers.serialize(self.bias_initializer),\n+                  'kernel_regularizer': regularizers.serialize(kernel_regularizer),\n+                  'recurrent_regularizer': regularizers.serialize(recurrent_regularizer),\n+                  'bias_regularizer': regularizers.serialize(bias_regularizer),\n+                  'kernel_constraint': constraints.serialize(self.kernel_constraint),\n+                  'recurrent_constraint': constraints.serialize(self.recurrent_constraint),\n+                  'bias_constraint': constraints.serialize(self.bias_constraint),\n+                  'dropout': self.dropout,\n+                  'recurrent_dropout': self.recurrent_dropout}\n         base_config = super(GRU, self).get_config()\n         return dict(list(base_config.items()) + list(config.items()))\n \n@@ -647,26 +777,50 @@ class LSTM(Recurrent):\n     [this tutorial](http://deeplearning.net/tutorial/lstm.html).\n \n     # Arguments\n-        output_dim: dimension of the internal projections and the final output.\n-        init: weight initializer function.\n-            Can be the name of an existing function (str),\n-            or a Theano function (see: [initializers](../initializers.md)).\n-        inner_init: initializer function of the inner cells.\n-        forget_bias_init: initializer function for the bias of the forget gate.\n-            [Jozefowicz et al.](http://www.jmlr.org/proceedings/papers/v37/jozefowicz15.pdf)\n-            recommend initializing with ones.\n-        activation: activation function.\n-            Can be the name of an existing function (str),\n-            or a Theano function (see: [activations](../activations.md)).\n-        inner_activation: activation function for the inner cells.\n-        W_regularizer: instance of [WeightRegularizer](../regularizers.md)\n-            (eg. L1 or L2 regularization), applied to the input weights matrices.\n-        U_regularizer: instance of [WeightRegularizer](../regularizers.md)\n-            (eg. L1 or L2 regularization), applied to the recurrent weights matrices.\n-        b_regularizer: instance of [WeightRegularizer](../regularizers.md),\n-            applied to the bias.\n-        dropout_W: float between 0 and 1. Fraction of the input units to drop for input gates.\n-        dropout_U: float between 0 and 1. Fraction of the input units to drop for recurrent connections.\n+        units: Positive integer, dimensionality of the output space.\n+        activation: Activation function to use\n+            (see [activations](../activations.md)).\n+            If you don't specify anything, no activation is applied\n+            (ie. \"linear\" activation: `a(x) = x`).\n+        use_bias: Boolean, whether the layer uses a bias vector.\n+        kernel_initializer: Initializer for the `kernel` weights matrix,\n+            used for the linear transformation of the inputs.\n+            (see [initializers](../initializers.md)).\n+        recurrent_initializer: Initializer for the `recurrent_kernel`\n+            weights matrix,\n+            used for the linear transformation of the recurrent state.\n+            (see [initializers](../initializers.md)).\n+        bias_initializer: Initializer for the bias vector\n+            (see [initializers](../initializers.md)).\n+        unit_forget_bias: Boolean.\n+            If True, add 1 to the bias of the forget gate at initialization.\n+            Use in combination with `bias_initializer=\"zeros\"`.\n+            This is recommended in [Jozefowicz et al.](http://www.jmlr.org/proceedings/papers/v37/jozefowicz15.pdf)\n+        kernel_regularizer: Regularizer function applied to\n+            the `kernel` weights matrix\n+            (see [regularizer](../regularizers.md)).\n+        recurrent_regularizer: Regularizer function applied to\n+            the `recurrent_kernel` weights matrix\n+            (see [regularizer](../regularizers.md)).\n+        bias_regularizer: Regularizer function applied to the bias vector\n+            (see [regularizer](../regularizers.md)).\n+        activity_regularizer: Regularizer function applied to\n+            the output of the layer (its \"activation\").\n+            (see [regularizer](../regularizers.md)).\n+        kernel_constraint: Constraint function applied to\n+            the `kernel` weights matrix\n+            (see [constraints](../constraints.md)).\n+        recurrent_constraint: Constraint function applied to\n+            the `recurrent_kernel` weights matrix\n+            (see [constraints](../constraints.md)).\n+        bias_constraint: Constraint function applied to the bias vector\n+            (see [constraints](../constraints.md)).\n+        dropout: Float between 0 and 1.\n+            Fraction of the units to drop for\n+            the linear transformation of the inputs.\n+        recurrent_dropout: Float between 0 and 1.\n+            Fraction of the units to drop for\n+            the linear transformation of the recurrent state.\n \n     # References\n         - [Long short-term memory](http://deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf) (original 1997 paper)\n@@ -675,27 +829,45 @@ class LSTM(Recurrent):\n         - [A Theoretically Grounded Application of Dropout in Recurrent Neural Networks](http://arxiv.org/abs/1512.05287)\n     \"\"\"\n \n-    def __init__(self, output_dim,\n-                 init='glorot_uniform', inner_init='orthogonal',\n-                 forget_bias_init='one', activation='tanh',\n-                 inner_activation='hard_sigmoid',\n-                 W_regularizer=None, U_regularizer=None, b_regularizer=None,\n-                 dropout_W=0., dropout_U=0., **kwargs):\n+    def __init__(self, units,\n+                 activation='tanh',\n+                 recurrent_activation='hard_sigmoid',\n+                 use_bias=True,\n+                 kernel_initializer='glorot_uniform',\n+                 recurrent_initializer='orthogonal',\n+                 bias_initializer='zeros',\n+                 unit_forget_bias=True,\n+                 kernel_regularizer=None,\n+                 recurrent_regularizer=None,\n+                 bias_regularizer=None,\n+                 activity_regularizer=None,\n+                 kernel_constraint=None,\n+                 recurrent_constraint=None,\n+                 bias_constraint=None,\n+                 dropout=0.,\n+                 recurrent_dropout=0.,\n+                 **kwargs):\n         super(LSTM, self).__init__(**kwargs)\n-        self.output_dim = output_dim\n-        self.init = initializers.get(init)\n-        self.inner_init = initializers.get(inner_init)\n-        self.forget_bias_init = initializers.get(forget_bias_init)\n+        self.units = units\n         self.activation = activations.get(activation)\n-        self.inner_activation = activations.get(inner_activation)\n-        self.W_regularizer = regularizers.get(W_regularizer)\n-        self.U_regularizer = regularizers.get(U_regularizer)\n-        self.b_regularizer = regularizers.get(b_regularizer)\n-        self.dropout_W = dropout_W\n-        self.dropout_U = dropout_U\n+        self.recurrent_activation = activations.get(recurrent_activation)\n+        self.use_bias = use_bias\n \n-        if self.dropout_W or self.dropout_U:\n-            self.uses_learning_phase = True\n+        self.kernel_initializer = initializers.get(kernel_initializer)\n+        self.recurrent_initializer = initializers.get(recurrent_initializer)\n+        self.bias_initializer = initializers.get(bias_initializer)\n+        self.unit_forget_bias = unit_forget_bias\n+\n+        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n+        self.recurrent_regularizer = regularizers.get(recurrent_regularizer)\n+        self.bias_regularizer = regularizers.get(bias_regularizer)\n+\n+        self.kernel_constraint = constraints.get(kernel_constraint)\n+        self.recurrent_constraint = constraints.get(recurrent_constraint)\n+        self.bias_constraint = constraints.get(bias_constraint)\n+\n+        self.dropout = min(1., max(0., dropout))\n+        self.recurrent_dropout = min(1., max(0., recurrent_dropout))\n \n     def build(self, input_shape):\n         self.input_spec = [InputSpec(shape=input_shape)]\n@@ -707,198 +879,182 @@ class LSTM(Recurrent):\n             # initial states: 2 all-zero tensors of shape (output_dim)\n             self.states = [None, None]\n \n-        if self.consume_less == 'gpu':\n-            self.W = self.add_weight((self.input_dim, 4 * self.output_dim),\n-                                     initializer=self.init,\n-                                     name='{}_W'.format(self.name),\n-                                     regularizer=self.W_regularizer)\n-            self.U = self.add_weight((self.output_dim, 4 * self.output_dim),\n-                                     initializer=self.inner_init,\n-                                     name='{}_U'.format(self.name),\n-                                     regularizer=self.U_regularizer)\n-\n-            def b_reg(shape, name=None):\n-                return K.variable(np.hstack((np.zeros(self.output_dim),\n-                                             K.get_value(self.forget_bias_init((self.output_dim,))),\n-                                             np.zeros(self.output_dim),\n-                                             np.zeros(self.output_dim))),\n-                                  name='{}_b'.format(self.name))\n-            self.b = self.add_weight((self.output_dim * 4,),\n-                                     initializer=b_reg,\n-                                     name='{}_b'.format(self.name),\n-                                     regularizer=self.b_regularizer)\n+        self.kernel = self.add_weight((self.input_dim, self.units * 4),\n+                                      name='kernel',\n+                                      initializer=self.kernel_initializer,\n+                                      regularizer=self.kernel_regularizer,\n+                                      constraint=self.kernel_constraint)\n+        self.recurrent_kernel = self.add_weight(\n+            (self.units, self.units * 4),\n+            name='recurrent_kernel',\n+            initializer=self.recurrent_initializer,\n+            regularizer=self.recurrent_regularizer,\n+            constraint=self.recurrent_constraint)\n+\n+        if self.use_bias:\n+            self.bias = self.add_weight((self.units * 4,),\n+                                        name='bias',\n+                                        initializer=self.bias_initializer,\n+                                        regularizer=self.bias_regularizer,\n+                                        constraint=self.bias_constraint)\n+            if self.unit_forget_bias:\n+                self.bias += K.reshape(K.ones((self.units,)),\n+                                       (1, self.units, 1, 1))\n         else:\n-            self.W_i = self.add_weight((self.input_dim, self.output_dim),\n-                                       initializer=self.init,\n-                                       name='{}_W_i'.format(self.name),\n-                                       regularizer=self.W_regularizer)\n-            self.U_i = self.add_weight((self.output_dim, self.output_dim),\n-                                       initializer=self.init,\n-                                       name='{}_U_i'.format(self.name),\n-                                       regularizer=self.W_regularizer)\n-            self.b_i = self.add_weight((self.output_dim,),\n-                                       initializer='zero',\n-                                       name='{}_b_i'.format(self.name),\n-                                       regularizer=self.b_regularizer)\n-            self.W_f = self.add_weight((self.input_dim, self.output_dim),\n-                                       initializer=self.init,\n-                                       name='{}_W_f'.format(self.name),\n-                                       regularizer=self.W_regularizer)\n-            self.U_f = self.add_weight((self.output_dim, self.output_dim),\n-                                       initializer=self.init,\n-                                       name='{}_U_f'.format(self.name),\n-                                       regularizer=self.W_regularizer)\n-            self.b_f = self.add_weight((self.output_dim,),\n-                                       initializer=self.forget_bias_init,\n-                                       name='{}_b_f'.format(self.name),\n-                                       regularizer=self.b_regularizer)\n-            self.W_c = self.add_weight((self.input_dim, self.output_dim),\n-                                       initializer=self.init,\n-                                       name='{}_W_c'.format(self.name),\n-                                       regularizer=self.W_regularizer)\n-            self.U_c = self.add_weight((self.output_dim, self.output_dim),\n-                                       initializer=self.init,\n-                                       name='{}_U_c'.format(self.name),\n-                                       regularizer=self.W_regularizer)\n-            self.b_c = self.add_weight((self.output_dim,),\n-                                       initializer='zero',\n-                                       name='{}_b_c'.format(self.name),\n-                                       regularizer=self.b_regularizer)\n-            self.W_o = self.add_weight((self.input_dim, self.output_dim),\n-                                       initializer=self.init,\n-                                       name='{}_W_o'.format(self.name),\n-                                       regularizer=self.W_regularizer)\n-            self.U_o = self.add_weight((self.output_dim, self.output_dim),\n-                                       initializer=self.init,\n-                                       name='{}_U_o'.format(self.name),\n-                                       regularizer=self.W_regularizer)\n-            self.b_o = self.add_weight((self.output_dim,),\n-                                       initializer='zero',\n-                                       name='{}_b_o'.format(self.name),\n-                                       regularizer=self.b_regularizer)\n-\n-            self.trainable_weights = [self.W_i, self.U_i, self.b_i,\n-                                      self.W_c, self.U_c, self.b_c,\n-                                      self.W_f, self.U_f, self.b_f,\n-                                      self.W_o, self.U_o, self.b_o]\n-            self.W = K.concatenate([self.W_i, self.W_f, self.W_c, self.W_o])\n-            self.U = K.concatenate([self.U_i, self.U_f, self.U_c, self.U_o])\n-            self.b = K.concatenate([self.b_i, self.b_f, self.b_c, self.b_o])\n-\n-        if self.initial_weights is not None:\n-            self.set_weights(self.initial_weights)\n-            del self.initial_weights\n-        self.built = True\n+            self.bias = None\n+\n+        self.kernel_i = self.kernel[:, :self.units]\n+        self.recurrent_kernel_i = self.recurrent_kernel[:, :self.units]\n+        self.kernel_f = self.kernel[:, self.units: self.units * 2]\n+        self.recurrent_kernel_f = self.recurrent_kernel[:, self.units: self.units * 2]\n+        self.kernel_c = self.kernel[:, self.units * 2: self.units * 3]\n+        self.recurrent_kernel_c = self.recurrent_kernel[:, self.units * 2: self.units * 3]\n+        self.kernel_o = self.kernel[:, self.units * 3:]\n+        self.recurrent_kernel_o = self.recurrent_kernel[:, self.units * 3:]\n+\n+        if self.use_bias:\n+            self.bias_i = self.bias[:self.units]\n+            self.bias_f = self.bias[self.units: self.units * 2]\n+            self.bias_c = self.bias[self.units * 2: self.units * 3]\n+            self.bias_o = self.bias[self.units * 3:]\n+        else:\n+            self.bias_i = None\n+            self.bias_f = None\n+            self.bias_c = None\n+            self.bias_o = None\n \n     def reset_states(self):\n         assert self.stateful, 'Layer must be stateful.'\n         input_shape = self.input_spec[0].shape\n         if not input_shape[0]:\n             raise ValueError('If a RNN is stateful, a complete ' +\n-                             'input_shape must be provided (including batch size).')\n+                             'input_shape must be provided '\n+                             '(including batch size).')\n         if hasattr(self, 'states'):\n             K.set_value(self.states[0],\n-                        np.zeros((input_shape[0], self.output_dim)))\n+                        np.zeros((input_shape[0], self.units)))\n             K.set_value(self.states[1],\n-                        np.zeros((input_shape[0], self.output_dim)))\n+                        np.zeros((input_shape[0], self.units)))\n         else:\n-            self.states = [K.zeros((input_shape[0], self.output_dim)),\n-                           K.zeros((input_shape[0], self.output_dim))]\n+            self.states = [K.zeros((input_shape[0], self.units)),\n+                           K.zeros((input_shape[0], self.units))]\n \n-    def preprocess_input(self, x):\n-        if self.consume_less == 'cpu':\n-            if 0 < self.dropout_W < 1:\n-                dropout = self.dropout_W\n-            else:\n-                dropout = 0\n-            input_shape = K.int_shape(x)\n+    def preprocess_input(self, inputs, training=None):\n+        if self.implementation == 0:\n+            input_shape = K.int_shape(inputs)\n             input_dim = input_shape[2]\n             timesteps = input_shape[1]\n \n-            x_i = time_distributed_dense(x, self.W_i, self.b_i, dropout,\n-                                         input_dim, self.output_dim, timesteps)\n-            x_f = time_distributed_dense(x, self.W_f, self.b_f, dropout,\n-                                         input_dim, self.output_dim, timesteps)\n-            x_c = time_distributed_dense(x, self.W_c, self.b_c, dropout,\n-                                         input_dim, self.output_dim, timesteps)\n-            x_o = time_distributed_dense(x, self.W_o, self.b_o, dropout,\n-                                         input_dim, self.output_dim, timesteps)\n+            x_i = _time_distributed_dense(inputs, self.kernel_i, self.bias_i,\n+                                          self.dropout, input_dim, self.units,\n+                                          timesteps, training=training)\n+            x_f = _time_distributed_dense(inputs, self.kernel_f, self.bias_f,\n+                                          self.dropout, input_dim, self.units,\n+                                          timesteps, training=training)\n+            x_c = _time_distributed_dense(inputs, self.kernel_c, self.bias_c,\n+                                          self.dropout, input_dim, self.units,\n+                                          timesteps, training=training)\n+            x_o = _time_distributed_dense(inputs, self.kernel_o, self.bias_o,\n+                                          self.dropout, input_dim, self.units,\n+                                          timesteps, training=training)\n             return K.concatenate([x_i, x_f, x_c, x_o], axis=2)\n         else:\n-            return x\n+            return inputs\n+\n+    def get_constants(self, inputs, training=None):\n+        constants = []\n+        if self.implementation == 0 and 0 < self.dropout < 1:\n+            input_shape = K.int_shape(inputs)\n+            input_dim = input_shape[-1]\n+            ones = K.ones_like(K.reshape(inputs[:, 0, 0], (-1, 1)))\n+            ones = K.tile(ones, (1, int(input_dim)))\n \n-    def step(self, x, states):\n+            def dropped_inputs():\n+                return K.dropout(ones, self.dropout)\n+\n+            dp_mask = [K.in_train_phase(dropped_inputs,\n+                                        ones,\n+                                        training=training) for _ in range(4)]\n+            constants.append(dp_mask)\n+        else:\n+            constants.append([K.cast_to_floatx(1.) for _ in range(4)])\n+\n+        if 0 < self.recurrent_dropout < 1:\n+            ones = K.ones_like(K.reshape(inputs[:, 0, 0], (-1, 1)))\n+            ones = K.tile(ones, (1, self.units))\n+\n+            def dropped_inputs():\n+                return K.dropout(ones, self.recurrent_dropout)\n+            rec_dp_mask = [K.in_train_phase(dropped_inputs,\n+                                            ones,\n+                                            training=training) for _ in range(4)]\n+            constants.append(rec_dp_mask)\n+        else:\n+            constants.append([K.cast_to_floatx(1.) for _ in range(4)])\n+        return constants\n+\n+    def step(self, inputs, states):\n         h_tm1 = states[0]\n         c_tm1 = states[1]\n-        B_U = states[2]\n-        B_W = states[3]\n+        dp_mask = states[2]\n+        rec_dp_mask = states[3]\n \n-        if self.consume_less == 'gpu':\n-            z = K.dot(x * B_W[0], self.W) + K.dot(h_tm1 * B_U[0], self.U) + self.b\n+        if self.implementation == 1:\n+            z = K.dot(inputs * dp_mask[0], self.kernel)\n+            z += K.dot(h_tm1 * rec_dp_mask[0], self.recurrent_kernel)\n+            if self.use_bias:\n+                z += self.bias\n \n-            z0 = z[:, :self.output_dim]\n-            z1 = z[:, self.output_dim: 2 * self.output_dim]\n-            z2 = z[:, 2 * self.output_dim: 3 * self.output_dim]\n-            z3 = z[:, 3 * self.output_dim:]\n+            z0 = z[:, :self.units]\n+            z1 = z[:, self.units: 2 * self.units]\n+            z2 = z[:, 2 * self.units: 3 * self.units]\n+            z3 = z[:, 3 * self.units:]\n \n             i = self.inner_activation(z0)\n             f = self.inner_activation(z1)\n             c = f * c_tm1 + i * self.activation(z2)\n             o = self.inner_activation(z3)\n         else:\n-            if self.consume_less == 'cpu':\n-                x_i = x[:, :self.output_dim]\n-                x_f = x[:, self.output_dim: 2 * self.output_dim]\n-                x_c = x[:, 2 * self.output_dim: 3 * self.output_dim]\n-                x_o = x[:, 3 * self.output_dim:]\n-            elif self.consume_less == 'mem':\n-                x_i = K.dot(x * B_W[0], self.W_i) + self.b_i\n-                x_f = K.dot(x * B_W[1], self.W_f) + self.b_f\n-                x_c = K.dot(x * B_W[2], self.W_c) + self.b_c\n-                x_o = K.dot(x * B_W[3], self.W_o) + self.b_o\n+            if self.implementation == 0:\n+                x_i = inputs[:, :self.units]\n+                x_f = inputs[:, self.units: 2 * self.units]\n+                x_c = inputs[:, 2 * self.units: 3 * self.units]\n+                x_o = inputs[:, 3 * self.units:]\n+            elif self.implementation == 2:\n+                x_i = K.dot(inputs * dp_mask[0], self.kernel_i) + self.bias_i\n+                x_f = K.dot(inputs * dp_mask[1], self.kernel_f) + self.bias_f\n+                x_c = K.dot(inputs * dp_mask[2], self.kernel_c) + self.bias_c\n+                x_o = K.dot(inputs * dp_mask[3], self.kernel_o) + self.bias_o\n             else:\n-                raise ValueError('Unknown `consume_less` mode.')\n-\n-            i = self.inner_activation(x_i + K.dot(h_tm1 * B_U[0], self.U_i))\n-            f = self.inner_activation(x_f + K.dot(h_tm1 * B_U[1], self.U_f))\n-            c = f * c_tm1 + i * self.activation(x_c + K.dot(h_tm1 * B_U[2], self.U_c))\n-            o = self.inner_activation(x_o + K.dot(h_tm1 * B_U[3], self.U_o))\n-\n+                raise ValueError('Unknown `implementation` mode.')\n+\n+            i = self.inner_activation(x_i + K.dot(h_tm1 * rec_dp_mask[0],\n+                                                  self.recurrent_kernel_i))\n+            f = self.inner_activation(x_f + K.dot(h_tm1 * rec_dp_mask[1],\n+                                                  self.recurrent_kernel_f))\n+            c = f * c_tm1 + i * self.activation(x_c + K.dot(h_tm1 * rec_dp_mask[2],\n+                                                self.recurrent_kernel_c))\n+            o = self.inner_activation(x_o + K.dot(h_tm1 * rec_dp_mask[3],\n+                                                  self.recurrent_kernel_o))\n         h = o * self.activation(c)\n         return h, [h, c]\n \n-    def get_constants(self, x):\n-        constants = []\n-        if 0 < self.dropout_U < 1:\n-            ones = K.ones_like(K.reshape(x[:, 0, 0], (-1, 1)))\n-            ones = K.tile(ones, (1, self.output_dim))\n-            B_U = [K.in_train_phase(K.dropout(ones, self.dropout_U), ones) for _ in range(4)]\n-            constants.append(B_U)\n-        else:\n-            constants.append([K.cast_to_floatx(1.) for _ in range(4)])\n-\n-        if 0 < self.dropout_W < 1:\n-            input_shape = K.int_shape(x)\n-            input_dim = input_shape[-1]\n-            ones = K.ones_like(K.reshape(x[:, 0, 0], (-1, 1)))\n-            ones = K.tile(ones, (1, int(input_dim)))\n-            B_W = [K.in_train_phase(K.dropout(ones, self.dropout_W), ones) for _ in range(4)]\n-            constants.append(B_W)\n-        else:\n-            constants.append([K.cast_to_floatx(1.) for _ in range(4)])\n-        return constants\n-\n     def get_config(self):\n-        config = {'output_dim': self.output_dim,\n-                  'init': initializers.get_config(self.init),\n-                  'inner_init': initializers.get_config(self.inner_init),\n-                  'forget_bias_init': initializers.get_config(self.forget_bias_init),\n-                  'activation': self.activation.__name__,\n-                  'inner_activation': self.inner_activation.__name__,\n-                  'W_regularizer': self.W_regularizer.get_config() if self.W_regularizer else None,\n-                  'U_regularizer': self.U_regularizer.get_config() if self.U_regularizer else None,\n-                  'b_regularizer': self.b_regularizer.get_config() if self.b_regularizer else None,\n-                  'dropout_W': self.dropout_W,\n-                  'dropout_U': self.dropout_U}\n+        config = {'units': self.units,\n+                  'activation': activations.serialize(self.activation),\n+                  'recurrent_activation': activations.serialize(self.recurrent_activation),\n+                  'use_bias': self.use_bias,\n+                  'kernel_initializer': initializers.serialize(self.kernel_initializer),\n+                  'recurrent_initializer': initializers.serialize(self.recurrent_initializer),\n+                  'bias_initializer': initializers.serialize(self.bias_initializer),\n+                  'unit_forget_bias': self.unit_forget_bias,\n+                  'kernel_regularizer': regularizers.serialize(kernel_regularizer),\n+                  'recurrent_regularizer': regularizers.serialize(recurrent_regularizer),\n+                  'bias_regularizer': regularizers.serialize(bias_regularizer),\n+                  'kernel_constraint': constraints.serialize(self.kernel_constraint),\n+                  'recurrent_constraint': constraints.serialize(self.recurrent_constraint),\n+                  'bias_constraint': constraints.serialize(self.bias_constraint),\n+                  'dropout': self.dropout,\n+                  'recurrent_dropout': self.recurrent_dropout}\n         base_config = super(LSTM, self).get_config()\n         return dict(list(base_config.items()) + list(config.items()))\n",
          "files_name_in_blame_commit": [
            "embeddings.py",
            "advanced_activations.py",
            "wrappers.py",
            "training.py",
            "noise.py",
            "pooling.py",
            "topology.py",
            "theano_backend.py",
            "normalization.py",
            "convolutional.py",
            "recurrent.py",
            "local.py",
            "core.py"
          ]
        }
      },
      "295bfe4e3ae7e98655b3630a9f83b2df4a82234f": {
        "commit": {
          "commit_id": "295bfe4e3ae7e98655b3630a9f83b2df4a82234f",
          "commit_message": "Keras 1.0 preview.",
          "commit_author": "Francois Chollet",
          "commit_date": "2016-03-31 11:35:27",
          "commit_parent": "1145fec39ff7f665f7bd352261549f8738de815e"
        },
        "function": {
          "function_name": "build",
          "function_code_before": "def build(self):\n    input_shape = self.input_shape\n    if self.stateful:\n        self.reset_states()\n    else:\n        self.states = [None]\n    input_dim = input_shape[2]\n    self.input_dim = input_dim\n    self.W = self.init((input_dim, self.output_dim), name='{}_W'.format(self.name))\n    self.U = self.inner_init((self.output_dim, self.output_dim), name='{}_U'.format(self.name))\n    self.b = K.zeros((self.output_dim,), name='{}_b'.format(self.name))\n    self.regularizers = []\n    if self.W_regularizer:\n        self.W_regularizer.set_param(self.W)\n        self.regularizers.append(self.W_regularizer)\n    if self.U_regularizer:\n        self.W_regularizer.set_param(self.U)\n        self.regularizers.append(self.U_regularizer)\n    if self.b_regularizer:\n        self.W_regularizer.set_param(self.b)\n        self.regularizers.append(self.b_regularizer)\n    self.trainable_weights = [self.W, self.U, self.b]\n    if self.initial_weights is not None:\n        self.set_weights(self.initial_weights)\n        del self.initial_weights",
          "function_code_after": "def build(self, input_shape):\n    self.input_spec = [InputSpec(shape=input_shape)]\n    if self.stateful:\n        self.reset_states()\n    else:\n        self.states = [None]\n    input_dim = input_shape[2]\n    self.input_dim = input_dim\n    self.W = self.init((input_dim, self.output_dim), name='{}_W'.format(self.name))\n    self.U = self.inner_init((self.output_dim, self.output_dim), name='{}_U'.format(self.name))\n    self.b = K.zeros((self.output_dim,), name='{}_b'.format(self.name))\n    self.regularizers = []\n    if self.W_regularizer:\n        self.W_regularizer.set_param(self.W)\n        self.regularizers.append(self.W_regularizer)\n    if self.U_regularizer:\n        self.W_regularizer.set_param(self.U)\n        self.regularizers.append(self.U_regularizer)\n    if self.b_regularizer:\n        self.W_regularizer.set_param(self.b)\n        self.regularizers.append(self.b_regularizer)\n    self.trainable_weights = [self.W, self.U, self.b]\n    if self.initial_weights is not None:\n        self.set_weights(self.initial_weights)\n        del self.initial_weights",
          "function_before_start_line": 259,
          "function_before_end_line": 290,
          "function_after_start_line": 278,
          "function_after_end_line": 309,
          "function_before_token_count": 240,
          "function_after_token_count": 249,
          "functions_name_modified_file": [
            "__init__",
            "compute_mask",
            "get_output_shape_for",
            "time_distributed_dense",
            "build",
            "call",
            "get_constants",
            "step",
            "reset_states",
            "get_initial_states",
            "preprocess_input",
            "get_config"
          ],
          "functions_name_all_files": [
            "test_temporal_regression",
            "pool3d",
            "output",
            "train_model",
            "add_input",
            "get_gradients",
            "display_table",
            "save_weights",
            "test_AveragePooling2D",
            "run_internal_graph",
            "print_summary",
            "test_graph_fit_generator",
            "dropout",
            "get_layer",
            "resize_volumes",
            "tanh",
            "evaluate",
            "create_node",
            "get_input_shape_at",
            "standardize_weights",
            "test_Flatten",
            "step",
            "test_merge_concat",
            "get_weights",
            "stateful",
            "rnn",
            "test_merge_overlap",
            "create_base_network",
            "get_test_data",
            "eval",
            "test_sequential_regression",
            "ones",
            "generator_queue",
            "test_1o_1i",
            "test_2o_1i_sample_weights",
            "check_layer_output_shape",
            "dtype",
            "test_vector_regression",
            "continuity_loss",
            "equal",
            "check_single_tensor_operation",
            "cast_to_floatx",
            "hard_sigmoid",
            "clear_previous",
            "add_shared_node",
            "test_vector_classification",
            "test_sequential",
            "__init__",
            "compute_mask",
            "sqrt",
            "get_output_shape_for",
            "spatial_2d_padding",
            "random_binomial",
            "weighted_objective",
            "evaluate_generator",
            "get_input_at",
            "test_rnn",
            "shape",
            "total_variation_loss",
            "expand_dims",
            "categorical_crossentropy",
            "conv2d",
            "collect_metrics",
            "get_constants",
            "set_state",
            "content_loss",
            "test_elementwise_operations",
            "load_data",
            "test_sequence_to_sequence",
            "set_input",
            "test_UpSampling1D",
            "test_shape_operations",
            "test_RepeatVector",
            "test_AveragePooling1D",
            "cast",
            "int_shape",
            "compile",
            "trainable_weights",
            "_get_test_data",
            "set_floatx",
            "layer_cache",
            "test_gradient",
            "test_nested_sequential",
            "get_output_shape_at",
            "test_ZeroPadding2D",
            "concatenate",
            "test_merge_dot",
            "set_epsilon",
            "reshape",
            "test_node_construction",
            "check_array_lengths",
            "get_updates",
            "test_Reshape",
            "standardize_sample_or_class_weights",
            "fit",
            "test_switch",
            "compute_accuracy",
            "test_on_batch",
            "get_output",
            "get_config",
            "softplus",
            "_fix_unknown_dimension",
            "create_input_layer",
            "test_repeat_elements",
            "standardize_input_data",
            "predict",
            "conv_output_length",
            "standardize_sample_weights",
            "test_random_normal",
            "test_2o_1i_save_weights",
            "add_inbound_node",
            "test_random_uniform",
            "add_output",
            "get_uid",
            "contrastive_loss",
            "grads",
            "std",
            "spatial_3d_padding",
            "test_UpSampling2D",
            "build",
            "merge",
            "call",
            "in_train_phase",
            "binary_accuracy",
            "permute_dimensions",
            "updates",
            "make_batches",
            "add_node",
            "test_image_classification",
            "test_nn_operations",
            "test_Permute",
            "l2_normalize",
            "test_TimeDistributedDense",
            "_test_loop",
            "fit_generator",
            "ones_like",
            "output_shape",
            "sigmoid",
            "prod",
            "test_Convolution2D",
            "test_function",
            "pack",
            "get_session",
            "random_uniform",
            "standardize_class_weights",
            "euclidean_distance",
            "conv3d",
            "_predict_loop",
            "pool2d",
            "preprocess_image",
            "repeat_elements",
            "predict_proba",
            "check_two_tensor_operation",
            "test_masking",
            "log",
            "to_list",
            "reset_states",
            "make_tuple",
            "switch",
            "relu",
            "test_SimpleRNN",
            "count_params",
            "normalize_axis",
            "any",
            "loss",
            "get_input",
            "get_from_module",
            "function",
            "test_linear_operations",
            "set_weights",
            "random_normal",
            "zeros_like",
            "get_output_mask_at",
            "input",
            "_pooling_function",
            "test_MaxPooling2D",
            "test_count_params",
            "test_learning_phase",
            "normalize",
            "test_siamese_1",
            "load_weights",
            "tokenize",
            "_fit_loop",
            "binary_crossentropy",
            "test_merge_recursivity",
            "get_source_inputs",
            "preprocess_input",
            "softmax",
            "get_state",
            "batch_flatten",
            "_gather_list_attr",
            "_get_node_attribute_at_index",
            "_standardize_user_data",
            "decode",
            "vectorize_stories",
            "non_trainable_weights",
            "square",
            "encode",
            "input_mask",
            "nb_input",
            "squeeze",
            "input_shape",
            "model_from_yaml",
            "test_create_output",
            "constraints",
            "__call__",
            "test_stacked_lstm_char_prediction",
            "epsilon",
            "learning_phase",
            "style_loss",
            "placeholder",
            "abs",
            "test_Dense",
            "mean",
            "clip",
            "test_recursive",
            "model_from_json",
            "dot",
            "floatx",
            "variable",
            "exp",
            "in_test_phase",
            "state_updates",
            "test_layer",
            "test_masked_temporal",
            "get_input_mask_at",
            "test_ZeroPadding1D",
            "resize_images",
            "create_pairs",
            "cache_enabled",
            "shape_cache",
            "max",
            "sum",
            "test_recursion",
            "predict_on_batch",
            "test_1o_1i_2",
            "slice_X",
            "deprocess_image",
            "test_multi_input_layer",
            "train_on_batch",
            "minimum",
            "_get_x",
            "argmax",
            "_arguments_validation",
            "zeros",
            "set_value",
            "test_lambda_serialization",
            "test_sequential_count_params",
            "to_yaml",
            "tile",
            "flatten",
            "pow",
            "test_Convolution1D",
            "regularizers",
            "set_previous",
            "categorical_accuracy",
            "_gather_dict_attr",
            "argmin",
            "add",
            "gram_matrix",
            "parse_stories",
            "clip_norm",
            "test_random_binomial",
            "test_value_manipulation",
            "round",
            "output_mask",
            "test_temporal_classification",
            "temporal_padding",
            "test_1o_2i",
            "set_session",
            "repeat",
            "Input",
            "nb_output",
            "get_initial_states",
            "get_value",
            "get",
            "to_json",
            "input_spec",
            "transpose",
            "get_output_at",
            "ndim",
            "uses_learning_phase",
            "summary",
            "predict_classes",
            "test_merge_sum",
            "gradients",
            "test_MaxPooling1D",
            "update",
            "time_distributed_dense",
            "from_config",
            "_get_y",
            "batch_shuffle",
            "flattened_layers",
            "kl_divergence",
            "assert_input_compatibility",
            "min",
            "layer_from_config",
            "sample",
            "test_functional_guide",
            "test_sequential_fit_generator",
            "_on_gpu",
            "not_equal",
            "get_stories",
            "eval_loss_and_grads",
            "gather",
            "maximum"
          ],
          "functions_name_co_evolved_modified_file": [
            "__init__",
            "compute_mask",
            "get_output_shape_for",
            "output_shape",
            "time_distributed_dense",
            "call",
            "get_output_mask",
            "reset_states",
            "step",
            "get_constants",
            "preprocess_input",
            "get_output",
            "get_config"
          ],
          "functions_name_co_evolved_all_files": [
            "output",
            "train_model",
            "add_input",
            "display_table",
            "save_weights",
            "get_params",
            "get_output_join",
            "run_internal_graph",
            "print_summary",
            "test_graph_fit_generator",
            "get_layer",
            "evaluate",
            "create_node",
            "get_input_shape_at",
            "standardize_weights",
            "step",
            "test_merge_concat",
            "get_weights",
            "stateful",
            "rnn",
            "test_merge_overlap",
            "test_sequential_regression",
            "check_layer_output_shape",
            "test_1o_1i",
            "generator_queue",
            "set_layer_input",
            "test_siamese_2",
            "get_output_ave",
            "get_output_shape",
            "clear_previous",
            "add_shared_node",
            "_fit",
            "test_vector_classification",
            "test_sequential",
            "__init__",
            "compute_mask",
            "get_output_shape_for",
            "weighted_objective",
            "evaluate_generator",
            "get_input_at",
            "test_rnn",
            "get_constants",
            "collect_metrics",
            "load_data",
            "set_input",
            "compile",
            "trainable_weights",
            "_get_test_data",
            "layer_cache",
            "test_nested_sequential",
            "get_output_shape_at",
            "test_node_construction",
            "check_array_lengths",
            "get_output_cos",
            "get_updates",
            "standardize_sample_or_class_weights",
            "fit",
            "test_Reshape",
            "test_on_batch",
            "get_output",
            "get_config",
            "container_from_config",
            "get_output_mask",
            "_fix_unknown_dimension",
            "create_input_layer",
            "predict",
            "standardize_input_data",
            "standardize_sample_weights",
            "test_2o_1i_save_weights",
            "add_inbound_node",
            "test_lambda",
            "add_output",
            "get_uid",
            "contrastive_loss",
            "make_submission",
            "merge",
            "binary_accuracy",
            "call",
            "in_train_phase",
            "test_image_classification",
            "updates",
            "make_batches",
            "add_node",
            "_test_loop",
            "fit_generator",
            "output_shape",
            "add_shared_layer",
            "standardize_class_weights",
            "euclidean_distance",
            "predict_proba",
            "_predict_loop",
            "test_masking",
            "model_summary",
            "name",
            "test_siamese_3",
            "to_list",
            "reset_states",
            "switch",
            "count_params",
            "get_input",
            "set_input_shape",
            "set_weights",
            "get_output_concat",
            "get_output_mask_at",
            "input",
            "test_count_params",
            "test_learning_phase",
            "test_siamese_1",
            "load_weights",
            "_fit_loop",
            "test_merge_recursivity",
            "get_source_inputs",
            "preprocess_input",
            "supports_masked_input",
            "get_output_dot",
            "_gather_list_attr",
            "_get_node_attribute_at_index",
            "_standardize_user_data",
            "output_reconstruction",
            "non_trainable_weights",
            "nb_input",
            "input_mask",
            "input_shape",
            "model_from_yaml",
            "test_create_output",
            "constraints",
            "standardize_X",
            "__call__",
            "test_recursive",
            "trainable",
            "model_from_json",
            "preprocess_labels",
            "in_test_phase",
            "state_updates",
            "get_input_mask",
            "test_masked_temporal",
            "test_layer",
            "get_input_mask_at",
            "cache_enabled",
            "shape_cache",
            "test_1o_1i_2",
            "test_recursion",
            "predict_on_batch",
            "standardize_y",
            "slice_X",
            "test_multi_input_layer",
            "train_on_batch",
            "model_from_config",
            "_get_x",
            "test_lambda_serialization",
            "_arguments_validation",
            "get_output_mul",
            "to_yaml",
            "test_sequential_count_params",
            "regularizers",
            "set_previous",
            "categorical_accuracy",
            "_gather_dict_attr",
            "test_siamese_5",
            "add",
            "test_2o_1i_weights",
            "test_siamese_4",
            "output_mask",
            "test_temporal_classification",
            "test_1o_2i",
            "preprocess_data",
            "nb_output",
            "Input",
            "to_json",
            "input_spec",
            "uses_learning_phase",
            "summary",
            "get_output_at",
            "predict_classes",
            "test_merge_sum",
            "time_distributed_dense",
            "from_config",
            "_get_y",
            "batch_shuffle",
            "get_output_sum",
            "flattened_layers",
            "assert_input_compatibility",
            "layer_from_config",
            "test_functional_guide",
            "test_sequential_fit_generator",
            "get_function_name",
            "_check_generator_output"
          ]
        },
        "file": {
          "file_name": "recurrent.py",
          "file_nloc": 688,
          "file_complexity": 109,
          "file_token_count": 5105,
          "file_before": "# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import\nimport numpy as np\n\nfrom .. import backend as K\nfrom .. import activations, initializations, regularizers\nfrom ..layers.core import MaskedLayer\n\n\ndef time_distributed_dense(x, w, b=None, dropout=None,\n                           input_dim=None, output_dim=None, timesteps=None):\n    '''Apply y.w + b for every temporal slice y of x.\n    '''\n    if not input_dim:\n        # won't work with TensorFlow\n        input_dim = K.shape(x)[2]\n    if not timesteps:\n        # won't work with TensorFlow\n        timesteps = K.shape(x)[1]\n    if not output_dim:\n        # won't work with TensorFlow\n        output_dim = K.shape(w)[1]\n\n    if dropout:\n        # apply the same dropout pattern at every timestep\n        ones = K.ones_like(K.reshape(x[:, 0, :], (-1, input_dim)))\n        dropout_matrix = K.dropout(ones, dropout)\n        expanded_dropout_matrix = K.repeat(dropout_matrix, timesteps)\n        x *= expanded_dropout_matrix\n\n    # collapse time dimension and batch dimension together\n    x = K.reshape(x, (-1, input_dim))\n\n    x = K.dot(x, w)\n    if b:\n        x = x + b\n    # reshape to 3D tensor\n    x = K.reshape(x, (-1, timesteps, output_dim))\n    return x\n\n\nclass Recurrent(MaskedLayer):\n    '''Abstract base class for recurrent layers.\n    Do not use in a model -- it's not a functional layer!\n\n    All recurrent layers (GRU, LSTM, SimpleRNN) also\n    follow the specifications of this class and accept\n    the keyword arguments listed below.\n\n    # Input shape\n        3D tensor with shape `(nb_samples, timesteps, input_dim)`.\n\n    # Output shape\n        - if `return_sequences`: 3D tensor with shape\n            `(nb_samples, timesteps, output_dim)`.\n        - else, 2D tensor with shape `(nb_samples, output_dim)`.\n\n    # Arguments\n        weights: list of numpy arrays to set as initial weights.\n            The list should have 3 elements, of shapes:\n            `[(input_dim, output_dim), (output_dim, output_dim), (output_dim,)]`.\n        return_sequences: Boolean. Whether to return the last output\n            in the output sequence, or the full sequence.\n        go_backwards: Boolean (default False).\n            If True, process the input sequence backwards.\n        stateful: Boolean (default False). If True, the last state\n            for each sample at index i in a batch will be used as initial\n            state for the sample of index i in the following batch.\n        input_dim: dimensionality of the input (integer).\n            This argument (or alternatively, the keyword argument `input_shape`)\n            is required when using this layer as the first layer in a model.\n        input_length: Length of input sequences, to be specified\n            when it is constant.\n            This argument is required if you are going to connect\n            `Flatten` then `Dense` layers upstream\n            (without it, the shape of the dense outputs cannot be computed).\n            Note that if the recurrent layer is not the first layer\n            in your model, you would need to specify the input length\n            at the level of the first layer\n            (e.g. via the `input_shape` argument)\n\n    # Masking\n        This layer supports masking for input data with a variable number\n        of timesteps. To introduce masks to your data,\n        use an [Embedding](embeddings.md) layer with the `mask_zero` parameter\n        set to `True`.\n\n    # TensorFlow warning\n        For the time being, when using the TensorFlow backend,\n        the number of timesteps used must be specified in your model.\n        Make sure to pass an `input_length` int argument to your\n        recurrent layer (if it comes first in your model),\n        or to pass a complete `input_shape` argument to the first layer\n        in your model otherwise.\n\n\n    # Note on using statefulness in RNNs\n        You can set RNN layers to be 'stateful', which means that the states\n        computed for the samples in one batch will be reused as initial states\n        for the samples in the next batch.\n        This assumes a one-to-one mapping between\n        samples in different successive batches.\n\n        To enable statefulness:\n            - specify `stateful=True` in the layer constructor.\n            - specify a fixed batch size for your model, by passing\n                a `batch_input_shape=(...)` to the first layer in your model.\n                This is the expected shape of your inputs *including the batch size*.\n                It should be a tuple of integers, e.g. `(32, 10, 100)`.\n\n        To reset the states of your model, call `.reset_states()` on either\n        a specific layer, or on your entire model.\n\n    # Note on using dropout with TensorFlow\n        When using the TensorFlow backend, specify a fixed batch size for your model\n        following the notes on statefulness RNNs.\n    '''\n    input_ndim = 3\n\n    def __init__(self, weights=None,\n                 return_sequences=False, go_backwards=False, stateful=False,\n                 input_dim=None, input_length=None, **kwargs):\n        self.return_sequences = return_sequences\n        self.initial_weights = weights\n        self.go_backwards = go_backwards\n        self.stateful = stateful\n\n        self.input_dim = input_dim\n        self.input_length = input_length\n        if self.input_dim:\n            kwargs['input_shape'] = (self.input_length, self.input_dim)\n        super(Recurrent, self).__init__(**kwargs)\n\n    def get_output_mask(self, train=False):\n        if self.return_sequences:\n            return super(Recurrent, self).get_output_mask(train)\n        else:\n            return None\n\n    @property\n    def output_shape(self):\n        input_shape = self.input_shape\n        if self.return_sequences:\n            return (input_shape[0], input_shape[1], self.output_dim)\n        else:\n            return (input_shape[0], self.output_dim)\n\n    def step(self, x, states):\n        raise NotImplementedError\n\n    def get_constants(self, x, train=False):\n        return []\n\n    def get_initial_states(self, x):\n        # build an all-zero tensor of shape (samples, output_dim)\n        initial_state = K.zeros_like(x)  # (samples, timesteps, input_dim)\n        initial_state = K.sum(initial_state, axis=1)  # (samples, input_dim)\n        reducer = K.zeros((self.input_dim, self.output_dim))\n        initial_state = K.dot(initial_state, reducer)  # (samples, output_dim)\n        initial_states = [initial_state for _ in range(len(self.states))]\n        return initial_states\n\n    def preprocess_input(self, x, train=False):\n        return x\n\n    def get_output(self, train=False):\n        # input shape: (nb_samples, time (padded with zeros), input_dim)\n        X = self.get_input(train)\n        mask = self.get_input_mask(train)\n\n        assert K.ndim(X) == 3\n        if K._BACKEND == 'tensorflow':\n            if not self.input_shape[1]:\n                raise Exception('When using TensorFlow, you should define ' +\n                                'explicitly the number of timesteps of ' +\n                                'your sequences.\\n' +\n                                'If your first layer is an Embedding, ' +\n                                'make sure to pass it an \"input_length\" ' +\n                                'argument. Otherwise, make sure ' +\n                                'the first layer has ' +\n                                'an \"input_shape\" or \"batch_input_shape\" ' +\n                                'argument, including the time axis.')\n        if self.stateful:\n            initial_states = self.states\n        else:\n            initial_states = self.get_initial_states(X)\n        constants = self.get_constants(X, train)\n        preprocessed_input = self.preprocess_input(X, train)\n\n        last_output, outputs, states = K.rnn(self.step, preprocessed_input,\n                                             initial_states,\n                                             go_backwards=self.go_backwards,\n                                             mask=mask,\n                                             constants=constants)\n        if self.stateful:\n            self.updates = []\n            for i in range(len(states)):\n                self.updates.append((self.states[i], states[i]))\n\n        if self.return_sequences:\n            return outputs\n        else:\n            return last_output\n\n    def get_config(self):\n        config = {\"name\": self.__class__.__name__,\n                  \"return_sequences\": self.return_sequences,\n                  \"go_backwards\": self.go_backwards,\n                  \"stateful\": self.stateful}\n        if self.stateful:\n            config['batch_input_shape'] = self.input_shape\n        else:\n            config['input_dim'] = self.input_dim\n            config['input_length'] = self.input_length\n\n        base_config = super(Recurrent, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\n\nclass SimpleRNN(Recurrent):\n    '''Fully-connected RNN where the output is to be fed back to input.\n\n    # Arguments\n        output_dim: dimension of the internal projections and the final output.\n        init: weight initialization function.\n            Can be the name of an existing function (str),\n            or a Theano function (see: [initializations](../initializations.md)).\n        inner_init: initialization function of the inner cells.\n        activation: activation function.\n            Can be the name of an existing function (str),\n            or a Theano function (see: [activations](../activations.md)).\n        W_regularizer: instance of [WeightRegularizer](../regularizers.md)\n            (eg. L1 or L2 regularization), applied to the input weights matrices.\n        U_regularizer: instance of [WeightRegularizer](../regularizers.md)\n            (eg. L1 or L2 regularization), applied to the recurrent weights matrices.\n        b_regularizer: instance of [WeightRegularizer](../regularizers.md),\n            applied to the bias.\n        dropout_W: float between 0 and 1. Fraction of the input units to drop for input gates.\n        dropout_U: float between 0 and 1. Fraction of the input units to drop for recurrent connections.\n\n    # References\n        - [A Theoretically Grounded Application of Dropout in Recurrent Neural Networks](http://arxiv.org/abs/1512.05287)\n    '''\n    def __init__(self, output_dim,\n                 init='glorot_uniform', inner_init='orthogonal',\n                 activation='tanh',\n                 W_regularizer=None, U_regularizer=None, b_regularizer=None,\n                 dropout_W=0., dropout_U=0., **kwargs):\n        self.output_dim = output_dim\n        self.init = initializations.get(init)\n        self.inner_init = initializations.get(inner_init)\n        self.activation = activations.get(activation)\n        self.W_regularizer = regularizers.get(W_regularizer)\n        self.U_regularizer = regularizers.get(U_regularizer)\n        self.b_regularizer = regularizers.get(b_regularizer)\n        self.dropout_W, self.dropout_U = dropout_W, dropout_U\n        super(SimpleRNN, self).__init__(**kwargs)\n\n    def build(self):\n        input_shape = self.input_shape\n        if self.stateful:\n            self.reset_states()\n        else:\n            # initial states: all-zero tensor of shape (output_dim)\n            self.states = [None]\n        input_dim = input_shape[2]\n        self.input_dim = input_dim\n\n        self.W = self.init((input_dim, self.output_dim),\n                           name='{}_W'.format(self.name))\n        self.U = self.inner_init((self.output_dim, self.output_dim),\n                                 name='{}_U'.format(self.name))\n        self.b = K.zeros((self.output_dim,), name='{}_b'.format(self.name))\n\n        self.regularizers = []\n        if self.W_regularizer:\n            self.W_regularizer.set_param(self.W)\n            self.regularizers.append(self.W_regularizer)\n        if self.U_regularizer:\n            self.W_regularizer.set_param(self.U)\n            self.regularizers.append(self.U_regularizer)\n        if self.b_regularizer:\n            self.W_regularizer.set_param(self.b)\n            self.regularizers.append(self.b_regularizer)\n\n        self.trainable_weights = [self.W, self.U, self.b]\n\n        if self.initial_weights is not None:\n            self.set_weights(self.initial_weights)\n            del self.initial_weights\n\n    def reset_states(self):\n        assert self.stateful, 'Layer must be stateful.'\n        input_shape = self.input_shape\n        if not input_shape[0]:\n            raise Exception('If a RNN is stateful, a complete ' +\n                            'input_shape must be provided (including batch size).')\n        if hasattr(self, 'states'):\n            K.set_value(self.states[0],\n                        np.zeros((input_shape[0], self.output_dim)))\n        else:\n            self.states = [K.zeros((input_shape[0], self.output_dim))]\n\n    def preprocess_input(self, x, train=False):\n        if train and (0 < self.dropout_W < 1):\n            dropout = self.dropout_W\n        else:\n            dropout = 0\n        input_shape = self.input_shape\n        input_dim = input_shape[2]\n        timesteps = input_shape[1]\n        return time_distributed_dense(x, self.W, self.b, dropout,\n                                      input_dim, self.output_dim, timesteps)\n\n    def step(self, h, states):\n        prev_output = states[0]\n        if len(states) == 2:\n            B_U = states[1]\n        else:\n            B_U = 1.\n        output = self.activation(h + K.dot(prev_output * B_U, self.U))\n        return output, [output]\n\n    def get_constants(self, x, train=False):\n        if train and (0 < self.dropout_U < 1):\n            ones = K.ones_like(K.reshape(x[:, 0, 0], (-1, 1)))\n            ones = K.concatenate([ones] * self.output_dim, 1)\n            B_U = K.dropout(ones, self.dropout_U)\n            return [B_U]\n        return []\n\n    def get_config(self):\n        config = {\"output_dim\": self.output_dim,\n                  \"init\": self.init.__name__,\n                  \"inner_init\": self.inner_init.__name__,\n                  \"activation\": self.activation.__name__,\n                  \"W_regularizer\": self.W_regularizer.get_config() if self.W_regularizer else None,\n                  \"U_regularizer\": self.U_regularizer.get_config() if self.U_regularizer else None,\n                  \"b_regularizer\": self.b_regularizer.get_config() if self.b_regularizer else None,\n                  \"dropout_W\": self.dropout_W,\n                  \"dropout_U\": self.dropout_U}\n        base_config = super(SimpleRNN, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\n\nclass GRU(Recurrent):\n    '''Gated Recurrent Unit - Cho et al. 2014.\n\n    # Arguments\n        output_dim: dimension of the internal projections and the final output.\n        init: weight initialization function.\n            Can be the name of an existing function (str),\n            or a Theano function (see: [initializations](../initializations.md)).\n        inner_init: initialization function of the inner cells.\n        activation: activation function.\n            Can be the name of an existing function (str),\n            or a Theano function (see: [activations](../activations.md)).\n        inner_activation: activation function for the inner cells.\n        W_regularizer: instance of [WeightRegularizer](../regularizers.md)\n            (eg. L1 or L2 regularization), applied to the input weights matrices.\n        U_regularizer: instance of [WeightRegularizer](../regularizers.md)\n            (eg. L1 or L2 regularization), applied to the recurrent weights matrices.\n        b_regularizer: instance of [WeightRegularizer](../regularizers.md),\n            applied to the bias.\n        dropout_W: float between 0 and 1. Fraction of the input units to drop for input gates.\n        dropout_U: float between 0 and 1. Fraction of the input units to drop for recurrent connections.\n\n    # References\n        - [On the Properties of Neural Machine Translation: Encoder\u2013Decoder Approaches](http://www.aclweb.org/anthology/W14-4012)\n        - [Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling](http://arxiv.org/pdf/1412.3555v1.pdf)\n        - [A Theoretically Grounded Application of Dropout in Recurrent Neural Networks](http://arxiv.org/abs/1512.05287)\n    '''\n    def __init__(self, output_dim,\n                 init='glorot_uniform', inner_init='orthogonal',\n                 activation='tanh', inner_activation='hard_sigmoid',\n                 W_regularizer=None, U_regularizer=None, b_regularizer=None,\n                 dropout_W=0., dropout_U=0., **kwargs):\n        self.output_dim = output_dim\n        self.init = initializations.get(init)\n        self.inner_init = initializations.get(inner_init)\n        self.activation = activations.get(activation)\n        self.inner_activation = activations.get(inner_activation)\n        self.W_regularizer = regularizers.get(W_regularizer)\n        self.U_regularizer = regularizers.get(U_regularizer)\n        self.b_regularizer = regularizers.get(b_regularizer)\n        self.dropout_W, self.dropout_U = dropout_W, dropout_U\n        super(GRU, self).__init__(**kwargs)\n\n    def build(self):\n        input_shape = self.input_shape\n        input_dim = input_shape[2]\n        self.input_dim = input_dim\n\n        self.W_z = self.init((input_dim, self.output_dim),\n                             name='{}_W_z'.format(self.name))\n        self.U_z = self.inner_init((self.output_dim, self.output_dim),\n                                   name='{}_U_z'.format(self.name))\n        self.b_z = K.zeros((self.output_dim,), name='{}_b_z'.format(self.name))\n\n        self.W_r = self.init((input_dim, self.output_dim),\n                             name='{}_W_r'.format(self.name))\n        self.U_r = self.inner_init((self.output_dim, self.output_dim),\n                                   name='{}_U_r'.format(self.name))\n        self.b_r = K.zeros((self.output_dim,), name='{}_b_r'.format(self.name))\n\n        self.W_h = self.init((input_dim, self.output_dim),\n                             name='{}_W_h'.format(self.name))\n        self.U_h = self.inner_init((self.output_dim, self.output_dim),\n                                   name='{}_U_h'.format(self.name))\n        self.b_h = K.zeros((self.output_dim,), name='{}_b_h'.format(self.name))\n\n        self.regularizers = []\n        if self.W_regularizer:\n            self.W_regularizer.set_param(K.concatenate([self.W_z,\n                                                        self.W_r,\n                                                        self.W_h]))\n            self.regularizers.append(self.W_regularizer)\n        if self.U_regularizer:\n            self.U_regularizer.set_param(K.concatenate([self.U_z,\n                                                        self.U_r,\n                                                        self.U_h]))\n            self.regularizers.append(self.U_regularizer)\n        if self.b_regularizer:\n            self.b_regularizer.set_param(K.concatenate([self.b_z,\n                                                        self.b_r,\n                                                        self.b_h]))\n            self.regularizers.append(self.b_regularizer)\n\n        self.trainable_weights = [self.W_z, self.U_z, self.b_z,\n                                  self.W_r, self.U_r, self.b_r,\n                                  self.W_h, self.U_h, self.b_h]\n        if self.stateful:\n            self.reset_states()\n        else:\n            # initial states: all-zero tensor of shape (output_dim)\n            self.states = [None]\n\n        if self.initial_weights is not None:\n            self.set_weights(self.initial_weights)\n            del self.initial_weights\n\n    def reset_states(self):\n        assert self.stateful, 'Layer must be stateful.'\n        input_shape = self.input_shape\n        if not input_shape[0]:\n            raise Exception('If a RNN is stateful, a complete ' +\n                            'input_shape must be provided (including batch size).')\n        if hasattr(self, 'states'):\n            K.set_value(self.states[0],\n                        np.zeros((input_shape[0], self.output_dim)))\n        else:\n            self.states = [K.zeros((input_shape[0], self.output_dim))]\n\n    def preprocess_input(self, x, train=False):\n        if train and (0 < self.dropout_W < 1):\n            dropout = self.dropout_W\n        else:\n            dropout = 0\n        input_shape = self.input_shape\n        input_dim = input_shape[2]\n        timesteps = input_shape[1]\n\n        x_z = time_distributed_dense(x, self.W_z, self.b_z, dropout,\n                                     input_dim, self.output_dim, timesteps)\n        x_r = time_distributed_dense(x, self.W_r, self.b_r, dropout,\n                                     input_dim, self.output_dim, timesteps)\n        x_h = time_distributed_dense(x, self.W_h, self.b_h, dropout,\n                                     input_dim, self.output_dim, timesteps)\n        return K.concatenate([x_z, x_r, x_h], axis=2)\n\n    def step(self, x, states):\n        h_tm1 = states[0]  # previous memory\n        if len(states) == 2:\n            B_U = states[1]  # dropout matrices for recurrent units\n        else:\n            B_U = [1., 1., 1.]\n\n        x_z = x[:, :self.output_dim]\n        x_r = x[:, self.output_dim: 2 * self.output_dim]\n        x_h = x[:, 2 * self.output_dim:]\n\n        z = self.inner_activation(x_z + K.dot(h_tm1 * B_U[0], self.U_z))\n        r = self.inner_activation(x_r + K.dot(h_tm1 * B_U[1], self.U_r))\n\n        hh = self.activation(x_h + K.dot(r * h_tm1 * B_U[2], self.U_h))\n        h = z * h_tm1 + (1 - z) * hh\n        return h, [h]\n\n    def get_constants(self, x, train=False):\n        if train and (0 < self.dropout_U < 1):\n            ones = K.ones_like(K.reshape(x[:, 0, 0], (-1, 1)))\n            ones = K.concatenate([ones] * self.output_dim, 1)\n            B_U = [K.dropout(ones, self.dropout_U) for _ in range(3)]\n            return [B_U]\n        return []\n\n    def get_config(self):\n        config = {\"output_dim\": self.output_dim,\n                  \"init\": self.init.__name__,\n                  \"inner_init\": self.inner_init.__name__,\n                  \"activation\": self.activation.__name__,\n                  \"inner_activation\": self.inner_activation.__name__,\n                  \"W_regularizer\": self.W_regularizer.get_config() if self.W_regularizer else None,\n                  \"U_regularizer\": self.U_regularizer.get_config() if self.U_regularizer else None,\n                  \"b_regularizer\": self.b_regularizer.get_config() if self.b_regularizer else None,\n                  \"dropout_W\": self.dropout_W,\n                  \"dropout_U\": self.dropout_U}\n        base_config = super(GRU, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\n\nclass LSTM(Recurrent):\n    '''Long-Short Term Memory unit - Hochreiter 1997.\n\n    For a step-by-step description of the algorithm, see\n    [this tutorial](http://deeplearning.net/tutorial/lstm.html).\n\n    # Arguments\n        output_dim: dimension of the internal projections and the final output.\n        init: weight initialization function.\n            Can be the name of an existing function (str),\n            or a Theano function (see: [initializations](../initializations.md)).\n        inner_init: initialization function of the inner cells.\n        forget_bias_init: initialization function for the bias of the forget gate.\n            [Jozefowicz et al.](http://www.jmlr.org/proceedings/papers/v37/jozefowicz15.pdf)\n            recommend initializing with ones.\n        activation: activation function.\n            Can be the name of an existing function (str),\n            or a Theano function (see: [activations](../activations.md)).\n        inner_activation: activation function for the inner cells.\n        W_regularizer: instance of [WeightRegularizer](../regularizers.md)\n            (eg. L1 or L2 regularization), applied to the input weights matrices.\n        U_regularizer: instance of [WeightRegularizer](../regularizers.md)\n            (eg. L1 or L2 regularization), applied to the recurrent weights matrices.\n        b_regularizer: instance of [WeightRegularizer](../regularizers.md),\n            applied to the bias.\n        dropout_W: float between 0 and 1. Fraction of the input units to drop for input gates.\n        dropout_U: float between 0 and 1. Fraction of the input units to drop for recurrent connections.\n\n    # References\n        - [Long short-term memory](http://deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf) (original 1997 paper)\n        - [Learning to forget: Continual prediction with LSTM](http://www.mitpressjournals.org/doi/pdf/10.1162/089976600300015015)\n        - [Supervised sequence labelling with recurrent neural networks](http://www.cs.toronto.edu/~graves/preprint.pdf)\n        - [A Theoretically Grounded Application of Dropout in Recurrent Neural Networks](http://arxiv.org/abs/1512.05287)\n    '''\n    def __init__(self, output_dim,\n                 init='glorot_uniform', inner_init='orthogonal',\n                 forget_bias_init='one', activation='tanh',\n                 inner_activation='hard_sigmoid',\n                 W_regularizer=None, U_regularizer=None, b_regularizer=None,\n                 dropout_W=0., dropout_U=0., **kwargs):\n        self.output_dim = output_dim\n        self.init = initializations.get(init)\n        self.inner_init = initializations.get(inner_init)\n        self.forget_bias_init = initializations.get(forget_bias_init)\n        self.activation = activations.get(activation)\n        self.inner_activation = activations.get(inner_activation)\n        self.W_regularizer = regularizers.get(W_regularizer)\n        self.U_regularizer = regularizers.get(U_regularizer)\n        self.b_regularizer = regularizers.get(b_regularizer)\n        self.dropout_W, self.dropout_U = dropout_W, dropout_U\n        super(LSTM, self).__init__(**kwargs)\n\n    def build(self):\n        input_shape = self.input_shape\n        input_dim = input_shape[2]\n        self.input_dim = input_dim\n\n        if self.stateful:\n            self.reset_states()\n        else:\n            # initial states: 2 all-zero tensors of shape (output_dim)\n            self.states = [None, None]\n\n        self.W_i = self.init((input_dim, self.output_dim),\n                             name='{}_W_i'.format(self.name))\n        self.U_i = self.inner_init((self.output_dim, self.output_dim),\n                                   name='{}_U_i'.format(self.name))\n        self.b_i = K.zeros((self.output_dim,), name='{}_b_i'.format(self.name))\n\n        self.W_f = self.init((input_dim, self.output_dim),\n                             name='{}_W_f'.format(self.name))\n        self.U_f = self.inner_init((self.output_dim, self.output_dim),\n                                   name='{}_U_f'.format(self.name))\n        self.b_f = self.forget_bias_init((self.output_dim,),\n                                         name='{}_b_f'.format(self.name))\n\n        self.W_c = self.init((input_dim, self.output_dim),\n                             name='{}_W_c'.format(self.name))\n        self.U_c = self.inner_init((self.output_dim, self.output_dim),\n                                   name='{}_U_c'.format(self.name))\n        self.b_c = K.zeros((self.output_dim,), name='{}_b_c'.format(self.name))\n\n        self.W_o = self.init((input_dim, self.output_dim),\n                             name='{}_W_o'.format(self.name))\n        self.U_o = self.inner_init((self.output_dim, self.output_dim),\n                                   name='{}_U_o'.format(self.name))\n        self.b_o = K.zeros((self.output_dim,), name='{}_b_o'.format(self.name))\n\n        self.regularizers = []\n        if self.W_regularizer:\n            self.W_regularizer.set_param(K.concatenate([self.W_i,\n                                                        self.W_f,\n                                                        self.W_c,\n                                                        self.W_o]))\n            self.regularizers.append(self.W_regularizer)\n        if self.U_regularizer:\n            self.U_regularizer.set_param(K.concatenate([self.U_i,\n                                                        self.U_f,\n                                                        self.U_c,\n                                                        self.U_o]))\n            self.regularizers.append(self.U_regularizer)\n        if self.b_regularizer:\n            self.b_regularizer.set_param(K.concatenate([self.b_i,\n                                                        self.b_f,\n                                                        self.b_c,\n                                                        self.b_o]))\n            self.regularizers.append(self.b_regularizer)\n\n        self.trainable_weights = [self.W_i, self.U_i, self.b_i,\n                                  self.W_c, self.U_c, self.b_c,\n                                  self.W_f, self.U_f, self.b_f,\n                                  self.W_o, self.U_o, self.b_o]\n\n        if self.initial_weights is not None:\n            self.set_weights(self.initial_weights)\n            del self.initial_weights\n\n    def reset_states(self):\n        assert self.stateful, 'Layer must be stateful.'\n        input_shape = self.input_shape\n        if not input_shape[0]:\n            raise Exception('If a RNN is stateful, a complete ' +\n                            'input_shape must be provided (including batch size).')\n        if hasattr(self, 'states'):\n            K.set_value(self.states[0],\n                        np.zeros((input_shape[0], self.output_dim)))\n            K.set_value(self.states[1],\n                        np.zeros((input_shape[0], self.output_dim)))\n        else:\n            self.states = [K.zeros((input_shape[0], self.output_dim)),\n                           K.zeros((input_shape[0], self.output_dim))]\n\n    def preprocess_input(self, x, train=False):\n        if train and (0 < self.dropout_W < 1):\n            dropout = self.dropout_W\n        else:\n            dropout = 0\n        input_shape = self.input_shape\n        input_dim = input_shape[2]\n        timesteps = input_shape[1]\n\n        x_i = time_distributed_dense(x, self.W_i, self.b_i, dropout,\n                                     input_dim, self.output_dim, timesteps)\n        x_f = time_distributed_dense(x, self.W_f, self.b_f, dropout,\n                                     input_dim, self.output_dim, timesteps)\n        x_c = time_distributed_dense(x, self.W_c, self.b_c, dropout,\n                                     input_dim, self.output_dim, timesteps)\n        x_o = time_distributed_dense(x, self.W_o, self.b_o, dropout,\n                                     input_dim, self.output_dim, timesteps)\n        return K.concatenate([x_i, x_f, x_c, x_o], axis=2)\n\n    def step(self, x, states):\n        h_tm1 = states[0]\n        c_tm1 = states[1]\n        if len(states) == 3:\n            B_U = states[2]\n        else:\n            B_U = [1. for _ in range(4)]\n\n        x_i = x[:, :self.output_dim]\n        x_f = x[:, self.output_dim: 2 * self.output_dim]\n        x_c = x[:, 2 * self.output_dim: 3 * self.output_dim]\n        x_o = x[:, 3 * self.output_dim:]\n\n        i = self.inner_activation(x_i + K.dot(h_tm1 * B_U[0], self.U_i))\n        f = self.inner_activation(x_f + K.dot(h_tm1 * B_U[1], self.U_f))\n        c = f * c_tm1 + i * self.activation(x_c + K.dot(h_tm1 * B_U[2], self.U_c))\n        o = self.inner_activation(x_o + K.dot(h_tm1 * B_U[3], self.U_o))\n\n        h = o * self.activation(c)\n        return h, [h, c]\n\n    def get_constants(self, x, train=False):\n        if train and (0 < self.dropout_U < 1):\n            ones = K.ones_like(K.reshape(x[:, 0, 0], (-1, 1)))\n            ones = K.concatenate([ones] * self.output_dim, 1)\n            B_U = [K.dropout(ones, self.dropout_U) for _ in range(4)]\n            return [B_U]\n        return []\n\n    def get_config(self):\n        config = {\"output_dim\": self.output_dim,\n                  \"init\": self.init.__name__,\n                  \"inner_init\": self.inner_init.__name__,\n                  \"forget_bias_init\": self.forget_bias_init.__name__,\n                  \"activation\": self.activation.__name__,\n                  \"inner_activation\": self.inner_activation.__name__,\n                  \"W_regularizer\": self.W_regularizer.get_config() if self.W_regularizer else None,\n                  \"U_regularizer\": self.U_regularizer.get_config() if self.U_regularizer else None,\n                  \"b_regularizer\": self.b_regularizer.get_config() if self.b_regularizer else None,\n                  \"dropout_W\": self.dropout_W,\n                  \"dropout_U\": self.dropout_U}\n        base_config = super(LSTM, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n",
          "file_after": "# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import\nimport numpy as np\n\nfrom .. import backend as K\nfrom .. import activations, initializations, regularizers\nfrom ..engine import Layer, InputSpec\n\n\ndef time_distributed_dense(x, w, b=None, dropout=None,\n                           input_dim=None, output_dim=None, timesteps=None):\n    '''Apply y.w + b for every temporal slice y of x.\n    '''\n    if not input_dim:\n        # won't work with TensorFlow\n        input_dim = K.shape(x)[2]\n    if not timesteps:\n        # won't work with TensorFlow\n        timesteps = K.shape(x)[1]\n    if not output_dim:\n        # won't work with TensorFlow\n        output_dim = K.shape(w)[1]\n\n    if dropout is not None and 0. < dropout < 1.:\n        # apply the same dropout pattern at every timestep\n        ones = K.ones_like(K.reshape(x[:, 0, :], (-1, input_dim)))\n        dropout_matrix = K.dropout(ones, dropout)\n        expanded_dropout_matrix = K.repeat(dropout_matrix, timesteps)\n        x = K.in_train_phase(x * expanded_dropout_matrix, x)\n\n    # collapse time dimension and batch dimension together\n    x = K.reshape(x, (-1, input_dim))\n\n    x = K.dot(x, w)\n    if b:\n        x = x + b\n    # reshape to 3D tensor\n    x = K.reshape(x, (-1, timesteps, output_dim))\n    return x\n\n\nclass Recurrent(Layer):\n    '''Abstract base class for recurrent layers.\n    Do not use in a model -- it's not a functional layer!\n\n    All recurrent layers (GRU, LSTM, SimpleRNN) also\n    follow the specifications of this class and accept\n    the keyword arguments listed below.\n\n    # Arguments\n        weights: list of numpy arrays to set as initial weights.\n            The list should have 3 elements, of shapes:\n            `[(input_dim, output_dim), (output_dim, output_dim), (output_dim,)]`.\n        return_sequences: Boolean. Whether to return the last output\n            in the output sequence, or the full sequence.\n        go_backwards: Boolean (default False).\n            If True, process the input sequence backwards.\n        stateful: Boolean (default False). If True, the last state\n            for each sample at index i in a batch will be used as initial\n            state for the sample of index i in the following batch.\n        unroll: Boolean (default False). If True, the network will be unrolled,\n            else a symbolic loop will be used. When using TensorFlow, the network\n            is always unrolled, so this argument does not do anything.\n            Unrolling can speed-up a RNN, although it tends to be more memory-intensive.\n            Unrolling is only suitable for short sequences.\n        consume_less: one of \"cpu\", \"mem\". If set to \"cpu\", the RNN will use\n            an implementation that uses fewer, larger matrix products,\n            thus running faster (at least on CPU) but consuming more memory.\n            If set to \"mem\", the RNN will use more matrix products,\n            but smaller ones, thus running slower (may actually be faster on GPU)\n            while consuming less memory.\n        input_dim: dimensionality of the input (integer).\n            This argument (or alternatively, the keyword argument `input_shape`)\n            is required when using this layer as the first layer in a model.\n        input_length: Length of input sequences, to be specified\n            when it is constant.\n            This argument is required if you are going to connect\n            `Flatten` then `Dense` layers upstream\n            (without it, the shape of the dense outputs cannot be computed).\n            Note that if the recurrent layer is not the first layer\n            in your model, you would need to specify the input length\n            at the level of the first layer\n            (e.g. via the `input_shape` argument)\n\n    # Input shape\n        3D tensor with shape `(nb_samples, timesteps, input_dim)`.\n\n    # Output shape\n        - if `return_sequences`: 3D tensor with shape\n            `(nb_samples, timesteps, output_dim)`.\n        - else, 2D tensor with shape `(nb_samples, output_dim)`.\n\n    # Masking\n        This layer supports masking for input data with a variable number\n        of timesteps. To introduce masks to your data,\n        use an [Embedding](embeddings.md) layer with the `mask_zero` parameter\n        set to `True`.\n\n    # TensorFlow warning\n        For the time being, when using the TensorFlow backend,\n        the number of timesteps used must be specified in your model.\n        Make sure to pass an `input_length` int argument to your\n        recurrent layer (if it comes first in your model),\n        or to pass a complete `input_shape` argument to the first layer\n        in your model otherwise.\n\n\n    # Note on using statefulness in RNNs\n        You can set RNN layers to be 'stateful', which means that the states\n        computed for the samples in one batch will be reused as initial states\n        for the samples in the next batch.\n        This assumes a one-to-one mapping between\n        samples in different successive batches.\n\n        To enable statefulness:\n            - specify `stateful=True` in the layer constructor.\n            - specify a fixed batch size for your model, by passing\n                a `batch_input_shape=(...)` to the first layer in your model.\n                This is the expected shape of your inputs *including the batch size*.\n                It should be a tuple of integers, e.g. `(32, 10, 100)`.\n\n        To reset the states of your model, call `.reset_states()` on either\n        a specific layer, or on your entire model.\n\n    # Note on using dropout with TensorFlow\n        When using the TensorFlow backend, specify a fixed batch size for your model\n        following the notes on statefulness RNNs.\n    '''\n    def __init__(self, weights=None,\n                 return_sequences=False, go_backwards=False, stateful=False,\n                 unroll=False, consume_less='cpu',\n                 input_dim=None, input_length=None, **kwargs):\n        self.return_sequences = return_sequences\n        self.initial_weights = weights\n        self.go_backwards = go_backwards\n        self.stateful = stateful\n        self.unroll = unroll\n        self.consume_less = consume_less\n\n        self.supports_masking = True\n        self.input_spec = [InputSpec(ndim=3)]\n        self.input_dim = input_dim\n        self.input_length = input_length\n        if self.input_dim:\n            kwargs['input_shape'] = (self.input_length, self.input_dim)\n        super(Recurrent, self).__init__(**kwargs)\n\n    def get_output_shape_for(self, input_shape):\n        if self.return_sequences:\n            return (input_shape[0], input_shape[1], self.output_dim)\n        else:\n            return (input_shape[0], self.output_dim)\n\n    def compute_mask(self, input, mask):\n        if self.return_sequences:\n            return mask\n        else:\n            return None\n\n    def step(self, x, states):\n        raise NotImplementedError\n\n    def get_constants(self, x):\n        return []\n\n    def get_initial_states(self, x):\n        # build an all-zero tensor of shape (samples, output_dim)\n        initial_state = K.zeros_like(x)  # (samples, timesteps, input_dim)\n        initial_state = K.sum(initial_state, axis=1)  # (samples, input_dim)\n        reducer = K.zeros((self.input_dim, self.output_dim))\n        initial_state = K.dot(initial_state, reducer)  # (samples, output_dim)\n        initial_states = [initial_state for _ in range(len(self.states))]\n        return initial_states\n\n    def preprocess_input(self, x):\n        return x\n\n    def call(self, x, mask=None):\n        # input shape: (nb_samples, time (padded with zeros), input_dim)\n        # note that the .build() method of subclasses MUST define\n        # self.input_sepc with a complete input shape.\n        input_shape = self.input_spec[0].shape\n        if K._BACKEND == 'tensorflow':\n            if not input_shape[1]:\n                raise Exception('When using TensorFlow, you should define '\n                                'explicitly the number of timesteps of '\n                                'your sequences.\\n'\n                                'If your first layer is an Embedding, '\n                                'make sure to pass it an \"input_length\" '\n                                'argument. Otherwise, make sure '\n                                'the first layer has '\n                                'an \"input_shape\" or \"batch_input_shape\" '\n                                'argument, including the time axis. '\n                                'Found input shape at layer ' + self.name +\n                                ': ' + str(input_shape))\n        if self.stateful:\n            initial_states = self.states\n        else:\n            initial_states = self.get_initial_states(x)\n        constants = self.get_constants(x)\n        preprocessed_input = self.preprocess_input(x)\n\n        last_output, outputs, states = K.rnn(self.step, preprocessed_input,\n                                             initial_states,\n                                             go_backwards=self.go_backwards,\n                                             mask=mask,\n                                             constants=constants,\n                                             unroll=self.unroll,\n                                             input_length=input_shape[1])\n        if self.stateful:\n            self.updates = []\n            for i in range(len(states)):\n                self.updates.append((self.states[i], states[i]))\n\n        if self.return_sequences:\n            return outputs\n        else:\n            return last_output\n\n    def get_config(self):\n        config = {'return_sequences': self.return_sequences,\n                  'go_backwards': self.go_backwards,\n                  'stateful': self.stateful,\n                  'unroll': self.unroll,\n                  'consume_less': self.consume_less}\n        if self.stateful:\n            config['batch_input_shape'] = self.input_shape\n        else:\n            config['input_dim'] = self.input_dim\n            config['input_length'] = self.input_length\n\n        base_config = super(Recurrent, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\n\nclass SimpleRNN(Recurrent):\n    '''Fully-connected RNN where the output is to be fed back to input.\n\n    # Arguments\n        output_dim: dimension of the internal projections and the final output.\n        init: weight initialization function.\n            Can be the name of an existing function (str),\n            or a Theano function (see: [initializations](../initializations.md)).\n        inner_init: initialization function of the inner cells.\n        activation: activation function.\n            Can be the name of an existing function (str),\n            or a Theano function (see: [activations](../activations.md)).\n        W_regularizer: instance of [WeightRegularizer](../regularizers.md)\n            (eg. L1 or L2 regularization), applied to the input weights matrices.\n        U_regularizer: instance of [WeightRegularizer](../regularizers.md)\n            (eg. L1 or L2 regularization), applied to the recurrent weights matrices.\n        b_regularizer: instance of [WeightRegularizer](../regularizers.md),\n            applied to the bias.\n        dropout_W: float between 0 and 1. Fraction of the input units to drop for input gates.\n        dropout_U: float between 0 and 1. Fraction of the input units to drop for recurrent connections.\n\n    # References\n        - [A Theoretically Grounded Application of Dropout in Recurrent Neural Networks](http://arxiv.org/abs/1512.05287)\n    '''\n    def __init__(self, output_dim,\n                 init='glorot_uniform', inner_init='orthogonal',\n                 activation='tanh',\n                 W_regularizer=None, U_regularizer=None, b_regularizer=None,\n                 dropout_W=0., dropout_U=0., **kwargs):\n        self.output_dim = output_dim\n        self.init = initializations.get(init)\n        self.inner_init = initializations.get(inner_init)\n        self.activation = activations.get(activation)\n        self.W_regularizer = regularizers.get(W_regularizer)\n        self.U_regularizer = regularizers.get(U_regularizer)\n        self.b_regularizer = regularizers.get(b_regularizer)\n        self.dropout_W, self.dropout_U = dropout_W, dropout_U\n\n        if self.dropout_W or self.dropout_U:\n            self.uses_learning_phase = True\n        super(SimpleRNN, self).__init__(**kwargs)\n\n    def build(self, input_shape):\n        self.input_spec = [InputSpec(shape=input_shape)]\n        if self.stateful:\n            self.reset_states()\n        else:\n            # initial states: all-zero tensor of shape (output_dim)\n            self.states = [None]\n        input_dim = input_shape[2]\n        self.input_dim = input_dim\n\n        self.W = self.init((input_dim, self.output_dim),\n                           name='{}_W'.format(self.name))\n        self.U = self.inner_init((self.output_dim, self.output_dim),\n                                 name='{}_U'.format(self.name))\n        self.b = K.zeros((self.output_dim,), name='{}_b'.format(self.name))\n\n        self.regularizers = []\n        if self.W_regularizer:\n            self.W_regularizer.set_param(self.W)\n            self.regularizers.append(self.W_regularizer)\n        if self.U_regularizer:\n            self.W_regularizer.set_param(self.U)\n            self.regularizers.append(self.U_regularizer)\n        if self.b_regularizer:\n            self.W_regularizer.set_param(self.b)\n            self.regularizers.append(self.b_regularizer)\n\n        self.trainable_weights = [self.W, self.U, self.b]\n\n        if self.initial_weights is not None:\n            self.set_weights(self.initial_weights)\n            del self.initial_weights\n\n    def reset_states(self):\n        assert self.stateful, 'Layer must be stateful.'\n        input_shape = self.input_spec[0].shape\n        if not input_shape[0]:\n            raise Exception('If a RNN is stateful, a complete ' +\n                            'input_shape must be provided (including batch size).')\n        if hasattr(self, 'states'):\n            K.set_value(self.states[0],\n                        np.zeros((input_shape[0], self.output_dim)))\n        else:\n            self.states = [K.zeros((input_shape[0], self.output_dim))]\n\n    def preprocess_input(self, x):\n        if self.consume_less == 'cpu':\n            input_shape = self.input_spec[0].shape\n            input_dim = input_shape[2]\n            timesteps = input_shape[1]\n            return time_distributed_dense(x, self.W, self.b, self.dropout_W,\n                                          input_dim, self.output_dim,\n                                          timesteps)\n        else:\n            return x\n\n    def step(self, x, states):\n        prev_output = states[0]\n        B_U = states[1]\n        B_W = states[2]\n\n        if self.consume_less == 'cpu':\n            h = x\n        else:\n            h = K.dot(x * B_W, self.W) + self.b\n\n        output = self.activation(h + K.dot(prev_output * B_U, self.U))\n        return output, [output]\n\n    def get_constants(self, x):\n        constants = []\n        if 0 < self.dropout_U < 1:\n            ones = K.ones_like(K.reshape(x[:, 0, 0], (-1, 1)))\n            ones = K.concatenate([ones] * self.output_dim, 1)\n            B_U = K.in_train_phase(K.dropout(ones, self.dropout_U), ones)\n            constants.append(B_U)\n        else:\n            constants.append(K.cast_to_floatx(1.))\n        if self.consume_less == 'cpu' and 0 < self.dropout_W < 1:\n            input_shape = self.input_spec[0].shape\n            input_dim = input_shape[-1]\n            ones = K.ones_like(K.reshape(x[:, 0, 0], (-1, 1)))\n            ones = K.concatenate([ones] * input_dim, 1)\n            B_W = K.in_train_phase(K.dropout(ones, self.dropout_W), ones)\n            constants.append(B_W)\n        else:\n            constants.append(K.cast_to_floatx(1.))\n        return constants\n\n    def get_config(self):\n        config = {\"output_dim\": self.output_dim,\n                  \"init\": self.init.__name__,\n                  \"inner_init\": self.inner_init.__name__,\n                  \"activation\": self.activation.__name__,\n                  \"W_regularizer\": self.W_regularizer.get_config() if self.W_regularizer else None,\n                  \"U_regularizer\": self.U_regularizer.get_config() if self.U_regularizer else None,\n                  \"b_regularizer\": self.b_regularizer.get_config() if self.b_regularizer else None,\n                  \"dropout_W\": self.dropout_W,\n                  \"dropout_U\": self.dropout_U}\n        base_config = super(SimpleRNN, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\n\nclass GRU(Recurrent):\n    '''Gated Recurrent Unit - Cho et al. 2014.\n\n    # Arguments\n        output_dim: dimension of the internal projections and the final output.\n        init: weight initialization function.\n            Can be the name of an existing function (str),\n            or a Theano function (see: [initializations](../initializations.md)).\n        inner_init: initialization function of the inner cells.\n        activation: activation function.\n            Can be the name of an existing function (str),\n            or a Theano function (see: [activations](../activations.md)).\n        inner_activation: activation function for the inner cells.\n        W_regularizer: instance of [WeightRegularizer](../regularizers.md)\n            (eg. L1 or L2 regularization), applied to the input weights matrices.\n        U_regularizer: instance of [WeightRegularizer](../regularizers.md)\n            (eg. L1 or L2 regularization), applied to the recurrent weights matrices.\n        b_regularizer: instance of [WeightRegularizer](../regularizers.md),\n            applied to the bias.\n        dropout_W: float between 0 and 1. Fraction of the input units to drop for input gates.\n        dropout_U: float between 0 and 1. Fraction of the input units to drop for recurrent connections.\n\n    # References\n        - [On the Properties of Neural Machine Translation: Encoder\u2013Decoder Approaches](http://www.aclweb.org/anthology/W14-4012)\n        - [Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling](http://arxiv.org/pdf/1412.3555v1.pdf)\n        - [A Theoretically Grounded Application of Dropout in Recurrent Neural Networks](http://arxiv.org/abs/1512.05287)\n    '''\n    def __init__(self, output_dim,\n                 init='glorot_uniform', inner_init='orthogonal',\n                 activation='tanh', inner_activation='hard_sigmoid',\n                 W_regularizer=None, U_regularizer=None, b_regularizer=None,\n                 dropout_W=0., dropout_U=0., **kwargs):\n        self.output_dim = output_dim\n        self.init = initializations.get(init)\n        self.inner_init = initializations.get(inner_init)\n        self.activation = activations.get(activation)\n        self.inner_activation = activations.get(inner_activation)\n        self.W_regularizer = regularizers.get(W_regularizer)\n        self.U_regularizer = regularizers.get(U_regularizer)\n        self.b_regularizer = regularizers.get(b_regularizer)\n        self.dropout_W, self.dropout_U = dropout_W, dropout_U\n\n        if self.dropout_W or self.dropout_U:\n            self.uses_learning_phase = True\n        super(GRU, self).__init__(**kwargs)\n\n    def build(self, input_shape):\n        self.input_spec = [InputSpec(shape=input_shape)]\n        input_dim = input_shape[2]\n        self.input_dim = input_dim\n\n        self.W_z = self.init((input_dim, self.output_dim),\n                             name='{}_W_z'.format(self.name))\n        self.U_z = self.inner_init((self.output_dim, self.output_dim),\n                                   name='{}_U_z'.format(self.name))\n        self.b_z = K.zeros((self.output_dim,), name='{}_b_z'.format(self.name))\n\n        self.W_r = self.init((input_dim, self.output_dim),\n                             name='{}_W_r'.format(self.name))\n        self.U_r = self.inner_init((self.output_dim, self.output_dim),\n                                   name='{}_U_r'.format(self.name))\n        self.b_r = K.zeros((self.output_dim,), name='{}_b_r'.format(self.name))\n\n        self.W_h = self.init((input_dim, self.output_dim),\n                             name='{}_W_h'.format(self.name))\n        self.U_h = self.inner_init((self.output_dim, self.output_dim),\n                                   name='{}_U_h'.format(self.name))\n        self.b_h = K.zeros((self.output_dim,), name='{}_b_h'.format(self.name))\n\n        self.regularizers = []\n        if self.W_regularizer:\n            self.W_regularizer.set_param(K.concatenate([self.W_z,\n                                                        self.W_r,\n                                                        self.W_h]))\n            self.regularizers.append(self.W_regularizer)\n        if self.U_regularizer:\n            self.U_regularizer.set_param(K.concatenate([self.U_z,\n                                                        self.U_r,\n                                                        self.U_h]))\n            self.regularizers.append(self.U_regularizer)\n        if self.b_regularizer:\n            self.b_regularizer.set_param(K.concatenate([self.b_z,\n                                                        self.b_r,\n                                                        self.b_h]))\n            self.regularizers.append(self.b_regularizer)\n\n        self.trainable_weights = [self.W_z, self.U_z, self.b_z,\n                                  self.W_r, self.U_r, self.b_r,\n                                  self.W_h, self.U_h, self.b_h]\n        if self.stateful:\n            self.reset_states()\n        else:\n            # initial states: all-zero tensor of shape (output_dim)\n            self.states = [None]\n\n        if self.initial_weights is not None:\n            self.set_weights(self.initial_weights)\n            del self.initial_weights\n\n    def reset_states(self):\n        assert self.stateful, 'Layer must be stateful.'\n        input_shape = self.input_shape\n        if not input_shape[0]:\n            raise Exception('If a RNN is stateful, a complete ' +\n                            'input_shape must be provided (including batch size).')\n        if hasattr(self, 'states'):\n            K.set_value(self.states[0],\n                        np.zeros((input_shape[0], self.output_dim)))\n        else:\n            self.states = [K.zeros((input_shape[0], self.output_dim))]\n\n    def preprocess_input(self, x):\n        if self.consume_less == 'cpu':\n            input_shape = self.input_spec[0].shape\n            input_dim = input_shape[2]\n            timesteps = input_shape[1]\n\n            x_z = time_distributed_dense(x, self.W_z, self.b_z, self.dropout_W,\n                                         input_dim, self.output_dim, timesteps)\n            x_r = time_distributed_dense(x, self.W_r, self.b_r, self.dropout_W,\n                                         input_dim, self.output_dim, timesteps)\n            x_h = time_distributed_dense(x, self.W_h, self.b_h, self.dropout_W,\n                                         input_dim, self.output_dim, timesteps)\n            return K.concatenate([x_z, x_r, x_h], axis=2)\n        else:\n            return x\n\n    def step(self, x, states):\n        h_tm1 = states[0]  # previous memory\n        B_U = states[1]  # dropout matrices for recurrent units\n        B_W = states[2]\n\n        if self.consume_less == 'cpu':\n            x_z = x[:, :self.output_dim]\n            x_r = x[:, self.output_dim: 2 * self.output_dim]\n            x_h = x[:, 2 * self.output_dim:]\n        else:\n            x_z = K.dot(x * B_W[0], self.W_z) + self.b_z\n            x_r = K.dot(x * B_W[1], self.W_r) + self.b_r\n            x_h = K.dot(x * B_W[2], self.W_h) + self.b_h\n\n        z = self.inner_activation(x_z + K.dot(h_tm1 * B_U[0], self.U_z))\n        r = self.inner_activation(x_r + K.dot(h_tm1 * B_U[1], self.U_r))\n\n        hh = self.activation(x_h + K.dot(r * h_tm1 * B_U[2], self.U_h))\n        h = z * h_tm1 + (1 - z) * hh\n        return h, [h]\n\n    def get_constants(self, x):\n        constants = []\n        if 0 < self.dropout_U < 1:\n            ones = K.ones_like(K.reshape(x[:, 0, 0], (-1, 1)))\n            ones = K.concatenate([ones] * self.output_dim, 1)\n            B_U = [K.dropout(ones, self.dropout_U) for _ in range(3)]\n            constants.append(B_U)\n        else:\n            constants.append([K.cast_to_floatx(1.) for _ in range(4)])\n\n        if self.consume_less == 'cpu' and 0 < self.dropout_W < 1:\n            input_shape = self.input_spec[0].shape\n            input_dim = input_shape[-1]\n            ones = K.ones_like(K.reshape(x[:, 0, 0], (-1, 1)))\n            ones = K.concatenate([ones] * input_dim, 1)\n            B_W = [K.dropout(ones, self.dropout_W) for _ in range(3)]\n            constants.append(B_W)\n        else:\n            constants.append([K.cast_to_floatx(1.) for _ in range(4)])\n        return constants\n\n    def get_config(self):\n        config = {\"output_dim\": self.output_dim,\n                  \"init\": self.init.__name__,\n                  \"inner_init\": self.inner_init.__name__,\n                  \"activation\": self.activation.__name__,\n                  \"inner_activation\": self.inner_activation.__name__,\n                  \"W_regularizer\": self.W_regularizer.get_config() if self.W_regularizer else None,\n                  \"U_regularizer\": self.U_regularizer.get_config() if self.U_regularizer else None,\n                  \"b_regularizer\": self.b_regularizer.get_config() if self.b_regularizer else None,\n                  \"dropout_W\": self.dropout_W,\n                  \"dropout_U\": self.dropout_U}\n        base_config = super(GRU, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\n\nclass LSTM(Recurrent):\n    '''Long-Short Term Memory unit - Hochreiter 1997.\n\n    For a step-by-step description of the algorithm, see\n    [this tutorial](http://deeplearning.net/tutorial/lstm.html).\n\n    # Arguments\n        output_dim: dimension of the internal projections and the final output.\n        init: weight initialization function.\n            Can be the name of an existing function (str),\n            or a Theano function (see: [initializations](../initializations.md)).\n        inner_init: initialization function of the inner cells.\n        forget_bias_init: initialization function for the bias of the forget gate.\n            [Jozefowicz et al.](http://www.jmlr.org/proceedings/papers/v37/jozefowicz15.pdf)\n            recommend initializing with ones.\n        activation: activation function.\n            Can be the name of an existing function (str),\n            or a Theano function (see: [activations](../activations.md)).\n        inner_activation: activation function for the inner cells.\n        W_regularizer: instance of [WeightRegularizer](../regularizers.md)\n            (eg. L1 or L2 regularization), applied to the input weights matrices.\n        U_regularizer: instance of [WeightRegularizer](../regularizers.md)\n            (eg. L1 or L2 regularization), applied to the recurrent weights matrices.\n        b_regularizer: instance of [WeightRegularizer](../regularizers.md),\n            applied to the bias.\n        dropout_W: float between 0 and 1. Fraction of the input units to drop for input gates.\n        dropout_U: float between 0 and 1. Fraction of the input units to drop for recurrent connections.\n\n    # References\n        - [Long short-term memory](http://deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf) (original 1997 paper)\n        - [Learning to forget: Continual prediction with LSTM](http://www.mitpressjournals.org/doi/pdf/10.1162/089976600300015015)\n        - [Supervised sequence labelling with recurrent neural networks](http://www.cs.toronto.edu/~graves/preprint.pdf)\n        - [A Theoretically Grounded Application of Dropout in Recurrent Neural Networks](http://arxiv.org/abs/1512.05287)\n    '''\n    def __init__(self, output_dim,\n                 init='glorot_uniform', inner_init='orthogonal',\n                 forget_bias_init='one', activation='tanh',\n                 inner_activation='hard_sigmoid',\n                 W_regularizer=None, U_regularizer=None, b_regularizer=None,\n                 dropout_W=0., dropout_U=0., **kwargs):\n        self.output_dim = output_dim\n        self.init = initializations.get(init)\n        self.inner_init = initializations.get(inner_init)\n        self.forget_bias_init = initializations.get(forget_bias_init)\n        self.activation = activations.get(activation)\n        self.inner_activation = activations.get(inner_activation)\n        self.W_regularizer = regularizers.get(W_regularizer)\n        self.U_regularizer = regularizers.get(U_regularizer)\n        self.b_regularizer = regularizers.get(b_regularizer)\n        self.dropout_W, self.dropout_U = dropout_W, dropout_U\n\n        if self.dropout_W or self.dropout_U:\n            self.uses_learning_phase = True\n        super(LSTM, self).__init__(**kwargs)\n\n    def build(self, input_shape):\n        self.input_spec = [InputSpec(shape=input_shape)]\n        input_dim = input_shape[2]\n        self.input_dim = input_dim\n\n        if self.stateful:\n            self.reset_states()\n        else:\n            # initial states: 2 all-zero tensors of shape (output_dim)\n            self.states = [None, None]\n\n        self.W_i = self.init((input_dim, self.output_dim),\n                             name='{}_W_i'.format(self.name))\n        self.U_i = self.inner_init((self.output_dim, self.output_dim),\n                                   name='{}_U_i'.format(self.name))\n        self.b_i = K.zeros((self.output_dim,), name='{}_b_i'.format(self.name))\n\n        self.W_f = self.init((input_dim, self.output_dim),\n                             name='{}_W_f'.format(self.name))\n        self.U_f = self.inner_init((self.output_dim, self.output_dim),\n                                   name='{}_U_f'.format(self.name))\n        self.b_f = self.forget_bias_init((self.output_dim,),\n                                         name='{}_b_f'.format(self.name))\n\n        self.W_c = self.init((input_dim, self.output_dim),\n                             name='{}_W_c'.format(self.name))\n        self.U_c = self.inner_init((self.output_dim, self.output_dim),\n                                   name='{}_U_c'.format(self.name))\n        self.b_c = K.zeros((self.output_dim,), name='{}_b_c'.format(self.name))\n\n        self.W_o = self.init((input_dim, self.output_dim),\n                             name='{}_W_o'.format(self.name))\n        self.U_o = self.inner_init((self.output_dim, self.output_dim),\n                                   name='{}_U_o'.format(self.name))\n        self.b_o = K.zeros((self.output_dim,), name='{}_b_o'.format(self.name))\n\n        self.regularizers = []\n        if self.W_regularizer:\n            self.W_regularizer.set_param(K.concatenate([self.W_i,\n                                                        self.W_f,\n                                                        self.W_c,\n                                                        self.W_o]))\n            self.regularizers.append(self.W_regularizer)\n        if self.U_regularizer:\n            self.U_regularizer.set_param(K.concatenate([self.U_i,\n                                                        self.U_f,\n                                                        self.U_c,\n                                                        self.U_o]))\n            self.regularizers.append(self.U_regularizer)\n        if self.b_regularizer:\n            self.b_regularizer.set_param(K.concatenate([self.b_i,\n                                                        self.b_f,\n                                                        self.b_c,\n                                                        self.b_o]))\n            self.regularizers.append(self.b_regularizer)\n\n        self.trainable_weights = [self.W_i, self.U_i, self.b_i,\n                                  self.W_c, self.U_c, self.b_c,\n                                  self.W_f, self.U_f, self.b_f,\n                                  self.W_o, self.U_o, self.b_o]\n\n        if self.initial_weights is not None:\n            self.set_weights(self.initial_weights)\n            del self.initial_weights\n\n    def reset_states(self):\n        assert self.stateful, 'Layer must be stateful.'\n        input_shape = self.input_spec[0].shape\n        if not input_shape[0]:\n            raise Exception('If a RNN is stateful, a complete ' +\n                            'input_shape must be provided (including batch size).')\n        if hasattr(self, 'states'):\n            K.set_value(self.states[0],\n                        np.zeros((input_shape[0], self.output_dim)))\n            K.set_value(self.states[1],\n                        np.zeros((input_shape[0], self.output_dim)))\n        else:\n            self.states = [K.zeros((input_shape[0], self.output_dim)),\n                           K.zeros((input_shape[0], self.output_dim))]\n\n    def preprocess_input(self, x, train=False):\n        if self.consume_less == 'cpu':\n            if train and (0 < self.dropout_W < 1):\n                dropout = self.dropout_W\n            else:\n                dropout = 0\n            input_shape = self.input_spec[0].shape\n            input_dim = input_shape[2]\n            timesteps = input_shape[1]\n\n            x_i = time_distributed_dense(x, self.W_i, self.b_i, dropout,\n                                         input_dim, self.output_dim, timesteps)\n            x_f = time_distributed_dense(x, self.W_f, self.b_f, dropout,\n                                         input_dim, self.output_dim, timesteps)\n            x_c = time_distributed_dense(x, self.W_c, self.b_c, dropout,\n                                         input_dim, self.output_dim, timesteps)\n            x_o = time_distributed_dense(x, self.W_o, self.b_o, dropout,\n                                         input_dim, self.output_dim, timesteps)\n            return K.concatenate([x_i, x_f, x_c, x_o], axis=2)\n        else:\n            return x\n\n    def step(self, x, states):\n        h_tm1 = states[0]\n        c_tm1 = states[1]\n        B_U = states[2]\n        B_W = states[3]\n\n        if self.consume_less == 'cpu':\n            x_i = x[:, :self.output_dim]\n            x_f = x[:, self.output_dim: 2 * self.output_dim]\n            x_c = x[:, 2 * self.output_dim: 3 * self.output_dim]\n            x_o = x[:, 3 * self.output_dim:]\n        else:\n            x_i = K.dot(x * B_W[0], self.W_i) + self.b_i\n            x_f = K.dot(x * B_W[1], self.W_f) + self.b_f\n            x_c = K.dot(x * B_W[2], self.W_c) + self.b_c\n            x_o = K.dot(x * B_W[3], self.W_o) + self.b_o\n\n        i = self.inner_activation(x_i + K.dot(h_tm1 * B_U[0], self.U_i))\n        f = self.inner_activation(x_f + K.dot(h_tm1 * B_U[1], self.U_f))\n        c = f * c_tm1 + i * self.activation(x_c + K.dot(h_tm1 * B_U[2], self.U_c))\n        o = self.inner_activation(x_o + K.dot(h_tm1 * B_U[3], self.U_o))\n\n        h = o * self.activation(c)\n        return h, [h, c]\n\n    def get_constants(self, x):\n        constants = []\n        if 0 < self.dropout_U < 1:\n            ones = K.ones_like(K.reshape(x[:, 0, 0], (-1, 1)))\n            ones = K.concatenate([ones] * self.output_dim, 1)\n            B_U = [K.dropout(ones, self.dropout_U) for _ in range(4)]\n            constants.append(B_U)\n        else:\n            constants.append([K.cast_to_floatx(1.) for _ in range(4)])\n\n        if self.consume_less == 'cpu' and 0 < self.dropout_W < 1:\n            input_shape = self.input_spec[0].shape\n            input_dim = input_shape[-1]\n            ones = K.ones_like(K.reshape(x[:, 0, 0], (-1, 1)))\n            ones = K.concatenate([ones] * input_dim, 1)\n            B_W = [K.dropout(ones, self.dropout_W) for _ in range(4)]\n            constants.append(B_W)\n        else:\n            constants.append([K.cast_to_floatx(1.) for _ in range(4)])\n        return constants\n\n    def get_config(self):\n        config = {\"output_dim\": self.output_dim,\n                  \"init\": self.init.__name__,\n                  \"inner_init\": self.inner_init.__name__,\n                  \"forget_bias_init\": self.forget_bias_init.__name__,\n                  \"activation\": self.activation.__name__,\n                  \"inner_activation\": self.inner_activation.__name__,\n                  \"W_regularizer\": self.W_regularizer.get_config() if self.W_regularizer else None,\n                  \"U_regularizer\": self.U_regularizer.get_config() if self.U_regularizer else None,\n                  \"b_regularizer\": self.b_regularizer.get_config() if self.b_regularizer else None,\n                  \"dropout_W\": self.dropout_W,\n                  \"dropout_U\": self.dropout_U}\n        base_config = super(LSTM, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n",
          "file_patch": "@@ -4,7 +4,7 @@ import numpy as np\n \n from .. import backend as K\n from .. import activations, initializations, regularizers\n-from ..layers.core import MaskedLayer\n+from ..engine import Layer, InputSpec\n \n \n def time_distributed_dense(x, w, b=None, dropout=None,\n@@ -21,12 +21,12 @@ def time_distributed_dense(x, w, b=None, dropout=None,\n         # won't work with TensorFlow\n         output_dim = K.shape(w)[1]\n \n-    if dropout:\n+    if dropout is not None and 0. < dropout < 1.:\n         # apply the same dropout pattern at every timestep\n         ones = K.ones_like(K.reshape(x[:, 0, :], (-1, input_dim)))\n         dropout_matrix = K.dropout(ones, dropout)\n         expanded_dropout_matrix = K.repeat(dropout_matrix, timesteps)\n-        x *= expanded_dropout_matrix\n+        x = K.in_train_phase(x * expanded_dropout_matrix, x)\n \n     # collapse time dimension and batch dimension together\n     x = K.reshape(x, (-1, input_dim))\n@@ -39,7 +39,7 @@ def time_distributed_dense(x, w, b=None, dropout=None,\n     return x\n \n \n-class Recurrent(MaskedLayer):\n+class Recurrent(Layer):\n     '''Abstract base class for recurrent layers.\n     Do not use in a model -- it's not a functional layer!\n \n@@ -47,14 +47,6 @@ class Recurrent(MaskedLayer):\n     follow the specifications of this class and accept\n     the keyword arguments listed below.\n \n-    # Input shape\n-        3D tensor with shape `(nb_samples, timesteps, input_dim)`.\n-\n-    # Output shape\n-        - if `return_sequences`: 3D tensor with shape\n-            `(nb_samples, timesteps, output_dim)`.\n-        - else, 2D tensor with shape `(nb_samples, output_dim)`.\n-\n     # Arguments\n         weights: list of numpy arrays to set as initial weights.\n             The list should have 3 elements, of shapes:\n@@ -66,6 +58,17 @@ class Recurrent(MaskedLayer):\n         stateful: Boolean (default False). If True, the last state\n             for each sample at index i in a batch will be used as initial\n             state for the sample of index i in the following batch.\n+        unroll: Boolean (default False). If True, the network will be unrolled,\n+            else a symbolic loop will be used. When using TensorFlow, the network\n+            is always unrolled, so this argument does not do anything.\n+            Unrolling can speed-up a RNN, although it tends to be more memory-intensive.\n+            Unrolling is only suitable for short sequences.\n+        consume_less: one of \"cpu\", \"mem\". If set to \"cpu\", the RNN will use\n+            an implementation that uses fewer, larger matrix products,\n+            thus running faster (at least on CPU) but consuming more memory.\n+            If set to \"mem\", the RNN will use more matrix products,\n+            but smaller ones, thus running slower (may actually be faster on GPU)\n+            while consuming less memory.\n         input_dim: dimensionality of the input (integer).\n             This argument (or alternatively, the keyword argument `input_shape`)\n             is required when using this layer as the first layer in a model.\n@@ -79,6 +82,14 @@ class Recurrent(MaskedLayer):\n             at the level of the first layer\n             (e.g. via the `input_shape` argument)\n \n+    # Input shape\n+        3D tensor with shape `(nb_samples, timesteps, input_dim)`.\n+\n+    # Output shape\n+        - if `return_sequences`: 3D tensor with shape\n+            `(nb_samples, timesteps, output_dim)`.\n+        - else, 2D tensor with shape `(nb_samples, output_dim)`.\n+\n     # Masking\n         This layer supports masking for input data with a variable number\n         of timesteps. To introduce masks to your data,\n@@ -115,40 +126,41 @@ class Recurrent(MaskedLayer):\n         When using the TensorFlow backend, specify a fixed batch size for your model\n         following the notes on statefulness RNNs.\n     '''\n-    input_ndim = 3\n-\n     def __init__(self, weights=None,\n                  return_sequences=False, go_backwards=False, stateful=False,\n+                 unroll=False, consume_less='cpu',\n                  input_dim=None, input_length=None, **kwargs):\n         self.return_sequences = return_sequences\n         self.initial_weights = weights\n         self.go_backwards = go_backwards\n         self.stateful = stateful\n+        self.unroll = unroll\n+        self.consume_less = consume_less\n \n+        self.supports_masking = True\n+        self.input_spec = [InputSpec(ndim=3)]\n         self.input_dim = input_dim\n         self.input_length = input_length\n         if self.input_dim:\n             kwargs['input_shape'] = (self.input_length, self.input_dim)\n         super(Recurrent, self).__init__(**kwargs)\n \n-    def get_output_mask(self, train=False):\n+    def get_output_shape_for(self, input_shape):\n         if self.return_sequences:\n-            return super(Recurrent, self).get_output_mask(train)\n+            return (input_shape[0], input_shape[1], self.output_dim)\n         else:\n-            return None\n+            return (input_shape[0], self.output_dim)\n \n-    @property\n-    def output_shape(self):\n-        input_shape = self.input_shape\n+    def compute_mask(self, input, mask):\n         if self.return_sequences:\n-            return (input_shape[0], input_shape[1], self.output_dim)\n+            return mask\n         else:\n-            return (input_shape[0], self.output_dim)\n+            return None\n \n     def step(self, x, states):\n         raise NotImplementedError\n \n-    def get_constants(self, x, train=False):\n+    def get_constants(self, x):\n         return []\n \n     def get_initial_states(self, x):\n@@ -160,38 +172,41 @@ class Recurrent(MaskedLayer):\n         initial_states = [initial_state for _ in range(len(self.states))]\n         return initial_states\n \n-    def preprocess_input(self, x, train=False):\n+    def preprocess_input(self, x):\n         return x\n \n-    def get_output(self, train=False):\n+    def call(self, x, mask=None):\n         # input shape: (nb_samples, time (padded with zeros), input_dim)\n-        X = self.get_input(train)\n-        mask = self.get_input_mask(train)\n-\n-        assert K.ndim(X) == 3\n+        # note that the .build() method of subclasses MUST define\n+        # self.input_sepc with a complete input shape.\n+        input_shape = self.input_spec[0].shape\n         if K._BACKEND == 'tensorflow':\n-            if not self.input_shape[1]:\n-                raise Exception('When using TensorFlow, you should define ' +\n-                                'explicitly the number of timesteps of ' +\n-                                'your sequences.\\n' +\n-                                'If your first layer is an Embedding, ' +\n-                                'make sure to pass it an \"input_length\" ' +\n-                                'argument. Otherwise, make sure ' +\n-                                'the first layer has ' +\n-                                'an \"input_shape\" or \"batch_input_shape\" ' +\n-                                'argument, including the time axis.')\n+            if not input_shape[1]:\n+                raise Exception('When using TensorFlow, you should define '\n+                                'explicitly the number of timesteps of '\n+                                'your sequences.\\n'\n+                                'If your first layer is an Embedding, '\n+                                'make sure to pass it an \"input_length\" '\n+                                'argument. Otherwise, make sure '\n+                                'the first layer has '\n+                                'an \"input_shape\" or \"batch_input_shape\" '\n+                                'argument, including the time axis. '\n+                                'Found input shape at layer ' + self.name +\n+                                ': ' + str(input_shape))\n         if self.stateful:\n             initial_states = self.states\n         else:\n-            initial_states = self.get_initial_states(X)\n-        constants = self.get_constants(X, train)\n-        preprocessed_input = self.preprocess_input(X, train)\n+            initial_states = self.get_initial_states(x)\n+        constants = self.get_constants(x)\n+        preprocessed_input = self.preprocess_input(x)\n \n         last_output, outputs, states = K.rnn(self.step, preprocessed_input,\n                                              initial_states,\n                                              go_backwards=self.go_backwards,\n                                              mask=mask,\n-                                             constants=constants)\n+                                             constants=constants,\n+                                             unroll=self.unroll,\n+                                             input_length=input_shape[1])\n         if self.stateful:\n             self.updates = []\n             for i in range(len(states)):\n@@ -203,10 +218,11 @@ class Recurrent(MaskedLayer):\n             return last_output\n \n     def get_config(self):\n-        config = {\"name\": self.__class__.__name__,\n-                  \"return_sequences\": self.return_sequences,\n-                  \"go_backwards\": self.go_backwards,\n-                  \"stateful\": self.stateful}\n+        config = {'return_sequences': self.return_sequences,\n+                  'go_backwards': self.go_backwards,\n+                  'stateful': self.stateful,\n+                  'unroll': self.unroll,\n+                  'consume_less': self.consume_less}\n         if self.stateful:\n             config['batch_input_shape'] = self.input_shape\n         else:\n@@ -254,10 +270,13 @@ class SimpleRNN(Recurrent):\n         self.U_regularizer = regularizers.get(U_regularizer)\n         self.b_regularizer = regularizers.get(b_regularizer)\n         self.dropout_W, self.dropout_U = dropout_W, dropout_U\n+\n+        if self.dropout_W or self.dropout_U:\n+            self.uses_learning_phase = True\n         super(SimpleRNN, self).__init__(**kwargs)\n \n-    def build(self):\n-        input_shape = self.input_shape\n+    def build(self, input_shape):\n+        self.input_spec = [InputSpec(shape=input_shape)]\n         if self.stateful:\n             self.reset_states()\n         else:\n@@ -291,7 +310,7 @@ class SimpleRNN(Recurrent):\n \n     def reset_states(self):\n         assert self.stateful, 'Layer must be stateful.'\n-        input_shape = self.input_shape\n+        input_shape = self.input_spec[0].shape\n         if not input_shape[0]:\n             raise Exception('If a RNN is stateful, a complete ' +\n                             'input_shape must be provided (including batch size).')\n@@ -301,33 +320,49 @@ class SimpleRNN(Recurrent):\n         else:\n             self.states = [K.zeros((input_shape[0], self.output_dim))]\n \n-    def preprocess_input(self, x, train=False):\n-        if train and (0 < self.dropout_W < 1):\n-            dropout = self.dropout_W\n+    def preprocess_input(self, x):\n+        if self.consume_less == 'cpu':\n+            input_shape = self.input_spec[0].shape\n+            input_dim = input_shape[2]\n+            timesteps = input_shape[1]\n+            return time_distributed_dense(x, self.W, self.b, self.dropout_W,\n+                                          input_dim, self.output_dim,\n+                                          timesteps)\n         else:\n-            dropout = 0\n-        input_shape = self.input_shape\n-        input_dim = input_shape[2]\n-        timesteps = input_shape[1]\n-        return time_distributed_dense(x, self.W, self.b, dropout,\n-                                      input_dim, self.output_dim, timesteps)\n+            return x\n \n-    def step(self, h, states):\n+    def step(self, x, states):\n         prev_output = states[0]\n-        if len(states) == 2:\n-            B_U = states[1]\n+        B_U = states[1]\n+        B_W = states[2]\n+\n+        if self.consume_less == 'cpu':\n+            h = x\n         else:\n-            B_U = 1.\n+            h = K.dot(x * B_W, self.W) + self.b\n+\n         output = self.activation(h + K.dot(prev_output * B_U, self.U))\n         return output, [output]\n \n-    def get_constants(self, x, train=False):\n-        if train and (0 < self.dropout_U < 1):\n+    def get_constants(self, x):\n+        constants = []\n+        if 0 < self.dropout_U < 1:\n             ones = K.ones_like(K.reshape(x[:, 0, 0], (-1, 1)))\n             ones = K.concatenate([ones] * self.output_dim, 1)\n-            B_U = K.dropout(ones, self.dropout_U)\n-            return [B_U]\n-        return []\n+            B_U = K.in_train_phase(K.dropout(ones, self.dropout_U), ones)\n+            constants.append(B_U)\n+        else:\n+            constants.append(K.cast_to_floatx(1.))\n+        if self.consume_less == 'cpu' and 0 < self.dropout_W < 1:\n+            input_shape = self.input_spec[0].shape\n+            input_dim = input_shape[-1]\n+            ones = K.ones_like(K.reshape(x[:, 0, 0], (-1, 1)))\n+            ones = K.concatenate([ones] * input_dim, 1)\n+            B_W = K.in_train_phase(K.dropout(ones, self.dropout_W), ones)\n+            constants.append(B_W)\n+        else:\n+            constants.append(K.cast_to_floatx(1.))\n+        return constants\n \n     def get_config(self):\n         config = {\"output_dim\": self.output_dim,\n@@ -384,10 +419,13 @@ class GRU(Recurrent):\n         self.U_regularizer = regularizers.get(U_regularizer)\n         self.b_regularizer = regularizers.get(b_regularizer)\n         self.dropout_W, self.dropout_U = dropout_W, dropout_U\n+\n+        if self.dropout_W or self.dropout_U:\n+            self.uses_learning_phase = True\n         super(GRU, self).__init__(**kwargs)\n \n-    def build(self):\n-        input_shape = self.input_shape\n+    def build(self, input_shape):\n+        self.input_spec = [InputSpec(shape=input_shape)]\n         input_dim = input_shape[2]\n         self.input_dim = input_dim\n \n@@ -451,33 +489,35 @@ class GRU(Recurrent):\n         else:\n             self.states = [K.zeros((input_shape[0], self.output_dim))]\n \n-    def preprocess_input(self, x, train=False):\n-        if train and (0 < self.dropout_W < 1):\n-            dropout = self.dropout_W\n+    def preprocess_input(self, x):\n+        if self.consume_less == 'cpu':\n+            input_shape = self.input_spec[0].shape\n+            input_dim = input_shape[2]\n+            timesteps = input_shape[1]\n+\n+            x_z = time_distributed_dense(x, self.W_z, self.b_z, self.dropout_W,\n+                                         input_dim, self.output_dim, timesteps)\n+            x_r = time_distributed_dense(x, self.W_r, self.b_r, self.dropout_W,\n+                                         input_dim, self.output_dim, timesteps)\n+            x_h = time_distributed_dense(x, self.W_h, self.b_h, self.dropout_W,\n+                                         input_dim, self.output_dim, timesteps)\n+            return K.concatenate([x_z, x_r, x_h], axis=2)\n         else:\n-            dropout = 0\n-        input_shape = self.input_shape\n-        input_dim = input_shape[2]\n-        timesteps = input_shape[1]\n-\n-        x_z = time_distributed_dense(x, self.W_z, self.b_z, dropout,\n-                                     input_dim, self.output_dim, timesteps)\n-        x_r = time_distributed_dense(x, self.W_r, self.b_r, dropout,\n-                                     input_dim, self.output_dim, timesteps)\n-        x_h = time_distributed_dense(x, self.W_h, self.b_h, dropout,\n-                                     input_dim, self.output_dim, timesteps)\n-        return K.concatenate([x_z, x_r, x_h], axis=2)\n+            return x\n \n     def step(self, x, states):\n         h_tm1 = states[0]  # previous memory\n-        if len(states) == 2:\n-            B_U = states[1]  # dropout matrices for recurrent units\n-        else:\n-            B_U = [1., 1., 1.]\n+        B_U = states[1]  # dropout matrices for recurrent units\n+        B_W = states[2]\n \n-        x_z = x[:, :self.output_dim]\n-        x_r = x[:, self.output_dim: 2 * self.output_dim]\n-        x_h = x[:, 2 * self.output_dim:]\n+        if self.consume_less == 'cpu':\n+            x_z = x[:, :self.output_dim]\n+            x_r = x[:, self.output_dim: 2 * self.output_dim]\n+            x_h = x[:, 2 * self.output_dim:]\n+        else:\n+            x_z = K.dot(x * B_W[0], self.W_z) + self.b_z\n+            x_r = K.dot(x * B_W[1], self.W_r) + self.b_r\n+            x_h = K.dot(x * B_W[2], self.W_h) + self.b_h\n \n         z = self.inner_activation(x_z + K.dot(h_tm1 * B_U[0], self.U_z))\n         r = self.inner_activation(x_r + K.dot(h_tm1 * B_U[1], self.U_r))\n@@ -486,13 +526,26 @@ class GRU(Recurrent):\n         h = z * h_tm1 + (1 - z) * hh\n         return h, [h]\n \n-    def get_constants(self, x, train=False):\n-        if train and (0 < self.dropout_U < 1):\n+    def get_constants(self, x):\n+        constants = []\n+        if 0 < self.dropout_U < 1:\n             ones = K.ones_like(K.reshape(x[:, 0, 0], (-1, 1)))\n             ones = K.concatenate([ones] * self.output_dim, 1)\n             B_U = [K.dropout(ones, self.dropout_U) for _ in range(3)]\n-            return [B_U]\n-        return []\n+            constants.append(B_U)\n+        else:\n+            constants.append([K.cast_to_floatx(1.) for _ in range(4)])\n+\n+        if self.consume_less == 'cpu' and 0 < self.dropout_W < 1:\n+            input_shape = self.input_spec[0].shape\n+            input_dim = input_shape[-1]\n+            ones = K.ones_like(K.reshape(x[:, 0, 0], (-1, 1)))\n+            ones = K.concatenate([ones] * input_dim, 1)\n+            B_W = [K.dropout(ones, self.dropout_W) for _ in range(3)]\n+            constants.append(B_W)\n+        else:\n+            constants.append([K.cast_to_floatx(1.) for _ in range(4)])\n+        return constants\n \n     def get_config(self):\n         config = {\"output_dim\": self.output_dim,\n@@ -559,10 +612,13 @@ class LSTM(Recurrent):\n         self.U_regularizer = regularizers.get(U_regularizer)\n         self.b_regularizer = regularizers.get(b_regularizer)\n         self.dropout_W, self.dropout_U = dropout_W, dropout_U\n+\n+        if self.dropout_W or self.dropout_U:\n+            self.uses_learning_phase = True\n         super(LSTM, self).__init__(**kwargs)\n \n-    def build(self):\n-        input_shape = self.input_shape\n+    def build(self, input_shape):\n+        self.input_spec = [InputSpec(shape=input_shape)]\n         input_dim = input_shape[2]\n         self.input_dim = input_dim\n \n@@ -628,7 +684,7 @@ class LSTM(Recurrent):\n \n     def reset_states(self):\n         assert self.stateful, 'Layer must be stateful.'\n-        input_shape = self.input_shape\n+        input_shape = self.input_spec[0].shape\n         if not input_shape[0]:\n             raise Exception('If a RNN is stateful, a complete ' +\n                             'input_shape must be provided (including batch size).')\n@@ -642,36 +698,43 @@ class LSTM(Recurrent):\n                            K.zeros((input_shape[0], self.output_dim))]\n \n     def preprocess_input(self, x, train=False):\n-        if train and (0 < self.dropout_W < 1):\n-            dropout = self.dropout_W\n+        if self.consume_less == 'cpu':\n+            if train and (0 < self.dropout_W < 1):\n+                dropout = self.dropout_W\n+            else:\n+                dropout = 0\n+            input_shape = self.input_spec[0].shape\n+            input_dim = input_shape[2]\n+            timesteps = input_shape[1]\n+\n+            x_i = time_distributed_dense(x, self.W_i, self.b_i, dropout,\n+                                         input_dim, self.output_dim, timesteps)\n+            x_f = time_distributed_dense(x, self.W_f, self.b_f, dropout,\n+                                         input_dim, self.output_dim, timesteps)\n+            x_c = time_distributed_dense(x, self.W_c, self.b_c, dropout,\n+                                         input_dim, self.output_dim, timesteps)\n+            x_o = time_distributed_dense(x, self.W_o, self.b_o, dropout,\n+                                         input_dim, self.output_dim, timesteps)\n+            return K.concatenate([x_i, x_f, x_c, x_o], axis=2)\n         else:\n-            dropout = 0\n-        input_shape = self.input_shape\n-        input_dim = input_shape[2]\n-        timesteps = input_shape[1]\n-\n-        x_i = time_distributed_dense(x, self.W_i, self.b_i, dropout,\n-                                     input_dim, self.output_dim, timesteps)\n-        x_f = time_distributed_dense(x, self.W_f, self.b_f, dropout,\n-                                     input_dim, self.output_dim, timesteps)\n-        x_c = time_distributed_dense(x, self.W_c, self.b_c, dropout,\n-                                     input_dim, self.output_dim, timesteps)\n-        x_o = time_distributed_dense(x, self.W_o, self.b_o, dropout,\n-                                     input_dim, self.output_dim, timesteps)\n-        return K.concatenate([x_i, x_f, x_c, x_o], axis=2)\n+            return x\n \n     def step(self, x, states):\n         h_tm1 = states[0]\n         c_tm1 = states[1]\n-        if len(states) == 3:\n-            B_U = states[2]\n+        B_U = states[2]\n+        B_W = states[3]\n+\n+        if self.consume_less == 'cpu':\n+            x_i = x[:, :self.output_dim]\n+            x_f = x[:, self.output_dim: 2 * self.output_dim]\n+            x_c = x[:, 2 * self.output_dim: 3 * self.output_dim]\n+            x_o = x[:, 3 * self.output_dim:]\n         else:\n-            B_U = [1. for _ in range(4)]\n-\n-        x_i = x[:, :self.output_dim]\n-        x_f = x[:, self.output_dim: 2 * self.output_dim]\n-        x_c = x[:, 2 * self.output_dim: 3 * self.output_dim]\n-        x_o = x[:, 3 * self.output_dim:]\n+            x_i = K.dot(x * B_W[0], self.W_i) + self.b_i\n+            x_f = K.dot(x * B_W[1], self.W_f) + self.b_f\n+            x_c = K.dot(x * B_W[2], self.W_c) + self.b_c\n+            x_o = K.dot(x * B_W[3], self.W_o) + self.b_o\n \n         i = self.inner_activation(x_i + K.dot(h_tm1 * B_U[0], self.U_i))\n         f = self.inner_activation(x_f + K.dot(h_tm1 * B_U[1], self.U_f))\n@@ -681,13 +744,26 @@ class LSTM(Recurrent):\n         h = o * self.activation(c)\n         return h, [h, c]\n \n-    def get_constants(self, x, train=False):\n-        if train and (0 < self.dropout_U < 1):\n+    def get_constants(self, x):\n+        constants = []\n+        if 0 < self.dropout_U < 1:\n             ones = K.ones_like(K.reshape(x[:, 0, 0], (-1, 1)))\n             ones = K.concatenate([ones] * self.output_dim, 1)\n             B_U = [K.dropout(ones, self.dropout_U) for _ in range(4)]\n-            return [B_U]\n-        return []\n+            constants.append(B_U)\n+        else:\n+            constants.append([K.cast_to_floatx(1.) for _ in range(4)])\n+\n+        if self.consume_less == 'cpu' and 0 < self.dropout_W < 1:\n+            input_shape = self.input_spec[0].shape\n+            input_dim = input_shape[-1]\n+            ones = K.ones_like(K.reshape(x[:, 0, 0], (-1, 1)))\n+            ones = K.concatenate([ones] * input_dim, 1)\n+            B_W = [K.dropout(ones, self.dropout_W) for _ in range(4)]\n+            constants.append(B_W)\n+        else:\n+            constants.append([K.cast_to_floatx(1.) for _ in range(4)])\n+        return constants\n \n     def get_config(self):\n         config = {\"output_dim\": self.output_dim,\n",
          "files_name_in_blame_commit": [
            "embeddings.py",
            "babi_memnn.py",
            "mnist_transfer_cnn.py",
            "noise.py",
            "generic_utils.py",
            "topology.py",
            "theano_backend.py",
            "test_backends.py",
            "lstm_text_generation.py",
            "__init__.py",
            "recurrent.py",
            "metrics.py",
            "core.py",
            "layer_utils.py",
            "common.py",
            "models.py",
            "mnist_cnn.py",
            "training.py",
            "test_training.py",
            "test_shape_inference.py",
            "imdb_cnn_lstm.py",
            "test_image_data_tasks.py",
            "normalization.py",
            "mnist_siamese_graph.py",
            "cifar10_cnn.py",
            "test_graph_model.py",
            "conv_filter_visualization.py",
            "imdb_bidirectional_lstm.py",
            "test_sequential_model.py",
            "imdb_lstm.py",
            "addition_rnn.py",
            "test_utils.py",
            "deep_dream.py",
            "wrappers.py",
            "optimizers.py",
            "imdb.py",
            "antirectifier.py",
            "convolutional.py",
            "reuters_mlp.py",
            "test_topology.py",
            "mnist_irnn.py",
            "advanced_activations.py",
            "kaggle_otto_nn.py",
            "test_temporal_data_tasks.py",
            "tensorflow_backend.py",
            "babi_rnn.py",
            "imdb_cnn.py",
            "neural_style_transfer.py",
            "containers.py",
            "mnist_mlp.py",
            "test_vector_data_tasks.py"
          ]
        }
      },
      "07ffc76b9318ab1197ce61838992960e9abb3c22": {
        "commit": {
          "commit_id": "07ffc76b9318ab1197ce61838992960e9abb3c22",
          "commit_message": "Finalize RNN statefulness functionality",
          "commit_author": "Francois Chollet",
          "commit_date": "2015-12-08 10:43:06",
          "commit_parent": "d400fc45129fbd4d3fe83345a69ec5973bb29a37"
        },
        "function": {
          "function_name": "build",
          "function_code_before": "def build(self):\n    input_shape = self.input_shape\n    if self.stateful:\n        if not input_shape[0]:\n            raise Exception('If a RNN is stateful, a complete ' + 'input_shape must be provided ' + '(including batch size).')\n        self.states = [K.zeros((input_shape[0], self.output_dim))]\n    else:\n        self.states = [None]\n    input_dim = input_shape[2]\n    self.input_dim = input_dim\n    self.W = self.init((input_dim, self.output_dim))\n    self.U = self.inner_init((self.output_dim, self.output_dim))\n    self.b = K.zeros(self.output_dim)\n    self.params = [self.W, self.U, self.b]\n    if self.initial_weights is not None:\n        self.set_weights(self.initial_weights)\n        del self.initial_weights",
          "function_code_after": "def build(self):\n    input_shape = self.input_shape\n    if self.stateful:\n        self.reset_states()\n    else:\n        self.states = [None]\n    input_dim = input_shape[2]\n    self.input_dim = input_dim\n    self.W = self.init((input_dim, self.output_dim))\n    self.U = self.inner_init((self.output_dim, self.output_dim))\n    self.b = K.zeros(self.output_dim)\n    self.params = [self.W, self.U, self.b]\n    if self.initial_weights is not None:\n        self.set_weights(self.initial_weights)\n        del self.initial_weights",
          "function_before_start_line": 123,
          "function_before_end_line": 144,
          "function_after_start_line": 122,
          "function_after_end_line": 139,
          "function_before_token_count": 157,
          "function_after_token_count": 125,
          "functions_name_modified_file": [
            "__init__",
            "output_shape",
            "build",
            "get_output_mask",
            "reset_states",
            "step",
            "get_initial_states",
            "get_output",
            "get_config"
          ],
          "functions_name_all_files": [
            "__init__",
            "output_shape",
            "build",
            "test_lstm",
            "get_output_mask",
            "reset_states",
            "step",
            "get_initial_states",
            "_runner",
            "test_gru",
            "test_simple",
            "get_output",
            "get_config"
          ],
          "functions_name_co_evolved_modified_file": [
            "reset_states",
            "get_output"
          ],
          "functions_name_co_evolved_all_files": [
            "_runner",
            "reset_states",
            "get_output"
          ]
        },
        "file": {
          "file_name": "recurrent.py",
          "file_nloc": 312,
          "file_complexity": 42,
          "file_token_count": 2352,
          "file_before": "# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import\nimport numpy as np\n\nfrom .. import backend as K\nfrom .. import activations, initializations\nfrom ..layers.core import MaskedLayer\n\n\nclass Recurrent(MaskedLayer):\n    input_ndim = 3\n\n    def __init__(self, weights=None,\n                 return_sequences=False, go_backwards=False, stateful=False,\n                 input_dim=None, input_length=None, **kwargs):\n        self.return_sequences = return_sequences\n        self.initial_weights = weights\n        self.go_backwards = go_backwards\n        self.stateful = stateful\n\n        self.input_dim = input_dim\n        self.input_length = input_length\n        if self.input_dim:\n            kwargs['input_shape'] = (self.input_length, self.input_dim)\n        super(Recurrent, self).__init__(**kwargs)\n\n    def get_output_mask(self, train=False):\n        if self.return_sequences:\n            return super(Recurrent, self).get_output_mask(train)\n        else:\n            return None\n\n    @property\n    def output_shape(self):\n        input_shape = self.input_shape\n        if self.return_sequences:\n            return (input_shape[0], input_shape[1], self.output_dim)\n        else:\n            return (input_shape[0], self.output_dim)\n\n    def step(self, x, states):\n        raise NotImplementedError\n\n    def get_initial_states(self, X):\n        # build an all-zero tensor of shape (samples, output_dim)\n        initial_state = K.zeros_like(X)  # (samples, timesteps, input_dim)\n        initial_state = K.sum(initial_state, axis=1)  # (samples, input_dim)\n        reducer = K.zeros((self.input_dim, self.output_dim))\n        initial_state = K.dot(initial_state, reducer)  # (samples, output_dim)\n        initial_states = [initial_state for _ in range(len(self.states))]\n        return initial_states\n\n    def get_output(self, train=False):\n        # input shape: (nb_samples, time (padded with zeros), input_dim)\n        X = self.get_input(train)\n        assert K.ndim(X) == 3\n        if K._BACKEND == 'tensorflow':\n            if not self.input_shape[1]:\n                raise Exception('When using TensorFlow, you should define ' +\n                                'explicitely the number of timesteps of ' +\n                                'your sequences. Make sure the first layer ' +\n                                'has a \"batch_input_shape\" argument ' +\n                                'including the samples axis.')\n\n        mask = self.get_output_mask(train)\n        if mask:\n            # apply mask\n            X *= K.expand_dims(mask)\n            masking = True\n        else:\n            masking = False\n\n        if self.stateful:\n            initial_states = self.states\n        else:\n            initial_states = self.get_initial_states(X)\n\n        last_output, outputs, states = K.rnn(self.step, X, initial_states,\n                                             go_backwards=self.go_backwards,\n                                             masking=masking)\n        if self.stateful:\n            for i in range(len(states)):\n                K.set_value(self.states[i], states[i])\n\n        if self.return_sequences:\n            return outputs\n        else:\n            return last_output\n\n    def get_config(self):\n        config = {\"name\": self.__class__.__name__,\n                  \"return_sequences\": self.return_sequences,\n                  \"input_dim\": self.input_dim,\n                  \"input_length\": self.input_length,\n                  \"go_backwards\": self.go_backwards,\n                  \"stateful\": self.stateful}\n        base_config = super(Recurrent, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\n\nclass SimpleRNN(Recurrent):\n    '''\n        Fully-connected RNN where the output is to fed back to input.\n        Takes inputs with shape:\n        (nb_samples, max_sample_length, input_dim)\n        (samples shorter than `max_sample_length`\n         are padded with zeros at the end)\n        and returns outputs with shape:\n        if not return_sequences:\n            (nb_samples, output_dim)\n        if return_sequences:\n            (nb_samples, max_sample_length, output_dim)\n    '''\n    def __init__(self, output_dim,\n                 init='glorot_uniform', inner_init='orthogonal',\n                 activation='sigmoid', **kwargs):\n        self.output_dim = output_dim\n        self.init = initializations.get(init)\n        self.inner_init = initializations.get(inner_init)\n        self.activation = activations.get(activation)\n        super(SimpleRNN, self).__init__(**kwargs)\n\n    def build(self):\n        input_shape = self.input_shape\n        if self.stateful:\n            if not input_shape[0]:\n                raise Exception('If a RNN is stateful, a complete ' +\n                                'input_shape must be provided ' +\n                                '(including batch size).')\n            self.states = [K.zeros((input_shape[0], self.output_dim))]\n        else:\n            # initial states: all-zero tensor of shape (output_dim)\n            self.states = [None]\n        input_dim = input_shape[2]\n        self.input_dim = input_dim\n\n        self.W = self.init((input_dim, self.output_dim))\n        self.U = self.inner_init((self.output_dim, self.output_dim))\n        self.b = K.zeros((self.output_dim))\n        self.params = [self.W, self.U, self.b]\n\n        if self.initial_weights is not None:\n            self.set_weights(self.initial_weights)\n            del self.initial_weights\n\n    def step(self, x, states):\n        # states only contains the previous output.\n        assert len(states) == 1\n        prev_output = states[0]\n        h = K.dot(x, self.W) + self.b\n        output = self.activation(h * K.dot(prev_output, self.U))\n        return output, [output]\n\n    def get_config(self):\n        config = {\"output_dim\": self.output_dim,\n                  \"init\": self.init.__name__,\n                  \"inner_init\": self.inner_init.__name__,\n                  \"activation\": self.activation.__name__}\n        base_config = super(SimpleRNN, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\n\nclass GRU(Recurrent):\n    '''\n        Gated Recurrent Unit - Cho et al. 2014\n        Acts as a spatiotemporal projection,\n        turning a sequence of vectors into a single vector.\n        Takes inputs with shape:\n        (nb_samples, max_sample_length, input_dim)\n        (samples shorter than `max_sample_length`\n         are padded with zeros at the end)\n        and returns outputs with shape:\n        if not return_sequences:\n            (nb_samples, output_dim)\n        if return_sequences:\n            (nb_samples, max_sample_length, output_dim)\n        References:\n            On the Properties of Neural Machine Translation:\n            Encoder\u2013Decoder Approaches\n                http://www.aclweb.org/anthology/W14-4012\n            Empirical Evaluation of Gated Recurrent Neural Networks\n            on Sequence Modeling\n                http://arxiv.org/pdf/1412.3555v1.pdf\n    '''\n    def __init__(self, output_dim,\n                 init='glorot_uniform', inner_init='orthogonal',\n                 activation='sigmoid', inner_activation='hard_sigmoid',\n                 **kwargs):\n        self.output_dim = output_dim\n        self.init = initializations.get(init)\n        self.inner_init = initializations.get(inner_init)\n        self.activation = activations.get(activation)\n        self.inner_activation = activations.get(inner_activation)\n        super(GRU, self).__init__(**kwargs)\n\n    def build(self):\n        input_shape = self.input_shape\n        input_dim = input_shape[2]\n        self.input_dim = input_dim\n        self.input = K.placeholder(input_shape)\n\n        self.W_z = self.init((input_dim, self.output_dim))\n        self.U_z = self.inner_init((self.output_dim, self.output_dim))\n        self.b_z = K.zeros((self.output_dim,))\n\n        self.W_r = self.init((input_dim, self.output_dim))\n        self.U_r = self.inner_init((self.output_dim, self.output_dim))\n        self.b_r = K.zeros((self.output_dim,))\n\n        self.W_h = self.init((input_dim, self.output_dim))\n        self.U_h = self.inner_init((self.output_dim, self.output_dim))\n        self.b_h = K.zeros((self.output_dim,))\n\n        self.params = [self.W_z, self.U_z, self.b_z,\n                       self.W_r, self.U_r, self.b_r,\n                       self.W_h, self.U_h, self.b_h]\n\n        if self.stateful:\n            if not input_shape[0]:\n                raise Exception('If a RNN is stateful, a complete ' +\n                                'input_shape must be provided ' +\n                                '(including batch size).')\n            self.states = [K.zeros((input_shape[0], self.output_dim))]\n        else:\n            # initial states: all-zero tensor of shape (output_dim)\n            self.states = [None]\n\n        if self.initial_weights is not None:\n            self.set_weights(self.initial_weights)\n            del self.initial_weights\n\n    def step(self, x, states):\n        assert len(states) == 1\n        x_z = K.dot(x, self.W_z) + self.b_z\n        x_r = K.dot(x, self.W_r) + self.b_r\n        x_h = K.dot(x, self.W_h) + self.b_h\n\n        h_tm1 = states[0]\n        z = self.inner_activation(x_z + K.dot(h_tm1, self.U_z))\n        r = self.inner_activation(x_r + K.dot(h_tm1, self.U_r))\n\n        hh = self.inner_activation(x_h + K.dot(r * h_tm1, self.U_h))\n        h = z * h_tm1 + (1 - z) * hh\n        return h, [h]\n\n    def get_config(self):\n        config = {\"output_dim\": self.output_dim,\n                  \"init\": self.init.__name__,\n                  \"inner_init\": self.inner_init.__name__,\n                  \"activation\": self.activation.__name__,\n                  \"inner_activation\": self.inner_activation.__name__}\n        base_config = super(GRU, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\n\nclass LSTM(Recurrent):\n    '''\n        Acts as a spatiotemporal projection,\n        turning a sequence of vectors into a single vector.\n        Takes inputs with shape:\n        (nb_samples, max_sample_length, input_dim)\n        (samples shorter than `max_sample_length`\n         are padded with zeros at the end)\n        and returns outputs with shape:\n        if not return_sequences:\n            (nb_samples, output_dim)\n        if return_sequences:\n            (nb_samples, max_sample_length, output_dim)\n        For a step-by-step description of the algorithm, see:\n        http://deeplearning.net/tutorial/lstm.html\n        References:\n            Long short-term memory (original 97 paper)\n                http://deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf\n            Learning to forget: Continual prediction with LSTM\n                http://www.mitpressjournals.org/doi/pdf/10.1162/089976600300015015\n            Supervised sequence labelling with recurrent neural networks\n                http://www.cs.toronto.edu/~graves/preprint.pdf\n    '''\n    def __init__(self, output_dim,\n                 init='glorot_uniform', inner_init='orthogonal',\n                 forget_bias_init='one', activation='tanh',\n                 inner_activation='hard_sigmoid', **kwargs):\n        self.output_dim = output_dim\n        self.init = initializations.get(init)\n        self.inner_init = initializations.get(inner_init)\n        self.forget_bias_init = initializations.get(forget_bias_init)\n        self.activation = activations.get(activation)\n        self.inner_activation = activations.get(inner_activation)\n        super(LSTM, self).__init__(**kwargs)\n\n    def build(self):\n        input_shape = self.input_shape\n        input_dim = input_shape[2]\n        self.input_dim = input_dim\n        self.input = K.placeholder(input_shape)\n\n        if self.stateful:\n            if not input_shape[0]:\n                raise Exception('If a RNN is stateful, a complete ' +\n                                'input_shape must be provided ' +\n                                '(including batch size).')\n            self.states = [K.zeros((input_shape[0], self.output_dim)),\n                           K.zeros((input_shape[0], self.output_dim))]\n        else:\n            # initial states: 2 all-zero tensor of shape (output_dim)\n            self.states = [None, None]\n\n        self.W_i = self.init((input_dim, self.output_dim))\n        self.U_i = self.inner_init((self.output_dim, self.output_dim))\n        self.b_i = K.zeros((self.output_dim))\n\n        self.W_f = self.init((input_dim, self.output_dim))\n        self.U_f = self.inner_init((self.output_dim, self.output_dim))\n        self.b_f = self.forget_bias_init((self.output_dim))\n\n        self.W_c = self.init((input_dim, self.output_dim))\n        self.U_c = self.inner_init((self.output_dim, self.output_dim))\n        self.b_c = K.zeros((self.output_dim))\n\n        self.W_o = self.init((input_dim, self.output_dim))\n        self.U_o = self.inner_init((self.output_dim, self.output_dim))\n        self.b_o = K.zeros((self.output_dim))\n\n        self.params = [self.W_i, self.U_i, self.b_i,\n                       self.W_c, self.U_c, self.b_c,\n                       self.W_f, self.U_f, self.b_f,\n                       self.W_o, self.U_o, self.b_o]\n\n        if self.initial_weights is not None:\n            self.set_weights(self.initial_weights)\n            del self.initial_weights\n\n    def step(self, x, states):\n        assert len(states) == 2\n        h_tm1 = states[0]\n        c_tm1 = states[1]\n\n        x_i = K.dot(x, self.W_i) + self.b_i\n        x_f = K.dot(x, self.W_f) + self.b_f\n        x_c = K.dot(x, self.W_c) + self.b_c\n        x_o = K.dot(x, self.W_o) + self.b_o\n\n        i = self.inner_activation(x_i + K.dot(h_tm1, self.U_i))\n        f = self.inner_activation(x_f + K.dot(h_tm1, self.U_f))\n        c = f * c_tm1 + i * self.activation(x_c + K.dot(h_tm1, self.U_c))\n        o = self.inner_activation(x_o + K.dot(h_tm1, self.U_o))\n        h = o * self.activation(c)\n        return h, [h, c]\n\n    def get_config(self):\n        config = {\"output_dim\": self.output_dim,\n                  \"init\": self.init.__name__,\n                  \"inner_init\": self.inner_init.__name__,\n                  \"forget_bias_init\": self.forget_bias_init.__name__,\n                  \"activation\": self.activation.__name__,\n                  \"inner_activation\": self.inner_activation.__name__}\n        base_config = super(LSTM, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n",
          "file_after": "# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import\n\nfrom .. import backend as K\nfrom .. import activations, initializations\nfrom ..layers.core import MaskedLayer\n\n\nclass Recurrent(MaskedLayer):\n    input_ndim = 3\n\n    def __init__(self, weights=None,\n                 return_sequences=False, go_backwards=False, stateful=False,\n                 input_dim=None, input_length=None, **kwargs):\n        self.return_sequences = return_sequences\n        self.initial_weights = weights\n        self.go_backwards = go_backwards\n        self.stateful = stateful\n\n        self.input_dim = input_dim\n        self.input_length = input_length\n        if self.input_dim:\n            kwargs['input_shape'] = (self.input_length, self.input_dim)\n        super(Recurrent, self).__init__(**kwargs)\n\n    def get_output_mask(self, train=False):\n        if self.return_sequences:\n            return super(Recurrent, self).get_output_mask(train)\n        else:\n            return None\n\n    @property\n    def output_shape(self):\n        input_shape = self.input_shape\n        if self.return_sequences:\n            return (input_shape[0], input_shape[1], self.output_dim)\n        else:\n            return (input_shape[0], self.output_dim)\n\n    def step(self, x, states):\n        raise NotImplementedError\n\n    def get_initial_states(self, X):\n        # build an all-zero tensor of shape (samples, output_dim)\n        initial_state = K.zeros_like(X)  # (samples, timesteps, input_dim)\n        initial_state = K.sum(initial_state, axis=1)  # (samples, input_dim)\n        reducer = K.zeros((self.input_dim, self.output_dim))\n        initial_state = K.dot(initial_state, reducer)  # (samples, output_dim)\n        initial_states = [initial_state for _ in range(len(self.states))]\n        return initial_states\n\n    def get_output(self, train=False):\n        # input shape: (nb_samples, time (padded with zeros), input_dim)\n        X = self.get_input(train)\n        assert K.ndim(X) == 3\n        if K._BACKEND == 'tensorflow':\n            if not self.input_shape[1]:\n                raise Exception('When using TensorFlow, you should define ' +\n                                'explicitely the number of timesteps of ' +\n                                'your sequences. Make sure the first layer ' +\n                                'has a \"batch_input_shape\" argument ' +\n                                'including the samples axis.')\n\n        mask = self.get_output_mask(train)\n        if mask:\n            # apply mask\n            X *= K.expand_dims(mask)\n            masking = True\n        else:\n            masking = False\n\n        if self.stateful:\n            initial_states = self.states\n        else:\n            initial_states = self.get_initial_states(X)\n\n        last_output, outputs, states = K.rnn(self.step, X, initial_states,\n                                             go_backwards=self.go_backwards,\n                                             masking=masking)\n        if self.stateful:\n            for i in range(len(states)):\n                K.set_value(self.states[i], K.eval(states[i]))\n\n        if self.return_sequences:\n            return outputs\n        else:\n            return last_output\n\n    def get_config(self):\n        config = {\"name\": self.__class__.__name__,\n                  \"return_sequences\": self.return_sequences,\n                  \"input_dim\": self.input_dim,\n                  \"input_length\": self.input_length,\n                  \"go_backwards\": self.go_backwards,\n                  \"stateful\": self.stateful}\n        base_config = super(Recurrent, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\n\nclass SimpleRNN(Recurrent):\n    '''\n        Fully-connected RNN where the output is to fed back to input.\n        Takes inputs with shape:\n        (nb_samples, max_sample_length, input_dim)\n        (samples shorter than `max_sample_length`\n         are padded with zeros at the end)\n        and returns outputs with shape:\n        if not return_sequences:\n            (nb_samples, output_dim)\n        if return_sequences:\n            (nb_samples, max_sample_length, output_dim)\n    '''\n    def __init__(self, output_dim,\n                 init='glorot_uniform', inner_init='orthogonal',\n                 activation='sigmoid', **kwargs):\n        self.output_dim = output_dim\n        self.init = initializations.get(init)\n        self.inner_init = initializations.get(inner_init)\n        self.activation = activations.get(activation)\n        super(SimpleRNN, self).__init__(**kwargs)\n\n    def build(self):\n        input_shape = self.input_shape\n        if self.stateful:\n            self.reset_states()\n        else:\n            # initial states: all-zero tensor of shape (output_dim)\n            self.states = [None]\n        input_dim = input_shape[2]\n        self.input_dim = input_dim\n\n        self.W = self.init((input_dim, self.output_dim))\n        self.U = self.inner_init((self.output_dim, self.output_dim))\n        self.b = K.zeros((self.output_dim))\n        self.params = [self.W, self.U, self.b]\n\n        if self.initial_weights is not None:\n            self.set_weights(self.initial_weights)\n            del self.initial_weights\n\n    def reset_states(self):\n        assert self.stateful, 'Layer must be stateful.'\n        input_shape = self.input_shape\n        if not input_shape[0]:\n            raise Exception('If a RNN is stateful, a complete ' +\n                            'input_shape must be provided ' +\n                            '(including batch size).')\n        self.states = [K.zeros((input_shape[0], self.output_dim))]\n\n    def step(self, x, states):\n        # states only contains the previous output.\n        assert len(states) == 1\n        prev_output = states[0]\n        h = K.dot(x, self.W) + self.b\n        output = self.activation(h * K.dot(prev_output, self.U))\n        return output, [output]\n\n    def get_config(self):\n        config = {\"output_dim\": self.output_dim,\n                  \"init\": self.init.__name__,\n                  \"inner_init\": self.inner_init.__name__,\n                  \"activation\": self.activation.__name__}\n        base_config = super(SimpleRNN, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\n\nclass GRU(Recurrent):\n    '''\n        Gated Recurrent Unit - Cho et al. 2014\n        Acts as a spatiotemporal projection,\n        turning a sequence of vectors into a single vector.\n        Takes inputs with shape:\n        (nb_samples, max_sample_length, input_dim)\n        (samples shorter than `max_sample_length`\n         are padded with zeros at the end)\n        and returns outputs with shape:\n        if not return_sequences:\n            (nb_samples, output_dim)\n        if return_sequences:\n            (nb_samples, max_sample_length, output_dim)\n        References:\n            On the Properties of Neural Machine Translation:\n            Encoder\u2013Decoder Approaches\n                http://www.aclweb.org/anthology/W14-4012\n            Empirical Evaluation of Gated Recurrent Neural Networks\n            on Sequence Modeling\n                http://arxiv.org/pdf/1412.3555v1.pdf\n    '''\n    def __init__(self, output_dim,\n                 init='glorot_uniform', inner_init='orthogonal',\n                 activation='sigmoid', inner_activation='hard_sigmoid',\n                 **kwargs):\n        self.output_dim = output_dim\n        self.init = initializations.get(init)\n        self.inner_init = initializations.get(inner_init)\n        self.activation = activations.get(activation)\n        self.inner_activation = activations.get(inner_activation)\n        super(GRU, self).__init__(**kwargs)\n\n    def build(self):\n        input_shape = self.input_shape\n        input_dim = input_shape[2]\n        self.input_dim = input_dim\n        self.input = K.placeholder(input_shape)\n\n        self.W_z = self.init((input_dim, self.output_dim))\n        self.U_z = self.inner_init((self.output_dim, self.output_dim))\n        self.b_z = K.zeros((self.output_dim,))\n\n        self.W_r = self.init((input_dim, self.output_dim))\n        self.U_r = self.inner_init((self.output_dim, self.output_dim))\n        self.b_r = K.zeros((self.output_dim,))\n\n        self.W_h = self.init((input_dim, self.output_dim))\n        self.U_h = self.inner_init((self.output_dim, self.output_dim))\n        self.b_h = K.zeros((self.output_dim,))\n\n        self.params = [self.W_z, self.U_z, self.b_z,\n                       self.W_r, self.U_r, self.b_r,\n                       self.W_h, self.U_h, self.b_h]\n\n        if self.stateful:\n            self.reset_states()\n        else:\n            # initial states: all-zero tensor of shape (output_dim)\n            self.states = [None]\n\n        if self.initial_weights is not None:\n            self.set_weights(self.initial_weights)\n            del self.initial_weights\n\n    def reset_states(self):\n        assert self.stateful, 'Layer must be stateful.'\n        input_shape = self.input_shape\n        if not input_shape[0]:\n            raise Exception('If a RNN is stateful, a complete ' +\n                            'input_shape must be provided ' +\n                            '(including batch size).')\n        self.states = [K.zeros((input_shape[0], self.output_dim))]\n\n    def step(self, x, states):\n        assert len(states) == 1\n        x_z = K.dot(x, self.W_z) + self.b_z\n        x_r = K.dot(x, self.W_r) + self.b_r\n        x_h = K.dot(x, self.W_h) + self.b_h\n\n        h_tm1 = states[0]\n        z = self.inner_activation(x_z + K.dot(h_tm1, self.U_z))\n        r = self.inner_activation(x_r + K.dot(h_tm1, self.U_r))\n\n        hh = self.inner_activation(x_h + K.dot(r * h_tm1, self.U_h))\n        h = z * h_tm1 + (1 - z) * hh\n        return h, [h]\n\n    def get_config(self):\n        config = {\"output_dim\": self.output_dim,\n                  \"init\": self.init.__name__,\n                  \"inner_init\": self.inner_init.__name__,\n                  \"activation\": self.activation.__name__,\n                  \"inner_activation\": self.inner_activation.__name__}\n        base_config = super(GRU, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\n\nclass LSTM(Recurrent):\n    '''\n        Acts as a spatiotemporal projection,\n        turning a sequence of vectors into a single vector.\n        Takes inputs with shape:\n        (nb_samples, max_sample_length, input_dim)\n        (samples shorter than `max_sample_length`\n         are padded with zeros at the end)\n        and returns outputs with shape:\n        if not return_sequences:\n            (nb_samples, output_dim)\n        if return_sequences:\n            (nb_samples, max_sample_length, output_dim)\n        For a step-by-step description of the algorithm, see:\n        http://deeplearning.net/tutorial/lstm.html\n        References:\n            Long short-term memory (original 97 paper)\n                http://deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf\n            Learning to forget: Continual prediction with LSTM\n                http://www.mitpressjournals.org/doi/pdf/10.1162/089976600300015015\n            Supervised sequence labelling with recurrent neural networks\n                http://www.cs.toronto.edu/~graves/preprint.pdf\n    '''\n    def __init__(self, output_dim,\n                 init='glorot_uniform', inner_init='orthogonal',\n                 forget_bias_init='one', activation='tanh',\n                 inner_activation='hard_sigmoid', **kwargs):\n        self.output_dim = output_dim\n        self.init = initializations.get(init)\n        self.inner_init = initializations.get(inner_init)\n        self.forget_bias_init = initializations.get(forget_bias_init)\n        self.activation = activations.get(activation)\n        self.inner_activation = activations.get(inner_activation)\n        super(LSTM, self).__init__(**kwargs)\n\n    def build(self):\n        input_shape = self.input_shape\n        input_dim = input_shape[2]\n        self.input_dim = input_dim\n        self.input = K.placeholder(input_shape)\n\n        if self.stateful:\n            self.reset_states()\n        else:\n            # initial states: 2 all-zero tensor of shape (output_dim)\n            self.states = [None, None]\n\n        self.W_i = self.init((input_dim, self.output_dim))\n        self.U_i = self.inner_init((self.output_dim, self.output_dim))\n        self.b_i = K.zeros((self.output_dim))\n\n        self.W_f = self.init((input_dim, self.output_dim))\n        self.U_f = self.inner_init((self.output_dim, self.output_dim))\n        self.b_f = self.forget_bias_init((self.output_dim))\n\n        self.W_c = self.init((input_dim, self.output_dim))\n        self.U_c = self.inner_init((self.output_dim, self.output_dim))\n        self.b_c = K.zeros((self.output_dim))\n\n        self.W_o = self.init((input_dim, self.output_dim))\n        self.U_o = self.inner_init((self.output_dim, self.output_dim))\n        self.b_o = K.zeros((self.output_dim))\n\n        self.params = [self.W_i, self.U_i, self.b_i,\n                       self.W_c, self.U_c, self.b_c,\n                       self.W_f, self.U_f, self.b_f,\n                       self.W_o, self.U_o, self.b_o]\n\n        if self.initial_weights is not None:\n            self.set_weights(self.initial_weights)\n            del self.initial_weights\n\n    def reset_states(self):\n        assert self.stateful, 'Layer must be stateful.'\n        input_shape = self.input_shape\n        if not input_shape[0]:\n            raise Exception('If a RNN is stateful, a complete ' +\n                            'input_shape must be provided ' +\n                            '(including batch size).')\n        self.states = [K.zeros((input_shape[0], self.output_dim)),\n                       K.zeros((input_shape[0], self.output_dim))]\n\n    def step(self, x, states):\n        assert len(states) == 2\n        h_tm1 = states[0]\n        c_tm1 = states[1]\n\n        x_i = K.dot(x, self.W_i) + self.b_i\n        x_f = K.dot(x, self.W_f) + self.b_f\n        x_c = K.dot(x, self.W_c) + self.b_c\n        x_o = K.dot(x, self.W_o) + self.b_o\n\n        i = self.inner_activation(x_i + K.dot(h_tm1, self.U_i))\n        f = self.inner_activation(x_f + K.dot(h_tm1, self.U_f))\n        c = f * c_tm1 + i * self.activation(x_c + K.dot(h_tm1, self.U_c))\n        o = self.inner_activation(x_o + K.dot(h_tm1, self.U_o))\n        h = o * self.activation(c)\n        return h, [h, c]\n\n    def get_config(self):\n        config = {\"output_dim\": self.output_dim,\n                  \"init\": self.init.__name__,\n                  \"inner_init\": self.inner_init.__name__,\n                  \"forget_bias_init\": self.forget_bias_init.__name__,\n                  \"activation\": self.activation.__name__,\n                  \"inner_activation\": self.inner_activation.__name__}\n        base_config = super(LSTM, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n",
          "file_patch": "@@ -1,6 +1,5 @@\n # -*- coding: utf-8 -*-\n from __future__ import absolute_import\n-import numpy as np\n \n from .. import backend as K\n from .. import activations, initializations\n@@ -80,7 +79,7 @@ class Recurrent(MaskedLayer):\n                                              masking=masking)\n         if self.stateful:\n             for i in range(len(states)):\n-                K.set_value(self.states[i], states[i])\n+                K.set_value(self.states[i], K.eval(states[i]))\n \n         if self.return_sequences:\n             return outputs\n@@ -123,11 +122,7 @@ class SimpleRNN(Recurrent):\n     def build(self):\n         input_shape = self.input_shape\n         if self.stateful:\n-            if not input_shape[0]:\n-                raise Exception('If a RNN is stateful, a complete ' +\n-                                'input_shape must be provided ' +\n-                                '(including batch size).')\n-            self.states = [K.zeros((input_shape[0], self.output_dim))]\n+            self.reset_states()\n         else:\n             # initial states: all-zero tensor of shape (output_dim)\n             self.states = [None]\n@@ -143,6 +138,15 @@ class SimpleRNN(Recurrent):\n             self.set_weights(self.initial_weights)\n             del self.initial_weights\n \n+    def reset_states(self):\n+        assert self.stateful, 'Layer must be stateful.'\n+        input_shape = self.input_shape\n+        if not input_shape[0]:\n+            raise Exception('If a RNN is stateful, a complete ' +\n+                            'input_shape must be provided ' +\n+                            '(including batch size).')\n+        self.states = [K.zeros((input_shape[0], self.output_dim))]\n+\n     def step(self, x, states):\n         # states only contains the previous output.\n         assert len(states) == 1\n@@ -216,11 +220,7 @@ class GRU(Recurrent):\n                        self.W_h, self.U_h, self.b_h]\n \n         if self.stateful:\n-            if not input_shape[0]:\n-                raise Exception('If a RNN is stateful, a complete ' +\n-                                'input_shape must be provided ' +\n-                                '(including batch size).')\n-            self.states = [K.zeros((input_shape[0], self.output_dim))]\n+            self.reset_states()\n         else:\n             # initial states: all-zero tensor of shape (output_dim)\n             self.states = [None]\n@@ -229,6 +229,15 @@ class GRU(Recurrent):\n             self.set_weights(self.initial_weights)\n             del self.initial_weights\n \n+    def reset_states(self):\n+        assert self.stateful, 'Layer must be stateful.'\n+        input_shape = self.input_shape\n+        if not input_shape[0]:\n+            raise Exception('If a RNN is stateful, a complete ' +\n+                            'input_shape must be provided ' +\n+                            '(including batch size).')\n+        self.states = [K.zeros((input_shape[0], self.output_dim))]\n+\n     def step(self, x, states):\n         assert len(states) == 1\n         x_z = K.dot(x, self.W_z) + self.b_z\n@@ -295,12 +304,7 @@ class LSTM(Recurrent):\n         self.input = K.placeholder(input_shape)\n \n         if self.stateful:\n-            if not input_shape[0]:\n-                raise Exception('If a RNN is stateful, a complete ' +\n-                                'input_shape must be provided ' +\n-                                '(including batch size).')\n-            self.states = [K.zeros((input_shape[0], self.output_dim)),\n-                           K.zeros((input_shape[0], self.output_dim))]\n+            self.reset_states()\n         else:\n             # initial states: 2 all-zero tensor of shape (output_dim)\n             self.states = [None, None]\n@@ -330,6 +334,16 @@ class LSTM(Recurrent):\n             self.set_weights(self.initial_weights)\n             del self.initial_weights\n \n+    def reset_states(self):\n+        assert self.stateful, 'Layer must be stateful.'\n+        input_shape = self.input_shape\n+        if not input_shape[0]:\n+            raise Exception('If a RNN is stateful, a complete ' +\n+                            'input_shape must be provided ' +\n+                            '(including batch size).')\n+        self.states = [K.zeros((input_shape[0], self.output_dim)),\n+                       K.zeros((input_shape[0], self.output_dim))]\n+\n     def step(self, x, states):\n         assert len(states) == 2\n         h_tm1 = states[0]\n",
          "files_name_in_blame_commit": [
            "recurrent.py",
            "test_recurrent.py"
          ]
        }
      },
      "47ed18a3af8be20dce91286a331d4671074ee0ca": {
        "commit": {
          "commit_id": "47ed18a3af8be20dce91286a331d4671074ee0ca",
          "commit_message": "Update backends with rnn support",
          "commit_author": "Francois Chollet",
          "commit_date": "2015-11-26 10:42:52",
          "commit_parent": "37ebbc3a1cf1d284d9ef2afefb405884effb6928"
        },
        "function": {
          "function_name": "build",
          "function_code_before": "def build(self):\n    input_dim = self.input_shape[2]\n    self.input = T.tensor3()\n    self.W = self.init((input_dim, self.output_dim))\n    self.U = self.inner_init((self.output_dim, self.output_dim))\n    self.b = shared_zeros(self.output_dim)\n    self.params = [self.W, self.U, self.b]\n    if self.initial_weights is not None:\n        self.set_weights(self.initial_weights)\n        del self.initial_weights",
          "function_code_after": "def build(self):\n    input_shape = self.input_shape\n    if self.stateful:\n        if not input_shape[0]:\n            raise Exception('If a RNN is stateful, a complete ' + 'input_shape must be provided ' + '(including batch size).')\n        self.states = [K.zeros(input_shape[0], self.output_dim)]\n    else:\n        self.states = [None]\n    input_dim = input_shape[2]\n    self.input_dim = input_dim\n    self.W = self.init((input_dim, self.output_dim))\n    self.U = self.inner_init((self.output_dim, self.output_dim))\n    self.b = K.zeros(self.output_dim)\n    self.params = [self.W, self.U, self.b]\n    if self.initial_weights is not None:\n        self.set_weights(self.initial_weights)\n        del self.initial_weights",
          "function_before_start_line": 76,
          "function_before_end_line": 87,
          "function_after_start_line": 114,
          "function_after_end_line": 135,
          "function_before_token_count": 105,
          "function_after_token_count": 155,
          "functions_name_modified_file": [
            "__init__",
            "output_shape",
            "build",
            "get_output_mask",
            "step",
            "get_output",
            "get_config"
          ],
          "functions_name_all_files": [
            "maxpool2d",
            "test_lstm",
            "get_params",
            "dropout",
            "tanh",
            "_get_hidden",
            "step",
            "get_weights",
            "rnn",
            "eval",
            "ones",
            "check_single_tensor_operation",
            "equal",
            "hard_sigmoid",
            "_get_session",
            "__init__",
            "sqrt",
            "spatial_2d_padding",
            "test_rnn",
            "shape",
            "expand_dims",
            "categorical_crossentropy",
            "conv2d",
            "test_elementwise_operations",
            "test_gru",
            "test_shape_operations",
            "cast",
            "test_gradient",
            "concatenate",
            "reshape",
            "test_switch",
            "get_output",
            "get_config",
            "get_output_mask",
            "softplus",
            "test_random_normal",
            "test_random_uniform",
            "std",
            "build",
            "permute_dimensions",
            "test_nn_operations",
            "ones_like",
            "output_shape",
            "sigmoid",
            "prod",
            "test_function",
            "random_uniform",
            "check_two_tensor_operation",
            "log",
            "switch",
            "relu",
            "count_params",
            "any",
            "_runner",
            "get_input",
            "function",
            "set_input_shape",
            "set_weights",
            "random_normal",
            "zeros_like",
            "test_linear_operations",
            "input",
            "binary_crossentropy",
            "supports_masked_input",
            "test_simple",
            "softmax",
            "square",
            "nb_input",
            "squeeze",
            "input_shape",
            "__call__",
            "placeholder",
            "abs",
            "mean",
            "clip",
            "trainable",
            "dot",
            "variable",
            "exp",
            "get_input_mask",
            "max",
            "sum",
            "minimum",
            "argmax",
            "zeros",
            "set_value",
            "tile",
            "flatten",
            "_set_session",
            "pow",
            "set_previous",
            "argmin",
            "test_value_manipulation",
            "round",
            "temporal_padding",
            "repeat",
            "nb_output",
            "get_value",
            "transpose",
            "ndim",
            "gradients",
            "min",
            "_on_gpu",
            "gather",
            "maximum"
          ],
          "functions_name_co_evolved_modified_file": [
            "__init__",
            "get_padded_shuffled_mask",
            "get_output_mask",
            "step",
            "_step",
            "get_output",
            "get_config"
          ],
          "functions_name_co_evolved_all_files": [
            "__init__",
            "get_padded_shuffled_mask",
            "test_rnn",
            "get_output_mask",
            "get_output",
            "step",
            "test_simple_deep",
            "conv2d",
            "_runner",
            "test_jzs2",
            "test_jzs1",
            "rnn",
            "test_jzs3",
            "_step",
            "get_config"
          ]
        },
        "file": {
          "file_name": "recurrent.py",
          "file_nloc": 432,
          "file_complexity": 49,
          "file_token_count": 3310,
          "file_before": "# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import\nimport theano\nimport theano.tensor as T\nimport numpy as np\n\nfrom .. import activations, initializations\nfrom ..utils.theano_utils import shared_scalar, shared_zeros, alloc_zeros_matrix\nfrom ..layers.core import Layer, MaskedLayer\nfrom six.moves import range\n\n\nclass Recurrent(MaskedLayer):\n    input_ndim = 3\n\n    def get_output_mask(self, train=None):\n        if self.return_sequences:\n            return super(Recurrent, self).get_output_mask(train)\n        else:\n            return None\n\n    def get_padded_shuffled_mask(self, train, X, pad=0):\n        mask = self.get_input_mask(train)\n        if mask is None:\n            mask = T.ones_like(X.sum(axis=-1))  # is there a better way to do this without a sum?\n\n        # TODO: reimplement\n        # mask is (nb_samples, time)\n        mask = T.shape_padright(mask)  # (nb_samples, time, 1)\n        mask = T.addbroadcast(mask, -1)  # the new dimension (the '1') is made broadcastable\n        # see http://deeplearning.net/software/theano/library/tensor/basic.html#broadcasting-in-theano-vs-numpy\n        mask = mask.dimshuffle(1, 0, 2)  # (time, nb_samples, 1)\n\n        if pad > 0:\n            # left-pad in time with 0\n            padding = alloc_zeros_matrix(pad, mask.shape[1], 1)\n            mask = T.concatenate([padding, mask], axis=0)\n        return mask.astype('int8')\n\n    @property\n    def output_shape(self):\n        input_shape = self.input_shape\n        if self.return_sequences:\n            return (input_shape[0], input_shape[1], self.output_dim)\n        else:\n            return (input_shape[0], self.output_dim)\n\n\nclass SimpleRNN(Recurrent):\n    '''\n        Fully connected RNN where output is to fed back to input.\n\n        Not a particularly useful model,\n        included for demonstration purposes\n        (demonstrates how to use theano.scan to build a basic RNN).\n    '''\n    def __init__(self, output_dim,\n                 init='glorot_uniform', inner_init='orthogonal', activation='sigmoid', weights=None,\n                 truncate_gradient=-1, return_sequences=False, input_dim=None,\n                 input_length=None, go_backwards=False, **kwargs):\n        self.output_dim = output_dim\n        self.init = initializations.get(init)\n        self.inner_init = initializations.get(inner_init)\n        self.truncate_gradient = truncate_gradient\n        self.activation = activations.get(activation)\n        self.return_sequences = return_sequences\n        self.initial_weights = weights\n        self.go_backwards = go_backwards\n\n        self.input_dim = input_dim\n        self.input_length = input_length\n        if self.input_dim:\n            kwargs['input_shape'] = (self.input_length, self.input_dim)\n        super(SimpleRNN, self).__init__(**kwargs)\n\n    def build(self):\n        input_dim = self.input_shape[2]\n        self.input = T.tensor3()\n\n        self.W = self.init((input_dim, self.output_dim))\n        self.U = self.inner_init((self.output_dim, self.output_dim))\n        self.b = shared_zeros((self.output_dim))\n        self.params = [self.W, self.U, self.b]\n\n        if self.initial_weights is not None:\n            self.set_weights(self.initial_weights)\n            del self.initial_weights\n\n    def _step(self, x_t, mask_tm1, h_tm1, u):\n        '''\n            Variable names follow the conventions from:\n            http://deeplearning.net/software/theano/library/scan.html\n\n        '''\n        return self.activation(x_t + mask_tm1 * T.dot(h_tm1, u))\n\n    def get_output(self, train=False):\n        X = self.get_input(train)  # shape: (nb_samples, time (padded with zeros), input_dim)\n        # new shape: (time, nb_samples, input_dim) -> because theano.scan iterates over main dimension\n        padded_mask = self.get_padded_shuffled_mask(train, X, pad=1)\n        X = X.dimshuffle((1, 0, 2))\n        x = T.dot(X, self.W) + self.b\n\n        # scan = theano symbolic loop.\n        # See: http://deeplearning.net/software/theano/library/scan.html\n        # Iterate over the first dimension of the x array (=time).\n        outputs, updates = theano.scan(\n            self._step,  # this will be called with arguments (sequences[i], outputs[i-1], non_sequences[i])\n            sequences=[x, dict(input=padded_mask, taps=[-1])],  # tensors to iterate over, inputs to _step\n            # initialization of the output. Input to _step with default tap=-1.\n            outputs_info=T.unbroadcast(alloc_zeros_matrix(X.shape[1], self.output_dim), 1),\n            non_sequences=self.U,  # static inputs to _step\n            truncate_gradient=self.truncate_gradient,\n            go_backwards=self.go_backwards)\n\n        if self.return_sequences:\n            return outputs.dimshuffle((1, 0, 2))\n        return outputs[-1]\n\n    def get_config(self):\n        config = {\"name\": self.__class__.__name__,\n                  \"output_dim\": self.output_dim,\n                  \"init\": self.init.__name__,\n                  \"inner_init\": self.inner_init.__name__,\n                  \"activation\": self.activation.__name__,\n                  \"truncate_gradient\": self.truncate_gradient,\n                  \"return_sequences\": self.return_sequences,\n                  \"input_dim\": self.input_dim,\n                  \"input_length\": self.input_length,\n                  \"go_backwards\": self.go_backwards}\n        base_config = super(SimpleRNN, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\n\nclass SimpleDeepRNN(Recurrent):\n    '''\n        Fully connected RNN where the output of multiple timesteps\n        (up to \"depth\" steps in the past) is fed back to the input:\n\n        output = activation( W.x_t + b + inner_activation(U_1.h_tm1) + inner_activation(U_2.h_tm2) + ... )\n\n        This demonstrates how to build RNNs with arbitrary lookback.\n        Also (probably) not a super useful model.\n    '''\n    def __init__(self, output_dim, depth=3,\n                 init='glorot_uniform', inner_init='orthogonal',\n                 activation='sigmoid', inner_activation='hard_sigmoid',\n                 weights=None, truncate_gradient=-1, return_sequences=False,\n                 input_dim=None, input_length=None, go_backwards=False, **kwargs):\n        self.output_dim = output_dim\n        self.init = initializations.get(init)\n        self.inner_init = initializations.get(inner_init)\n        self.truncate_gradient = truncate_gradient\n        self.activation = activations.get(activation)\n        self.inner_activation = activations.get(inner_activation)\n        self.depth = depth\n        self.return_sequences = return_sequences\n        self.initial_weights = weights\n        self.go_backwards = go_backwards\n\n        self.input_dim = input_dim\n        self.input_length = input_length\n        if self.input_dim:\n            kwargs['input_shape'] = (self.input_length, self.input_dim)\n        super(SimpleDeepRNN, self).__init__(**kwargs)\n\n    def build(self):\n        input_dim = self.input_shape[2]\n        self.input = T.tensor3()\n        self.W = self.init((input_dim, self.output_dim))\n        self.Us = [self.inner_init((self.output_dim, self.output_dim)) for _ in range(self.depth)]\n        self.b = shared_zeros((self.output_dim))\n        self.params = [self.W] + self.Us + [self.b]\n\n        if self.initial_weights is not None:\n            self.set_weights(self.initial_weights)\n            del self.initial_weights\n\n    def _step(self, x_t, *args):\n        o = x_t\n        for i in range(self.depth):\n            mask_tmi = args[i]\n            h_tmi = args[i + self.depth]\n            U_tmi = args[i + 2*self.depth]\n            o += mask_tmi*self.inner_activation(T.dot(h_tmi, U_tmi))\n        return self.activation(o)\n\n    def get_output(self, train=False):\n        X = self.get_input(train)\n        padded_mask = self.get_padded_shuffled_mask(train, X, pad=self.depth)\n        X = X.dimshuffle((1, 0, 2))\n\n        x = T.dot(X, self.W) + self.b\n\n        if self.depth == 1:\n            initial = T.unbroadcast(alloc_zeros_matrix(X.shape[1], self.output_dim), 1)\n        else:\n            initial = T.unbroadcast(T.unbroadcast(alloc_zeros_matrix(self.depth, X.shape[1], self.output_dim), 0), 2)\n\n        outputs, updates = theano.scan(\n            self._step,\n            sequences=[x, dict(\n                input=padded_mask,\n                taps=[(-i) for i in range(self.depth)]\n            )],\n            outputs_info=[dict(\n                initial=initial,\n                taps=[(-i-1) for i in range(self.depth)]\n            )],\n            non_sequences=self.Us,\n            truncate_gradient=self.truncate_gradient,\n            go_backwards=self.go_backwards)\n\n        if self.return_sequences:\n            return outputs.dimshuffle((1, 0, 2))\n        return outputs[-1]\n\n    def get_config(self):\n        config = {\"name\": self.__class__.__name__,\n                  \"output_dim\": self.output_dim,\n                  \"depth\": self.depth,\n                  \"init\": self.init.__name__,\n                  \"inner_init\": self.inner_init.__name__,\n                  \"activation\": self.activation.__name__,\n                  \"inner_activation\": self.inner_activation.__name__,\n                  \"truncate_gradient\": self.truncate_gradient,\n                  \"return_sequences\": self.return_sequences,\n                  \"input_dim\": self.input_dim,\n                  \"input_length\": self.input_length,\n                  \"go_backwards\": self.go_backwards}\n        base_config = super(SimpleDeepRNN, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\n\nclass GRU(Recurrent):\n    '''\n        Gated Recurrent Unit - Cho et al. 2014\n\n        Acts as a spatiotemporal projection,\n        turning a sequence of vectors into a single vector.\n\n        Eats inputs with shape:\n        (nb_samples, max_sample_length (samples shorter than this are padded with zeros at the end), input_dim)\n\n        and returns outputs with shape:\n        if not return_sequences:\n            (nb_samples, output_dim)\n        if return_sequences:\n            (nb_samples, max_sample_length, output_dim)\n\n        References:\n            On the Properties of Neural Machine Translation: Encoder\u2013Decoder Approaches\n                http://www.aclweb.org/anthology/W14-4012\n            Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling\n                http://arxiv.org/pdf/1412.3555v1.pdf\n    '''\n    def __init__(self, output_dim,\n                 init='glorot_uniform', inner_init='orthogonal',\n                 activation='sigmoid', inner_activation='hard_sigmoid',\n                 weights=None, truncate_gradient=-1, return_sequences=False,\n                 input_dim=None, input_length=None, go_backwards=False, **kwargs):\n        self.output_dim = output_dim\n        self.init = initializations.get(init)\n        self.inner_init = initializations.get(inner_init)\n        self.activation = activations.get(activation)\n        self.inner_activation = activations.get(inner_activation)\n        self.truncate_gradient = truncate_gradient\n        self.return_sequences = return_sequences\n        self.initial_weights = weights\n        self.go_backwards = go_backwards\n\n        self.input_dim = input_dim\n        self.input_length = input_length\n        if self.input_dim:\n            kwargs['input_shape'] = (self.input_length, self.input_dim)\n        super(GRU, self).__init__(**kwargs)\n\n    def build(self):\n        input_dim = self.input_shape[2]\n        self.input = T.tensor3()\n\n        self.W_z = self.init((input_dim, self.output_dim))\n        self.U_z = self.inner_init((self.output_dim, self.output_dim))\n        self.b_z = shared_zeros((self.output_dim))\n\n        self.W_r = self.init((input_dim, self.output_dim))\n        self.U_r = self.inner_init((self.output_dim, self.output_dim))\n        self.b_r = shared_zeros((self.output_dim))\n\n        self.W_h = self.init((input_dim, self.output_dim))\n        self.U_h = self.inner_init((self.output_dim, self.output_dim))\n        self.b_h = shared_zeros((self.output_dim))\n\n        self.params = [\n            self.W_z, self.U_z, self.b_z,\n            self.W_r, self.U_r, self.b_r,\n            self.W_h, self.U_h, self.b_h,\n        ]\n\n        if self.initial_weights is not None:\n            self.set_weights(self.initial_weights)\n            del self.initial_weights\n\n    def _step(self,\n              xz_t, xr_t, xh_t, mask_tm1,\n              h_tm1,\n              u_z, u_r, u_h):\n        h_mask_tm1 = mask_tm1 * h_tm1\n        z = self.inner_activation(xz_t + T.dot(h_mask_tm1, u_z))\n        r = self.inner_activation(xr_t + T.dot(h_mask_tm1, u_r))\n        hh_t = self.activation(xh_t + T.dot(r * h_mask_tm1, u_h))\n        h_t = z * h_mask_tm1 + (1 - z) * hh_t\n        return h_t\n\n    def get_output(self, train=False):\n        X = self.get_input(train)\n        padded_mask = self.get_padded_shuffled_mask(train, X, pad=1)\n        X = X.dimshuffle((1, 0, 2))\n\n        x_z = T.dot(X, self.W_z) + self.b_z\n        x_r = T.dot(X, self.W_r) + self.b_r\n        x_h = T.dot(X, self.W_h) + self.b_h\n        outputs, updates = theano.scan(\n            self._step,\n            sequences=[x_z, x_r, x_h, padded_mask],\n            outputs_info=T.unbroadcast(alloc_zeros_matrix(X.shape[1], self.output_dim), 1),\n            non_sequences=[self.U_z, self.U_r, self.U_h],\n            truncate_gradient=self.truncate_gradient,\n            go_backwards=self.go_backwards)\n\n        if self.return_sequences:\n            return outputs.dimshuffle((1, 0, 2))\n        return outputs[-1]\n\n    def get_config(self):\n        config = {\"name\": self.__class__.__name__,\n                  \"output_dim\": self.output_dim,\n                  \"init\": self.init.__name__,\n                  \"inner_init\": self.inner_init.__name__,\n                  \"activation\": self.activation.__name__,\n                  \"inner_activation\": self.inner_activation.__name__,\n                  \"truncate_gradient\": self.truncate_gradient,\n                  \"return_sequences\": self.return_sequences,\n                  \"input_dim\": self.input_dim,\n                  \"input_length\": self.input_length,\n                  \"go_backwards\": self.go_backwards}\n        base_config = super(GRU, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\n\nclass LSTM(Recurrent):\n    '''\n        Acts as a spatiotemporal projection,\n        turning a sequence of vectors into a single vector.\n\n        Eats inputs with shape:\n        (nb_samples, max_sample_length (samples shorter than this are padded with zeros at the end), input_dim)\n\n        and returns outputs with shape:\n        if not return_sequences:\n            (nb_samples, output_dim)\n        if return_sequences:\n            (nb_samples, max_sample_length, output_dim)\n\n        For a step-by-step description of the algorithm, see:\n        http://deeplearning.net/tutorial/lstm.html\n\n        References:\n            Long short-term memory (original 97 paper)\n                http://deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf\n            Learning to forget: Continual prediction with LSTM\n                http://www.mitpressjournals.org/doi/pdf/10.1162/089976600300015015\n            Supervised sequence labelling with recurrent neural networks\n                http://www.cs.toronto.edu/~graves/preprint.pdf\n    '''\n    def __init__(self, output_dim,\n                 init='glorot_uniform', inner_init='orthogonal', forget_bias_init='one',\n                 activation='tanh', inner_activation='hard_sigmoid',\n                 weights=None, truncate_gradient=-1, return_sequences=False,\n                 input_dim=None, input_length=None, go_backwards=False, **kwargs):\n        self.output_dim = output_dim\n        self.init = initializations.get(init)\n        self.inner_init = initializations.get(inner_init)\n        self.forget_bias_init = initializations.get(forget_bias_init)\n        self.activation = activations.get(activation)\n        self.inner_activation = activations.get(inner_activation)\n        self.truncate_gradient = truncate_gradient\n        self.return_sequences = return_sequences\n        self.initial_weights = weights\n        self.go_backwards = go_backwards\n\n        self.input_dim = input_dim\n        self.input_length = input_length\n        if self.input_dim:\n            kwargs['input_shape'] = (self.input_length, self.input_dim)\n        super(LSTM, self).__init__(**kwargs)\n\n    def build(self):\n        input_dim = self.input_shape[2]\n        self.input = T.tensor3()\n\n        self.W_i = self.init((input_dim, self.output_dim))\n        self.U_i = self.inner_init((self.output_dim, self.output_dim))\n        self.b_i = shared_zeros((self.output_dim))\n\n        self.W_f = self.init((input_dim, self.output_dim))\n        self.U_f = self.inner_init((self.output_dim, self.output_dim))\n        self.b_f = self.forget_bias_init((self.output_dim))\n\n        self.W_c = self.init((input_dim, self.output_dim))\n        self.U_c = self.inner_init((self.output_dim, self.output_dim))\n        self.b_c = shared_zeros((self.output_dim))\n\n        self.W_o = self.init((input_dim, self.output_dim))\n        self.U_o = self.inner_init((self.output_dim, self.output_dim))\n        self.b_o = shared_zeros((self.output_dim))\n\n        self.params = [\n            self.W_i, self.U_i, self.b_i,\n            self.W_c, self.U_c, self.b_c,\n            self.W_f, self.U_f, self.b_f,\n            self.W_o, self.U_o, self.b_o,\n        ]\n\n        if self.initial_weights is not None:\n            self.set_weights(self.initial_weights)\n            del self.initial_weights\n\n    def _step(self,\n              xi_t, xf_t, xo_t, xc_t, mask_tm1,\n              h_tm1, c_tm1,\n              u_i, u_f, u_o, u_c):\n        h_mask_tm1 = mask_tm1 * h_tm1\n        c_mask_tm1 = mask_tm1 * c_tm1\n\n        i_t = self.inner_activation(xi_t + T.dot(h_mask_tm1, u_i))\n        f_t = self.inner_activation(xf_t + T.dot(h_mask_tm1, u_f))\n        c_t = f_t * c_mask_tm1 + i_t * self.activation(xc_t + T.dot(h_mask_tm1, u_c))\n        o_t = self.inner_activation(xo_t + T.dot(h_mask_tm1, u_o))\n        h_t = o_t * self.activation(c_t)\n        return h_t, c_t\n\n    def get_output(self, train=False):\n        X = self.get_input(train)\n        padded_mask = self.get_padded_shuffled_mask(train, X, pad=1)\n        X = X.dimshuffle((1, 0, 2))\n\n        xi = T.dot(X, self.W_i) + self.b_i\n        xf = T.dot(X, self.W_f) + self.b_f\n        xc = T.dot(X, self.W_c) + self.b_c\n        xo = T.dot(X, self.W_o) + self.b_o\n\n        [outputs, memories], updates = theano.scan(\n            self._step,\n            sequences=[xi, xf, xo, xc, padded_mask],\n            outputs_info=[\n                T.unbroadcast(alloc_zeros_matrix(X.shape[1], self.output_dim), 1),\n                T.unbroadcast(alloc_zeros_matrix(X.shape[1], self.output_dim), 1)\n            ],\n            non_sequences=[self.U_i, self.U_f, self.U_o, self.U_c],\n            truncate_gradient=self.truncate_gradient,\n            go_backwards=self.go_backwards)\n\n        if self.return_sequences:\n            return outputs.dimshuffle((1, 0, 2))\n        return outputs[-1]\n\n    def get_config(self):\n        config = {\"name\": self.__class__.__name__,\n                  \"output_dim\": self.output_dim,\n                  \"init\": self.init.__name__,\n                  \"inner_init\": self.inner_init.__name__,\n                  \"forget_bias_init\": self.forget_bias_init.__name__,\n                  \"activation\": self.activation.__name__,\n                  \"inner_activation\": self.inner_activation.__name__,\n                  \"truncate_gradient\": self.truncate_gradient,\n                  \"return_sequences\": self.return_sequences,\n                  \"input_dim\": self.input_dim,\n                  \"input_length\": self.input_length,\n                  \"go_backwards\": self.go_backwards}\n        base_config = super(LSTM, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\n\nclass JZS1(Recurrent):\n    '''\n        Evolved recurrent neural network architectures from the evaluation of thousands\n        of models, serving as alternatives to LSTMs and GRUs. See Jozefowicz et al. 2015.\n\n        This corresponds to the `MUT1` architecture described in the paper.\n\n        Takes inputs with shape:\n        (nb_samples, max_sample_length (samples shorter than this are padded with zeros at the end), input_dim)\n\n        and returns outputs with shape:\n        if not return_sequences:\n            (nb_samples, output_dim)\n        if return_sequences:\n            (nb_samples, max_sample_length, output_dim)\n\n        References:\n            An Empirical Exploration of Recurrent Network Architectures\n                http://www.jmlr.org/proceedings/papers/v37/jozefowicz15.pdf\n    '''\n    def __init__(self, output_dim,\n                 init='glorot_uniform', inner_init='orthogonal',\n                 activation='tanh', inner_activation='sigmoid',\n                 weights=None, truncate_gradient=-1, return_sequences=False,\n                 input_dim=None, input_length=None, go_backwards=False, **kwargs):\n        self.output_dim = output_dim\n        self.init = initializations.get(init)\n        self.inner_init = initializations.get(inner_init)\n        self.activation = activations.get(activation)\n        self.inner_activation = activations.get(inner_activation)\n        self.truncate_gradient = truncate_gradient\n        self.return_sequences = return_sequences\n        self.initial_weights = weights\n        self.go_backwards = go_backwards\n\n        self.input_dim = input_dim\n        self.input_length = input_length\n        if self.input_dim:\n            kwargs['input_shape'] = (self.input_length, self.input_dim)\n        super(JZS1, self).__init__(**kwargs)\n\n    def build(self):\n        input_dim = self.input_shape[2]\n        self.input = T.tensor3()\n\n        self.W_z = self.init((input_dim, self.output_dim))\n        self.b_z = shared_zeros((self.output_dim))\n\n        self.W_r = self.init((input_dim, self.output_dim))\n        self.U_r = self.inner_init((self.output_dim, self.output_dim))\n        self.b_r = shared_zeros((self.output_dim))\n\n        self.U_h = self.inner_init((self.output_dim, self.output_dim))\n        self.b_h = shared_zeros((self.output_dim))\n\n        # P_h used to project X onto different dimension, using sparse random projections\n        if input_dim == self.output_dim:\n            self.Pmat = theano.shared(np.identity(self.output_dim, dtype=theano.config.floatX), name=None)\n        else:\n            P = np.random.binomial(1, 0.5, size=(input_dim, self.output_dim)).astype(theano.config.floatX) * 2 - 1\n            P = 1 / np.sqrt(input_dim) * P\n            self.Pmat = theano.shared(P, name=None)\n\n        self.params = [\n            self.W_z, self.b_z,\n            self.W_r, self.U_r, self.b_r,\n            self.U_h, self.b_h,\n            self.Pmat\n        ]\n\n        if self.initial_weights is not None:\n            self.set_weights(self.initial_weights)\n            del self.initial_weights\n\n    def _step(self,\n              xz_t, xr_t, xh_t, mask_tm1,\n              h_tm1,\n              u_r, u_h):\n        h_mask_tm1 = mask_tm1 * h_tm1\n        z = self.inner_activation(xz_t)\n        r = self.inner_activation(xr_t + T.dot(h_mask_tm1, u_r))\n        hh_t = self.activation(xh_t + T.dot(r * h_mask_tm1, u_h))\n        h_t = hh_t * z + h_mask_tm1 * (1 - z)\n        return h_t\n\n    def get_output(self, train=False):\n        X = self.get_input(train)\n        padded_mask = self.get_padded_shuffled_mask(train, X, pad=1)\n        X = X.dimshuffle((1, 0, 2))\n\n        x_z = T.dot(X, self.W_z) + self.b_z\n        x_r = T.dot(X, self.W_r) + self.b_r\n        x_h = T.tanh(T.dot(X, self.Pmat)) + self.b_h\n        outputs, updates = theano.scan(\n            self._step,\n            sequences=[x_z, x_r, x_h, padded_mask],\n            outputs_info=T.unbroadcast(alloc_zeros_matrix(X.shape[1], self.output_dim), 1),\n            non_sequences=[self.U_r, self.U_h],\n            truncate_gradient=self.truncate_gradient,\n            go_backwards=self.go_backwards)\n        if self.return_sequences:\n            return outputs.dimshuffle((1, 0, 2))\n        return outputs[-1]\n\n    def get_config(self):\n        config = {\"name\": self.__class__.__name__,\n                  \"output_dim\": self.output_dim,\n                  \"init\": self.init.__name__,\n                  \"inner_init\": self.inner_init.__name__,\n                  \"activation\": self.activation.__name__,\n                  \"inner_activation\": self.inner_activation.__name__,\n                  \"truncate_gradient\": self.truncate_gradient,\n                  \"return_sequences\": self.return_sequences,\n                  \"input_dim\": self.input_dim,\n                  \"input_length\": self.input_length,\n                  \"go_backwards\": self.go_backwards}\n        base_config = super(JZS1, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\n\nclass JZS2(Recurrent):\n    '''\n        Evolved recurrent neural network architectures from the evaluation of thousands\n        of models, serving as alternatives to LSTMs and GRUs. See Jozefowicz et al. 2015.\n\n        This corresponds to the `MUT2` architecture described in the paper.\n\n        Takes inputs with shape:\n        (nb_samples, max_sample_length (samples shorter than this are padded with zeros at the end), input_dim)\n\n        and returns outputs with shape:\n        if not return_sequences:\n            (nb_samples, output_dim)\n        if return_sequences:\n            (nb_samples, max_sample_length, output_dim)\n\n        References:\n            An Empirical Exploration of Recurrent Network Architectures\n                http://www.jmlr.org/proceedings/papers/v37/jozefowicz15.pdf\n    '''\n    def __init__(self, output_dim,\n                 init='glorot_uniform', inner_init='orthogonal',\n                 activation='tanh', inner_activation='sigmoid',\n                 weights=None, truncate_gradient=-1, return_sequences=False,\n                 input_dim=None, input_length=None, go_backwards=False, **kwargs):\n        self.output_dim = output_dim\n        self.init = initializations.get(init)\n        self.inner_init = initializations.get(inner_init)\n        self.activation = activations.get(activation)\n        self.inner_activation = activations.get(inner_activation)\n        self.truncate_gradient = truncate_gradient\n        self.return_sequences = return_sequences\n        self.initial_weights = weights\n        self.go_backwards = go_backwards\n\n        self.input_dim = input_dim\n        self.input_length = input_length\n        if self.input_dim:\n            kwargs['input_shape'] = (self.input_length, self.input_dim)\n        super(JZS2, self).__init__(**kwargs)\n\n    def build(self):\n        input_dim = self.input_shape[2]\n        self.input = T.tensor3()\n\n        self.W_z = self.init((input_dim, self.output_dim))\n        self.U_z = self.inner_init((self.output_dim, self.output_dim))\n        self.b_z = shared_zeros((self.output_dim))\n\n        self.U_r = self.inner_init((self.output_dim, self.output_dim))\n        self.b_r = shared_zeros((self.output_dim))\n\n        self.W_h = self.init((input_dim, self.output_dim))\n        self.U_h = self.inner_init((self.output_dim, self.output_dim))\n        self.b_h = shared_zeros((self.output_dim))\n\n        # P_h used to project X onto different dimension, using sparse random projections\n        if input_dim == self.output_dim:\n            self.Pmat = theano.shared(np.identity(self.output_dim, dtype=theano.config.floatX), name=None)\n        else:\n            P = np.random.binomial(1, 0.5, size=(input_dim, self.output_dim)).astype(theano.config.floatX) * 2 - 1\n            P = 1 / np.sqrt(input_dim) * P\n            self.Pmat = theano.shared(P, name=None)\n\n        self.params = [\n            self.W_z, self.U_z, self.b_z,\n            self.U_r, self.b_r,\n            self.W_h, self.U_h, self.b_h,\n            self.Pmat\n        ]\n\n        if self.initial_weights is not None:\n            self.set_weights(self.initial_weights)\n            del self.initial_weights\n\n    def _step(self,\n              xz_t, xr_t, xh_t, mask_tm1,\n              h_tm1,\n              u_z, u_r, u_h):\n        h_mask_tm1 = mask_tm1 * h_tm1\n        z = self.inner_activation(xz_t + T.dot(h_mask_tm1, u_z))\n        r = self.inner_activation(xr_t + T.dot(h_mask_tm1, u_r))\n        hh_t = self.activation(xh_t + T.dot(r * h_mask_tm1, u_h))\n        h_t = hh_t * z + h_mask_tm1 * (1 - z)\n        return h_t\n\n    def get_output(self, train=False):\n        X = self.get_input(train)\n        padded_mask = self.get_padded_shuffled_mask(train, X, pad=1)\n        X = X.dimshuffle((1, 0, 2))\n\n        x_z = T.dot(X, self.W_z) + self.b_z\n        x_r = T.dot(X, self.Pmat) + self.b_r\n        x_h = T.dot(X, self.W_h) + self.b_h\n        outputs, updates = theano.scan(\n            self._step,\n            sequences=[x_z, x_r, x_h, padded_mask],\n            outputs_info=T.unbroadcast(alloc_zeros_matrix(X.shape[1], self.output_dim), 1),\n            non_sequences=[self.U_z, self.U_r, self.U_h],\n            truncate_gradient=self.truncate_gradient,\n            go_backwards=self.go_backwards)\n\n        if self.return_sequences:\n            return outputs.dimshuffle((1, 0, 2))\n        return outputs[-1]\n\n    def get_config(self):\n        config = {\"name\": self.__class__.__name__,\n                  \"output_dim\": self.output_dim,\n                  \"init\": self.init.__name__,\n                  \"inner_init\": self.inner_init.__name__,\n                  \"activation\": self.activation.__name__,\n                  \"inner_activation\": self.inner_activation.__name__,\n                  \"truncate_gradient\": self.truncate_gradient,\n                  \"return_sequences\": self.return_sequences,\n                  \"input_dim\": self.input_dim,\n                  \"input_length\": self.input_length,\n                  \"go_backwards\": self.go_backwards}\n        base_config = super(JZS2, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\n\nclass JZS3(Recurrent):\n    '''\n        Evolved recurrent neural network architectures from the evaluation of thousands\n        of models, serving as alternatives to LSTMs and GRUs. See Jozefowicz et al. 2015.\n\n        This corresponds to the `MUT3` architecture described in the paper.\n\n        Takes inputs with shape:\n        (nb_samples, max_sample_length (samples shorter than this are padded with zeros at the end), input_dim)\n\n        and returns outputs with shape:\n        if not return_sequences:\n            (nb_samples, output_dim)\n        if return_sequences:\n            (nb_samples, max_sample_length, output_dim)\n\n        References:\n            An Empirical Exploration of Recurrent Network Architectures\n                http://www.jmlr.org/proceedings/papers/v37/jozefowicz15.pdf\n    '''\n    def __init__(self, output_dim,\n                 init='glorot_uniform', inner_init='orthogonal',\n                 activation='tanh', inner_activation='sigmoid',\n                 weights=None, truncate_gradient=-1, return_sequences=False,\n                 input_dim=None, input_length=None, go_backwards=False, **kwargs):\n        self.output_dim = output_dim\n        self.init = initializations.get(init)\n        self.inner_init = initializations.get(inner_init)\n        self.activation = activations.get(activation)\n        self.inner_activation = activations.get(inner_activation)\n        self.truncate_gradient = truncate_gradient\n        self.return_sequences = return_sequences\n        self.initial_weights = weights\n        self.go_backwards = go_backwards\n\n        self.input_dim = input_dim\n        self.input_length = input_length\n        if self.input_dim:\n            kwargs['input_shape'] = (self.input_length, self.input_dim)\n        super(JZS3, self).__init__(**kwargs)\n\n    def build(self):\n        input_dim = self.input_shape[2]\n        self.input = T.tensor3()\n\n        self.W_z = self.init((input_dim, self.output_dim))\n        self.U_z = self.inner_init((self.output_dim, self.output_dim))\n        self.b_z = shared_zeros((self.output_dim))\n\n        self.W_r = self.init((input_dim, self.output_dim))\n        self.U_r = self.inner_init((self.output_dim, self.output_dim))\n        self.b_r = shared_zeros((self.output_dim))\n\n        self.W_h = self.init((input_dim, self.output_dim))\n        self.U_h = self.inner_init((self.output_dim, self.output_dim))\n        self.b_h = shared_zeros((self.output_dim))\n\n        self.params = [\n            self.W_z, self.U_z, self.b_z,\n            self.W_r, self.U_r, self.b_r,\n            self.W_h, self.U_h, self.b_h,\n        ]\n\n        if self.initial_weights is not None:\n            self.set_weights(self.initial_weights)\n            del self.initial_weights\n\n    def _step(self,\n              xz_t, xr_t, xh_t, mask_tm1,\n              h_tm1,\n              u_z, u_r, u_h):\n        h_mask_tm1 = mask_tm1 * h_tm1\n        z = self.inner_activation(xz_t + T.dot(T.tanh(h_mask_tm1), u_z))\n        r = self.inner_activation(xr_t + T.dot(h_mask_tm1, u_r))\n        hh_t = self.activation(xh_t + T.dot(r * h_mask_tm1, u_h))\n        h_t = hh_t * z + h_mask_tm1 * (1 - z)\n        return h_t\n\n    def get_output(self, train=False):\n        X = self.get_input(train)\n        padded_mask = self.get_padded_shuffled_mask(train, X, pad=1)\n        X = X.dimshuffle((1, 0, 2))\n\n        x_z = T.dot(X, self.W_z) + self.b_z\n        x_r = T.dot(X, self.W_r) + self.b_r\n        x_h = T.dot(X, self.W_h) + self.b_h\n        outputs, updates = theano.scan(\n            self._step,\n            sequences=[x_z, x_r, x_h, padded_mask],\n            outputs_info=T.unbroadcast(alloc_zeros_matrix(X.shape[1], self.output_dim), 1),\n            non_sequences=[self.U_z, self.U_r, self.U_h],\n            truncate_gradient=self.truncate_gradient,\n            go_backwards=self.go_backwards)\n        \n        if self.return_sequences:\n            return outputs.dimshuffle((1, 0, 2))\n        return outputs[-1]\n\n    def get_config(self):\n        config = {\"name\": self.__class__.__name__,\n                  \"output_dim\": self.output_dim,\n                  \"init\": self.init.__name__,\n                  \"inner_init\": self.inner_init.__name__,\n                  \"activation\": self.activation.__name__,\n                  \"inner_activation\": self.inner_activation.__name__,\n                  \"truncate_gradient\": self.truncate_gradient,\n                  \"return_sequences\": self.return_sequences,\n                  \"input_dim\": self.input_dim,\n                  \"input_length\": self.input_length,\n                  \"go_backwards\": self.go_backwards}\n        base_config = super(JZS3, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n",
          "file_after": "# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import\nimport numpy as np\n\nfrom .. import backend as K\nfrom .. import activations, initializations\nfrom ..layers.core import MaskedLayer\n\n\nclass Recurrent(MaskedLayer):\n    input_ndim = 3\n\n    def __init__(self, weights=None,\n                 return_sequences=False, input_dim=None,\n                 input_length=None, go_backwards=False,\n                 stateful=False, **kwargs):\n        self.return_sequences = return_sequences\n        self.initial_weights = weights\n        self.go_backwards = go_backwards\n        self.stateful = stateful\n\n        self.input_dim = input_dim\n        self.input_length = input_length\n        if self.input_dim:\n            kwargs['input_shape'] = (self.input_length, self.input_dim)\n        super(Recurrent, self).__init__(**kwargs)\n\n    def get_output_mask(self, train=False):\n        if self.return_sequences:\n            return super(Recurrent, self).get_output_mask(train)\n        else:\n            return None\n\n    @property\n    def output_shape(self):\n        input_shape = self.input_shape\n        if self.return_sequences:\n            return (input_shape[0], input_shape[1], self.output_dim)\n        else:\n            return (input_shape[0], self.output_dim)\n\n    def step(self, x, states):\n        raise NotImplementedError\n\n    def get_output(self, train=False):\n        # input shape: (nb_samples, time (padded with zeros), input_dim)\n        X = self.get_input(train)\n        assert K.ndim(X) == 3\n\n        mask = self.get_output_mask(train)\n        if mask:\n            # apply mask\n            X *= K.expand_dims(mask)\n            masking = True\n        else:\n            masking = False\n\n        if self.stateful:\n            initial_states = self.states\n        else:\n            # build an all-zero tensor of shape (samples, output_dim)\n            initial_state = K.zeros_like(X)  # (samples, timesteps, input_dim)\n            initial_state = K.sum(initial_state, axis=1)  # (samples, input_dim)\n            reducer = K.zeros((self.input_dim, self.output_dim))\n            initial_state = K.dot(initial_state, reducer)  # (samples, output_dim)\n            initial_states = [initial_state for _ in range(len(self.states))]\n        last_output, outputs, states = K.rnn(self.step, X, initial_states,\n                                             go_backwards=self.go_backwards,\n                                             masking=masking)\n        if self.stateful:\n            for i in range(len(states)):\n                K.set_value(self.states[i], states[i])\n\n        if self.return_sequences:\n            return outputs\n        else:\n            return last_output\n\n    def get_config(self):\n        config = {\"name\": self.__class__.__name__,\n                  \"return_sequences\": self.return_sequences,\n                  \"input_dim\": self.input_dim,\n                  \"input_length\": self.input_length,\n                  \"go_backwards\": self.go_backwards,\n                  \"stateful\": self.stateful}\n        base_config = super(Recurrent, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\n\nclass SimpleRNN(Recurrent):\n    '''\n        Fully-connected RNN where the output is to fed back to input.\n\n        Takes inputs with shape:\n        (nb_samples, max_sample_length, input_dim)\n        (samples shorter than `max_sample_length`\n         are padded with zeros at the end)\n\n        and returns outputs with shape:\n        if not return_sequences:\n            (nb_samples, output_dim)\n        if return_sequences:\n            (nb_samples, max_sample_length, output_dim)\n    '''\n    def __init__(self, output_dim,\n                 init='glorot_uniform', inner_init='orthogonal',\n                 activation='sigmoid', **kwargs):\n        self.output_dim = output_dim\n        self.init = initializations.get(init)\n        self.inner_init = initializations.get(inner_init)\n        self.activation = activations.get(activation)\n        super(SimpleRNN, self).__init__(**kwargs)\n\n    def build(self):\n        input_shape = self.input_shape\n        if self.stateful:\n            if not input_shape[0]:\n                raise Exception('If a RNN is stateful, a complete ' +\n                                'input_shape must be provided ' +\n                                '(including batch size).')\n            self.states = [K.zeros(input_shape[0], self.output_dim)]\n        else:\n            # initial states: all-zero tensor of shape (output_dim)\n            self.states = [None]\n        input_dim = input_shape[2]\n        self.input_dim = input_dim\n\n        self.W = self.init((input_dim, self.output_dim))\n        self.U = self.inner_init((self.output_dim, self.output_dim))\n        self.b = K.zeros((self.output_dim))\n        self.params = [self.W, self.U, self.b]\n\n        if self.initial_weights is not None:\n            self.set_weights(self.initial_weights)\n            del self.initial_weights\n\n    def step(self, x, states):\n        # states only contains the previous output.\n        assert len(states) == 1\n        prev_output = states[0]\n        h = K.dot(x, self.W) + self.b\n        output = self.activation(h * K.dot(prev_output, self.U))\n        return output, [output]\n\n    def get_config(self):\n        config = {\"output_dim\": self.output_dim,\n                  \"init\": self.init.__name__,\n                  \"inner_init\": self.inner_init.__name__,\n                  \"activation\": self.activation.__name__}\n        base_config = super(SimpleRNN, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\n\nclass GRU(Recurrent):\n    '''\n        Gated Recurrent Unit - Cho et al. 2014\n\n        Acts as a spatiotemporal projection,\n        turning a sequence of vectors into a single vector.\n\n        Takes inputs with shape:\n        (nb_samples, max_sample_length, input_dim)\n        (samples shorter than `max_sample_length`\n         are padded with zeros at the end)\n\n        and returns outputs with shape:\n        if not return_sequences:\n            (nb_samples, output_dim)\n        if return_sequences:\n            (nb_samples, max_sample_length, output_dim)\n\n        References:\n            On the Properties of Neural Machine Translation:\n            Encoder\u2013Decoder Approaches\n                http://www.aclweb.org/anthology/W14-4012\n            Empirical Evaluation of Gated Recurrent Neural Networks\n            on Sequence Modeling\n                http://arxiv.org/pdf/1412.3555v1.pdf\n    '''\n    def __init__(self, output_dim,\n                 init='glorot_uniform', inner_init='orthogonal',\n                 activation='sigmoid', inner_activation='hard_sigmoid',\n                 **kwargs):\n        self.output_dim = output_dim\n        self.init = initializations.get(init)\n        self.inner_init = initializations.get(inner_init)\n        self.activation = activations.get(activation)\n        self.inner_activation = activations.get(inner_activation)\n        super(GRU, self).__init__(**kwargs)\n\n    def build(self):\n        input_shape = self.input_shape\n        input_dim = input_shape[2]\n        self.input_dim = input_dim\n        self.input = K.placeholder(input_shape)\n\n        self.W_z = self.init((input_dim, self.output_dim))\n        self.U_z = self.inner_init((self.output_dim, self.output_dim))\n        self.b_z = K.zeros((self.output_dim,))\n\n        self.W_r = self.init((input_dim, self.output_dim))\n        self.U_r = self.inner_init((self.output_dim, self.output_dim))\n        self.b_r = K.zeros((self.output_dim,))\n\n        self.W_h = self.init((input_dim, self.output_dim))\n        self.U_h = self.inner_init((self.output_dim, self.output_dim))\n        self.b_h = K.zeros((self.output_dim,))\n\n        self.params = [self.W_z, self.U_z, self.b_z,\n                       self.W_r, self.U_r, self.b_r,\n                       self.W_h, self.U_h, self.b_h]\n\n        if self.stateful:\n            if not input_shape[0]:\n                raise Exception('If a RNN is stateful, a complete ' +\n                                'input_shape must be provided ' +\n                                '(including batch size).')\n            self.states = [K.zeros(input_shape[0], self.output_dim)]\n        else:\n            # initial states: all-zero tensor of shape (output_dim)\n            self.states = [None]\n\n        if self.initial_weights is not None:\n            self.set_weights(self.initial_weights)\n            del self.initial_weights\n\n    def step(self, x, states):\n        assert len(states) == 1\n        x_z = K.dot(x, self.W_z) + self.b_z\n        x_r = K.dot(x, self.W_r) + self.b_r\n        x_h = K.dot(x, self.W_h) + self.b_h\n\n        h_tm1 = states[0]\n        z = self.inner_activation(x_z + K.dot(h_tm1, self.U_z))\n        r = self.inner_activation(x_r + K.dot(h_tm1, self.U_r))\n\n        hh = self.inner_activation(x_h + K.dot(r * h_tm1, self.U_h))\n        h = z * h_tm1 + (1 - z) * hh\n        return h, [h]\n\n    def get_config(self):\n        config = {\"output_dim\": self.output_dim,\n                  \"init\": self.init.__name__,\n                  \"inner_init\": self.inner_init.__name__,\n                  \"activation\": self.activation.__name__,\n                  \"inner_activation\": self.inner_activation.__name__}\n        base_config = super(GRU, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\n\nclass LSTM(Recurrent):\n    '''\n        Acts as a spatiotemporal projection,\n        turning a sequence of vectors into a single vector.\n\n        Takes inputs with shape:\n        (nb_samples, max_sample_length, input_dim)\n        (samples shorter than `max_sample_length`\n         are padded with zeros at the end)\n\n        and returns outputs with shape:\n        if not return_sequences:\n            (nb_samples, output_dim)\n        if return_sequences:\n            (nb_samples, max_sample_length, output_dim)\n\n        For a step-by-step description of the algorithm, see:\n        http://deeplearning.net/tutorial/lstm.html\n\n        References:\n            Long short-term memory (original 97 paper)\n                http://deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf\n            Learning to forget: Continual prediction with LSTM\n                http://www.mitpressjournals.org/doi/pdf/10.1162/089976600300015015\n            Supervised sequence labelling with recurrent neural networks\n                http://www.cs.toronto.edu/~graves/preprint.pdf\n    '''\n    def __init__(self, output_dim,\n                 init='glorot_uniform', inner_init='orthogonal',\n                 forget_bias_init='one', activation='tanh',\n                 inner_activation='hard_sigmoid', **kwargs):\n        self.output_dim = output_dim\n        self.init = initializations.get(init)\n        self.inner_init = initializations.get(inner_init)\n        self.forget_bias_init = initializations.get(forget_bias_init)\n        self.activation = activations.get(activation)\n        self.inner_activation = activations.get(inner_activation)\n        super(LSTM, self).__init__(**kwargs)\n\n    def build(self):\n        input_shape = self.input_shape\n        input_dim = input_shape[2]\n        self.input_dim = input_dim\n        self.input = K.placeholder(input_shape)\n\n        if self.stateful:\n            if not input_shape[0]:\n                raise Exception('If a RNN is stateful, a complete ' +\n                                'input_shape must be provided ' +\n                                '(including batch size).')\n            self.states = [K.zeros(input_shape[0], self.output_dim),\n                           K.zeros(input_shape[0], self.output_dim)]\n        else:\n            # initial states: 2 all-zero tensor of shape (output_dim)\n            self.states = [None, None]\n\n        self.W_i = self.init((input_dim, self.output_dim))\n        self.U_i = self.inner_init((self.output_dim, self.output_dim))\n        self.b_i = K.zeros((self.output_dim))\n\n        self.W_f = self.init((input_dim, self.output_dim))\n        self.U_f = self.inner_init((self.output_dim, self.output_dim))\n        self.b_f = self.forget_bias_init((self.output_dim))\n\n        self.W_c = self.init((input_dim, self.output_dim))\n        self.U_c = self.inner_init((self.output_dim, self.output_dim))\n        self.b_c = K.zeros((self.output_dim))\n\n        self.W_o = self.init((input_dim, self.output_dim))\n        self.U_o = self.inner_init((self.output_dim, self.output_dim))\n        self.b_o = K.zeros((self.output_dim))\n\n        self.params = [self.W_i, self.U_i, self.b_i,\n                       self.W_c, self.U_c, self.b_c,\n                       self.W_f, self.U_f, self.b_f,\n                       self.W_o, self.U_o, self.b_o]\n\n        if self.initial_weights is not None:\n            self.set_weights(self.initial_weights)\n            del self.initial_weights\n\n    def step(self, x, states):\n        assert len(states) == 2\n        h_tm1 = states[0]\n        c_tm1 = states[1]\n\n        x_i = K.dot(x, self.W_i) + self.b_i\n        x_f = K.dot(x, self.W_f) + self.b_f\n        x_c = K.dot(x, self.W_c) + self.b_c\n        x_o = K.dot(x, self.W_o) + self.b_o\n\n        i = self.inner_activation(x_i + K.dot(h_tm1, self.U_i))\n        f = self.inner_activation(x_f + K.dot(h_tm1, self.U_f))\n        c = f * c_tm1 + i * self.activation(x_c + K.dot(h_tm1, self.U_c))\n        o = self.inner_activation(x_o + K.dot(h_tm1, self.U_o))\n        h = o * self.activation(c)\n        return h, [h, c]\n\n    def get_config(self):\n        config = {\"output_dim\": self.output_dim,\n                  \"init\": self.init.__name__,\n                  \"inner_init\": self.inner_init.__name__,\n                  \"forget_bias_init\": self.forget_bias_init.__name__,\n                  \"activation\": self.activation.__name__,\n                  \"inner_activation\": self.inner_activation.__name__}\n        base_config = super(LSTM, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\n\nclass JZS(Recurrent):\n    '''\n        Evolved recurrent neural network architectures\n        from the evaluation of thousands\n        of models, serving as alternatives to LSTMs and GRUs.\n        See Jozefowicz et al. 2015.\n\n        References:\n            An Empirical Exploration of Recurrent Network Architectures\n                http://www.jmlr.org/proceedings/papers/v37/jozefowicz15.pdf\n    '''\n    def __init__(self, output_dim,\n                 init='glorot_uniform', inner_init='orthogonal',\n                 activation='tanh', inner_activation='sigmoid',\n                 **kwargs):\n        self.output_dim = output_dim\n        self.init = initializations.get(init)\n        self.inner_init = initializations.get(inner_init)\n        self.activation = activations.get(activation)\n        self.inner_activation = activations.get(inner_activation)\n        super(JZS, self).__init__(**kwargs)\n\n    def build(self):\n        input_shape = self.input_shape\n        input_dim = input_shape[2]\n        self.input_dim = input_dim\n        self.input = K.placeholder(input_shape)\n\n        if self.stateful:\n            if not input_shape[0]:\n                raise Exception('If a RNN is stateful, a complete ' +\n                                'input_shape must be provided ' +\n                                '(including batch size).')\n            self.states = [K.zeros(input_shape[0], self.output_dim)]\n        else:\n            # initial states: all-zero tensor of shape (output_dim)\n            self.states = [None]\n\n        self.W_z = self.init((input_dim, self.output_dim))\n        self.U_z = self.inner_init((self.output_dim, self.output_dim))\n        self.b_z = K.zeros((self.output_dim))\n\n        self.W_r = self.init((input_dim, self.output_dim))\n        self.U_r = self.inner_init((self.output_dim, self.output_dim))\n        self.b_r = K.zeros((self.output_dim))\n\n        self.W_h = self.init((input_dim, self.output_dim))\n        self.U_h = self.inner_init((self.output_dim, self.output_dim))\n        self.b_h = K.zeros((self.output_dim))\n\n        # P matrix used to project X onto different dimension,\n        # using sparse random projections\n        if input_dim == self.output_dim:\n            self.P = K.variable(np.identity(self.output_dim))\n        else:\n            P = np.random.binomial(1, 0.5, size=(input_dim, self.output_dim))\n            P = 1. / np.sqrt(input_dim) * (P * 2. - 1.)\n            self.P = K.variable(P)\n\n        self.params = [self.W_z, self.b_z,\n                       self.W_r, self.U_r, self.b_r,\n                       self.U_h, self.b_h,\n                       self.P]\n\n        if self.initial_weights is not None:\n            self.set_weights(self.initial_weights)\n            del self.initial_weights\n\n    def get_config(self):\n        config = {\"output_dim\": self.output_dim,\n                  \"init\": self.init.__name__,\n                  \"inner_init\": self.inner_init.__name__,\n                  \"activation\": self.activation.__name__,\n                  \"inner_activation\": self.inner_activation.__name__}\n        base_config = super(JZS, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\n\nclass JZS1(JZS):\n    '''\n        Evolved recurrent neural network architectures\n        from the evaluation of thousands\n        of models, serving as alternatives to LSTMs and GRUs.\n        See Jozefowicz et al. 2015.\n\n        This corresponds to the `MUT1` architecture described in the paper.\n    '''\n    def __init__(self, *args, **kwargs):\n        super(JZS1, self).__init__(*args, **kwargs)\n\n    def step(self, x, states):\n        assert len(states) == 1\n        h_tm1 = states[0]\n\n        x_z = K.dot(x, self.W_z) + self.b_z\n        x_r = K.dot(x, self.P) + self.b_r\n        x_h = K.dot(x, self.W_h) + self.b_h\n\n        z = self.inner_activation(x_z)\n        r = self.inner_activation(x_r + K.dot(h_tm1, self.U_r))\n        hh = self.activation(x_h + K.dot(r * h_tm1, self.U_h))\n        h = hh * z + h_tm1 * (1. - z)\n        return h, [h]\n\n\nclass JZS2(JZS):\n    '''\n        Evolved recurrent neural network architectures\n        from the evaluation of thousands\n        of models, serving as alternatives to LSTMs and GRUs.\n        See Jozefowicz et al. 2015.\n\n        This corresponds to the `MUT2` architecture described in the paper.\n    '''\n    def __init__(self, *args, **kwargs):\n        super(JZS2, self).__init__(*args, **kwargs)\n\n    def step(self, x, states):\n        assert len(states) == 1\n        h_tm1 = states[0]\n\n        x_z = K.dot(x, self.W_z) + self.b_z\n        x_r = K.dot(x, self.P) + self.b_r\n        x_h = K.dot(x, self.W_h) + self.b_h\n\n        z = self.inner_activation(x_z + K.dot(h_tm1, self.U_z))\n        r = self.inner_activation(x_r + K.dot(h_tm1, self.U_r))\n        hh = self.activation(x_h + K.dot(r * h_tm1, self.U_h))\n        h = hh * z + h_tm1 * (1. - z)\n        return h, [h]\n\n\nclass JZS3(Recurrent):\n    '''\n        Evolved recurrent neural network architectures\n        from the evaluation of thousands\n        of models, serving as alternatives to LSTMs and GRUs.\n        See Jozefowicz et al. 2015.\n\n        This corresponds to the `MUT3` architecture described in the paper.\n    '''\n    def __init__(self, *args, **kwargs):\n        super(JZS3, self).__init__(*args, **kwargs)\n\n    def step(self, x, states):\n        assert len(states) == 1\n        h_tm1 = states[0]\n\n        x_z = K.dot(x, self.W_z) + self.b_z\n        x_r = K.dot(x, self.P) + self.b_r\n        x_h = K.dot(x, self.W_h) + self.b_h\n\n        z = self.inner_activation(x_z + K.dot(K.tanh(h_tm1), self.U_z))\n        r = self.inner_activation(x_r + K.dot(h_tm1, self.U_r))\n        hh = self.activation(x_h + K.dot(r * h_tm1, self.U_h))\n        h = hh * z + h_tm1 * (1. - z)\n        return h, [h]\n",
          "file_patch": "@@ -1,42 +1,36 @@\n # -*- coding: utf-8 -*-\n from __future__ import absolute_import\n-import theano\n-import theano.tensor as T\n import numpy as np\n \n+from .. import backend as K\n from .. import activations, initializations\n-from ..utils.theano_utils import shared_scalar, shared_zeros, alloc_zeros_matrix\n-from ..layers.core import Layer, MaskedLayer\n-from six.moves import range\n+from ..layers.core import MaskedLayer\n \n \n class Recurrent(MaskedLayer):\n     input_ndim = 3\n \n-    def get_output_mask(self, train=None):\n+    def __init__(self, weights=None,\n+                 return_sequences=False, input_dim=None,\n+                 input_length=None, go_backwards=False,\n+                 stateful=False, **kwargs):\n+        self.return_sequences = return_sequences\n+        self.initial_weights = weights\n+        self.go_backwards = go_backwards\n+        self.stateful = stateful\n+\n+        self.input_dim = input_dim\n+        self.input_length = input_length\n+        if self.input_dim:\n+            kwargs['input_shape'] = (self.input_length, self.input_dim)\n+        super(Recurrent, self).__init__(**kwargs)\n+\n+    def get_output_mask(self, train=False):\n         if self.return_sequences:\n             return super(Recurrent, self).get_output_mask(train)\n         else:\n             return None\n \n-    def get_padded_shuffled_mask(self, train, X, pad=0):\n-        mask = self.get_input_mask(train)\n-        if mask is None:\n-            mask = T.ones_like(X.sum(axis=-1))  # is there a better way to do this without a sum?\n-\n-        # TODO: reimplement\n-        # mask is (nb_samples, time)\n-        mask = T.shape_padright(mask)  # (nb_samples, time, 1)\n-        mask = T.addbroadcast(mask, -1)  # the new dimension (the '1') is made broadcastable\n-        # see http://deeplearning.net/software/theano/library/tensor/basic.html#broadcasting-in-theano-vs-numpy\n-        mask = mask.dimshuffle(1, 0, 2)  # (time, nb_samples, 1)\n-\n-        if pad > 0:\n-            # left-pad in time with 0\n-            padding = alloc_zeros_matrix(pad, mask.shape[1], 1)\n-            mask = T.concatenate([padding, mask], axis=0)\n-        return mask.astype('int8')\n-\n     @property\n     def output_shape(self):\n         input_shape = self.input_shape\n@@ -45,190 +39,115 @@ class Recurrent(MaskedLayer):\n         else:\n             return (input_shape[0], self.output_dim)\n \n+    def step(self, x, states):\n+        raise NotImplementedError\n \n-class SimpleRNN(Recurrent):\n-    '''\n-        Fully connected RNN where output is to fed back to input.\n-\n-        Not a particularly useful model,\n-        included for demonstration purposes\n-        (demonstrates how to use theano.scan to build a basic RNN).\n-    '''\n-    def __init__(self, output_dim,\n-                 init='glorot_uniform', inner_init='orthogonal', activation='sigmoid', weights=None,\n-                 truncate_gradient=-1, return_sequences=False, input_dim=None,\n-                 input_length=None, go_backwards=False, **kwargs):\n-        self.output_dim = output_dim\n-        self.init = initializations.get(init)\n-        self.inner_init = initializations.get(inner_init)\n-        self.truncate_gradient = truncate_gradient\n-        self.activation = activations.get(activation)\n-        self.return_sequences = return_sequences\n-        self.initial_weights = weights\n-        self.go_backwards = go_backwards\n-\n-        self.input_dim = input_dim\n-        self.input_length = input_length\n-        if self.input_dim:\n-            kwargs['input_shape'] = (self.input_length, self.input_dim)\n-        super(SimpleRNN, self).__init__(**kwargs)\n-\n-    def build(self):\n-        input_dim = self.input_shape[2]\n-        self.input = T.tensor3()\n-\n-        self.W = self.init((input_dim, self.output_dim))\n-        self.U = self.inner_init((self.output_dim, self.output_dim))\n-        self.b = shared_zeros((self.output_dim))\n-        self.params = [self.W, self.U, self.b]\n-\n-        if self.initial_weights is not None:\n-            self.set_weights(self.initial_weights)\n-            del self.initial_weights\n-\n-    def _step(self, x_t, mask_tm1, h_tm1, u):\n-        '''\n-            Variable names follow the conventions from:\n-            http://deeplearning.net/software/theano/library/scan.html\n+    def get_output(self, train=False):\n+        # input shape: (nb_samples, time (padded with zeros), input_dim)\n+        X = self.get_input(train)\n+        assert K.ndim(X) == 3\n \n-        '''\n-        return self.activation(x_t + mask_tm1 * T.dot(h_tm1, u))\n+        mask = self.get_output_mask(train)\n+        if mask:\n+            # apply mask\n+            X *= K.expand_dims(mask)\n+            masking = True\n+        else:\n+            masking = False\n \n-    def get_output(self, train=False):\n-        X = self.get_input(train)  # shape: (nb_samples, time (padded with zeros), input_dim)\n-        # new shape: (time, nb_samples, input_dim) -> because theano.scan iterates over main dimension\n-        padded_mask = self.get_padded_shuffled_mask(train, X, pad=1)\n-        X = X.dimshuffle((1, 0, 2))\n-        x = T.dot(X, self.W) + self.b\n-\n-        # scan = theano symbolic loop.\n-        # See: http://deeplearning.net/software/theano/library/scan.html\n-        # Iterate over the first dimension of the x array (=time).\n-        outputs, updates = theano.scan(\n-            self._step,  # this will be called with arguments (sequences[i], outputs[i-1], non_sequences[i])\n-            sequences=[x, dict(input=padded_mask, taps=[-1])],  # tensors to iterate over, inputs to _step\n-            # initialization of the output. Input to _step with default tap=-1.\n-            outputs_info=T.unbroadcast(alloc_zeros_matrix(X.shape[1], self.output_dim), 1),\n-            non_sequences=self.U,  # static inputs to _step\n-            truncate_gradient=self.truncate_gradient,\n-            go_backwards=self.go_backwards)\n+        if self.stateful:\n+            initial_states = self.states\n+        else:\n+            # build an all-zero tensor of shape (samples, output_dim)\n+            initial_state = K.zeros_like(X)  # (samples, timesteps, input_dim)\n+            initial_state = K.sum(initial_state, axis=1)  # (samples, input_dim)\n+            reducer = K.zeros((self.input_dim, self.output_dim))\n+            initial_state = K.dot(initial_state, reducer)  # (samples, output_dim)\n+            initial_states = [initial_state for _ in range(len(self.states))]\n+        last_output, outputs, states = K.rnn(self.step, X, initial_states,\n+                                             go_backwards=self.go_backwards,\n+                                             masking=masking)\n+        if self.stateful:\n+            for i in range(len(states)):\n+                K.set_value(self.states[i], states[i])\n \n         if self.return_sequences:\n-            return outputs.dimshuffle((1, 0, 2))\n-        return outputs[-1]\n+            return outputs\n+        else:\n+            return last_output\n \n     def get_config(self):\n         config = {\"name\": self.__class__.__name__,\n-                  \"output_dim\": self.output_dim,\n-                  \"init\": self.init.__name__,\n-                  \"inner_init\": self.inner_init.__name__,\n-                  \"activation\": self.activation.__name__,\n-                  \"truncate_gradient\": self.truncate_gradient,\n                   \"return_sequences\": self.return_sequences,\n                   \"input_dim\": self.input_dim,\n                   \"input_length\": self.input_length,\n-                  \"go_backwards\": self.go_backwards}\n-        base_config = super(SimpleRNN, self).get_config()\n+                  \"go_backwards\": self.go_backwards,\n+                  \"stateful\": self.stateful}\n+        base_config = super(Recurrent, self).get_config()\n         return dict(list(base_config.items()) + list(config.items()))\n \n \n-class SimpleDeepRNN(Recurrent):\n+class SimpleRNN(Recurrent):\n     '''\n-        Fully connected RNN where the output of multiple timesteps\n-        (up to \"depth\" steps in the past) is fed back to the input:\n+        Fully-connected RNN where the output is to fed back to input.\n \n-        output = activation( W.x_t + b + inner_activation(U_1.h_tm1) + inner_activation(U_2.h_tm2) + ... )\n+        Takes inputs with shape:\n+        (nb_samples, max_sample_length, input_dim)\n+        (samples shorter than `max_sample_length`\n+         are padded with zeros at the end)\n \n-        This demonstrates how to build RNNs with arbitrary lookback.\n-        Also (probably) not a super useful model.\n+        and returns outputs with shape:\n+        if not return_sequences:\n+            (nb_samples, output_dim)\n+        if return_sequences:\n+            (nb_samples, max_sample_length, output_dim)\n     '''\n-    def __init__(self, output_dim, depth=3,\n+    def __init__(self, output_dim,\n                  init='glorot_uniform', inner_init='orthogonal',\n-                 activation='sigmoid', inner_activation='hard_sigmoid',\n-                 weights=None, truncate_gradient=-1, return_sequences=False,\n-                 input_dim=None, input_length=None, go_backwards=False, **kwargs):\n+                 activation='sigmoid', **kwargs):\n         self.output_dim = output_dim\n         self.init = initializations.get(init)\n         self.inner_init = initializations.get(inner_init)\n-        self.truncate_gradient = truncate_gradient\n         self.activation = activations.get(activation)\n-        self.inner_activation = activations.get(inner_activation)\n-        self.depth = depth\n-        self.return_sequences = return_sequences\n-        self.initial_weights = weights\n-        self.go_backwards = go_backwards\n+        super(SimpleRNN, self).__init__(**kwargs)\n \n+    def build(self):\n+        input_shape = self.input_shape\n+        if self.stateful:\n+            if not input_shape[0]:\n+                raise Exception('If a RNN is stateful, a complete ' +\n+                                'input_shape must be provided ' +\n+                                '(including batch size).')\n+            self.states = [K.zeros(input_shape[0], self.output_dim)]\n+        else:\n+            # initial states: all-zero tensor of shape (output_dim)\n+            self.states = [None]\n+        input_dim = input_shape[2]\n         self.input_dim = input_dim\n-        self.input_length = input_length\n-        if self.input_dim:\n-            kwargs['input_shape'] = (self.input_length, self.input_dim)\n-        super(SimpleDeepRNN, self).__init__(**kwargs)\n \n-    def build(self):\n-        input_dim = self.input_shape[2]\n-        self.input = T.tensor3()\n         self.W = self.init((input_dim, self.output_dim))\n-        self.Us = [self.inner_init((self.output_dim, self.output_dim)) for _ in range(self.depth)]\n-        self.b = shared_zeros((self.output_dim))\n-        self.params = [self.W] + self.Us + [self.b]\n+        self.U = self.inner_init((self.output_dim, self.output_dim))\n+        self.b = K.zeros((self.output_dim))\n+        self.params = [self.W, self.U, self.b]\n \n         if self.initial_weights is not None:\n             self.set_weights(self.initial_weights)\n             del self.initial_weights\n \n-    def _step(self, x_t, *args):\n-        o = x_t\n-        for i in range(self.depth):\n-            mask_tmi = args[i]\n-            h_tmi = args[i + self.depth]\n-            U_tmi = args[i + 2*self.depth]\n-            o += mask_tmi*self.inner_activation(T.dot(h_tmi, U_tmi))\n-        return self.activation(o)\n-\n-    def get_output(self, train=False):\n-        X = self.get_input(train)\n-        padded_mask = self.get_padded_shuffled_mask(train, X, pad=self.depth)\n-        X = X.dimshuffle((1, 0, 2))\n-\n-        x = T.dot(X, self.W) + self.b\n-\n-        if self.depth == 1:\n-            initial = T.unbroadcast(alloc_zeros_matrix(X.shape[1], self.output_dim), 1)\n-        else:\n-            initial = T.unbroadcast(T.unbroadcast(alloc_zeros_matrix(self.depth, X.shape[1], self.output_dim), 0), 2)\n-\n-        outputs, updates = theano.scan(\n-            self._step,\n-            sequences=[x, dict(\n-                input=padded_mask,\n-                taps=[(-i) for i in range(self.depth)]\n-            )],\n-            outputs_info=[dict(\n-                initial=initial,\n-                taps=[(-i-1) for i in range(self.depth)]\n-            )],\n-            non_sequences=self.Us,\n-            truncate_gradient=self.truncate_gradient,\n-            go_backwards=self.go_backwards)\n-\n-        if self.return_sequences:\n-            return outputs.dimshuffle((1, 0, 2))\n-        return outputs[-1]\n+    def step(self, x, states):\n+        # states only contains the previous output.\n+        assert len(states) == 1\n+        prev_output = states[0]\n+        h = K.dot(x, self.W) + self.b\n+        output = self.activation(h * K.dot(prev_output, self.U))\n+        return output, [output]\n \n     def get_config(self):\n-        config = {\"name\": self.__class__.__name__,\n-                  \"output_dim\": self.output_dim,\n-                  \"depth\": self.depth,\n+        config = {\"output_dim\": self.output_dim,\n                   \"init\": self.init.__name__,\n                   \"inner_init\": self.inner_init.__name__,\n-                  \"activation\": self.activation.__name__,\n-                  \"inner_activation\": self.inner_activation.__name__,\n-                  \"truncate_gradient\": self.truncate_gradient,\n-                  \"return_sequences\": self.return_sequences,\n-                  \"input_dim\": self.input_dim,\n-                  \"input_length\": self.input_length,\n-                  \"go_backwards\": self.go_backwards}\n-        base_config = super(SimpleDeepRNN, self).get_config()\n+                  \"activation\": self.activation.__name__}\n+        base_config = super(SimpleRNN, self).get_config()\n         return dict(list(base_config.items()) + list(config.items()))\n \n \n@@ -239,8 +158,10 @@ class GRU(Recurrent):\n         Acts as a spatiotemporal projection,\n         turning a sequence of vectors into a single vector.\n \n-        Eats inputs with shape:\n-        (nb_samples, max_sample_length (samples shorter than this are padded with zeros at the end), input_dim)\n+        Takes inputs with shape:\n+        (nb_samples, max_sample_length, input_dim)\n+        (samples shorter than `max_sample_length`\n+         are padded with zeros at the end)\n \n         and returns outputs with shape:\n         if not return_sequences:\n@@ -249,101 +170,80 @@ class GRU(Recurrent):\n             (nb_samples, max_sample_length, output_dim)\n \n         References:\n-            On the Properties of Neural Machine Translation: Encoder\u2013Decoder Approaches\n+            On the Properties of Neural Machine Translation:\n+            Encoder\u2013Decoder Approaches\n                 http://www.aclweb.org/anthology/W14-4012\n-            Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling\n+            Empirical Evaluation of Gated Recurrent Neural Networks\n+            on Sequence Modeling\n                 http://arxiv.org/pdf/1412.3555v1.pdf\n     '''\n     def __init__(self, output_dim,\n                  init='glorot_uniform', inner_init='orthogonal',\n                  activation='sigmoid', inner_activation='hard_sigmoid',\n-                 weights=None, truncate_gradient=-1, return_sequences=False,\n-                 input_dim=None, input_length=None, go_backwards=False, **kwargs):\n+                 **kwargs):\n         self.output_dim = output_dim\n         self.init = initializations.get(init)\n         self.inner_init = initializations.get(inner_init)\n         self.activation = activations.get(activation)\n         self.inner_activation = activations.get(inner_activation)\n-        self.truncate_gradient = truncate_gradient\n-        self.return_sequences = return_sequences\n-        self.initial_weights = weights\n-        self.go_backwards = go_backwards\n-\n-        self.input_dim = input_dim\n-        self.input_length = input_length\n-        if self.input_dim:\n-            kwargs['input_shape'] = (self.input_length, self.input_dim)\n         super(GRU, self).__init__(**kwargs)\n \n     def build(self):\n-        input_dim = self.input_shape[2]\n-        self.input = T.tensor3()\n+        input_shape = self.input_shape\n+        input_dim = input_shape[2]\n+        self.input_dim = input_dim\n+        self.input = K.placeholder(input_shape)\n \n         self.W_z = self.init((input_dim, self.output_dim))\n         self.U_z = self.inner_init((self.output_dim, self.output_dim))\n-        self.b_z = shared_zeros((self.output_dim))\n+        self.b_z = K.zeros((self.output_dim,))\n \n         self.W_r = self.init((input_dim, self.output_dim))\n         self.U_r = self.inner_init((self.output_dim, self.output_dim))\n-        self.b_r = shared_zeros((self.output_dim))\n+        self.b_r = K.zeros((self.output_dim,))\n \n         self.W_h = self.init((input_dim, self.output_dim))\n         self.U_h = self.inner_init((self.output_dim, self.output_dim))\n-        self.b_h = shared_zeros((self.output_dim))\n-\n-        self.params = [\n-            self.W_z, self.U_z, self.b_z,\n-            self.W_r, self.U_r, self.b_r,\n-            self.W_h, self.U_h, self.b_h,\n-        ]\n+        self.b_h = K.zeros((self.output_dim,))\n+\n+        self.params = [self.W_z, self.U_z, self.b_z,\n+                       self.W_r, self.U_r, self.b_r,\n+                       self.W_h, self.U_h, self.b_h]\n+\n+        if self.stateful:\n+            if not input_shape[0]:\n+                raise Exception('If a RNN is stateful, a complete ' +\n+                                'input_shape must be provided ' +\n+                                '(including batch size).')\n+            self.states = [K.zeros(input_shape[0], self.output_dim)]\n+        else:\n+            # initial states: all-zero tensor of shape (output_dim)\n+            self.states = [None]\n \n         if self.initial_weights is not None:\n             self.set_weights(self.initial_weights)\n             del self.initial_weights\n \n-    def _step(self,\n-              xz_t, xr_t, xh_t, mask_tm1,\n-              h_tm1,\n-              u_z, u_r, u_h):\n-        h_mask_tm1 = mask_tm1 * h_tm1\n-        z = self.inner_activation(xz_t + T.dot(h_mask_tm1, u_z))\n-        r = self.inner_activation(xr_t + T.dot(h_mask_tm1, u_r))\n-        hh_t = self.activation(xh_t + T.dot(r * h_mask_tm1, u_h))\n-        h_t = z * h_mask_tm1 + (1 - z) * hh_t\n-        return h_t\n+    def step(self, x, states):\n+        assert len(states) == 1\n+        x_z = K.dot(x, self.W_z) + self.b_z\n+        x_r = K.dot(x, self.W_r) + self.b_r\n+        x_h = K.dot(x, self.W_h) + self.b_h\n \n-    def get_output(self, train=False):\n-        X = self.get_input(train)\n-        padded_mask = self.get_padded_shuffled_mask(train, X, pad=1)\n-        X = X.dimshuffle((1, 0, 2))\n-\n-        x_z = T.dot(X, self.W_z) + self.b_z\n-        x_r = T.dot(X, self.W_r) + self.b_r\n-        x_h = T.dot(X, self.W_h) + self.b_h\n-        outputs, updates = theano.scan(\n-            self._step,\n-            sequences=[x_z, x_r, x_h, padded_mask],\n-            outputs_info=T.unbroadcast(alloc_zeros_matrix(X.shape[1], self.output_dim), 1),\n-            non_sequences=[self.U_z, self.U_r, self.U_h],\n-            truncate_gradient=self.truncate_gradient,\n-            go_backwards=self.go_backwards)\n+        h_tm1 = states[0]\n+        z = self.inner_activation(x_z + K.dot(h_tm1, self.U_z))\n+        r = self.inner_activation(x_r + K.dot(h_tm1, self.U_r))\n \n-        if self.return_sequences:\n-            return outputs.dimshuffle((1, 0, 2))\n-        return outputs[-1]\n+        hh = self.inner_activation(x_h + K.dot(r * h_tm1, self.U_h))\n+        h = z * h_tm1 + (1 - z) * hh\n+        return h, [h]\n \n     def get_config(self):\n-        config = {\"name\": self.__class__.__name__,\n-                  \"output_dim\": self.output_dim,\n+        config = {\"output_dim\": self.output_dim,\n                   \"init\": self.init.__name__,\n                   \"inner_init\": self.inner_init.__name__,\n                   \"activation\": self.activation.__name__,\n-                  \"inner_activation\": self.inner_activation.__name__,\n-                  \"truncate_gradient\": self.truncate_gradient,\n-                  \"return_sequences\": self.return_sequences,\n-                  \"input_dim\": self.input_dim,\n-                  \"input_length\": self.input_length,\n-                  \"go_backwards\": self.go_backwards}\n+                  \"inner_activation\": self.inner_activation.__name__}\n         base_config = super(GRU, self).get_config()\n         return dict(list(base_config.items()) + list(config.items()))\n \n@@ -353,8 +253,10 @@ class LSTM(Recurrent):\n         Acts as a spatiotemporal projection,\n         turning a sequence of vectors into a single vector.\n \n-        Eats inputs with shape:\n-        (nb_samples, max_sample_length (samples shorter than this are padded with zeros at the end), input_dim)\n+        Takes inputs with shape:\n+        (nb_samples, max_sample_length, input_dim)\n+        (samples shorter than `max_sample_length`\n+         are padded with zeros at the end)\n \n         and returns outputs with shape:\n         if not return_sequences:\n@@ -374,34 +276,37 @@ class LSTM(Recurrent):\n                 http://www.cs.toronto.edu/~graves/preprint.pdf\n     '''\n     def __init__(self, output_dim,\n-                 init='glorot_uniform', inner_init='orthogonal', forget_bias_init='one',\n-                 activation='tanh', inner_activation='hard_sigmoid',\n-                 weights=None, truncate_gradient=-1, return_sequences=False,\n-                 input_dim=None, input_length=None, go_backwards=False, **kwargs):\n+                 init='glorot_uniform', inner_init='orthogonal',\n+                 forget_bias_init='one', activation='tanh',\n+                 inner_activation='hard_sigmoid', **kwargs):\n         self.output_dim = output_dim\n         self.init = initializations.get(init)\n         self.inner_init = initializations.get(inner_init)\n         self.forget_bias_init = initializations.get(forget_bias_init)\n         self.activation = activations.get(activation)\n         self.inner_activation = activations.get(inner_activation)\n-        self.truncate_gradient = truncate_gradient\n-        self.return_sequences = return_sequences\n-        self.initial_weights = weights\n-        self.go_backwards = go_backwards\n-\n-        self.input_dim = input_dim\n-        self.input_length = input_length\n-        if self.input_dim:\n-            kwargs['input_shape'] = (self.input_length, self.input_dim)\n         super(LSTM, self).__init__(**kwargs)\n \n     def build(self):\n-        input_dim = self.input_shape[2]\n-        self.input = T.tensor3()\n+        input_shape = self.input_shape\n+        input_dim = input_shape[2]\n+        self.input_dim = input_dim\n+        self.input = K.placeholder(input_shape)\n+\n+        if self.stateful:\n+            if not input_shape[0]:\n+                raise Exception('If a RNN is stateful, a complete ' +\n+                                'input_shape must be provided ' +\n+                                '(including batch size).')\n+            self.states = [K.zeros(input_shape[0], self.output_dim),\n+                           K.zeros(input_shape[0], self.output_dim)]\n+        else:\n+            # initial states: 2 all-zero tensor of shape (output_dim)\n+            self.states = [None, None]\n \n         self.W_i = self.init((input_dim, self.output_dim))\n         self.U_i = self.inner_init((self.output_dim, self.output_dim))\n-        self.b_i = shared_zeros((self.output_dim))\n+        self.b_i = K.zeros((self.output_dim))\n \n         self.W_f = self.init((input_dim, self.output_dim))\n         self.U_f = self.inner_init((self.output_dim, self.output_dim))\n@@ -409,94 +314,55 @@ class LSTM(Recurrent):\n \n         self.W_c = self.init((input_dim, self.output_dim))\n         self.U_c = self.inner_init((self.output_dim, self.output_dim))\n-        self.b_c = shared_zeros((self.output_dim))\n+        self.b_c = K.zeros((self.output_dim))\n \n         self.W_o = self.init((input_dim, self.output_dim))\n         self.U_o = self.inner_init((self.output_dim, self.output_dim))\n-        self.b_o = shared_zeros((self.output_dim))\n+        self.b_o = K.zeros((self.output_dim))\n \n-        self.params = [\n-            self.W_i, self.U_i, self.b_i,\n-            self.W_c, self.U_c, self.b_c,\n-            self.W_f, self.U_f, self.b_f,\n-            self.W_o, self.U_o, self.b_o,\n-        ]\n+        self.params = [self.W_i, self.U_i, self.b_i,\n+                       self.W_c, self.U_c, self.b_c,\n+                       self.W_f, self.U_f, self.b_f,\n+                       self.W_o, self.U_o, self.b_o]\n \n         if self.initial_weights is not None:\n             self.set_weights(self.initial_weights)\n             del self.initial_weights\n \n-    def _step(self,\n-              xi_t, xf_t, xo_t, xc_t, mask_tm1,\n-              h_tm1, c_tm1,\n-              u_i, u_f, u_o, u_c):\n-        h_mask_tm1 = mask_tm1 * h_tm1\n-        c_mask_tm1 = mask_tm1 * c_tm1\n-\n-        i_t = self.inner_activation(xi_t + T.dot(h_mask_tm1, u_i))\n-        f_t = self.inner_activation(xf_t + T.dot(h_mask_tm1, u_f))\n-        c_t = f_t * c_mask_tm1 + i_t * self.activation(xc_t + T.dot(h_mask_tm1, u_c))\n-        o_t = self.inner_activation(xo_t + T.dot(h_mask_tm1, u_o))\n-        h_t = o_t * self.activation(c_t)\n-        return h_t, c_t\n+    def step(self, x, states):\n+        assert len(states) == 2\n+        h_tm1 = states[0]\n+        c_tm1 = states[1]\n \n-    def get_output(self, train=False):\n-        X = self.get_input(train)\n-        padded_mask = self.get_padded_shuffled_mask(train, X, pad=1)\n-        X = X.dimshuffle((1, 0, 2))\n-\n-        xi = T.dot(X, self.W_i) + self.b_i\n-        xf = T.dot(X, self.W_f) + self.b_f\n-        xc = T.dot(X, self.W_c) + self.b_c\n-        xo = T.dot(X, self.W_o) + self.b_o\n-\n-        [outputs, memories], updates = theano.scan(\n-            self._step,\n-            sequences=[xi, xf, xo, xc, padded_mask],\n-            outputs_info=[\n-                T.unbroadcast(alloc_zeros_matrix(X.shape[1], self.output_dim), 1),\n-                T.unbroadcast(alloc_zeros_matrix(X.shape[1], self.output_dim), 1)\n-            ],\n-            non_sequences=[self.U_i, self.U_f, self.U_o, self.U_c],\n-            truncate_gradient=self.truncate_gradient,\n-            go_backwards=self.go_backwards)\n+        x_i = K.dot(x, self.W_i) + self.b_i\n+        x_f = K.dot(x, self.W_f) + self.b_f\n+        x_c = K.dot(x, self.W_c) + self.b_c\n+        x_o = K.dot(x, self.W_o) + self.b_o\n \n-        if self.return_sequences:\n-            return outputs.dimshuffle((1, 0, 2))\n-        return outputs[-1]\n+        i = self.inner_activation(x_i + K.dot(h_tm1, self.U_i))\n+        f = self.inner_activation(x_f + K.dot(h_tm1, self.U_f))\n+        c = f * c_tm1 + i * self.activation(x_c + K.dot(h_tm1, self.U_c))\n+        o = self.inner_activation(x_o + K.dot(h_tm1, self.U_o))\n+        h = o * self.activation(c)\n+        return h, [h, c]\n \n     def get_config(self):\n-        config = {\"name\": self.__class__.__name__,\n-                  \"output_dim\": self.output_dim,\n+        config = {\"output_dim\": self.output_dim,\n                   \"init\": self.init.__name__,\n                   \"inner_init\": self.inner_init.__name__,\n                   \"forget_bias_init\": self.forget_bias_init.__name__,\n                   \"activation\": self.activation.__name__,\n-                  \"inner_activation\": self.inner_activation.__name__,\n-                  \"truncate_gradient\": self.truncate_gradient,\n-                  \"return_sequences\": self.return_sequences,\n-                  \"input_dim\": self.input_dim,\n-                  \"input_length\": self.input_length,\n-                  \"go_backwards\": self.go_backwards}\n+                  \"inner_activation\": self.inner_activation.__name__}\n         base_config = super(LSTM, self).get_config()\n         return dict(list(base_config.items()) + list(config.items()))\n \n \n-class JZS1(Recurrent):\n+class JZS(Recurrent):\n     '''\n-        Evolved recurrent neural network architectures from the evaluation of thousands\n-        of models, serving as alternatives to LSTMs and GRUs. See Jozefowicz et al. 2015.\n-\n-        This corresponds to the `MUT1` architecture described in the paper.\n-\n-        Takes inputs with shape:\n-        (nb_samples, max_sample_length (samples shorter than this are padded with zeros at the end), input_dim)\n-\n-        and returns outputs with shape:\n-        if not return_sequences:\n-            (nb_samples, output_dim)\n-        if return_sequences:\n-            (nb_samples, max_sample_length, output_dim)\n+        Evolved recurrent neural network architectures\n+        from the evaluation of thousands\n+        of models, serving as alternatives to LSTMs and GRUs.\n+        See Jozefowicz et al. 2015.\n \n         References:\n             An Empirical Exploration of Recurrent Network Architectures\n@@ -505,334 +371,146 @@ class JZS1(Recurrent):\n     def __init__(self, output_dim,\n                  init='glorot_uniform', inner_init='orthogonal',\n                  activation='tanh', inner_activation='sigmoid',\n-                 weights=None, truncate_gradient=-1, return_sequences=False,\n-                 input_dim=None, input_length=None, go_backwards=False, **kwargs):\n+                 **kwargs):\n         self.output_dim = output_dim\n         self.init = initializations.get(init)\n         self.inner_init = initializations.get(inner_init)\n         self.activation = activations.get(activation)\n         self.inner_activation = activations.get(inner_activation)\n-        self.truncate_gradient = truncate_gradient\n-        self.return_sequences = return_sequences\n-        self.initial_weights = weights\n-        self.go_backwards = go_backwards\n-\n-        self.input_dim = input_dim\n-        self.input_length = input_length\n-        if self.input_dim:\n-            kwargs['input_shape'] = (self.input_length, self.input_dim)\n-        super(JZS1, self).__init__(**kwargs)\n+        super(JZS, self).__init__(**kwargs)\n \n     def build(self):\n-        input_dim = self.input_shape[2]\n-        self.input = T.tensor3()\n+        input_shape = self.input_shape\n+        input_dim = input_shape[2]\n+        self.input_dim = input_dim\n+        self.input = K.placeholder(input_shape)\n+\n+        if self.stateful:\n+            if not input_shape[0]:\n+                raise Exception('If a RNN is stateful, a complete ' +\n+                                'input_shape must be provided ' +\n+                                '(including batch size).')\n+            self.states = [K.zeros(input_shape[0], self.output_dim)]\n+        else:\n+            # initial states: all-zero tensor of shape (output_dim)\n+            self.states = [None]\n \n         self.W_z = self.init((input_dim, self.output_dim))\n-        self.b_z = shared_zeros((self.output_dim))\n+        self.U_z = self.inner_init((self.output_dim, self.output_dim))\n+        self.b_z = K.zeros((self.output_dim))\n \n         self.W_r = self.init((input_dim, self.output_dim))\n         self.U_r = self.inner_init((self.output_dim, self.output_dim))\n-        self.b_r = shared_zeros((self.output_dim))\n+        self.b_r = K.zeros((self.output_dim))\n \n+        self.W_h = self.init((input_dim, self.output_dim))\n         self.U_h = self.inner_init((self.output_dim, self.output_dim))\n-        self.b_h = shared_zeros((self.output_dim))\n+        self.b_h = K.zeros((self.output_dim))\n \n-        # P_h used to project X onto different dimension, using sparse random projections\n+        # P matrix used to project X onto different dimension,\n+        # using sparse random projections\n         if input_dim == self.output_dim:\n-            self.Pmat = theano.shared(np.identity(self.output_dim, dtype=theano.config.floatX), name=None)\n+            self.P = K.variable(np.identity(self.output_dim))\n         else:\n-            P = np.random.binomial(1, 0.5, size=(input_dim, self.output_dim)).astype(theano.config.floatX) * 2 - 1\n-            P = 1 / np.sqrt(input_dim) * P\n-            self.Pmat = theano.shared(P, name=None)\n+            P = np.random.binomial(1, 0.5, size=(input_dim, self.output_dim))\n+            P = 1. / np.sqrt(input_dim) * (P * 2. - 1.)\n+            self.P = K.variable(P)\n \n-        self.params = [\n-            self.W_z, self.b_z,\n-            self.W_r, self.U_r, self.b_r,\n-            self.U_h, self.b_h,\n-            self.Pmat\n-        ]\n+        self.params = [self.W_z, self.b_z,\n+                       self.W_r, self.U_r, self.b_r,\n+                       self.U_h, self.b_h,\n+                       self.P]\n \n         if self.initial_weights is not None:\n             self.set_weights(self.initial_weights)\n             del self.initial_weights\n \n-    def _step(self,\n-              xz_t, xr_t, xh_t, mask_tm1,\n-              h_tm1,\n-              u_r, u_h):\n-        h_mask_tm1 = mask_tm1 * h_tm1\n-        z = self.inner_activation(xz_t)\n-        r = self.inner_activation(xr_t + T.dot(h_mask_tm1, u_r))\n-        hh_t = self.activation(xh_t + T.dot(r * h_mask_tm1, u_h))\n-        h_t = hh_t * z + h_mask_tm1 * (1 - z)\n-        return h_t\n-\n-    def get_output(self, train=False):\n-        X = self.get_input(train)\n-        padded_mask = self.get_padded_shuffled_mask(train, X, pad=1)\n-        X = X.dimshuffle((1, 0, 2))\n-\n-        x_z = T.dot(X, self.W_z) + self.b_z\n-        x_r = T.dot(X, self.W_r) + self.b_r\n-        x_h = T.tanh(T.dot(X, self.Pmat)) + self.b_h\n-        outputs, updates = theano.scan(\n-            self._step,\n-            sequences=[x_z, x_r, x_h, padded_mask],\n-            outputs_info=T.unbroadcast(alloc_zeros_matrix(X.shape[1], self.output_dim), 1),\n-            non_sequences=[self.U_r, self.U_h],\n-            truncate_gradient=self.truncate_gradient,\n-            go_backwards=self.go_backwards)\n-        if self.return_sequences:\n-            return outputs.dimshuffle((1, 0, 2))\n-        return outputs[-1]\n-\n     def get_config(self):\n-        config = {\"name\": self.__class__.__name__,\n-                  \"output_dim\": self.output_dim,\n+        config = {\"output_dim\": self.output_dim,\n                   \"init\": self.init.__name__,\n                   \"inner_init\": self.inner_init.__name__,\n                   \"activation\": self.activation.__name__,\n-                  \"inner_activation\": self.inner_activation.__name__,\n-                  \"truncate_gradient\": self.truncate_gradient,\n-                  \"return_sequences\": self.return_sequences,\n-                  \"input_dim\": self.input_dim,\n-                  \"input_length\": self.input_length,\n-                  \"go_backwards\": self.go_backwards}\n-        base_config = super(JZS1, self).get_config()\n+                  \"inner_activation\": self.inner_activation.__name__}\n+        base_config = super(JZS, self).get_config()\n         return dict(list(base_config.items()) + list(config.items()))\n \n \n-class JZS2(Recurrent):\n+class JZS1(JZS):\n     '''\n-        Evolved recurrent neural network architectures from the evaluation of thousands\n-        of models, serving as alternatives to LSTMs and GRUs. See Jozefowicz et al. 2015.\n+        Evolved recurrent neural network architectures\n+        from the evaluation of thousands\n+        of models, serving as alternatives to LSTMs and GRUs.\n+        See Jozefowicz et al. 2015.\n \n-        This corresponds to the `MUT2` architecture described in the paper.\n-\n-        Takes inputs with shape:\n-        (nb_samples, max_sample_length (samples shorter than this are padded with zeros at the end), input_dim)\n-\n-        and returns outputs with shape:\n-        if not return_sequences:\n-            (nb_samples, output_dim)\n-        if return_sequences:\n-            (nb_samples, max_sample_length, output_dim)\n-\n-        References:\n-            An Empirical Exploration of Recurrent Network Architectures\n-                http://www.jmlr.org/proceedings/papers/v37/jozefowicz15.pdf\n+        This corresponds to the `MUT1` architecture described in the paper.\n     '''\n-    def __init__(self, output_dim,\n-                 init='glorot_uniform', inner_init='orthogonal',\n-                 activation='tanh', inner_activation='sigmoid',\n-                 weights=None, truncate_gradient=-1, return_sequences=False,\n-                 input_dim=None, input_length=None, go_backwards=False, **kwargs):\n-        self.output_dim = output_dim\n-        self.init = initializations.get(init)\n-        self.inner_init = initializations.get(inner_init)\n-        self.activation = activations.get(activation)\n-        self.inner_activation = activations.get(inner_activation)\n-        self.truncate_gradient = truncate_gradient\n-        self.return_sequences = return_sequences\n-        self.initial_weights = weights\n-        self.go_backwards = go_backwards\n+    def __init__(self, *args, **kwargs):\n+        super(JZS1, self).__init__(*args, **kwargs)\n \n-        self.input_dim = input_dim\n-        self.input_length = input_length\n-        if self.input_dim:\n-            kwargs['input_shape'] = (self.input_length, self.input_dim)\n-        super(JZS2, self).__init__(**kwargs)\n+    def step(self, x, states):\n+        assert len(states) == 1\n+        h_tm1 = states[0]\n \n-    def build(self):\n-        input_dim = self.input_shape[2]\n-        self.input = T.tensor3()\n+        x_z = K.dot(x, self.W_z) + self.b_z\n+        x_r = K.dot(x, self.P) + self.b_r\n+        x_h = K.dot(x, self.W_h) + self.b_h\n \n-        self.W_z = self.init((input_dim, self.output_dim))\n-        self.U_z = self.inner_init((self.output_dim, self.output_dim))\n-        self.b_z = shared_zeros((self.output_dim))\n-\n-        self.U_r = self.inner_init((self.output_dim, self.output_dim))\n-        self.b_r = shared_zeros((self.output_dim))\n+        z = self.inner_activation(x_z)\n+        r = self.inner_activation(x_r + K.dot(h_tm1, self.U_r))\n+        hh = self.activation(x_h + K.dot(r * h_tm1, self.U_h))\n+        h = hh * z + h_tm1 * (1. - z)\n+        return h, [h]\n \n-        self.W_h = self.init((input_dim, self.output_dim))\n-        self.U_h = self.inner_init((self.output_dim, self.output_dim))\n-        self.b_h = shared_zeros((self.output_dim))\n \n-        # P_h used to project X onto different dimension, using sparse random projections\n-        if input_dim == self.output_dim:\n-            self.Pmat = theano.shared(np.identity(self.output_dim, dtype=theano.config.floatX), name=None)\n-        else:\n-            P = np.random.binomial(1, 0.5, size=(input_dim, self.output_dim)).astype(theano.config.floatX) * 2 - 1\n-            P = 1 / np.sqrt(input_dim) * P\n-            self.Pmat = theano.shared(P, name=None)\n+class JZS2(JZS):\n+    '''\n+        Evolved recurrent neural network architectures\n+        from the evaluation of thousands\n+        of models, serving as alternatives to LSTMs and GRUs.\n+        See Jozefowicz et al. 2015.\n \n-        self.params = [\n-            self.W_z, self.U_z, self.b_z,\n-            self.U_r, self.b_r,\n-            self.W_h, self.U_h, self.b_h,\n-            self.Pmat\n-        ]\n+        This corresponds to the `MUT2` architecture described in the paper.\n+    '''\n+    def __init__(self, *args, **kwargs):\n+        super(JZS2, self).__init__(*args, **kwargs)\n \n-        if self.initial_weights is not None:\n-            self.set_weights(self.initial_weights)\n-            del self.initial_weights\n+    def step(self, x, states):\n+        assert len(states) == 1\n+        h_tm1 = states[0]\n \n-    def _step(self,\n-              xz_t, xr_t, xh_t, mask_tm1,\n-              h_tm1,\n-              u_z, u_r, u_h):\n-        h_mask_tm1 = mask_tm1 * h_tm1\n-        z = self.inner_activation(xz_t + T.dot(h_mask_tm1, u_z))\n-        r = self.inner_activation(xr_t + T.dot(h_mask_tm1, u_r))\n-        hh_t = self.activation(xh_t + T.dot(r * h_mask_tm1, u_h))\n-        h_t = hh_t * z + h_mask_tm1 * (1 - z)\n-        return h_t\n+        x_z = K.dot(x, self.W_z) + self.b_z\n+        x_r = K.dot(x, self.P) + self.b_r\n+        x_h = K.dot(x, self.W_h) + self.b_h\n \n-    def get_output(self, train=False):\n-        X = self.get_input(train)\n-        padded_mask = self.get_padded_shuffled_mask(train, X, pad=1)\n-        X = X.dimshuffle((1, 0, 2))\n-\n-        x_z = T.dot(X, self.W_z) + self.b_z\n-        x_r = T.dot(X, self.Pmat) + self.b_r\n-        x_h = T.dot(X, self.W_h) + self.b_h\n-        outputs, updates = theano.scan(\n-            self._step,\n-            sequences=[x_z, x_r, x_h, padded_mask],\n-            outputs_info=T.unbroadcast(alloc_zeros_matrix(X.shape[1], self.output_dim), 1),\n-            non_sequences=[self.U_z, self.U_r, self.U_h],\n-            truncate_gradient=self.truncate_gradient,\n-            go_backwards=self.go_backwards)\n-\n-        if self.return_sequences:\n-            return outputs.dimshuffle((1, 0, 2))\n-        return outputs[-1]\n-\n-    def get_config(self):\n-        config = {\"name\": self.__class__.__name__,\n-                  \"output_dim\": self.output_dim,\n-                  \"init\": self.init.__name__,\n-                  \"inner_init\": self.inner_init.__name__,\n-                  \"activation\": self.activation.__name__,\n-                  \"inner_activation\": self.inner_activation.__name__,\n-                  \"truncate_gradient\": self.truncate_gradient,\n-                  \"return_sequences\": self.return_sequences,\n-                  \"input_dim\": self.input_dim,\n-                  \"input_length\": self.input_length,\n-                  \"go_backwards\": self.go_backwards}\n-        base_config = super(JZS2, self).get_config()\n-        return dict(list(base_config.items()) + list(config.items()))\n+        z = self.inner_activation(x_z + K.dot(h_tm1, self.U_z))\n+        r = self.inner_activation(x_r + K.dot(h_tm1, self.U_r))\n+        hh = self.activation(x_h + K.dot(r * h_tm1, self.U_h))\n+        h = hh * z + h_tm1 * (1. - z)\n+        return h, [h]\n \n \n class JZS3(Recurrent):\n     '''\n-        Evolved recurrent neural network architectures from the evaluation of thousands\n-        of models, serving as alternatives to LSTMs and GRUs. See Jozefowicz et al. 2015.\n+        Evolved recurrent neural network architectures\n+        from the evaluation of thousands\n+        of models, serving as alternatives to LSTMs and GRUs.\n+        See Jozefowicz et al. 2015.\n \n         This corresponds to the `MUT3` architecture described in the paper.\n-\n-        Takes inputs with shape:\n-        (nb_samples, max_sample_length (samples shorter than this are padded with zeros at the end), input_dim)\n-\n-        and returns outputs with shape:\n-        if not return_sequences:\n-            (nb_samples, output_dim)\n-        if return_sequences:\n-            (nb_samples, max_sample_length, output_dim)\n-\n-        References:\n-            An Empirical Exploration of Recurrent Network Architectures\n-                http://www.jmlr.org/proceedings/papers/v37/jozefowicz15.pdf\n     '''\n-    def __init__(self, output_dim,\n-                 init='glorot_uniform', inner_init='orthogonal',\n-                 activation='tanh', inner_activation='sigmoid',\n-                 weights=None, truncate_gradient=-1, return_sequences=False,\n-                 input_dim=None, input_length=None, go_backwards=False, **kwargs):\n-        self.output_dim = output_dim\n-        self.init = initializations.get(init)\n-        self.inner_init = initializations.get(inner_init)\n-        self.activation = activations.get(activation)\n-        self.inner_activation = activations.get(inner_activation)\n-        self.truncate_gradient = truncate_gradient\n-        self.return_sequences = return_sequences\n-        self.initial_weights = weights\n-        self.go_backwards = go_backwards\n-\n-        self.input_dim = input_dim\n-        self.input_length = input_length\n-        if self.input_dim:\n-            kwargs['input_shape'] = (self.input_length, self.input_dim)\n-        super(JZS3, self).__init__(**kwargs)\n-\n-    def build(self):\n-        input_dim = self.input_shape[2]\n-        self.input = T.tensor3()\n-\n-        self.W_z = self.init((input_dim, self.output_dim))\n-        self.U_z = self.inner_init((self.output_dim, self.output_dim))\n-        self.b_z = shared_zeros((self.output_dim))\n-\n-        self.W_r = self.init((input_dim, self.output_dim))\n-        self.U_r = self.inner_init((self.output_dim, self.output_dim))\n-        self.b_r = shared_zeros((self.output_dim))\n-\n-        self.W_h = self.init((input_dim, self.output_dim))\n-        self.U_h = self.inner_init((self.output_dim, self.output_dim))\n-        self.b_h = shared_zeros((self.output_dim))\n-\n-        self.params = [\n-            self.W_z, self.U_z, self.b_z,\n-            self.W_r, self.U_r, self.b_r,\n-            self.W_h, self.U_h, self.b_h,\n-        ]\n-\n-        if self.initial_weights is not None:\n-            self.set_weights(self.initial_weights)\n-            del self.initial_weights\n-\n-    def _step(self,\n-              xz_t, xr_t, xh_t, mask_tm1,\n-              h_tm1,\n-              u_z, u_r, u_h):\n-        h_mask_tm1 = mask_tm1 * h_tm1\n-        z = self.inner_activation(xz_t + T.dot(T.tanh(h_mask_tm1), u_z))\n-        r = self.inner_activation(xr_t + T.dot(h_mask_tm1, u_r))\n-        hh_t = self.activation(xh_t + T.dot(r * h_mask_tm1, u_h))\n-        h_t = hh_t * z + h_mask_tm1 * (1 - z)\n-        return h_t\n-\n-    def get_output(self, train=False):\n-        X = self.get_input(train)\n-        padded_mask = self.get_padded_shuffled_mask(train, X, pad=1)\n-        X = X.dimshuffle((1, 0, 2))\n-\n-        x_z = T.dot(X, self.W_z) + self.b_z\n-        x_r = T.dot(X, self.W_r) + self.b_r\n-        x_h = T.dot(X, self.W_h) + self.b_h\n-        outputs, updates = theano.scan(\n-            self._step,\n-            sequences=[x_z, x_r, x_h, padded_mask],\n-            outputs_info=T.unbroadcast(alloc_zeros_matrix(X.shape[1], self.output_dim), 1),\n-            non_sequences=[self.U_z, self.U_r, self.U_h],\n-            truncate_gradient=self.truncate_gradient,\n-            go_backwards=self.go_backwards)\n-        \n-        if self.return_sequences:\n-            return outputs.dimshuffle((1, 0, 2))\n-        return outputs[-1]\n-\n-    def get_config(self):\n-        config = {\"name\": self.__class__.__name__,\n-                  \"output_dim\": self.output_dim,\n-                  \"init\": self.init.__name__,\n-                  \"inner_init\": self.inner_init.__name__,\n-                  \"activation\": self.activation.__name__,\n-                  \"inner_activation\": self.inner_activation.__name__,\n-                  \"truncate_gradient\": self.truncate_gradient,\n-                  \"return_sequences\": self.return_sequences,\n-                  \"input_dim\": self.input_dim,\n-                  \"input_length\": self.input_length,\n-                  \"go_backwards\": self.go_backwards}\n-        base_config = super(JZS3, self).get_config()\n-        return dict(list(base_config.items()) + list(config.items()))\n+    def __init__(self, *args, **kwargs):\n+        super(JZS3, self).__init__(*args, **kwargs)\n+\n+    def step(self, x, states):\n+        assert len(states) == 1\n+        h_tm1 = states[0]\n+\n+        x_z = K.dot(x, self.W_z) + self.b_z\n+        x_r = K.dot(x, self.P) + self.b_r\n+        x_h = K.dot(x, self.W_h) + self.b_h\n+\n+        z = self.inner_activation(x_z + K.dot(K.tanh(h_tm1), self.U_z))\n+        r = self.inner_activation(x_r + K.dot(h_tm1, self.U_r))\n+        hh = self.activation(x_h + K.dot(r * h_tm1, self.U_h))\n+        h = hh * z + h_tm1 * (1. - z)\n+        return h, [h]\n",
          "files_name_in_blame_commit": [
            "tensorflow_backend.py",
            "test_backends.py",
            "theano_backend.py",
            "recurrent.py",
            "core.py",
            "test_recurrent.py"
          ]
        }
      }
    }
  }
}