{
  "id": "43",
  "blame_commit": {
    "commit": {
      "commit_id": "045880c29e4d40dc0458d2a3272d145174b5122d",
      "commit_message": "BUG: Don't cast categorical nan to int (#28438)",
      "commit_author": "Daniel Saxton",
      "commit_date": "2019-09-18 08:35:03",
      "commit_parent": "367670e80ff008d77c791437a7a5df3e1abe012b"
    },
    "function": {
      "function_name": "get_indexer_non_unique",
      "function_code_before": "@Appender(_index_shared_docs['get_indexer_non_unique'] % _index_doc_kwargs)\ndef get_indexer_non_unique(self, target):\n    target = ensure_index(target)\n    if is_categorical(target):\n        target = target.astype(target.dtype.categories.dtype)\n    (pself, ptarget) = self._maybe_promote(target)\n    if pself is not self or ptarget is not target:\n        return pself.get_indexer_non_unique(ptarget)\n    if self.is_all_dates:\n        tgt_values = target.asi8\n    else:\n        tgt_values = target._ndarray_values\n    (indexer, missing) = self._engine.get_indexer_non_unique(tgt_values)\n    return (ensure_platform_int(indexer), missing)",
      "function_code_after": "@Appender(_index_shared_docs['get_indexer_non_unique'] % _index_doc_kwargs)\ndef get_indexer_non_unique(self, target):\n    target = ensure_index(target)\n    (pself, ptarget) = self._maybe_promote(target)\n    if pself is not self or ptarget is not target:\n        return pself.get_indexer_non_unique(ptarget)\n    if is_categorical(target):\n        tgt_values = np.asarray(target)\n    elif self.is_all_dates:\n        tgt_values = target.asi8\n    else:\n        tgt_values = target._ndarray_values\n    (indexer, missing) = self._engine.get_indexer_non_unique(tgt_values)\n    return (ensure_platform_int(indexer), missing)",
      "function_before_start_line": 4716,
      "function_before_end_line": 4730,
      "function_after_start_line": 4716,
      "function_after_end_line": 4730,
      "function_before_token_count": 97,
      "function_after_token_count": 91,
      "functions_name_modified_file": [
        "_get_fill_indexer_searchsorted",
        "_sort_levels_monotonic",
        "map",
        "to_flat_index",
        "format",
        "__or__",
        "_validate_sort_keyword",
        "sort_values",
        "is_floating",
        "memory_usage",
        "is_monotonic_decreasing",
        "_add_logical_methods_disabled",
        "_cleanup",
        "is_boolean",
        "_get_names",
        "_add_comparison_methods",
        "get_loc",
        "to_native_types",
        "_union_incompatible_dtypes",
        "is_lexsorted_for_tuple",
        "_convert_slice_indexer",
        "_format_space",
        "_has_complex_internals",
        "_reindex_non_unique",
        "_wrap_joined_index",
        "identical",
        "_get_unique_index",
        "_add_numeric_methods_disabled",
        "_internal_get_values",
        "dtype",
        "is_mixed",
        "__new__",
        "default_index",
        "_can_hold_identifiers_and_holds_name",
        "notna",
        "_engine",
        "_format_native_types",
        "unique",
        "__getitem__",
        "_get_grouper_for_level",
        "__sub__",
        "isin",
        "shape",
        "_is_memory_usage_qualified",
        "__setstate__",
        "_string_data_error",
        "_validate_join_method",
        "contains",
        "_outer_indexer",
        "has_duplicates",
        "__rsub__",
        "get_indexer_non_unique",
        "ensure_index",
        "drop_duplicates",
        "is_numeric",
        "_simple_new",
        "_validate_indexer",
        "_isnan",
        "set_names",
        "_values",
        "get_indexer_for",
        "_convert_for_op",
        "_update_inplace",
        "shift",
        "is_",
        "_validate_index_level",
        "_formatter_func",
        "_convert_can_do_setop",
        "_add_numeric_methods_add_sub_disabled",
        "hasnans",
        "_searchsorted_monotonic",
        "_join_multi",
        "nlevels",
        "_trim_front",
        "argsort",
        "_new_Index",
        "__contains__",
        "get_indexer",
        "_get_fill_indexer",
        "drop",
        "asof_locs",
        "_join_level",
        "take",
        "_make_arithmetic_op",
        "_is_strictly_monotonic_increasing",
        "_join_monotonic",
        "__xor__",
        "__reduce__",
        "_try_convert_to_int_index",
        "_add_numeric_methods_unary",
        "_get_level_number",
        "union",
        "_add_numeric_methods_binary",
        "holds_integer",
        "is_all_dates",
        "_invalid_indexer",
        "inferred_type",
        "__len__",
        "__nonzero__",
        "_wrap_setop_result",
        "ravel",
        "slice_indexer",
        "_assert_take_fillable",
        "_shallow_copy",
        "_nan_idxs",
        "astype",
        "groupby",
        "asof",
        "is_monotonic",
        "_assert_can_do_op",
        "append",
        "_inner_indexer",
        "_summary",
        "_convert_tolerance",
        "_is_compatible_with_other",
        "insert",
        "asi8",
        "_format_with_header",
        "_get_level_values",
        "__hash__",
        "isna",
        "_get_reconciled_name_object",
        "_convert_index_indexer",
        "__radd__",
        "_reset_identity",
        "is_categorical",
        "where",
        "is_integer",
        "_maybe_cast_slice_bound",
        "__array__",
        "dtype_str",
        "__add__",
        "is_interval",
        "_get_attributes_dict",
        "__repr__",
        "difference",
        "duplicated",
        "fillna",
        "_maybe_cast_indexer",
        "putmask",
        "_assert_can_do_setop",
        "_concat",
        "__deepcopy__",
        "_can_reindex",
        "sortlevel",
        "__iadd__",
        "copy",
        "is_type_compatible",
        "_mpl_repr",
        "__setitem__",
        "_make_comparison_op",
        "_left_indexer",
        "_format_data",
        "_add_logical_methods",
        "_is_strictly_monotonic_decreasing",
        "get_slice_bound",
        "_join_non_unique",
        "reindex",
        "_coerce_to_ndarray",
        "_format_attrs",
        "_add_numeric_methods",
        "is_monotonic_increasing",
        "set_value",
        "equals",
        "ensure_index_from_sequences",
        "__and__",
        "_get_string_slice",
        "get_duplicates",
        "is_unique",
        "rename",
        "intersection",
        "_convert_scalar_indexer",
        "_convert_list_indexer",
        "_to_safe_for_reshape",
        "_shallow_copy_with_infer",
        "__array_wrap__",
        "_coerce_scalar_to_index",
        "_set_names",
        "repeat",
        "get_value",
        "slice_locs",
        "_constructor",
        "_ensure_has_len",
        "get_values",
        "summary",
        "droplevel",
        "_get_nearest_indexer",
        "_left_indexer_unique",
        "_filter_indexer_tolerance",
        "_convert_listlike_indexer",
        "_validate_names",
        "is_object",
        "_maybe_promote",
        "delete",
        "values",
        "_union",
        "__copy__",
        "join",
        "to_series",
        "_scalar_data_error",
        "to_frame",
        "_concat_same_dtype",
        "dropna",
        "_convert_arr_indexer",
        "sort",
        "symmetric_difference",
        "view"
      ],
      "functions_name_all_files": [
        "_get_fill_indexer_searchsorted",
        "_sort_levels_monotonic",
        "map",
        "check_for_ordered",
        "format",
        "to_flat_index",
        "__or__",
        "test_divmod_series_array",
        "categories",
        "_validate_sort_keyword",
        "sort_values",
        "test_searchsorted",
        "memory_usage",
        "_recode_for_categories",
        "is_monotonic_decreasing",
        "is_floating",
        "_add_logical_methods_disabled",
        "_cleanup",
        "test_ravel",
        "is_boolean",
        "_get_names",
        "_check_divmod_op",
        "_add_comparison_methods",
        "is_dtype_equal",
        "get_loc",
        "to_native_types",
        "_union_incompatible_dtypes",
        "is_lexsorted_for_tuple",
        "_convert_slice_indexer",
        "_format_space",
        "nbytes",
        "_reindex_non_unique",
        "_wrap_joined_index",
        "_has_complex_internals",
        "_get_unique_index",
        "identical",
        "tolist",
        "ordered",
        "_add_numeric_methods_disabled",
        "test_fillna_limit_pad",
        "test_getitem_scalar",
        "_internal_get_values",
        "dtype",
        "is_mixed",
        "searchsorted",
        "__new__",
        "test_take",
        "_delegate_property_set",
        "_can_hold_identifiers_and_holds_name",
        "default_index",
        "notna",
        "_engine",
        "_format_native_types",
        "unique",
        "test_take_pandas_style_negative_raises",
        "as_unordered",
        "test_take_series",
        "codes",
        "add_categories",
        "__getitem__",
        "__init__",
        "_maybe_coerce_indexer",
        "_get_grouper_for_level",
        "__sub__",
        "describe",
        "isin",
        "shape",
        "data_for_sorting",
        "_is_memory_usage_qualified",
        "__setstate__",
        "_filter_indexer_tolerance",
        "_string_data_error",
        "_delegate_property_get",
        "as_ordered",
        "_validate_join_method",
        "contains",
        "remove_unused_categories",
        "_outer_indexer",
        "has_duplicates",
        "__rsub__",
        "get_indexer_non_unique",
        "ensure_index",
        "drop_duplicates",
        "is_numeric",
        "_simple_new",
        "test_add_series_with_extension_array",
        "_validate_indexer",
        "_isnan",
        "itemsize",
        "set_names",
        "_values",
        "get_indexer_for",
        "data_missing_for_sorting",
        "_convert_for_op",
        "_update_inplace",
        "take_nd",
        "shift",
        "is_",
        "_convert_listlike_indexer",
        "_delegate_method",
        "size",
        "_formatter_func",
        "_validate_index_level",
        "_convert_can_do_setop",
        "_add_numeric_methods_add_sub_disabled",
        "hasnans",
        "data_missing",
        "_get_repr",
        "_searchsorted_monotonic",
        "_join_multi",
        "_from_inferred_categories",
        "nlevels",
        "_compare_other",
        "base",
        "_ndarray_values",
        "argsort",
        "__contains__",
        "_new_Index",
        "_trim_front",
        "get_indexer",
        "_get_fill_indexer",
        "test_reindex_non_na_fill_value",
        "drop",
        "_factorize_from_iterables",
        "asof_locs",
        "_join_level",
        "take",
        "_make_arithmetic_op",
        "_is_strictly_monotonic_increasing",
        "_join_monotonic",
        "__xor__",
        "__reduce__",
        "_try_convert_to_int_index",
        "_validate",
        "test_value_counts",
        "_set_codes",
        "_add_numeric_methods_unary",
        "_get_level_number",
        "union",
        "_add_numeric_methods_binary",
        "holds_integer",
        "is_all_dates",
        "_invalid_indexer",
        "inferred_type",
        "__len__",
        "__nonzero__",
        "_wrap_setop_result",
        "test_fillna_length_mismatch",
        "make_data",
        "ravel",
        "slice_indexer",
        "_assert_take_fillable",
        "_shallow_copy",
        "_nan_idxs",
        "_reverse_indexer",
        "_values_for_factorize",
        "astype",
        "name",
        "groupby",
        "asof",
        "_repr_categories_info",
        "is_monotonic",
        "_assert_can_do_op",
        "append",
        "_convert_to_list_like",
        "_inner_indexer",
        "_summary",
        "delete",
        "rename_categories",
        "from_codes",
        "_is_compatible_with_other",
        "_convert_tolerance",
        "insert",
        "__array_ufunc__",
        "value_counts",
        "asi8",
        "test_take_empty",
        "_repr_categories",
        "_format_with_header",
        "remove_categories",
        "_get_level_values",
        "__hash__",
        "isna",
        "_get_reconciled_name_object",
        "mode",
        "_convert_index_indexer",
        "__radd__",
        "test_take_non_na_fill_value",
        "_reset_identity",
        "is_categorical",
        "where",
        "is_integer",
        "_factorize_from_iterable",
        "_maybe_cast_slice_bound",
        "__array__",
        "dtype_str",
        "__add__",
        "test_cast_nan_to_int",
        "_get_codes_for_values",
        "_values_for_argsort",
        "is_interval",
        "_formatter",
        "_get_attributes_dict",
        "data_for_grouping",
        "__repr__",
        "difference",
        "duplicated",
        "fillna",
        "_maybe_cast_indexer",
        "putmask",
        "_assert_can_do_setop",
        "_concat",
        "__deepcopy__",
        "_can_reindex",
        "sortlevel",
        "__iadd__",
        "copy",
        "is_type_compatible",
        "_mpl_repr",
        "__setitem__",
        "_cat_compare_op",
        "_repr_footer",
        "_make_comparison_op",
        "T",
        "_left_indexer",
        "_format_data",
        "_add_logical_methods",
        "max",
        "_values_for_rank",
        "_is_strictly_monotonic_decreasing",
        "get_slice_bound",
        "_from_sequence",
        "_join_non_unique",
        "_reduce",
        "reindex",
        "_coerce_to_ndarray",
        "_format_attrs",
        "set_categories",
        "is_monotonic_increasing",
        "set_value",
        "equals",
        "_add_numeric_methods",
        "_tidy_repr",
        "to_dense",
        "__and__",
        "ensure_index_from_sequences",
        "test_take_out_of_bounds_raises",
        "_get_string_slice",
        "get_duplicates",
        "__iter__",
        "is_unique",
        "rename",
        "intersection",
        "_convert_scalar_indexer",
        "_convert_list_indexer",
        "_to_safe_for_reshape",
        "na_value",
        "test_memory_usage",
        "data",
        "put",
        "_concat_same_type",
        "_shallow_copy_with_infer",
        "__array_wrap__",
        "index",
        "repeat",
        "categorical",
        "_set_names",
        "_coerce_scalar_to_index",
        "get_value",
        "slice_locs",
        "_constructor",
        "_ensure_has_len",
        "test_reindex",
        "test_fillna_limit_backfill",
        "test_arith_series_with_scalar",
        "get_values",
        "summary",
        "reorder_categories",
        "_set_categories",
        "_left_indexer_unique",
        "droplevel",
        "_get_nearest_indexer",
        "_validate_names",
        "_get_codes",
        "set_ordered",
        "_from_factorized",
        "is_object",
        "_maybe_promote",
        "values",
        "_set_dtype",
        "_union",
        "__copy__",
        "join",
        "to_series",
        "test_combine_add",
        "min",
        "_scalar_data_error",
        "_can_hold_na",
        "to_frame",
        "_concat_same_dtype",
        "test_take_negative",
        "dropna",
        "_convert_arr_indexer",
        "sort",
        "symmetric_difference",
        "view"
      ],
      "functions_name_co_evolved_modified_file": [
        "get_indexer_for"
      ],
      "functions_name_co_evolved_all_files": [
        "get_indexer_for",
        "astype",
        "test_cast_nan_to_int"
      ]
    },
    "file": {
      "file_name": "base.py",
      "file_nloc": 2884,
      "file_complexity": 759,
      "file_token_count": 15914,
      "file_before": "from datetime import datetime\nimport operator\nfrom textwrap import dedent\nfrom typing import Union\nimport warnings\n\nimport numpy as np\n\nfrom pandas._libs import algos as libalgos, index as libindex, lib\nimport pandas._libs.join as libjoin\nfrom pandas._libs.lib import is_datetime_array\nfrom pandas._libs.tslibs import OutOfBoundsDatetime, Timestamp\nfrom pandas._libs.tslibs.period import IncompatibleFrequency\nfrom pandas._libs.tslibs.timezones import tz_compare\nfrom pandas.compat import set_function_name\nfrom pandas.compat.numpy import function as nv\nfrom pandas.util._decorators import Appender, Substitution, cache_readonly\n\nfrom pandas.core.dtypes import concat as _concat\nfrom pandas.core.dtypes.cast import maybe_cast_to_integer_array\nfrom pandas.core.dtypes.common import (\n    ensure_categorical,\n    ensure_int64,\n    ensure_object,\n    ensure_platform_int,\n    is_bool,\n    is_bool_dtype,\n    is_categorical,\n    is_categorical_dtype,\n    is_datetime64_any_dtype,\n    is_datetime64tz_dtype,\n    is_dtype_equal,\n    is_extension_array_dtype,\n    is_float,\n    is_float_dtype,\n    is_hashable,\n    is_integer,\n    is_integer_dtype,\n    is_interval_dtype,\n    is_iterator,\n    is_list_like,\n    is_object_dtype,\n    is_period_dtype,\n    is_scalar,\n    is_signed_integer_dtype,\n    is_timedelta64_dtype,\n    is_unsigned_integer_dtype,\n    pandas_dtype,\n)\nfrom pandas.core.dtypes.concat import concat_compat\nfrom pandas.core.dtypes.generic import (\n    ABCCategorical,\n    ABCDataFrame,\n    ABCDatetimeArray,\n    ABCDatetimeIndex,\n    ABCIndexClass,\n    ABCMultiIndex,\n    ABCPandasArray,\n    ABCPeriodIndex,\n    ABCSeries,\n    ABCTimedeltaIndex,\n)\nfrom pandas.core.dtypes.missing import array_equivalent, isna\n\nfrom pandas.core import ops\nfrom pandas.core.accessor import CachedAccessor, DirNamesMixin\nimport pandas.core.algorithms as algos\nfrom pandas.core.arrays import ExtensionArray\nfrom pandas.core.base import IndexOpsMixin, PandasObject\nimport pandas.core.common as com\nfrom pandas.core.indexers import maybe_convert_indices\nfrom pandas.core.indexes.frozen import FrozenList\nimport pandas.core.missing as missing\nfrom pandas.core.ops import get_op_result_name\nfrom pandas.core.ops.invalid import make_invalid_op\nimport pandas.core.sorting as sorting\nfrom pandas.core.strings import StringMethods\n\nfrom pandas.io.formats.printing import (\n    default_pprint,\n    format_object_attrs,\n    format_object_summary,\n    pprint_thing,\n)\n\n__all__ = [\"Index\"]\n\n_unsortable_types = frozenset((\"mixed\", \"mixed-integer\"))\n\n_index_doc_kwargs = dict(\n    klass=\"Index\",\n    inplace=\"\",\n    target_klass=\"Index\",\n    raises_section=\"\",\n    unique=\"Index\",\n    duplicated=\"np.ndarray\",\n)\n_index_shared_docs = dict()\n\n\ndef _make_comparison_op(op, cls):\n    def cmp_method(self, other):\n        if isinstance(other, (np.ndarray, Index, ABCSeries, ExtensionArray)):\n            if other.ndim > 0 and len(self) != len(other):\n                raise ValueError(\"Lengths must match to compare\")\n\n        if is_object_dtype(self) and isinstance(other, ABCCategorical):\n            left = type(other)(self._values, dtype=other.dtype)\n            return op(left, other)\n        elif is_object_dtype(self) and not isinstance(self, ABCMultiIndex):\n            # don't pass MultiIndex\n            with np.errstate(all=\"ignore\"):\n                result = ops.comp_method_OBJECT_ARRAY(op, self.values, other)\n\n        else:\n            with np.errstate(all=\"ignore\"):\n                result = op(self.values, np.asarray(other))\n\n        if is_bool_dtype(result):\n            return result\n        return ops.invalid_comparison(self, other, op)\n\n    name = \"__{name}__\".format(name=op.__name__)\n    return set_function_name(cmp_method, name, cls)\n\n\ndef _make_arithmetic_op(op, cls):\n    def index_arithmetic_method(self, other):\n        if isinstance(other, (ABCSeries, ABCDataFrame, ABCTimedeltaIndex)):\n            return NotImplemented\n\n        from pandas import Series\n\n        result = op(Series(self), other)\n        if isinstance(result, tuple):\n            return (Index(result[0]), Index(result[1]))\n        return Index(result)\n\n    name = \"__{name}__\".format(name=op.__name__)\n    # TODO: docstring?\n    return set_function_name(index_arithmetic_method, name, cls)\n\n\nclass InvalidIndexError(Exception):\n    pass\n\n\n_o_dtype = np.dtype(object)\n_Identity = object\n\n\ndef _new_Index(cls, d):\n    \"\"\"\n    This is called upon unpickling, rather than the default which doesn't\n    have arguments and breaks __new__.\n    \"\"\"\n    # required for backward compat, because PI can't be instantiated with\n    # ordinals through __new__ GH #13277\n    if issubclass(cls, ABCPeriodIndex):\n        from pandas.core.indexes.period import _new_PeriodIndex\n\n        return _new_PeriodIndex(cls, **d)\n    return cls.__new__(cls, **d)\n\n\nclass Index(IndexOpsMixin, PandasObject):\n    \"\"\"\n    Immutable ndarray implementing an ordered, sliceable set. The basic object\n    storing axis labels for all pandas objects.\n\n    Parameters\n    ----------\n    data : array-like (1-dimensional)\n    dtype : NumPy dtype (default: object)\n        If dtype is None, we find the dtype that best fits the data.\n        If an actual dtype is provided, we coerce to that dtype if it's safe.\n        Otherwise, an error will be raised.\n    copy : bool\n        Make a copy of input ndarray\n    name : object\n        Name to be stored in the index\n    tupleize_cols : bool (default: True)\n        When True, attempt to create a MultiIndex if possible\n\n    See Also\n    --------\n    RangeIndex : Index implementing a monotonic integer range.\n    CategoricalIndex : Index of :class:`Categorical` s.\n    MultiIndex : A multi-level, or hierarchical, Index.\n    IntervalIndex : An Index of :class:`Interval` s.\n    DatetimeIndex, TimedeltaIndex, PeriodIndex\n    Int64Index, UInt64Index,  Float64Index\n\n    Notes\n    -----\n    An Index instance can **only** contain hashable objects\n\n    Examples\n    --------\n    >>> pd.Index([1, 2, 3])\n    Int64Index([1, 2, 3], dtype='int64')\n\n    >>> pd.Index(list('abc'))\n    Index(['a', 'b', 'c'], dtype='object')\n    \"\"\"\n\n    # tolist is not actually deprecated, just suppressed in the __dir__\n    _deprecations = DirNamesMixin._deprecations | frozenset([\"tolist\"])\n\n    # To hand over control to subclasses\n    _join_precedence = 1\n\n    # Cython methods; see github.com/cython/cython/issues/2647\n    #  for why we need to wrap these instead of making them class attributes\n    # Moreover, cython will choose the appropriate-dtyped sub-function\n    #  given the dtypes of the passed arguments\n    def _left_indexer_unique(self, left, right):\n        return libjoin.left_join_indexer_unique(left, right)\n\n    def _left_indexer(self, left, right):\n        return libjoin.left_join_indexer(left, right)\n\n    def _inner_indexer(self, left, right):\n        return libjoin.inner_join_indexer(left, right)\n\n    def _outer_indexer(self, left, right):\n        return libjoin.outer_join_indexer(left, right)\n\n    _typ = \"index\"\n    _data = None\n    _id = None\n    name = None\n    _comparables = [\"name\"]\n    _attributes = [\"name\"]\n    _is_numeric_dtype = False\n    _can_hold_na = True\n\n    # would we like our indexing holder to defer to us\n    _defer_to_indexing = False\n\n    # prioritize current class for _shallow_copy_with_infer,\n    # used to infer integers as datetime-likes\n    _infer_as_myclass = False\n\n    _engine_type = libindex.ObjectEngine\n    # whether we support partial string indexing. Overridden\n    # in DatetimeIndex and PeriodIndex\n    _supports_partial_string_indexing = False\n\n    _accessors = {\"str\"}\n\n    str = CachedAccessor(\"str\", StringMethods)\n\n    # --------------------------------------------------------------------\n    # Constructors\n\n    def __new__(\n        cls,\n        data=None,\n        dtype=None,\n        copy=False,\n        name=None,\n        fastpath=None,\n        tupleize_cols=True,\n        **kwargs\n    ) -> \"Index\":\n\n        from .range import RangeIndex\n        from pandas import PeriodIndex, DatetimeIndex, TimedeltaIndex\n        from .numeric import Float64Index, Int64Index, UInt64Index\n        from .interval import IntervalIndex\n        from .category import CategoricalIndex\n\n        if name is None and hasattr(data, \"name\"):\n            name = data.name\n\n        if fastpath is not None:\n            warnings.warn(\n                \"The 'fastpath' keyword is deprecated, and will be \"\n                \"removed in a future version.\",\n                FutureWarning,\n                stacklevel=2,\n            )\n            if fastpath:\n                return cls._simple_new(data, name)\n\n        if isinstance(data, ABCPandasArray):\n            # ensure users don't accidentally put a PandasArray in an index.\n            data = data.to_numpy()\n\n        # range\n        if isinstance(data, RangeIndex):\n            return RangeIndex(start=data, copy=copy, dtype=dtype, name=name)\n        elif isinstance(data, range):\n            return RangeIndex.from_range(data, dtype=dtype, name=name)\n\n        # categorical\n        elif is_categorical_dtype(data) or is_categorical_dtype(dtype):\n            return CategoricalIndex(data, dtype=dtype, copy=copy, name=name, **kwargs)\n\n        # interval\n        elif (\n            is_interval_dtype(data) or is_interval_dtype(dtype)\n        ) and not is_object_dtype(dtype):\n            closed = kwargs.get(\"closed\", None)\n            return IntervalIndex(data, dtype=dtype, name=name, copy=copy, closed=closed)\n\n        elif (\n            is_datetime64_any_dtype(data)\n            or is_datetime64_any_dtype(dtype)\n            or \"tz\" in kwargs\n        ):\n            if is_dtype_equal(_o_dtype, dtype):\n                # GH#23524 passing `dtype=object` to DatetimeIndex is invalid,\n                #  will raise in the where `data` is already tz-aware.  So\n                #  we leave it out of this step and cast to object-dtype after\n                #  the DatetimeIndex construction.\n                # Note we can pass copy=False because the .astype below\n                #  will always make a copy\n                result = DatetimeIndex(\n                    data, copy=False, name=name, **kwargs\n                )  # type: \"Index\"\n                return result.astype(object)\n            else:\n                return DatetimeIndex(data, copy=copy, name=name, dtype=dtype, **kwargs)\n\n        elif is_timedelta64_dtype(data) or is_timedelta64_dtype(dtype):\n            if is_dtype_equal(_o_dtype, dtype):\n                # Note we can pass copy=False because the .astype below\n                #  will always make a copy\n                result = TimedeltaIndex(data, copy=False, name=name, **kwargs)\n                return result.astype(object)\n            else:\n                return TimedeltaIndex(data, copy=copy, name=name, dtype=dtype, **kwargs)\n\n        elif is_period_dtype(data) and not is_object_dtype(dtype):\n            return PeriodIndex(data, copy=copy, name=name, **kwargs)\n\n        # extension dtype\n        elif is_extension_array_dtype(data) or is_extension_array_dtype(dtype):\n            data = np.asarray(data)\n            if not (dtype is None or is_object_dtype(dtype)):\n                # coerce to the provided dtype\n                ea_cls = dtype.construct_array_type()\n                data = ea_cls._from_sequence(data, dtype=dtype, copy=False)\n\n            # coerce to the object dtype\n            data = data.astype(object)\n            return Index(data, dtype=object, copy=copy, name=name, **kwargs)\n\n        # index-like\n        elif isinstance(data, (np.ndarray, Index, ABCSeries)):\n            if dtype is not None:\n                # we need to avoid having numpy coerce\n                # things that look like ints/floats to ints unless\n                # they are actually ints, e.g. '0' and 0.0\n                # should not be coerced\n                # GH 11836\n                if is_integer_dtype(dtype):\n                    inferred = lib.infer_dtype(data, skipna=False)\n                    if inferred == \"integer\":\n                        data = maybe_cast_to_integer_array(data, dtype, copy=copy)\n                    elif inferred in [\"floating\", \"mixed-integer-float\"]:\n                        if isna(data).any():\n                            raise ValueError(\"cannot convert float NaN to integer\")\n\n                        if inferred == \"mixed-integer-float\":\n                            data = maybe_cast_to_integer_array(data, dtype)\n\n                        # If we are actually all equal to integers,\n                        # then coerce to integer.\n                        try:\n                            return cls._try_convert_to_int_index(\n                                data, copy, name, dtype\n                            )\n                        except ValueError:\n                            pass\n\n                        # Return an actual float index.\n                        return Float64Index(data, copy=copy, dtype=dtype, name=name)\n\n                    elif inferred == \"string\":\n                        pass\n                    else:\n                        data = data.astype(dtype)\n                elif is_float_dtype(dtype):\n                    inferred = lib.infer_dtype(data, skipna=False)\n                    if inferred == \"string\":\n                        pass\n                    else:\n                        data = data.astype(dtype)\n                else:\n                    data = np.array(data, dtype=dtype, copy=copy)\n\n            # maybe coerce to a sub-class\n            if is_signed_integer_dtype(data.dtype):\n                return Int64Index(data, copy=copy, dtype=dtype, name=name)\n            elif is_unsigned_integer_dtype(data.dtype):\n                return UInt64Index(data, copy=copy, dtype=dtype, name=name)\n            elif is_float_dtype(data.dtype):\n                return Float64Index(data, copy=copy, dtype=dtype, name=name)\n            elif issubclass(data.dtype.type, np.bool) or is_bool_dtype(data):\n                subarr = data.astype(\"object\")\n            else:\n                subarr = com.asarray_tuplesafe(data, dtype=object)\n\n            # asarray_tuplesafe does not always copy underlying data,\n            # so need to make sure that this happens\n            if copy:\n                subarr = subarr.copy()\n\n            if dtype is None:\n                inferred = lib.infer_dtype(subarr, skipna=False)\n                if inferred == \"integer\":\n                    try:\n                        return cls._try_convert_to_int_index(subarr, copy, name, dtype)\n                    except ValueError:\n                        pass\n\n                    return Index(subarr, copy=copy, dtype=object, name=name)\n                elif inferred in [\"floating\", \"mixed-integer-float\", \"integer-na\"]:\n                    # TODO: Returns IntegerArray for integer-na case in the future\n                    return Float64Index(subarr, copy=copy, name=name)\n                elif inferred == \"interval\":\n                    try:\n                        return IntervalIndex(subarr, name=name, copy=copy)\n                    except ValueError:\n                        # GH27172: mixed closed Intervals --> object dtype\n                        pass\n                elif inferred == \"boolean\":\n                    # don't support boolean explicitly ATM\n                    pass\n                elif inferred != \"string\":\n                    if inferred.startswith(\"datetime\"):\n                        try:\n                            return DatetimeIndex(subarr, copy=copy, name=name, **kwargs)\n                        except (ValueError, OutOfBoundsDatetime):\n                            # GH 27011\n                            # If we have mixed timezones, just send it\n                            # down the base constructor\n                            pass\n\n                    elif inferred.startswith(\"timedelta\"):\n                        return TimedeltaIndex(subarr, copy=copy, name=name, **kwargs)\n                    elif inferred == \"period\":\n                        try:\n                            return PeriodIndex(subarr, name=name, **kwargs)\n                        except IncompatibleFrequency:\n                            pass\n            return cls._simple_new(subarr, name)\n\n        elif hasattr(data, \"__array__\"):\n            return Index(np.asarray(data), dtype=dtype, copy=copy, name=name, **kwargs)\n        elif data is None or is_scalar(data):\n            raise cls._scalar_data_error(data)\n        else:\n            if tupleize_cols and is_list_like(data):\n                # GH21470: convert iterable to list before determining if empty\n                if is_iterator(data):\n                    data = list(data)\n\n                if data and all(isinstance(e, tuple) for e in data):\n                    # we must be all tuples, otherwise don't construct\n                    # 10697\n                    from .multi import MultiIndex\n\n                    return MultiIndex.from_tuples(\n                        data, names=name or kwargs.get(\"names\")\n                    )\n            # other iterable of some kind\n            subarr = com.asarray_tuplesafe(data, dtype=object)\n            return Index(subarr, dtype=dtype, copy=copy, name=name, **kwargs)\n\n    \"\"\"\n    NOTE for new Index creation:\n\n    - _simple_new: It returns new Index with the same type as the caller.\n      All metadata (such as name) must be provided by caller's responsibility.\n      Using _shallow_copy is recommended because it fills these metadata\n      otherwise specified.\n\n    - _shallow_copy: It returns new Index with the same type (using\n      _simple_new), but fills caller's metadata otherwise specified. Passed\n      kwargs will overwrite corresponding metadata.\n\n    - _shallow_copy_with_infer: It returns new Index inferring its type\n      from passed values. It fills caller's metadata otherwise specified as the\n      same as _shallow_copy.\n\n    See each method's docstring.\n    \"\"\"\n\n    @property\n    def asi8(self):\n        \"\"\"\n        Integer representation of the values.\n\n        Returns\n        -------\n        ndarray\n            An ndarray with int64 dtype.\n        \"\"\"\n        return None\n\n    @classmethod\n    def _simple_new(cls, values, name=None, dtype=None, **kwargs):\n        \"\"\"\n        We require that we have a dtype compat for the values. If we are passed\n        a non-dtype compat, then coerce using the constructor.\n\n        Must be careful not to recurse.\n        \"\"\"\n        if isinstance(values, (ABCSeries, ABCIndexClass)):\n            # Index._data must always be an ndarray.\n            # This is no-copy for when _values is an ndarray,\n            # which should be always at this point.\n            values = np.asarray(values._values)\n\n        result = object.__new__(cls)\n        result._data = values\n        # _index_data is a (temporary?) fix to ensure that the direct data\n        # manipulation we do in `_libs/reduction.pyx` continues to work.\n        # We need access to the actual ndarray, since we're messing with\n        # data buffers and strides. We don't re-use `_ndarray_values`, since\n        # we actually set this value too.\n        result._index_data = values\n        result.name = name\n        for k, v in kwargs.items():\n            setattr(result, k, v)\n        return result._reset_identity()\n\n    @cache_readonly\n    def _constructor(self):\n        return type(self)\n\n    # --------------------------------------------------------------------\n    # Index Internals Methods\n\n    def _get_attributes_dict(self):\n        \"\"\"\n        Return an attributes dict for my class.\n        \"\"\"\n        return {k: getattr(self, k, None) for k in self._attributes}\n\n    _index_shared_docs[\n        \"_shallow_copy\"\n    ] = \"\"\"\n        Create a new Index with the same class as the caller, don't copy the\n        data, use the same object attributes with passed in attributes taking\n        precedence.\n\n        *this is an internal non-public method*\n\n        Parameters\n        ----------\n        values : the values to create the new Index, optional\n        kwargs : updates the default attributes for this Index\n        \"\"\"\n\n    @Appender(_index_shared_docs[\"_shallow_copy\"])\n    def _shallow_copy(self, values=None, **kwargs):\n        if values is None:\n            values = self.values\n        attributes = self._get_attributes_dict()\n        attributes.update(kwargs)\n        if not len(values) and \"dtype\" not in kwargs:\n            attributes[\"dtype\"] = self.dtype\n\n        # _simple_new expects an the type of self._data\n        values = getattr(values, \"_values\", values)\n        if isinstance(values, ABCDatetimeArray):\n            # `self.values` returns `self` for tz-aware, so we need to unwrap\n            #  more specifically\n            values = values.asi8\n\n        return self._simple_new(values, **attributes)\n\n    def _shallow_copy_with_infer(self, values, **kwargs):\n        \"\"\"\n        Create a new Index inferring the class with passed value, don't copy\n        the data, use the same object attributes with passed in attributes\n        taking precedence.\n\n        *this is an internal non-public method*\n\n        Parameters\n        ----------\n        values : the values to create the new Index, optional\n        kwargs : updates the default attributes for this Index\n        \"\"\"\n        attributes = self._get_attributes_dict()\n        attributes.update(kwargs)\n        attributes[\"copy\"] = False\n        if not len(values) and \"dtype\" not in kwargs:\n            attributes[\"dtype\"] = self.dtype\n        if self._infer_as_myclass:\n            try:\n                return self._constructor(values, **attributes)\n            except (TypeError, ValueError):\n                pass\n        return Index(values, **attributes)\n\n    def _update_inplace(self, result, **kwargs):\n        # guard when called from IndexOpsMixin\n        raise TypeError(\"Index can't be updated inplace\")\n\n    def is_(self, other):\n        \"\"\"\n        More flexible, faster check like ``is`` but that works through views.\n\n        Note: this is *not* the same as ``Index.identical()``, which checks\n        that metadata is also the same.\n\n        Parameters\n        ----------\n        other : object\n            other object to compare against.\n\n        Returns\n        -------\n        True if both have same underlying data, False otherwise : bool\n        \"\"\"\n        # use something other than None to be clearer\n        return self._id is getattr(other, \"_id\", Ellipsis) and self._id is not None\n\n    def _reset_identity(self):\n        \"\"\"\n        Initializes or resets ``_id`` attribute with new object.\n        \"\"\"\n        self._id = _Identity()\n        return self\n\n    def _cleanup(self):\n        self._engine.clear_mapping()\n\n    @cache_readonly\n    def _engine(self):\n        # property, for now, slow to look up\n\n        # to avoid a reference cycle, bind `_ndarray_values` to a local variable, so\n        # `self` is not passed into the lambda.\n        _ndarray_values = self._ndarray_values\n        return self._engine_type(lambda: _ndarray_values, len(self))\n\n    # --------------------------------------------------------------------\n    # Array-Like Methods\n\n    # ndarray compat\n    def __len__(self):\n        \"\"\"\n        Return the length of the Index.\n        \"\"\"\n        return len(self._data)\n\n    def __array__(self, dtype=None):\n        \"\"\"\n        The array interface, return my values.\n        \"\"\"\n        return np.asarray(self._data, dtype=dtype)\n\n    def __array_wrap__(self, result, context=None):\n        \"\"\"\n        Gets called after a ufunc.\n        \"\"\"\n        result = lib.item_from_zerodim(result)\n        if is_bool_dtype(result) or lib.is_scalar(result):\n            return result\n\n        attrs = self._get_attributes_dict()\n        return Index(result, **attrs)\n\n    @cache_readonly\n    def dtype(self):\n        \"\"\"\n        Return the dtype object of the underlying data.\n        \"\"\"\n        return self._data.dtype\n\n    @property\n    def dtype_str(self):\n        \"\"\"\n        Return the dtype str of the underlying data.\n\n        .. deprecated:: 0.25.0\n        \"\"\"\n        warnings.warn(\n            \"`dtype_str` has been deprecated. Call `str` on the \"\n            \"dtype attribute instead.\",\n            FutureWarning,\n            stacklevel=2,\n        )\n        return str(self.dtype)\n\n    def ravel(self, order=\"C\"):\n        \"\"\"\n        Return an ndarray of the flattened values of the underlying data.\n\n        Returns\n        -------\n        numpy.ndarray\n            Flattened array.\n\n        See Also\n        --------\n        numpy.ndarray.ravel\n        \"\"\"\n        return self._ndarray_values.ravel(order=order)\n\n    def view(self, cls=None):\n\n        # we need to see if we are subclassing an\n        # index type here\n        if cls is not None and not hasattr(cls, \"_typ\"):\n            result = self._data.view(cls)\n        else:\n            result = self._shallow_copy()\n        if isinstance(result, Index):\n            result._id = self._id\n        return result\n\n    _index_shared_docs[\n        \"astype\"\n    ] = \"\"\"\n        Create an Index with values cast to dtypes. The class of a new Index\n        is determined by dtype. When conversion is impossible, a ValueError\n        exception is raised.\n\n        Parameters\n        ----------\n        dtype : numpy dtype or pandas type\n            Note that any signed integer `dtype` is treated as ``'int64'``,\n            and any unsigned integer `dtype` is treated as ``'uint64'``,\n            regardless of the size.\n        copy : bool, default True\n            By default, astype always returns a newly allocated object.\n            If copy is set to False and internal requirements on dtype are\n            satisfied, the original data is used to create a new Index\n            or the original Index is returned.\n\n        Returns\n        -------\n        Index\n            Index with values cast to specified dtype.\n        \"\"\"\n\n    @Appender(_index_shared_docs[\"astype\"])\n    def astype(self, dtype, copy=True):\n        if is_dtype_equal(self.dtype, dtype):\n            return self.copy() if copy else self\n\n        elif is_categorical_dtype(dtype):\n            from .category import CategoricalIndex\n\n            return CategoricalIndex(self.values, name=self.name, dtype=dtype, copy=copy)\n        elif is_datetime64tz_dtype(dtype):\n            # TODO(GH-24559): Remove this block, use the following elif.\n            # avoid FutureWarning from DatetimeIndex constructor.\n            from pandas import DatetimeIndex\n\n            tz = pandas_dtype(dtype).tz\n            return DatetimeIndex(np.asarray(self)).tz_localize(\"UTC\").tz_convert(tz)\n\n        elif is_extension_array_dtype(dtype):\n            return Index(np.asarray(self), dtype=dtype, copy=copy)\n\n        try:\n            if is_datetime64tz_dtype(dtype):\n                from pandas import DatetimeIndex\n\n                return DatetimeIndex(\n                    self.values, name=self.name, dtype=dtype, copy=copy\n                )\n            return Index(\n                self.values.astype(dtype, copy=copy), name=self.name, dtype=dtype\n            )\n        except (TypeError, ValueError):\n            msg = \"Cannot cast {name} to dtype {dtype}\"\n            raise TypeError(msg.format(name=type(self).__name__, dtype=dtype))\n\n    _index_shared_docs[\n        \"take\"\n    ] = \"\"\"\n        Return a new %(klass)s of the values selected by the indices.\n\n        For internal compatibility with numpy arrays.\n\n        Parameters\n        ----------\n        indices : list\n            Indices to be taken\n        axis : int, optional\n            The axis over which to select values, always 0.\n        allow_fill : bool, default True\n        fill_value : bool, default None\n            If allow_fill=True and fill_value is not None, indices specified by\n            -1 is regarded as NA. If Index doesn't hold NA, raise ValueError\n\n        Returns\n        -------\n        numpy.ndarray\n            Elements of given indices.\n\n        See Also\n        --------\n        numpy.ndarray.take\n        \"\"\"\n\n    @Appender(_index_shared_docs[\"take\"] % _index_doc_kwargs)\n    def take(self, indices, axis=0, allow_fill=True, fill_value=None, **kwargs):\n        if kwargs:\n            nv.validate_take(tuple(), kwargs)\n        indices = ensure_platform_int(indices)\n        if self._can_hold_na:\n            taken = self._assert_take_fillable(\n                self.values,\n                indices,\n                allow_fill=allow_fill,\n                fill_value=fill_value,\n                na_value=self._na_value,\n            )\n        else:\n            if allow_fill and fill_value is not None:\n                msg = \"Unable to fill values because {0} cannot contain NA\"\n                raise ValueError(msg.format(self.__class__.__name__))\n            taken = self.values.take(indices)\n        return self._shallow_copy(taken)\n\n    def _assert_take_fillable(\n        self, values, indices, allow_fill=True, fill_value=None, na_value=np.nan\n    ):\n        \"\"\"\n        Internal method to handle NA filling of take.\n        \"\"\"\n        indices = ensure_platform_int(indices)\n\n        # only fill if we are passing a non-None fill_value\n        if allow_fill and fill_value is not None:\n            if (indices < -1).any():\n                msg = (\n                    \"When allow_fill=True and fill_value is not None, \"\n                    \"all indices must be >= -1\"\n                )\n                raise ValueError(msg)\n            taken = algos.take(\n                values, indices, allow_fill=allow_fill, fill_value=na_value\n            )\n        else:\n            taken = values.take(indices)\n        return taken\n\n    _index_shared_docs[\n        \"repeat\"\n    ] = \"\"\"\n        Repeat elements of a %(klass)s.\n\n        Returns a new %(klass)s where each element of the current %(klass)s\n        is repeated consecutively a given number of times.\n\n        Parameters\n        ----------\n        repeats : int or array of ints\n            The number of repetitions for each element. This should be a\n            non-negative integer. Repeating 0 times will return an empty\n            %(klass)s.\n        axis : None\n            Must be ``None``. Has no effect but is accepted for compatibility\n            with numpy.\n\n        Returns\n        -------\n        repeated_index : %(klass)s\n            Newly created %(klass)s with repeated elements.\n\n        See Also\n        --------\n        Series.repeat : Equivalent function for Series.\n        numpy.repeat : Similar method for :class:`numpy.ndarray`.\n\n        Examples\n        --------\n        >>> idx = pd.Index(['a', 'b', 'c'])\n        >>> idx\n        Index(['a', 'b', 'c'], dtype='object')\n        >>> idx.repeat(2)\n        Index(['a', 'a', 'b', 'b', 'c', 'c'], dtype='object')\n        >>> idx.repeat([1, 2, 3])\n        Index(['a', 'b', 'b', 'c', 'c', 'c'], dtype='object')\n        \"\"\"\n\n    @Appender(_index_shared_docs[\"repeat\"] % _index_doc_kwargs)\n    def repeat(self, repeats, axis=None):\n        repeats = ensure_platform_int(repeats)\n        nv.validate_repeat(tuple(), dict(axis=axis))\n        return self._shallow_copy(self._values.repeat(repeats))\n\n    # --------------------------------------------------------------------\n    # Copying Methods\n\n    _index_shared_docs[\n        \"copy\"\n    ] = \"\"\"\n        Make a copy of this object.  Name and dtype sets those attributes on\n        the new object.\n\n        Parameters\n        ----------\n        name : string, optional\n        deep : boolean, default False\n        dtype : numpy dtype or pandas type\n\n        Returns\n        -------\n        copy : Index\n\n        Notes\n        -----\n        In most cases, there should be no functional difference from using\n        ``deep``, but if ``deep`` is passed it will attempt to deepcopy.\n        \"\"\"\n\n    @Appender(_index_shared_docs[\"copy\"])\n    def copy(self, name=None, deep=False, dtype=None, **kwargs):\n        if deep:\n            new_index = self._shallow_copy(self._data.copy())\n        else:\n            new_index = self._shallow_copy()\n\n        names = kwargs.get(\"names\")\n        names = self._validate_names(name=name, names=names, deep=deep)\n        new_index = new_index.set_names(names)\n\n        if dtype:\n            new_index = new_index.astype(dtype)\n        return new_index\n\n    def __copy__(self, **kwargs):\n        return self.copy(**kwargs)\n\n    def __deepcopy__(self, memo=None):\n        \"\"\"\n        Parameters\n        ----------\n        memo, default None\n            Standard signature. Unused\n        \"\"\"\n        if memo is None:\n            memo = {}\n        return self.copy(deep=True)\n\n    # --------------------------------------------------------------------\n    # Rendering Methods\n\n    def __repr__(self):\n        \"\"\"\n        Return a string representation for this object.\n        \"\"\"\n        klass = self.__class__.__name__\n        data = self._format_data()\n        attrs = self._format_attrs()\n        space = self._format_space()\n\n        prepr = (\",%s\" % space).join(\"%s=%s\" % (k, v) for k, v in attrs)\n\n        # no data provided, just attributes\n        if data is None:\n            data = \"\"\n\n        res = \"%s(%s%s)\" % (klass, data, prepr)\n\n        return res\n\n    def _format_space(self):\n\n        # using space here controls if the attributes\n        # are line separated or not (the default)\n\n        # max_seq_items = get_option('display.max_seq_items')\n        # if len(self) > max_seq_items:\n        #    space = \"\\n%s\" % (' ' * (len(klass) + 1))\n        return \" \"\n\n    @property\n    def _formatter_func(self):\n        \"\"\"\n        Return the formatter function.\n        \"\"\"\n        return default_pprint\n\n    def _format_data(self, name=None):\n        \"\"\"\n        Return the formatted data as a unicode string.\n        \"\"\"\n\n        # do we want to justify (only do so for non-objects)\n        is_justify = not (\n            self.inferred_type in (\"string\", \"unicode\")\n            or (\n                self.inferred_type == \"categorical\" and is_object_dtype(self.categories)\n            )\n        )\n\n        return format_object_summary(\n            self, self._formatter_func, is_justify=is_justify, name=name\n        )\n\n    def _format_attrs(self):\n        \"\"\"\n        Return a list of tuples of the (attr,formatted_value).\n        \"\"\"\n        return format_object_attrs(self)\n\n    def _mpl_repr(self):\n        # how to represent ourselves to matplotlib\n        return self.values\n\n    def format(self, name=False, formatter=None, **kwargs):\n        \"\"\"\n        Render a string representation of the Index.\n        \"\"\"\n        header = []\n        if name:\n            header.append(\n                pprint_thing(self.name, escape_chars=(\"\\t\", \"\\r\", \"\\n\"))\n                if self.name is not None\n                else \"\"\n            )\n\n        if formatter is not None:\n            return header + list(self.map(formatter))\n\n        return self._format_with_header(header, **kwargs)\n\n    def _format_with_header(self, header, na_rep=\"NaN\", **kwargs):\n        values = self.values\n\n        from pandas.io.formats.format import format_array\n\n        if is_categorical_dtype(values.dtype):\n            values = np.array(values)\n\n        elif is_object_dtype(values.dtype):\n            values = lib.maybe_convert_objects(values, safe=1)\n\n        if is_object_dtype(values.dtype):\n            result = [pprint_thing(x, escape_chars=(\"\\t\", \"\\r\", \"\\n\")) for x in values]\n\n            # could have nans\n            mask = isna(values)\n            if mask.any():\n                result = np.array(result)\n                result[mask] = na_rep\n                result = result.tolist()\n\n        else:\n            result = _trim_front(format_array(values, None, justify=\"left\"))\n        return header + result\n\n    def to_native_types(self, slicer=None, **kwargs):\n        \"\"\"\n        Format specified values of `self` and return them.\n\n        Parameters\n        ----------\n        slicer : int, array-like\n            An indexer into `self` that specifies which values\n            are used in the formatting process.\n        kwargs : dict\n            Options for specifying how the values should be formatted.\n            These options include the following:\n\n            1) na_rep : str\n                The value that serves as a placeholder for NULL values\n            2) quoting : bool or None\n                Whether or not there are quoted values in `self`\n            3) date_format : str\n                The format used to represent date-like values\n\n        Returns\n        -------\n        numpy.ndarray\n            Formatted values.\n        \"\"\"\n\n        values = self\n        if slicer is not None:\n            values = values[slicer]\n        return values._format_native_types(**kwargs)\n\n    def _format_native_types(self, na_rep=\"\", quoting=None, **kwargs):\n        \"\"\"\n        Actually format specific types of the index.\n        \"\"\"\n        mask = isna(self)\n        if not self.is_object() and not quoting:\n            values = np.asarray(self).astype(str)\n        else:\n            values = np.array(self, dtype=object, copy=True)\n\n        values[mask] = na_rep\n        return values\n\n    def _summary(self, name=None):\n        \"\"\"\n        Return a summarized representation.\n\n        Parameters\n        ----------\n        name : str\n            name to use in the summary representation\n\n        Returns\n        -------\n        String with a summarized representation of the index\n        \"\"\"\n        if len(self) > 0:\n            head = self[0]\n            if hasattr(head, \"format\") and not isinstance(head, str):\n                head = head.format()\n            tail = self[-1]\n            if hasattr(tail, \"format\") and not isinstance(tail, str):\n                tail = tail.format()\n            index_summary = \", %s to %s\" % (pprint_thing(head), pprint_thing(tail))\n        else:\n            index_summary = \"\"\n\n        if name is None:\n            name = type(self).__name__\n        return \"%s: %s entries%s\" % (name, len(self), index_summary)\n\n    def summary(self, name=None):\n        \"\"\"\n        Return a summarized representation.\n\n        .. deprecated:: 0.23.0\n        \"\"\"\n        warnings.warn(\n            \"'summary' is deprecated and will be removed in a future version.\",\n            FutureWarning,\n            stacklevel=2,\n        )\n        return self._summary(name)\n\n    # --------------------------------------------------------------------\n    # Conversion Methods\n\n    def to_flat_index(self):\n        \"\"\"\n        Identity method.\n\n        .. versionadded:: 0.24.0\n\n        This is implemented for compatibility with subclass implementations\n        when chaining.\n\n        Returns\n        -------\n        pd.Index\n            Caller.\n\n        See Also\n        --------\n        MultiIndex.to_flat_index : Subclass implementation.\n        \"\"\"\n        return self\n\n    def to_series(self, index=None, name=None):\n        \"\"\"\n        Create a Series with both index and values equal to the index keys\n        useful with map for returning an indexer based on an index.\n\n        Parameters\n        ----------\n        index : Index, optional\n            index of resulting Series. If None, defaults to original index\n        name : string, optional\n            name of resulting Series. If None, defaults to name of original\n            index\n\n        Returns\n        -------\n        Series : dtype will be based on the type of the Index values.\n        \"\"\"\n\n        from pandas import Series\n\n        if index is None:\n            index = self._shallow_copy()\n        if name is None:\n            name = self.name\n\n        return Series(self.values.copy(), index=index, name=name)\n\n    def to_frame(self, index=True, name=None):\n        \"\"\"\n        Create a DataFrame with a column containing the Index.\n\n        .. versionadded:: 0.24.0\n\n        Parameters\n        ----------\n        index : boolean, default True\n            Set the index of the returned DataFrame as the original Index.\n\n        name : object, default None\n            The passed name should substitute for the index name (if it has\n            one).\n\n        Returns\n        -------\n        DataFrame\n            DataFrame containing the original Index data.\n\n        See Also\n        --------\n        Index.to_series : Convert an Index to a Series.\n        Series.to_frame : Convert Series to DataFrame.\n\n        Examples\n        --------\n        >>> idx = pd.Index(['Ant', 'Bear', 'Cow'], name='animal')\n        >>> idx.to_frame()\n               animal\n        animal\n        Ant       Ant\n        Bear     Bear\n        Cow       Cow\n\n        By default, the original Index is reused. To enforce a new Index:\n\n        >>> idx.to_frame(index=False)\n            animal\n        0   Ant\n        1  Bear\n        2   Cow\n\n        To override the name of the resulting column, specify `name`:\n\n        >>> idx.to_frame(index=False, name='zoo')\n            zoo\n        0   Ant\n        1  Bear\n        2   Cow\n        \"\"\"\n\n        from pandas import DataFrame\n\n        if name is None:\n            name = self.name or 0\n        result = DataFrame({name: self._values.copy()})\n\n        if index:\n            result.index = self\n        return result\n\n    # --------------------------------------------------------------------\n    # Name-Centric Methods\n\n    def _validate_names(self, name=None, names=None, deep=False):\n        \"\"\"\n        Handles the quirks of having a singular 'name' parameter for general\n        Index and plural 'names' parameter for MultiIndex.\n        \"\"\"\n        from copy import deepcopy\n\n        if names is not None and name is not None:\n            raise TypeError(\"Can only provide one of `names` and `name`\")\n        elif names is None and name is None:\n            return deepcopy(self.names) if deep else self.names\n        elif names is not None:\n            if not is_list_like(names):\n                raise TypeError(\"Must pass list-like as `names`.\")\n            return names\n        else:\n            if not is_list_like(name):\n                return [name]\n            return name\n\n    def _get_names(self):\n        return FrozenList((self.name,))\n\n    def _set_names(self, values, level=None):\n        \"\"\"\n        Set new names on index. Each name has to be a hashable type.\n\n        Parameters\n        ----------\n        values : str or sequence\n            name(s) to set\n        level : int, level name, or sequence of int/level names (default None)\n            If the index is a MultiIndex (hierarchical), level(s) to set (None\n            for all levels).  Otherwise level must be None\n\n        Raises\n        ------\n        TypeError if each name is not hashable.\n        \"\"\"\n        if not is_list_like(values):\n            raise ValueError(\"Names must be a list-like\")\n        if len(values) != 1:\n            raise ValueError(\"Length of new names must be 1, got %d\" % len(values))\n\n        # GH 20527\n        # All items in 'name' need to be hashable:\n        for name in values:\n            if not is_hashable(name):\n                raise TypeError(\n                    \"{}.name must be a hashable type\".format(self.__class__.__name__)\n                )\n        self.name = values[0]\n\n    names = property(fset=_set_names, fget=_get_names)\n\n    def set_names(self, names, level=None, inplace=False):\n        \"\"\"\n        Set Index or MultiIndex name.\n\n        Able to set new names partially and by level.\n\n        Parameters\n        ----------\n        names : label or list of label\n            Name(s) to set.\n        level : int, label or list of int or label, optional\n            If the index is a MultiIndex, level(s) to set (None for all\n            levels). Otherwise level must be None.\n        inplace : bool, default False\n            Modifies the object directly, instead of creating a new Index or\n            MultiIndex.\n\n        Returns\n        -------\n        Index\n            The same type as the caller or None if inplace is True.\n\n        See Also\n        --------\n        Index.rename : Able to set new names without level.\n\n        Examples\n        --------\n        >>> idx = pd.Index([1, 2, 3, 4])\n        >>> idx\n        Int64Index([1, 2, 3, 4], dtype='int64')\n        >>> idx.set_names('quarter')\n        Int64Index([1, 2, 3, 4], dtype='int64', name='quarter')\n\n        >>> idx = pd.MultiIndex.from_product([['python', 'cobra'],\n        ...                                   [2018, 2019]])\n        >>> idx\n        MultiIndex([('python', 2018),\n                    ('python', 2019),\n                    ( 'cobra', 2018),\n                    ( 'cobra', 2019)],\n                   )\n        >>> idx.set_names(['kind', 'year'], inplace=True)\n        >>> idx\n        MultiIndex([('python', 2018),\n                    ('python', 2019),\n                    ( 'cobra', 2018),\n                    ( 'cobra', 2019)],\n                   names=['kind', 'year'])\n        >>> idx.set_names('species', level=0)\n        MultiIndex([('python', 2018),\n                    ('python', 2019),\n                    ( 'cobra', 2018),\n                    ( 'cobra', 2019)],\n                   names=['species', 'year'])\n        \"\"\"\n\n        if level is not None and not isinstance(self, ABCMultiIndex):\n            raise ValueError(\"Level must be None for non-MultiIndex\")\n\n        if level is not None and not is_list_like(level) and is_list_like(names):\n            msg = \"Names must be a string when a single level is provided.\"\n            raise TypeError(msg)\n\n        if not is_list_like(names) and level is None and self.nlevels > 1:\n            raise TypeError(\"Must pass list-like as `names`.\")\n\n        if not is_list_like(names):\n            names = [names]\n        if level is not None and not is_list_like(level):\n            level = [level]\n\n        if inplace:\n            idx = self\n        else:\n            idx = self._shallow_copy()\n        idx._set_names(names, level=level)\n        if not inplace:\n            return idx\n\n    def rename(self, name, inplace=False):\n        \"\"\"\n        Alter Index or MultiIndex name.\n\n        Able to set new names without level. Defaults to returning new index.\n        Length of names must match number of levels in MultiIndex.\n\n        Parameters\n        ----------\n        name : label or list of labels\n            Name(s) to set.\n        inplace : boolean, default False\n            Modifies the object directly, instead of creating a new Index or\n            MultiIndex.\n\n        Returns\n        -------\n        Index\n            The same type as the caller or None if inplace is True.\n\n        See Also\n        --------\n        Index.set_names : Able to set new names partially and by level.\n\n        Examples\n        --------\n        >>> idx = pd.Index(['A', 'C', 'A', 'B'], name='score')\n        >>> idx.rename('grade')\n        Index(['A', 'C', 'A', 'B'], dtype='object', name='grade')\n\n        >>> idx = pd.MultiIndex.from_product([['python', 'cobra'],\n        ...                                   [2018, 2019]],\n        ...                                   names=['kind', 'year'])\n        >>> idx\n        MultiIndex([('python', 2018),\n                    ('python', 2019),\n                    ( 'cobra', 2018),\n                    ( 'cobra', 2019)],\n                   names=['kind', 'year'])\n        >>> idx.rename(['species', 'year'])\n        MultiIndex([('python', 2018),\n                    ('python', 2019),\n                    ( 'cobra', 2018),\n                    ( 'cobra', 2019)],\n                   names=['species', 'year'])\n        >>> idx.rename('species')\n        Traceback (most recent call last):\n        TypeError: Must pass list-like as `names`.\n        \"\"\"\n        return self.set_names([name], inplace=inplace)\n\n    # --------------------------------------------------------------------\n    # Level-Centric Methods\n\n    @property\n    def nlevels(self):\n        \"\"\"\n        Number of levels.\n        \"\"\"\n        return 1\n\n    def _sort_levels_monotonic(self):\n        \"\"\"\n        Compat with MultiIndex.\n        \"\"\"\n        return self\n\n    def _validate_index_level(self, level):\n        \"\"\"\n        Validate index level.\n\n        For single-level Index getting level number is a no-op, but some\n        verification must be done like in MultiIndex.\n\n        \"\"\"\n        if isinstance(level, int):\n            if level < 0 and level != -1:\n                raise IndexError(\n                    \"Too many levels: Index has only 1 level,\"\n                    \" %d is not a valid level number\" % (level,)\n                )\n            elif level > 0:\n                raise IndexError(\n                    \"Too many levels: Index has only 1 level, not %d\" % (level + 1)\n                )\n        elif level != self.name:\n            raise KeyError(\n                \"Requested level ({}) does not match index name ({})\".format(\n                    level, self.name\n                )\n            )\n\n    def _get_level_number(self, level):\n        self._validate_index_level(level)\n        return 0\n\n    def sortlevel(self, level=None, ascending=True, sort_remaining=None):\n        \"\"\"\n        For internal compatibility with with the Index API.\n\n        Sort the Index. This is for compat with MultiIndex\n\n        Parameters\n        ----------\n        ascending : boolean, default True\n            False to sort in descending order\n\n        level, sort_remaining are compat parameters\n\n        Returns\n        -------\n        Index\n        \"\"\"\n        return self.sort_values(return_indexer=True, ascending=ascending)\n\n    def _get_level_values(self, level):\n        \"\"\"\n        Return an Index of values for requested level.\n\n        This is primarily useful to get an individual level of values from a\n        MultiIndex, but is provided on Index as well for compatibility.\n\n        Parameters\n        ----------\n        level : int or str\n            It is either the integer position or the name of the level.\n\n        Returns\n        -------\n        Index\n            Calling object, as there is only one level in the Index.\n\n        See Also\n        --------\n        MultiIndex.get_level_values : Get values for a level of a MultiIndex.\n\n        Notes\n        -----\n        For Index, level should be 0, since there are no multiple levels.\n\n        Examples\n        --------\n\n        >>> idx = pd.Index(list('abc'))\n        >>> idx\n        Index(['a', 'b', 'c'], dtype='object')\n\n        Get level values by supplying `level` as integer:\n\n        >>> idx.get_level_values(0)\n        Index(['a', 'b', 'c'], dtype='object')\n        \"\"\"\n        self._validate_index_level(level)\n        return self\n\n    get_level_values = _get_level_values\n\n    def droplevel(self, level=0):\n        \"\"\"\n        Return index with requested level(s) removed.\n\n        If resulting index has only 1 level left, the result will be\n        of Index type, not MultiIndex.\n\n        .. versionadded:: 0.23.1 (support for non-MultiIndex)\n\n        Parameters\n        ----------\n        level : int, str, or list-like, default 0\n            If a string is given, must be the name of a level\n            If list-like, elements must be names or indexes of levels.\n\n        Returns\n        -------\n        Index or MultiIndex\n        \"\"\"\n        if not isinstance(level, (tuple, list)):\n            level = [level]\n\n        levnums = sorted(self._get_level_number(lev) for lev in level)[::-1]\n\n        if len(level) == 0:\n            return self\n        if len(level) >= self.nlevels:\n            raise ValueError(\n                \"Cannot remove {} levels from an index with {} \"\n                \"levels: at least one level must be \"\n                \"left.\".format(len(level), self.nlevels)\n            )\n        # The two checks above guarantee that here self is a MultiIndex\n\n        new_levels = list(self.levels)\n        new_codes = list(self.codes)\n        new_names = list(self.names)\n\n        for i in levnums:\n            new_levels.pop(i)\n            new_codes.pop(i)\n            new_names.pop(i)\n\n        if len(new_levels) == 1:\n\n            # set nan if needed\n            mask = new_codes[0] == -1\n            result = new_levels[0].take(new_codes[0])\n            if mask.any():\n                result = result.putmask(mask, np.nan)\n\n            result.name = new_names[0]\n            return result\n        else:\n            from .multi import MultiIndex\n\n            return MultiIndex(\n                levels=new_levels,\n                codes=new_codes,\n                names=new_names,\n                verify_integrity=False,\n            )\n\n    _index_shared_docs[\n        \"_get_grouper_for_level\"\n    ] = \"\"\"\n        Get index grouper corresponding to an index level\n\n        Parameters\n        ----------\n        mapper: Group mapping function or None\n            Function mapping index values to groups\n        level : int or None\n            Index level\n\n        Returns\n        -------\n        grouper : Index\n            Index of values to group on.\n        labels : ndarray of int or None\n            Array of locations in level_index.\n        uniques : Index or None\n            Index of unique values for level.\n        \"\"\"\n\n    @Appender(_index_shared_docs[\"_get_grouper_for_level\"])\n    def _get_grouper_for_level(self, mapper, level=None):\n        assert level is None or level == 0\n        if mapper is None:\n            grouper = self\n        else:\n            grouper = self.map(mapper)\n\n        return grouper, None, None\n\n    # --------------------------------------------------------------------\n    # Introspection Methods\n\n    @property\n    def is_monotonic(self):\n        \"\"\"\n        Alias for is_monotonic_increasing.\n        \"\"\"\n        return self.is_monotonic_increasing\n\n    @property\n    def is_monotonic_increasing(self):\n        \"\"\"\n        Return if the index is monotonic increasing (only equal or\n        increasing) values.\n\n        Examples\n        --------\n        >>> Index([1, 2, 3]).is_monotonic_increasing\n        True\n        >>> Index([1, 2, 2]).is_monotonic_increasing\n        True\n        >>> Index([1, 3, 2]).is_monotonic_increasing\n        False\n        \"\"\"\n        return self._engine.is_monotonic_increasing\n\n    @property\n    def is_monotonic_decreasing(self):\n        \"\"\"\n        Return if the index is monotonic decreasing (only equal or\n        decreasing) values.\n\n        Examples\n        --------\n        >>> Index([3, 2, 1]).is_monotonic_decreasing\n        True\n        >>> Index([3, 2, 2]).is_monotonic_decreasing\n        True\n        >>> Index([3, 1, 2]).is_monotonic_decreasing\n        False\n        \"\"\"\n        return self._engine.is_monotonic_decreasing\n\n    @property\n    def _is_strictly_monotonic_increasing(self):\n        \"\"\"\n        Return if the index is strictly monotonic increasing\n        (only increasing) values.\n\n        Examples\n        --------\n        >>> Index([1, 2, 3])._is_strictly_monotonic_increasing\n        True\n        >>> Index([1, 2, 2])._is_strictly_monotonic_increasing\n        False\n        >>> Index([1, 3, 2])._is_strictly_monotonic_increasing\n        False\n        \"\"\"\n        return self.is_unique and self.is_monotonic_increasing\n\n    @property\n    def _is_strictly_monotonic_decreasing(self):\n        \"\"\"\n        Return if the index is strictly monotonic decreasing\n        (only decreasing) values.\n\n        Examples\n        --------\n        >>> Index([3, 2, 1])._is_strictly_monotonic_decreasing\n        True\n        >>> Index([3, 2, 2])._is_strictly_monotonic_decreasing\n        False\n        >>> Index([3, 1, 2])._is_strictly_monotonic_decreasing\n        False\n        \"\"\"\n        return self.is_unique and self.is_monotonic_decreasing\n\n    def is_lexsorted_for_tuple(self, tup):\n        return True\n\n    @cache_readonly\n    def is_unique(self):\n        \"\"\"\n        Return if the index has unique values.\n        \"\"\"\n        return self._engine.is_unique\n\n    @property\n    def has_duplicates(self):\n        return not self.is_unique\n\n    def is_boolean(self):\n        return self.inferred_type in [\"boolean\"]\n\n    def is_integer(self):\n        return self.inferred_type in [\"integer\"]\n\n    def is_floating(self):\n        return self.inferred_type in [\"floating\", \"mixed-integer-float\", \"integer-na\"]\n\n    def is_numeric(self):\n        return self.inferred_type in [\"integer\", \"floating\"]\n\n    def is_object(self):\n        return is_object_dtype(self.dtype)\n\n    def is_categorical(self):\n        \"\"\"\n        Check if the Index holds categorical data.\n\n        Returns\n        -------\n        boolean\n            True if the Index is categorical.\n\n        See Also\n        --------\n        CategoricalIndex : Index for categorical data.\n\n        Examples\n        --------\n        >>> idx = pd.Index([\"Watermelon\", \"Orange\", \"Apple\",\n        ...                 \"Watermelon\"]).astype(\"category\")\n        >>> idx.is_categorical()\n        True\n\n        >>> idx = pd.Index([1, 3, 5, 7])\n        >>> idx.is_categorical()\n        False\n\n        >>> s = pd.Series([\"Peter\", \"Victor\", \"Elisabeth\", \"Mar\"])\n        >>> s\n        0        Peter\n        1       Victor\n        2    Elisabeth\n        3          Mar\n        dtype: object\n        >>> s.index.is_categorical()\n        False\n        \"\"\"\n        return self.inferred_type in [\"categorical\"]\n\n    def is_interval(self):\n        return self.inferred_type in [\"interval\"]\n\n    def is_mixed(self):\n        return self.inferred_type in [\"mixed\"]\n\n    def holds_integer(self):\n        \"\"\"\n        Whether the type is an integer type.\n        \"\"\"\n        return self.inferred_type in [\"integer\", \"mixed-integer\"]\n\n    @cache_readonly\n    def inferred_type(self):\n        \"\"\"\n        Return a string of the type inferred from the values.\n        \"\"\"\n        return lib.infer_dtype(self, skipna=False)\n\n    @cache_readonly\n    def is_all_dates(self):\n        return is_datetime_array(ensure_object(self.values))\n\n    # --------------------------------------------------------------------\n    # Pickle Methods\n\n    def __reduce__(self):\n        d = dict(data=self._data)\n        d.update(self._get_attributes_dict())\n        return _new_Index, (self.__class__, d), None\n\n    def __setstate__(self, state):\n        \"\"\"\n        Necessary for making this object picklable.\n        \"\"\"\n\n        if isinstance(state, dict):\n            self._data = state.pop(\"data\")\n            for k, v in state.items():\n                setattr(self, k, v)\n\n        elif isinstance(state, tuple):\n\n            if len(state) == 2:\n                nd_state, own_state = state\n                data = np.empty(nd_state[1], dtype=nd_state[2])\n                np.ndarray.__setstate__(data, nd_state)\n                self.name = own_state[0]\n\n            else:  # pragma: no cover\n                data = np.empty(state)\n                np.ndarray.__setstate__(data, state)\n\n            self._data = data\n            self._reset_identity()\n        else:\n            raise Exception(\"invalid pickle state\")\n\n    _unpickle_compat = __setstate__\n\n    # --------------------------------------------------------------------\n    # Null Handling Methods\n\n    _na_value = np.nan\n    \"\"\"The expected NA value to use with this index.\"\"\"\n\n    @cache_readonly\n    def _isnan(self):\n        \"\"\"\n        Return if each value is NaN.\n        \"\"\"\n        if self._can_hold_na:\n            return isna(self)\n        else:\n            # shouldn't reach to this condition by checking hasnans beforehand\n            values = np.empty(len(self), dtype=np.bool_)\n            values.fill(False)\n            return values\n\n    @cache_readonly\n    def _nan_idxs(self):\n        if self._can_hold_na:\n            w, = self._isnan.nonzero()\n            return w\n        else:\n            return np.array([], dtype=np.int64)\n\n    @cache_readonly\n    def hasnans(self):\n        \"\"\"\n        Return if I have any nans; enables various perf speedups.\n        \"\"\"\n        if self._can_hold_na:\n            return bool(self._isnan.any())\n        else:\n            return False\n\n    def isna(self):\n        \"\"\"\n        Detect missing values.\n\n        Return a boolean same-sized object indicating if the values are NA.\n        NA values, such as ``None``, :attr:`numpy.NaN` or :attr:`pd.NaT`, get\n        mapped to ``True`` values.\n        Everything else get mapped to ``False`` values. Characters such as\n        empty strings `''` or :attr:`numpy.inf` are not considered NA values\n        (unless you set ``pandas.options.mode.use_inf_as_na = True``).\n\n        .. versionadded:: 0.20.0\n\n        Returns\n        -------\n        numpy.ndarray\n            A boolean array of whether my values are NA.\n\n        See Also\n        --------\n        Index.notna : Boolean inverse of isna.\n        Index.dropna : Omit entries with missing values.\n        isna : Top-level isna.\n        Series.isna : Detect missing values in Series object.\n\n        Examples\n        --------\n        Show which entries in a pandas.Index are NA. The result is an\n        array.\n\n        >>> idx = pd.Index([5.2, 6.0, np.NaN])\n        >>> idx\n        Float64Index([5.2, 6.0, nan], dtype='float64')\n        >>> idx.isna()\n        array([False, False,  True], dtype=bool)\n\n        Empty strings are not considered NA values. None is considered an NA\n        value.\n\n        >>> idx = pd.Index(['black', '', 'red', None])\n        >>> idx\n        Index(['black', '', 'red', None], dtype='object')\n        >>> idx.isna()\n        array([False, False, False,  True], dtype=bool)\n\n        For datetimes, `NaT` (Not a Time) is considered as an NA value.\n\n        >>> idx = pd.DatetimeIndex([pd.Timestamp('1940-04-25'),\n        ...                         pd.Timestamp(''), None, pd.NaT])\n        >>> idx\n        DatetimeIndex(['1940-04-25', 'NaT', 'NaT', 'NaT'],\n                      dtype='datetime64[ns]', freq=None)\n        >>> idx.isna()\n        array([False,  True,  True,  True], dtype=bool)\n        \"\"\"\n        return self._isnan\n\n    isnull = isna\n\n    def notna(self):\n        \"\"\"\n        Detect existing (non-missing) values.\n\n        Return a boolean same-sized object indicating if the values are not NA.\n        Non-missing values get mapped to ``True``. Characters such as empty\n        strings ``''`` or :attr:`numpy.inf` are not considered NA values\n        (unless you set ``pandas.options.mode.use_inf_as_na = True``).\n        NA values, such as None or :attr:`numpy.NaN`, get mapped to ``False``\n        values.\n\n        .. versionadded:: 0.20.0\n\n        Returns\n        -------\n        numpy.ndarray\n            Boolean array to indicate which entries are not NA.\n\n        See Also\n        --------\n        Index.notnull : Alias of notna.\n        Index.isna: Inverse of notna.\n        notna : Top-level notna.\n\n        Examples\n        --------\n        Show which entries in an Index are not NA. The result is an\n        array.\n\n        >>> idx = pd.Index([5.2, 6.0, np.NaN])\n        >>> idx\n        Float64Index([5.2, 6.0, nan], dtype='float64')\n        >>> idx.notna()\n        array([ True,  True, False])\n\n        Empty strings are not considered NA values. None is considered a NA\n        value.\n\n        >>> idx = pd.Index(['black', '', 'red', None])\n        >>> idx\n        Index(['black', '', 'red', None], dtype='object')\n        >>> idx.notna()\n        array([ True,  True,  True, False])\n        \"\"\"\n        return ~self.isna()\n\n    notnull = notna\n\n    _index_shared_docs[\n        \"fillna\"\n    ] = \"\"\"\n        Fill NA/NaN values with the specified value.\n\n        Parameters\n        ----------\n        value : scalar\n            Scalar value to use to fill holes (e.g. 0).\n            This value cannot be a list-likes.\n        downcast : dict, default is None\n            a dict of item->dtype of what to downcast if possible,\n            or the string 'infer' which will try to downcast to an appropriate\n            equal type (e.g. float64 to int64 if possible)\n\n        Returns\n        -------\n        filled : Index\n        \"\"\"\n\n    @Appender(_index_shared_docs[\"fillna\"])\n    def fillna(self, value=None, downcast=None):\n        self._assert_can_do_op(value)\n        if self.hasnans:\n            result = self.putmask(self._isnan, value)\n            if downcast is None:\n                # no need to care metadata other than name\n                # because it can't have freq if\n                return Index(result, name=self.name)\n        return self._shallow_copy()\n\n    _index_shared_docs[\n        \"dropna\"\n    ] = \"\"\"\n        Return Index without NA/NaN values.\n\n        Parameters\n        ----------\n        how :  {'any', 'all'}, default 'any'\n            If the Index is a MultiIndex, drop the value when any or all levels\n            are NaN.\n\n        Returns\n        -------\n        valid : Index\n        \"\"\"\n\n    @Appender(_index_shared_docs[\"dropna\"])\n    def dropna(self, how=\"any\"):\n        if how not in (\"any\", \"all\"):\n            raise ValueError(\"invalid how option: {0}\".format(how))\n\n        if self.hasnans:\n            return self._shallow_copy(self.values[~self._isnan])\n        return self._shallow_copy()\n\n    # --------------------------------------------------------------------\n    # Uniqueness Methods\n\n    _index_shared_docs[\n        \"index_unique\"\n    ] = \"\"\"\n        Return unique values in the index. Uniques are returned in order\n        of appearance, this does NOT sort.\n\n        Parameters\n        ----------\n        level : int or str, optional, default None\n            Only return values from specified level (for MultiIndex)\n\n            .. versionadded:: 0.23.0\n\n        Returns\n        -------\n        Index without duplicates\n\n        See Also\n        --------\n        unique\n        Series.unique\n        \"\"\"\n\n    @Appender(_index_shared_docs[\"index_unique\"] % _index_doc_kwargs)\n    def unique(self, level=None):\n        if level is not None:\n            self._validate_index_level(level)\n        result = super().unique()\n        return self._shallow_copy(result)\n\n    def drop_duplicates(self, keep=\"first\"):\n        \"\"\"\n        Return Index with duplicate values removed.\n\n        Parameters\n        ----------\n        keep : {'first', 'last', ``False``}, default 'first'\n            - 'first' : Drop duplicates except for the first occurrence.\n            - 'last' : Drop duplicates except for the last occurrence.\n            - ``False`` : Drop all duplicates.\n\n        Returns\n        -------\n        deduplicated : Index\n\n        See Also\n        --------\n        Series.drop_duplicates : Equivalent method on Series.\n        DataFrame.drop_duplicates : Equivalent method on DataFrame.\n        Index.duplicated : Related method on Index, indicating duplicate\n            Index values.\n\n        Examples\n        --------\n        Generate an pandas.Index with duplicate values.\n\n        >>> idx = pd.Index(['lama', 'cow', 'lama', 'beetle', 'lama', 'hippo'])\n\n        The `keep` parameter controls  which duplicate values are removed.\n        The value 'first' keeps the first occurrence for each\n        set of duplicated entries. The default value of keep is 'first'.\n\n        >>> idx.drop_duplicates(keep='first')\n        Index(['lama', 'cow', 'beetle', 'hippo'], dtype='object')\n\n        The value 'last' keeps the last occurrence for each set of duplicated\n        entries.\n\n        >>> idx.drop_duplicates(keep='last')\n        Index(['cow', 'beetle', 'lama', 'hippo'], dtype='object')\n\n        The value ``False`` discards all sets of duplicated entries.\n\n        >>> idx.drop_duplicates(keep=False)\n        Index(['cow', 'beetle', 'hippo'], dtype='object')\n        \"\"\"\n        return super().drop_duplicates(keep=keep)\n\n    def duplicated(self, keep=\"first\"):\n        \"\"\"\n        Indicate duplicate index values.\n\n        Duplicated values are indicated as ``True`` values in the resulting\n        array. Either all duplicates, all except the first, or all except the\n        last occurrence of duplicates can be indicated.\n\n        Parameters\n        ----------\n        keep : {'first', 'last', False}, default 'first'\n            The value or values in a set of duplicates to mark as missing.\n\n            - 'first' : Mark duplicates as ``True`` except for the first\n              occurrence.\n            - 'last' : Mark duplicates as ``True`` except for the last\n              occurrence.\n            - ``False`` : Mark all duplicates as ``True``.\n\n        Returns\n        -------\n        numpy.ndarray\n\n        See Also\n        --------\n        Series.duplicated : Equivalent method on pandas.Series.\n        DataFrame.duplicated : Equivalent method on pandas.DataFrame.\n        Index.drop_duplicates : Remove duplicate values from Index.\n\n        Examples\n        --------\n        By default, for each set of duplicated values, the first occurrence is\n        set to False and all others to True:\n\n        >>> idx = pd.Index(['lama', 'cow', 'lama', 'beetle', 'lama'])\n        >>> idx.duplicated()\n        array([False, False,  True, False,  True])\n\n        which is equivalent to\n\n        >>> idx.duplicated(keep='first')\n        array([False, False,  True, False,  True])\n\n        By using 'last', the last occurrence of each set of duplicated values\n        is set on False and all others on True:\n\n        >>> idx.duplicated(keep='last')\n        array([ True, False,  True, False, False])\n\n        By setting keep on ``False``, all duplicates are True:\n\n        >>> idx.duplicated(keep=False)\n        array([ True, False,  True, False,  True])\n        \"\"\"\n        return super().duplicated(keep=keep)\n\n    def get_duplicates(self):\n        \"\"\"\n        Extract duplicated index elements.\n\n        .. deprecated:: 0.23.0\n            Use idx[idx.duplicated()].unique() instead\n\n        Returns a sorted list of index elements which appear more than once in\n        the index.\n\n        Returns\n        -------\n        array-like\n            List of duplicated indexes.\n\n        See Also\n        --------\n        Index.duplicated : Return boolean array denoting duplicates.\n        Index.drop_duplicates : Return Index with duplicates removed.\n\n        Examples\n        --------\n\n        Works on different Index of types.\n\n        >>> pd.Index([1, 2, 2, 3, 3, 3, 4]).get_duplicates()  # doctest: +SKIP\n        [2, 3]\n\n        Note that for a DatetimeIndex, it does not return a list but a new\n        DatetimeIndex:\n\n        >>> dates = pd.to_datetime(['2018-01-01', '2018-01-02', '2018-01-03',\n        ...                         '2018-01-03', '2018-01-04', '2018-01-04'],\n        ...                        format='%Y-%m-%d')\n        >>> pd.Index(dates).get_duplicates()  # doctest: +SKIP\n        DatetimeIndex(['2018-01-03', '2018-01-04'],\n                      dtype='datetime64[ns]', freq=None)\n\n        Sorts duplicated elements even when indexes are unordered.\n\n        >>> pd.Index([1, 2, 3, 2, 3, 4, 3]).get_duplicates()  # doctest: +SKIP\n        [2, 3]\n\n        Return empty array-like structure when all elements are unique.\n\n        >>> pd.Index([1, 2, 3, 4]).get_duplicates()  # doctest: +SKIP\n        []\n        >>> dates = pd.to_datetime(['2018-01-01', '2018-01-02', '2018-01-03'],\n        ...                        format='%Y-%m-%d')\n        >>> pd.Index(dates).get_duplicates()  # doctest: +SKIP\n        DatetimeIndex([], dtype='datetime64[ns]', freq=None)\n        \"\"\"\n        warnings.warn(\n            \"'get_duplicates' is deprecated and will be removed in \"\n            \"a future release. You can use \"\n            \"idx[idx.duplicated()].unique() instead\",\n            FutureWarning,\n            stacklevel=2,\n        )\n\n        return self[self.duplicated()].unique()\n\n    def _get_unique_index(self, dropna=False):\n        \"\"\"\n        Returns an index containing unique values.\n\n        Parameters\n        ----------\n        dropna : bool\n            If True, NaN values are dropped.\n\n        Returns\n        -------\n        uniques : index\n        \"\"\"\n        if self.is_unique and not dropna:\n            return self\n\n        values = self.values\n\n        if not self.is_unique:\n            values = self.unique()\n\n        if dropna:\n            try:\n                if self.hasnans:\n                    values = values[~isna(values)]\n            except NotImplementedError:\n                pass\n\n        return self._shallow_copy(values)\n\n    # --------------------------------------------------------------------\n    # Arithmetic & Logical Methods\n\n    def __add__(self, other):\n        if isinstance(other, (ABCSeries, ABCDataFrame)):\n            return NotImplemented\n        from pandas import Series\n\n        return Index(Series(self) + other)\n\n    def __radd__(self, other):\n        from pandas import Series\n\n        return Index(other + Series(self))\n\n    def __iadd__(self, other):\n        # alias for __add__\n        return self + other\n\n    def __sub__(self, other):\n        return Index(np.array(self) - other)\n\n    def __rsub__(self, other):\n        # wrap Series to ensure we pin name correctly\n        from pandas import Series\n\n        return Index(other - Series(self))\n\n    def __and__(self, other):\n        return self.intersection(other)\n\n    def __or__(self, other):\n        return self.union(other)\n\n    def __xor__(self, other):\n        return self.symmetric_difference(other)\n\n    def __nonzero__(self):\n        raise ValueError(\n            \"The truth value of a {0} is ambiguous. \"\n            \"Use a.empty, a.bool(), a.item(), a.any() or a.all().\".format(\n                self.__class__.__name__\n            )\n        )\n\n    __bool__ = __nonzero__\n\n    # --------------------------------------------------------------------\n    # Set Operation Methods\n\n    def _get_reconciled_name_object(self, other):\n        \"\"\"\n        If the result of a set operation will be self,\n        return self, unless the name changes, in which\n        case make a shallow copy of self.\n        \"\"\"\n        name = get_op_result_name(self, other)\n        if self.name != name:\n            return self._shallow_copy(name=name)\n        return self\n\n    def _union_incompatible_dtypes(self, other, sort):\n        \"\"\"\n        Casts this and other index to object dtype to allow the formation\n        of a union between incompatible types.\n\n        Parameters\n        ----------\n        other : Index or array-like\n        sort : False or None, default False\n            Whether to sort the resulting index.\n\n            * False : do not sort the result.\n            * None : sort the result, except when `self` and `other` are equal\n              or when the values cannot be compared.\n\n        Returns\n        -------\n        Index\n        \"\"\"\n        this = self.astype(object, copy=False)\n        # cast to Index for when `other` is list-like\n        other = Index(other).astype(object, copy=False)\n        return Index.union(this, other, sort=sort).astype(object, copy=False)\n\n    def _is_compatible_with_other(self, other):\n        \"\"\"\n        Check whether this and the other dtype are compatible with each other.\n        Meaning a union can be formed between them without needing to be cast\n        to dtype object.\n\n        Parameters\n        ----------\n        other : Index or array-like\n\n        Returns\n        -------\n        bool\n        \"\"\"\n        return type(self) is type(other) and is_dtype_equal(self.dtype, other.dtype)\n\n    def _validate_sort_keyword(self, sort):\n        if sort not in [None, False]:\n            raise ValueError(\n                \"The 'sort' keyword only takes the values of \"\n                \"None or False; {0} was passed.\".format(sort)\n            )\n\n    def union(self, other, sort=None):\n        \"\"\"\n        Form the union of two Index objects.\n\n        If the Index objects are incompatible, both Index objects will be\n        cast to dtype('object') first.\n\n            .. versionchanged:: 0.25.0\n\n        Parameters\n        ----------\n        other : Index or array-like\n        sort : bool or None, default None\n            Whether to sort the resulting Index.\n\n            * None : Sort the result, except when\n\n              1. `self` and `other` are equal.\n              2. `self` or `other` has length 0.\n              3. Some values in `self` or `other` cannot be compared.\n                 A RuntimeWarning is issued in this case.\n\n            * False : do not sort the result.\n\n            .. versionadded:: 0.24.0\n\n            .. versionchanged:: 0.24.1\n\n               Changed the default value from ``True`` to ``None``\n               (without change in behaviour).\n\n        Returns\n        -------\n        union : Index\n\n        Examples\n        --------\n\n        Union matching dtypes\n\n        >>> idx1 = pd.Index([1, 2, 3, 4])\n        >>> idx2 = pd.Index([3, 4, 5, 6])\n        >>> idx1.union(idx2)\n        Int64Index([1, 2, 3, 4, 5, 6], dtype='int64')\n\n        Union mismatched dtypes\n\n        >>> idx1 = pd.Index(['a', 'b', 'c', 'd'])\n        >>> idx2 = pd.Index([1, 2, 3, 4])\n        >>> idx1.union(idx2)\n        Index(['a', 'b', 'c', 'd', 1, 2, 3, 4], dtype='object')\n        \"\"\"\n        self._validate_sort_keyword(sort)\n        self._assert_can_do_setop(other)\n\n        if not self._is_compatible_with_other(other):\n            return self._union_incompatible_dtypes(other, sort=sort)\n\n        return self._union(other, sort=sort)\n\n    def _union(self, other, sort):\n        \"\"\"\n        Specific union logic should go here. In subclasses, union behavior\n        should be overwritten here rather than in `self.union`.\n\n        Parameters\n        ----------\n        other : Index or array-like\n        sort : False or None, default False\n            Whether to sort the resulting index.\n\n            * False : do not sort the result.\n            * None : sort the result, except when `self` and `other` are equal\n              or when the values cannot be compared.\n\n        Returns\n        -------\n        Index\n        \"\"\"\n\n        if not len(other) or self.equals(other):\n            return self._get_reconciled_name_object(other)\n\n        if not len(self):\n            return other._get_reconciled_name_object(self)\n\n        # TODO(EA): setops-refactor, clean all this up\n        if is_period_dtype(self) or is_datetime64tz_dtype(self):\n            lvals = self._ndarray_values\n        else:\n            lvals = self._values\n        if is_period_dtype(other) or is_datetime64tz_dtype(other):\n            rvals = other._ndarray_values\n        else:\n            rvals = other._values\n\n        if sort is None and self.is_monotonic and other.is_monotonic:\n            try:\n                result = self._outer_indexer(lvals, rvals)[0]\n            except TypeError:\n                # incomparable objects\n                result = list(lvals)\n\n                # worth making this faster? a very unusual case\n                value_set = set(lvals)\n                result.extend([x for x in rvals if x not in value_set])\n        else:\n            indexer = self.get_indexer(other)\n            indexer, = (indexer == -1).nonzero()\n\n            if len(indexer) > 0:\n                other_diff = algos.take_nd(rvals, indexer, allow_fill=False)\n                result = concat_compat((lvals, other_diff))\n\n            else:\n                result = lvals\n\n            if sort is None:\n                try:\n                    result = sorting.safe_sort(result)\n                except TypeError as e:\n                    warnings.warn(\n                        \"{}, sort order is undefined for \"\n                        \"incomparable objects\".format(e),\n                        RuntimeWarning,\n                        stacklevel=3,\n                    )\n\n        # for subclasses\n        return self._wrap_setop_result(other, result)\n\n    def _wrap_setop_result(self, other, result):\n        return self._constructor(result, name=get_op_result_name(self, other))\n\n    _index_shared_docs[\n        \"intersection\"\n    ] = \"\"\"\n        Form the intersection of two Index objects.\n\n        This returns a new Index with elements common to the index and `other`.\n\n        Parameters\n        ----------\n        other : Index or array-like\n        sort : False or None, default False\n            Whether to sort the resulting index.\n\n            * False : do not sort the result.\n            * None : sort the result, except when `self` and `other` are equal\n              or when the values cannot be compared.\n\n            .. versionadded:: 0.24.0\n\n            .. versionchanged:: 0.24.1\n\n               Changed the default from ``True`` to ``False``, to match\n               the behaviour of 0.23.4 and earlier.\n\n        Returns\n        -------\n        intersection : Index\n\n        Examples\n        --------\n\n        >>> idx1 = pd.Index([1, 2, 3, 4])\n        >>> idx2 = pd.Index([3, 4, 5, 6])\n        >>> idx1.intersection(idx2)\n        Int64Index([3, 4], dtype='int64')\n        \"\"\"\n\n    # TODO: standardize return type of non-union setops type(self vs other)\n    @Appender(_index_shared_docs[\"intersection\"])\n    def intersection(self, other, sort=False):\n        self._validate_sort_keyword(sort)\n        self._assert_can_do_setop(other)\n        other = ensure_index(other)\n\n        if self.equals(other):\n            return self._get_reconciled_name_object(other)\n\n        if not is_dtype_equal(self.dtype, other.dtype):\n            this = self.astype(\"O\")\n            other = other.astype(\"O\")\n            return this.intersection(other, sort=sort)\n\n        # TODO(EA): setops-refactor, clean all this up\n        if is_period_dtype(self):\n            lvals = self._ndarray_values\n        else:\n            lvals = self._values\n        if is_period_dtype(other):\n            rvals = other._ndarray_values\n        else:\n            rvals = other._values\n\n        if self.is_monotonic and other.is_monotonic:\n            try:\n                result = self._inner_indexer(lvals, rvals)[0]\n                return self._wrap_setop_result(other, result)\n            except TypeError:\n                pass\n\n        try:\n            indexer = Index(rvals).get_indexer(lvals)\n            indexer = indexer.take((indexer != -1).nonzero()[0])\n        except Exception:\n            # duplicates\n            indexer = algos.unique1d(Index(rvals).get_indexer_non_unique(lvals)[0])\n            indexer = indexer[indexer != -1]\n\n        taken = other.take(indexer)\n\n        if sort is None:\n            taken = sorting.safe_sort(taken.values)\n            if self.name != other.name:\n                name = None\n            else:\n                name = self.name\n            return self._shallow_copy(taken, name=name)\n\n        if self.name != other.name:\n            taken.name = None\n\n        return taken\n\n    def difference(self, other, sort=None):\n        \"\"\"\n        Return a new Index with elements from the index that are not in\n        `other`.\n\n        This is the set difference of two Index objects.\n\n        Parameters\n        ----------\n        other : Index or array-like\n        sort : False or None, default None\n            Whether to sort the resulting index. By default, the\n            values are attempted to be sorted, but any TypeError from\n            incomparable elements is caught by pandas.\n\n            * None : Attempt to sort the result, but catch any TypeErrors\n              from comparing incomparable elements.\n            * False : Do not sort the result.\n\n            .. versionadded:: 0.24.0\n\n            .. versionchanged:: 0.24.1\n\n               Changed the default value from ``True`` to ``None``\n               (without change in behaviour).\n\n        Returns\n        -------\n        difference : Index\n\n        Examples\n        --------\n\n        >>> idx1 = pd.Index([2, 1, 3, 4])\n        >>> idx2 = pd.Index([3, 4, 5, 6])\n        >>> idx1.difference(idx2)\n        Int64Index([1, 2], dtype='int64')\n        >>> idx1.difference(idx2, sort=False)\n        Int64Index([2, 1], dtype='int64')\n        \"\"\"\n        self._validate_sort_keyword(sort)\n        self._assert_can_do_setop(other)\n\n        if self.equals(other):\n            # pass an empty np.ndarray with the appropriate dtype\n            return self._shallow_copy(self._data[:0])\n\n        other, result_name = self._convert_can_do_setop(other)\n\n        this = self._get_unique_index()\n\n        indexer = this.get_indexer(other)\n        indexer = indexer.take((indexer != -1).nonzero()[0])\n\n        label_diff = np.setdiff1d(np.arange(this.size), indexer, assume_unique=True)\n        the_diff = this.values.take(label_diff)\n        if sort is None:\n            try:\n                the_diff = sorting.safe_sort(the_diff)\n            except TypeError:\n                pass\n\n        return this._shallow_copy(the_diff, name=result_name, freq=None)\n\n    def symmetric_difference(self, other, result_name=None, sort=None):\n        \"\"\"\n        Compute the symmetric difference of two Index objects.\n\n        Parameters\n        ----------\n        other : Index or array-like\n        result_name : str\n        sort : False or None, default None\n            Whether to sort the resulting index. By default, the\n            values are attempted to be sorted, but any TypeError from\n            incomparable elements is caught by pandas.\n\n            * None : Attempt to sort the result, but catch any TypeErrors\n              from comparing incomparable elements.\n            * False : Do not sort the result.\n\n            .. versionadded:: 0.24.0\n\n            .. versionchanged:: 0.24.1\n\n               Changed the default value from ``True`` to ``None``\n               (without change in behaviour).\n\n        Returns\n        -------\n        symmetric_difference : Index\n\n        Notes\n        -----\n        ``symmetric_difference`` contains elements that appear in either\n        ``idx1`` or ``idx2`` but not both. Equivalent to the Index created by\n        ``idx1.difference(idx2) | idx2.difference(idx1)`` with duplicates\n        dropped.\n\n        Examples\n        --------\n        >>> idx1 = pd.Index([1, 2, 3, 4])\n        >>> idx2 = pd.Index([2, 3, 4, 5])\n        >>> idx1.symmetric_difference(idx2)\n        Int64Index([1, 5], dtype='int64')\n\n        You can also use the ``^`` operator:\n\n        >>> idx1 ^ idx2\n        Int64Index([1, 5], dtype='int64')\n        \"\"\"\n        self._validate_sort_keyword(sort)\n        self._assert_can_do_setop(other)\n        other, result_name_update = self._convert_can_do_setop(other)\n        if result_name is None:\n            result_name = result_name_update\n\n        this = self._get_unique_index()\n        other = other._get_unique_index()\n        indexer = this.get_indexer(other)\n\n        # {this} minus {other}\n        common_indexer = indexer.take((indexer != -1).nonzero()[0])\n        left_indexer = np.setdiff1d(\n            np.arange(this.size), common_indexer, assume_unique=True\n        )\n        left_diff = this.values.take(left_indexer)\n\n        # {other} minus {this}\n        right_indexer = (indexer == -1).nonzero()[0]\n        right_diff = other.values.take(right_indexer)\n\n        the_diff = concat_compat([left_diff, right_diff])\n        if sort is None:\n            try:\n                the_diff = sorting.safe_sort(the_diff)\n            except TypeError:\n                pass\n\n        attribs = self._get_attributes_dict()\n        attribs[\"name\"] = result_name\n        if \"freq\" in attribs:\n            attribs[\"freq\"] = None\n        return self._shallow_copy_with_infer(the_diff, **attribs)\n\n    def _assert_can_do_setop(self, other):\n        if not is_list_like(other):\n            raise TypeError(\"Input must be Index or array-like\")\n        return True\n\n    def _convert_can_do_setop(self, other):\n        if not isinstance(other, Index):\n            other = Index(other, name=self.name)\n            result_name = self.name\n        else:\n            result_name = get_op_result_name(self, other)\n        return other, result_name\n\n    # --------------------------------------------------------------------\n    # Indexing Methods\n\n    _index_shared_docs[\n        \"get_loc\"\n    ] = \"\"\"\n        Get integer location, slice or boolean mask for requested label.\n\n        Parameters\n        ----------\n        key : label\n        method : {None, 'pad'/'ffill', 'backfill'/'bfill', 'nearest'}, optional\n            * default: exact matches only.\n            * pad / ffill: find the PREVIOUS index value if no exact match.\n            * backfill / bfill: use NEXT index value if no exact match\n            * nearest: use the NEAREST index value if no exact match. Tied\n              distances are broken by preferring the larger index value.\n        tolerance : int or float, optional\n            Maximum distance from index value for inexact matches. The value of\n            the index at the matching location most satisfy the equation\n            ``abs(index[loc] - key) <= tolerance``.\n\n            .. versionadded:: 0.21.0 (list-like tolerance)\n\n        Returns\n        -------\n        loc : int if unique index, slice if monotonic index, else mask\n\n        Examples\n        --------\n        >>> unique_index = pd.Index(list('abc'))\n        >>> unique_index.get_loc('b')\n        1\n\n        >>> monotonic_index = pd.Index(list('abbc'))\n        >>> monotonic_index.get_loc('b')\n        slice(1, 3, None)\n\n        >>> non_monotonic_index = pd.Index(list('abcb'))\n        >>> non_monotonic_index.get_loc('b')\n        array([False,  True, False,  True], dtype=bool)\n        \"\"\"\n\n    @Appender(_index_shared_docs[\"get_loc\"])\n    def get_loc(self, key, method=None, tolerance=None):\n        if method is None:\n            if tolerance is not None:\n                raise ValueError(\n                    \"tolerance argument only valid if using pad, \"\n                    \"backfill or nearest lookups\"\n                )\n            try:\n                return self._engine.get_loc(key)\n            except KeyError:\n                return self._engine.get_loc(self._maybe_cast_indexer(key))\n        indexer = self.get_indexer([key], method=method, tolerance=tolerance)\n        if indexer.ndim > 1 or indexer.size > 1:\n            raise TypeError(\"get_loc requires scalar valued input\")\n        loc = indexer.item()\n        if loc == -1:\n            raise KeyError(key)\n        return loc\n\n    _index_shared_docs[\n        \"get_indexer\"\n    ] = \"\"\"\n        Compute indexer and mask for new index given the current index. The\n        indexer should be then used as an input to ndarray.take to align the\n        current data to the new index.\n\n        Parameters\n        ----------\n        target : %(target_klass)s\n        method : {None, 'pad'/'ffill', 'backfill'/'bfill', 'nearest'}, optional\n            * default: exact matches only.\n            * pad / ffill: find the PREVIOUS index value if no exact match.\n            * backfill / bfill: use NEXT index value if no exact match\n            * nearest: use the NEAREST index value if no exact match. Tied\n              distances are broken by preferring the larger index value.\n        limit : int, optional\n            Maximum number of consecutive labels in ``target`` to match for\n            inexact matches.\n        tolerance : optional\n            Maximum distance between original and new labels for inexact\n            matches. The values of the index at the matching locations most\n            satisfy the equation ``abs(index[indexer] - target) <= tolerance``.\n\n            Tolerance may be a scalar value, which applies the same tolerance\n            to all values, or list-like, which applies variable tolerance per\n            element. List-like includes list, tuple, array, Series, and must be\n            the same size as the index and its dtype must exactly match the\n            index's type.\n\n            .. versionadded:: 0.21.0 (list-like tolerance)\n\n        Returns\n        -------\n        indexer : ndarray of int\n            Integers from 0 to n - 1 indicating that the index at these\n            positions matches the corresponding target values. Missing values\n            in the target are marked by -1.\n        %(raises_section)s\n        Examples\n        --------\n        >>> index = pd.Index(['c', 'a', 'b'])\n        >>> index.get_indexer(['a', 'b', 'x'])\n        array([ 1,  2, -1])\n\n        Notice that the return value is an array of locations in ``index``\n        and ``x`` is marked by -1, as it is not in ``index``.\n        \"\"\"\n\n    @Appender(_index_shared_docs[\"get_indexer\"] % _index_doc_kwargs)\n    def get_indexer(self, target, method=None, limit=None, tolerance=None):\n        method = missing.clean_reindex_fill_method(method)\n        target = ensure_index(target)\n        if tolerance is not None:\n            tolerance = self._convert_tolerance(tolerance, target)\n\n        # Treat boolean labels passed to a numeric index as not found. Without\n        # this fix False and True would be treated as 0 and 1 respectively.\n        # (GH #16877)\n        if target.is_boolean() and self.is_numeric():\n            return ensure_platform_int(np.repeat(-1, target.size))\n\n        pself, ptarget = self._maybe_promote(target)\n        if pself is not self or ptarget is not target:\n            return pself.get_indexer(\n                ptarget, method=method, limit=limit, tolerance=tolerance\n            )\n\n        if not is_dtype_equal(self.dtype, target.dtype):\n            this = self.astype(object)\n            target = target.astype(object)\n            return this.get_indexer(\n                target, method=method, limit=limit, tolerance=tolerance\n            )\n\n        if not self.is_unique:\n            raise InvalidIndexError(\n                \"Reindexing only valid with uniquely valued Index objects\"\n            )\n\n        if method == \"pad\" or method == \"backfill\":\n            indexer = self._get_fill_indexer(target, method, limit, tolerance)\n        elif method == \"nearest\":\n            indexer = self._get_nearest_indexer(target, limit, tolerance)\n        else:\n            if tolerance is not None:\n                raise ValueError(\n                    \"tolerance argument only valid if doing pad, \"\n                    \"backfill or nearest reindexing\"\n                )\n            if limit is not None:\n                raise ValueError(\n                    \"limit argument only valid if doing pad, \"\n                    \"backfill or nearest reindexing\"\n                )\n\n            indexer = self._engine.get_indexer(target._ndarray_values)\n\n        return ensure_platform_int(indexer)\n\n    def _convert_tolerance(self, tolerance, target):\n        # override this method on subclasses\n        tolerance = np.asarray(tolerance)\n        if target.size != tolerance.size and tolerance.size > 1:\n            raise ValueError(\"list-like tolerance size must match target index size\")\n        return tolerance\n\n    def _get_fill_indexer(self, target, method, limit=None, tolerance=None):\n        if self.is_monotonic_increasing and target.is_monotonic_increasing:\n            method = (\n                self._engine.get_pad_indexer\n                if method == \"pad\"\n                else self._engine.get_backfill_indexer\n            )\n            indexer = method(target._ndarray_values, limit)\n        else:\n            indexer = self._get_fill_indexer_searchsorted(target, method, limit)\n        if tolerance is not None:\n            indexer = self._filter_indexer_tolerance(\n                target._ndarray_values, indexer, tolerance\n            )\n        return indexer\n\n    def _get_fill_indexer_searchsorted(self, target, method, limit=None):\n        \"\"\"\n        Fallback pad/backfill get_indexer that works for monotonic decreasing\n        indexes and non-monotonic targets.\n        \"\"\"\n        if limit is not None:\n            raise ValueError(\n                \"limit argument for %r method only well-defined \"\n                \"if index and target are monotonic\" % method\n            )\n\n        side = \"left\" if method == \"pad\" else \"right\"\n\n        # find exact matches first (this simplifies the algorithm)\n        indexer = self.get_indexer(target)\n        nonexact = indexer == -1\n        indexer[nonexact] = self._searchsorted_monotonic(target[nonexact], side)\n        if side == \"left\":\n            # searchsorted returns \"indices into a sorted array such that,\n            # if the corresponding elements in v were inserted before the\n            # indices, the order of a would be preserved\".\n            # Thus, we need to subtract 1 to find values to the left.\n            indexer[nonexact] -= 1\n            # This also mapped not found values (values of 0 from\n            # np.searchsorted) to -1, which conveniently is also our\n            # sentinel for missing values\n        else:\n            # Mark indices to the right of the largest value as not found\n            indexer[indexer == len(self)] = -1\n        return indexer\n\n    def _get_nearest_indexer(self, target, limit, tolerance):\n        \"\"\"\n        Get the indexer for the nearest index labels; requires an index with\n        values that can be subtracted from each other (e.g., not strings or\n        tuples).\n        \"\"\"\n        left_indexer = self.get_indexer(target, \"pad\", limit=limit)\n        right_indexer = self.get_indexer(target, \"backfill\", limit=limit)\n\n        target = np.asarray(target)\n        left_distances = abs(self.values[left_indexer] - target)\n        right_distances = abs(self.values[right_indexer] - target)\n\n        op = operator.lt if self.is_monotonic_increasing else operator.le\n        indexer = np.where(\n            op(left_distances, right_distances) | (right_indexer == -1),\n            left_indexer,\n            right_indexer,\n        )\n        if tolerance is not None:\n            indexer = self._filter_indexer_tolerance(target, indexer, tolerance)\n        return indexer\n\n    def _filter_indexer_tolerance(self, target, indexer, tolerance):\n        distance = abs(self.values[indexer] - target)\n        indexer = np.where(distance <= tolerance, indexer, -1)\n        return indexer\n\n    # --------------------------------------------------------------------\n    # Indexer Conversion Methods\n\n    _index_shared_docs[\n        \"_convert_scalar_indexer\"\n    ] = \"\"\"\n        Convert a scalar indexer.\n\n        Parameters\n        ----------\n        key : label of the slice bound\n        kind : {'ix', 'loc', 'getitem', 'iloc'} or None\n    \"\"\"\n\n    @Appender(_index_shared_docs[\"_convert_scalar_indexer\"])\n    def _convert_scalar_indexer(self, key, kind=None):\n        assert kind in [\"ix\", \"loc\", \"getitem\", \"iloc\", None]\n\n        if kind == \"iloc\":\n            return self._validate_indexer(\"positional\", key, kind)\n\n        if len(self) and not isinstance(self, ABCMultiIndex):\n\n            # we can raise here if we are definitive that this\n            # is positional indexing (eg. .ix on with a float)\n            # or label indexing if we are using a type able\n            # to be represented in the index\n\n            if kind in [\"getitem\", \"ix\"] and is_float(key):\n                if not self.is_floating():\n                    return self._invalid_indexer(\"label\", key)\n\n            elif kind in [\"loc\"] and is_float(key):\n\n                # we want to raise KeyError on string/mixed here\n                # technically we *could* raise a TypeError\n                # on anything but mixed though\n                if self.inferred_type not in [\n                    \"floating\",\n                    \"mixed-integer-float\",\n                    \"integer-na\",\n                    \"string\",\n                    \"unicode\",\n                    \"mixed\",\n                ]:\n                    return self._invalid_indexer(\"label\", key)\n\n            elif kind in [\"loc\"] and is_integer(key):\n                if not self.holds_integer():\n                    return self._invalid_indexer(\"label\", key)\n\n        return key\n\n    _index_shared_docs[\n        \"_convert_slice_indexer\"\n    ] = \"\"\"\n        Convert a slice indexer.\n\n        By definition, these are labels unless 'iloc' is passed in.\n        Floats are not allowed as the start, step, or stop of the slice.\n\n        Parameters\n        ----------\n        key : label of the slice bound\n        kind : {'ix', 'loc', 'getitem', 'iloc'} or None\n    \"\"\"\n\n    @Appender(_index_shared_docs[\"_convert_slice_indexer\"])\n    def _convert_slice_indexer(self, key: slice, kind=None):\n        assert kind in [\"ix\", \"loc\", \"getitem\", \"iloc\", None]\n\n        # validate iloc\n        if kind == \"iloc\":\n            return slice(\n                self._validate_indexer(\"slice\", key.start, kind),\n                self._validate_indexer(\"slice\", key.stop, kind),\n                self._validate_indexer(\"slice\", key.step, kind),\n            )\n\n        # potentially cast the bounds to integers\n        start, stop, step = key.start, key.stop, key.step\n\n        # figure out if this is a positional indexer\n        def is_int(v):\n            return v is None or is_integer(v)\n\n        is_null_slicer = start is None and stop is None\n        is_index_slice = is_int(start) and is_int(stop)\n        is_positional = is_index_slice and not self.is_integer()\n\n        if kind == \"getitem\":\n            \"\"\"\n            called from the getitem slicers, validate that we are in fact\n            integers\n            \"\"\"\n            if self.is_integer() or is_index_slice:\n                return slice(\n                    self._validate_indexer(\"slice\", key.start, kind),\n                    self._validate_indexer(\"slice\", key.stop, kind),\n                    self._validate_indexer(\"slice\", key.step, kind),\n                )\n\n        # convert the slice to an indexer here\n\n        # if we are mixed and have integers\n        try:\n            if is_positional and self.is_mixed():\n                # Validate start & stop\n                if start is not None:\n                    self.get_loc(start)\n                if stop is not None:\n                    self.get_loc(stop)\n                is_positional = False\n        except KeyError:\n            if self.inferred_type in [\"mixed-integer-float\", \"integer-na\"]:\n                raise\n\n        if is_null_slicer:\n            indexer = key\n        elif is_positional:\n            indexer = key\n        else:\n            try:\n                indexer = self.slice_indexer(start, stop, step, kind=kind)\n            except Exception:\n                if is_index_slice:\n                    if self.is_integer():\n                        raise\n                    else:\n                        indexer = key\n                else:\n                    raise\n\n        return indexer\n\n    def _convert_listlike_indexer(self, keyarr, kind=None):\n        \"\"\"\n        Parameters\n        ----------\n        keyarr : list-like\n            Indexer to convert.\n\n        Returns\n        -------\n        indexer : numpy.ndarray or None\n            Return an ndarray or None if cannot convert.\n        keyarr : numpy.ndarray\n            Return tuple-safe keys.\n        \"\"\"\n        if isinstance(keyarr, Index):\n            keyarr = self._convert_index_indexer(keyarr)\n        else:\n            keyarr = self._convert_arr_indexer(keyarr)\n\n        indexer = self._convert_list_indexer(keyarr, kind=kind)\n        return indexer, keyarr\n\n    _index_shared_docs[\n        \"_convert_arr_indexer\"\n    ] = \"\"\"\n        Convert an array-like indexer to the appropriate dtype.\n\n        Parameters\n        ----------\n        keyarr : array-like\n            Indexer to convert.\n\n        Returns\n        -------\n        converted_keyarr : array-like\n    \"\"\"\n\n    @Appender(_index_shared_docs[\"_convert_arr_indexer\"])\n    def _convert_arr_indexer(self, keyarr):\n        keyarr = com.asarray_tuplesafe(keyarr)\n        return keyarr\n\n    _index_shared_docs[\n        \"_convert_index_indexer\"\n    ] = \"\"\"\n        Convert an Index indexer to the appropriate dtype.\n\n        Parameters\n        ----------\n        keyarr : Index (or sub-class)\n            Indexer to convert.\n\n        Returns\n        -------\n        converted_keyarr : Index (or sub-class)\n    \"\"\"\n\n    @Appender(_index_shared_docs[\"_convert_index_indexer\"])\n    def _convert_index_indexer(self, keyarr):\n        return keyarr\n\n    _index_shared_docs[\n        \"_convert_list_indexer\"\n    ] = \"\"\"\n        Convert a list-like indexer to the appropriate dtype.\n\n        Parameters\n        ----------\n        keyarr : Index (or sub-class)\n            Indexer to convert.\n        kind : iloc, ix, loc, optional\n\n        Returns\n        -------\n        positional indexer or None\n    \"\"\"\n\n    @Appender(_index_shared_docs[\"_convert_list_indexer\"])\n    def _convert_list_indexer(self, keyarr, kind=None):\n        if (\n            kind in [None, \"iloc\", \"ix\"]\n            and is_integer_dtype(keyarr)\n            and not self.is_floating()\n            and not isinstance(keyarr, ABCPeriodIndex)\n        ):\n\n            if self.inferred_type == \"mixed-integer\":\n                indexer = self.get_indexer(keyarr)\n                if (indexer >= 0).all():\n                    return indexer\n                # missing values are flagged as -1 by get_indexer and negative\n                # indices are already converted to positive indices in the\n                # above if-statement, so the negative flags are changed to\n                # values outside the range of indices so as to trigger an\n                # IndexError in maybe_convert_indices\n                indexer[indexer < 0] = len(self)\n\n                return maybe_convert_indices(indexer, len(self))\n\n            elif not self.inferred_type == \"integer\":\n                keyarr = np.where(keyarr < 0, len(self) + keyarr, keyarr)\n                return keyarr\n\n        return None\n\n    def _invalid_indexer(self, form, key):\n        \"\"\"\n        Consistent invalid indexer message.\n        \"\"\"\n        raise TypeError(\n            \"cannot do {form} indexing on {klass} with these \"\n            \"indexers [{key}] of {kind}\".format(\n                form=form, klass=type(self), key=key, kind=type(key)\n            )\n        )\n\n    # --------------------------------------------------------------------\n    # Reindex Methods\n\n    def _can_reindex(self, indexer):\n        \"\"\"\n        Check if we are allowing reindexing with this particular indexer.\n\n        Parameters\n        ----------\n        indexer : an integer indexer\n\n        Raises\n        ------\n        ValueError if its a duplicate axis\n        \"\"\"\n\n        # trying to reindex on an axis with duplicates\n        if not self.is_unique and len(indexer):\n            raise ValueError(\"cannot reindex from a duplicate axis\")\n\n    def reindex(self, target, method=None, level=None, limit=None, tolerance=None):\n        \"\"\"\n        Create index with target's values (move/add/delete values\n        as necessary).\n\n        Parameters\n        ----------\n        target : an iterable\n\n        Returns\n        -------\n        new_index : pd.Index\n            Resulting index.\n        indexer : np.ndarray or None\n            Indices of output values in original index.\n        \"\"\"\n        # GH6552: preserve names when reindexing to non-named target\n        # (i.e. neither Index nor Series).\n        preserve_names = not hasattr(target, \"name\")\n\n        # GH7774: preserve dtype/tz if target is empty and not an Index.\n        target = _ensure_has_len(target)  # target may be an iterator\n\n        if not isinstance(target, Index) and len(target) == 0:\n            attrs = self._get_attributes_dict()\n            attrs.pop(\"freq\", None)  # don't preserve freq\n            values = self._data[:0]  # appropriately-dtyped empty array\n            target = self._simple_new(values, dtype=self.dtype, **attrs)\n        else:\n            target = ensure_index(target)\n\n        if level is not None:\n            if method is not None:\n                raise TypeError(\"Fill method not supported if level passed\")\n            _, indexer, _ = self._join_level(\n                target, level, how=\"right\", return_indexers=True\n            )\n        else:\n            if self.equals(target):\n                indexer = None\n            else:\n                # check is_overlapping for IntervalIndex compat\n                if self.is_unique and not getattr(self, \"is_overlapping\", False):\n                    indexer = self.get_indexer(\n                        target, method=method, limit=limit, tolerance=tolerance\n                    )\n                else:\n                    if method is not None or limit is not None:\n                        raise ValueError(\n                            \"cannot reindex a non-unique index \"\n                            \"with a method or limit\"\n                        )\n                    indexer, missing = self.get_indexer_non_unique(target)\n\n        if preserve_names and target.nlevels == 1 and target.name != self.name:\n            target = target.copy()\n            target.name = self.name\n\n        return target, indexer\n\n    def _reindex_non_unique(self, target):\n        \"\"\"\n        Create a new index with target's values (move/add/delete values as\n        necessary) use with non-unique Index and a possibly non-unique target.\n\n        Parameters\n        ----------\n        target : an iterable\n\n        Returns\n        -------\n        new_index : pd.Index\n            Resulting index.\n        indexer : np.ndarray or None\n            Indices of output values in original index.\n\n        \"\"\"\n\n        target = ensure_index(target)\n        indexer, missing = self.get_indexer_non_unique(target)\n        check = indexer != -1\n        new_labels = self.take(indexer[check])\n        new_indexer = None\n\n        if len(missing):\n            length = np.arange(len(indexer))\n\n            missing = ensure_platform_int(missing)\n            missing_labels = target.take(missing)\n            missing_indexer = ensure_int64(length[~check])\n            cur_labels = self.take(indexer[check]).values\n            cur_indexer = ensure_int64(length[check])\n\n            new_labels = np.empty(tuple([len(indexer)]), dtype=object)\n            new_labels[cur_indexer] = cur_labels\n            new_labels[missing_indexer] = missing_labels\n\n            # a unique indexer\n            if target.is_unique:\n\n                # see GH5553, make sure we use the right indexer\n                new_indexer = np.arange(len(indexer))\n                new_indexer[cur_indexer] = np.arange(len(cur_labels))\n                new_indexer[missing_indexer] = -1\n\n            # we have a non_unique selector, need to use the original\n            # indexer here\n            else:\n\n                # need to retake to have the same size as the indexer\n                indexer[~check] = -1\n\n                # reset the new indexer to account for the new size\n                new_indexer = np.arange(len(self.take(indexer)))\n                new_indexer[~check] = -1\n\n        new_index = self._shallow_copy_with_infer(new_labels, freq=None)\n        return new_index, indexer, new_indexer\n\n    # --------------------------------------------------------------------\n    # Join Methods\n\n    _index_shared_docs[\n        \"join\"\n    ] = \"\"\"\n        Compute join_index and indexers to conform data\n        structures to the new index.\n\n        Parameters\n        ----------\n        other : Index\n        how : {'left', 'right', 'inner', 'outer'}\n        level : int or level name, default None\n        return_indexers : boolean, default False\n        sort : boolean, default False\n            Sort the join keys lexicographically in the result Index. If False,\n            the order of the join keys depends on the join type (how keyword)\n\n            .. versionadded:: 0.20.0\n\n        Returns\n        -------\n        join_index, (left_indexer, right_indexer)\n        \"\"\"\n\n    @Appender(_index_shared_docs[\"join\"])\n    def join(self, other, how=\"left\", level=None, return_indexers=False, sort=False):\n        self_is_mi = isinstance(self, ABCMultiIndex)\n        other_is_mi = isinstance(other, ABCMultiIndex)\n\n        # try to figure out the join level\n        # GH3662\n        if level is None and (self_is_mi or other_is_mi):\n\n            # have the same levels/names so a simple join\n            if self.names == other.names:\n                pass\n            else:\n                return self._join_multi(other, how=how, return_indexers=return_indexers)\n\n        # join on the level\n        if level is not None and (self_is_mi or other_is_mi):\n            return self._join_level(\n                other, level, how=how, return_indexers=return_indexers\n            )\n\n        other = ensure_index(other)\n\n        if len(other) == 0 and how in (\"left\", \"outer\"):\n            join_index = self._shallow_copy()\n            if return_indexers:\n                rindexer = np.repeat(-1, len(join_index))\n                return join_index, None, rindexer\n            else:\n                return join_index\n\n        if len(self) == 0 and how in (\"right\", \"outer\"):\n            join_index = other._shallow_copy()\n            if return_indexers:\n                lindexer = np.repeat(-1, len(join_index))\n                return join_index, lindexer, None\n            else:\n                return join_index\n\n        if self._join_precedence < other._join_precedence:\n            how = {\"right\": \"left\", \"left\": \"right\"}.get(how, how)\n            result = other.join(\n                self, how=how, level=level, return_indexers=return_indexers\n            )\n            if return_indexers:\n                x, y, z = result\n                result = x, z, y\n            return result\n\n        if not is_dtype_equal(self.dtype, other.dtype):\n            this = self.astype(\"O\")\n            other = other.astype(\"O\")\n            return this.join(other, how=how, return_indexers=return_indexers)\n\n        _validate_join_method(how)\n\n        if not self.is_unique and not other.is_unique:\n            return self._join_non_unique(\n                other, how=how, return_indexers=return_indexers\n            )\n        elif not self.is_unique or not other.is_unique:\n            if self.is_monotonic and other.is_monotonic:\n                return self._join_monotonic(\n                    other, how=how, return_indexers=return_indexers\n                )\n            else:\n                return self._join_non_unique(\n                    other, how=how, return_indexers=return_indexers\n                )\n        elif self.is_monotonic and other.is_monotonic:\n            try:\n                return self._join_monotonic(\n                    other, how=how, return_indexers=return_indexers\n                )\n            except TypeError:\n                pass\n\n        if how == \"left\":\n            join_index = self\n        elif how == \"right\":\n            join_index = other\n        elif how == \"inner\":\n            # TODO: sort=False here for backwards compat. It may\n            # be better to use the sort parameter passed into join\n            join_index = self.intersection(other, sort=False)\n        elif how == \"outer\":\n            # TODO: sort=True here for backwards compat. It may\n            # be better to use the sort parameter passed into join\n            join_index = self.union(other)\n\n        if sort:\n            join_index = join_index.sort_values()\n\n        if return_indexers:\n            if join_index is self:\n                lindexer = None\n            else:\n                lindexer = self.get_indexer(join_index)\n            if join_index is other:\n                rindexer = None\n            else:\n                rindexer = other.get_indexer(join_index)\n            return join_index, lindexer, rindexer\n        else:\n            return join_index\n\n    def _join_multi(self, other, how, return_indexers=True):\n        from .multi import MultiIndex\n        from pandas.core.reshape.merge import _restore_dropped_levels_multijoin\n\n        # figure out join names\n        self_names = set(com.not_none(*self.names))\n        other_names = set(com.not_none(*other.names))\n        overlap = self_names & other_names\n\n        # need at least 1 in common\n        if not overlap:\n            raise ValueError(\"cannot join with no overlapping index names\")\n\n        self_is_mi = isinstance(self, MultiIndex)\n        other_is_mi = isinstance(other, MultiIndex)\n\n        if self_is_mi and other_is_mi:\n\n            # Drop the non-matching levels from left and right respectively\n            ldrop_names = list(self_names - overlap)\n            rdrop_names = list(other_names - overlap)\n\n            self_jnlevels = self.droplevel(ldrop_names)\n            other_jnlevels = other.droplevel(rdrop_names)\n\n            # Join left and right\n            # Join on same leveled multi-index frames is supported\n            join_idx, lidx, ridx = self_jnlevels.join(\n                other_jnlevels, how, return_indexers=True\n            )\n\n            # Restore the dropped levels\n            # Returned index level order is\n            # common levels, ldrop_names, rdrop_names\n            dropped_names = ldrop_names + rdrop_names\n\n            levels, codes, names = _restore_dropped_levels_multijoin(\n                self, other, dropped_names, join_idx, lidx, ridx\n            )\n\n            # Re-create the multi-index\n            multi_join_idx = MultiIndex(\n                levels=levels, codes=codes, names=names, verify_integrity=False\n            )\n\n            multi_join_idx = multi_join_idx.remove_unused_levels()\n\n            return multi_join_idx, lidx, ridx\n\n        jl = list(overlap)[0]\n\n        # Case where only one index is multi\n        # make the indices into mi's that match\n        flip_order = False\n        if self_is_mi:\n            self, other = other, self\n            flip_order = True\n            # flip if join method is right or left\n            how = {\"right\": \"left\", \"left\": \"right\"}.get(how, how)\n\n        level = other.names.index(jl)\n        result = self._join_level(\n            other, level, how=how, return_indexers=return_indexers\n        )\n\n        if flip_order:\n            if isinstance(result, tuple):\n                return result[0], result[2], result[1]\n        return result\n\n    def _join_non_unique(self, other, how=\"left\", return_indexers=False):\n        from pandas.core.reshape.merge import _get_join_indexers\n\n        left_idx, right_idx = _get_join_indexers(\n            [self._ndarray_values], [other._ndarray_values], how=how, sort=True\n        )\n\n        left_idx = ensure_platform_int(left_idx)\n        right_idx = ensure_platform_int(right_idx)\n\n        join_index = np.asarray(self._ndarray_values.take(left_idx))\n        mask = left_idx == -1\n        np.putmask(join_index, mask, other._ndarray_values.take(right_idx))\n\n        join_index = self._wrap_joined_index(join_index, other)\n\n        if return_indexers:\n            return join_index, left_idx, right_idx\n        else:\n            return join_index\n\n    def _join_level(\n        self, other, level, how=\"left\", return_indexers=False, keep_order=True\n    ):\n        \"\"\"\n        The join method *only* affects the level of the resulting\n        MultiIndex. Otherwise it just exactly aligns the Index data to the\n        labels of the level in the MultiIndex.\n\n        If ```keep_order == True```, the order of the data indexed by the\n        MultiIndex will not be changed; otherwise, it will tie out\n        with `other`.\n        \"\"\"\n        from .multi import MultiIndex\n\n        def _get_leaf_sorter(labels):\n            \"\"\"\n            Returns sorter for the inner most level while preserving the\n            order of higher levels.\n            \"\"\"\n            if labels[0].size == 0:\n                return np.empty(0, dtype=\"int64\")\n\n            if len(labels) == 1:\n                lab = ensure_int64(labels[0])\n                sorter, _ = libalgos.groupsort_indexer(lab, 1 + lab.max())\n                return sorter\n\n            # find indexers of beginning of each set of\n            # same-key labels w.r.t all but last level\n            tic = labels[0][:-1] != labels[0][1:]\n            for lab in labels[1:-1]:\n                tic |= lab[:-1] != lab[1:]\n\n            starts = np.hstack(([True], tic, [True])).nonzero()[0]\n            lab = ensure_int64(labels[-1])\n            return lib.get_level_sorter(lab, ensure_int64(starts))\n\n        if isinstance(self, MultiIndex) and isinstance(other, MultiIndex):\n            raise TypeError(\"Join on level between two MultiIndex objects is ambiguous\")\n\n        left, right = self, other\n\n        flip_order = not isinstance(self, MultiIndex)\n        if flip_order:\n            left, right = right, left\n            how = {\"right\": \"left\", \"left\": \"right\"}.get(how, how)\n\n        level = left._get_level_number(level)\n        old_level = left.levels[level]\n\n        if not right.is_unique:\n            raise NotImplementedError(\n                \"Index._join_level on non-unique index is not implemented\"\n            )\n\n        new_level, left_lev_indexer, right_lev_indexer = old_level.join(\n            right, how=how, return_indexers=True\n        )\n\n        if left_lev_indexer is None:\n            if keep_order or len(left) == 0:\n                left_indexer = None\n                join_index = left\n            else:  # sort the leaves\n                left_indexer = _get_leaf_sorter(left.codes[: level + 1])\n                join_index = left[left_indexer]\n\n        else:\n            left_lev_indexer = ensure_int64(left_lev_indexer)\n            rev_indexer = lib.get_reverse_indexer(left_lev_indexer, len(old_level))\n\n            new_lev_codes = algos.take_nd(\n                rev_indexer, left.codes[level], allow_fill=False\n            )\n\n            new_codes = list(left.codes)\n            new_codes[level] = new_lev_codes\n\n            new_levels = list(left.levels)\n            new_levels[level] = new_level\n\n            if keep_order:  # just drop missing values. o.w. keep order\n                left_indexer = np.arange(len(left), dtype=np.intp)\n                mask = new_lev_codes != -1\n                if not mask.all():\n                    new_codes = [lab[mask] for lab in new_codes]\n                    left_indexer = left_indexer[mask]\n\n            else:  # tie out the order with other\n                if level == 0:  # outer most level, take the fast route\n                    ngroups = 1 + new_lev_codes.max()\n                    left_indexer, counts = libalgos.groupsort_indexer(\n                        new_lev_codes, ngroups\n                    )\n\n                    # missing values are placed first; drop them!\n                    left_indexer = left_indexer[counts[0] :]\n                    new_codes = [lab[left_indexer] for lab in new_codes]\n\n                else:  # sort the leaves\n                    mask = new_lev_codes != -1\n                    mask_all = mask.all()\n                    if not mask_all:\n                        new_codes = [lab[mask] for lab in new_codes]\n\n                    left_indexer = _get_leaf_sorter(new_codes[: level + 1])\n                    new_codes = [lab[left_indexer] for lab in new_codes]\n\n                    # left_indexers are w.r.t masked frame.\n                    # reverse to original frame!\n                    if not mask_all:\n                        left_indexer = mask.nonzero()[0][left_indexer]\n\n            join_index = MultiIndex(\n                levels=new_levels,\n                codes=new_codes,\n                names=left.names,\n                verify_integrity=False,\n            )\n\n        if right_lev_indexer is not None:\n            right_indexer = algos.take_nd(\n                right_lev_indexer, join_index.codes[level], allow_fill=False\n            )\n        else:\n            right_indexer = join_index.codes[level]\n\n        if flip_order:\n            left_indexer, right_indexer = right_indexer, left_indexer\n\n        if return_indexers:\n            left_indexer = (\n                None if left_indexer is None else ensure_platform_int(left_indexer)\n            )\n            right_indexer = (\n                None if right_indexer is None else ensure_platform_int(right_indexer)\n            )\n            return join_index, left_indexer, right_indexer\n        else:\n            return join_index\n\n    def _join_monotonic(self, other, how=\"left\", return_indexers=False):\n        if self.equals(other):\n            ret_index = other if how == \"right\" else self\n            if return_indexers:\n                return ret_index, None, None\n            else:\n                return ret_index\n\n        sv = self._ndarray_values\n        ov = other._ndarray_values\n\n        if self.is_unique and other.is_unique:\n            # We can perform much better than the general case\n            if how == \"left\":\n                join_index = self\n                lidx = None\n                ridx = self._left_indexer_unique(sv, ov)\n            elif how == \"right\":\n                join_index = other\n                lidx = self._left_indexer_unique(ov, sv)\n                ridx = None\n            elif how == \"inner\":\n                join_index, lidx, ridx = self._inner_indexer(sv, ov)\n                join_index = self._wrap_joined_index(join_index, other)\n            elif how == \"outer\":\n                join_index, lidx, ridx = self._outer_indexer(sv, ov)\n                join_index = self._wrap_joined_index(join_index, other)\n        else:\n            if how == \"left\":\n                join_index, lidx, ridx = self._left_indexer(sv, ov)\n            elif how == \"right\":\n                join_index, ridx, lidx = self._left_indexer(ov, sv)\n            elif how == \"inner\":\n                join_index, lidx, ridx = self._inner_indexer(sv, ov)\n            elif how == \"outer\":\n                join_index, lidx, ridx = self._outer_indexer(sv, ov)\n            join_index = self._wrap_joined_index(join_index, other)\n\n        if return_indexers:\n            lidx = None if lidx is None else ensure_platform_int(lidx)\n            ridx = None if ridx is None else ensure_platform_int(ridx)\n            return join_index, lidx, ridx\n        else:\n            return join_index\n\n    def _wrap_joined_index(self, joined, other):\n        name = get_op_result_name(self, other)\n        return Index(joined, name=name)\n\n    # --------------------------------------------------------------------\n    # Uncategorized Methods\n\n    @property\n    def values(self):\n        \"\"\"\n        Return an array representing the data in the Index.\n\n        .. warning::\n\n           We recommend using :attr:`Index.array` or\n           :meth:`Index.to_numpy`, depending on whether you need\n           a reference to the underlying data or a NumPy array.\n\n        Returns\n        -------\n        array: numpy.ndarray or ExtensionArray\n\n        See Also\n        --------\n        Index.array : Reference to the underlying data.\n        Index.to_numpy : A NumPy array representing the underlying data.\n        \"\"\"\n        return self._data.view(np.ndarray)\n\n    @property\n    def _values(self) -> Union[ExtensionArray, ABCIndexClass, np.ndarray]:\n        # TODO(EA): remove index types as they become extension arrays\n        \"\"\"\n        The best array representation.\n\n        This is an ndarray, ExtensionArray, or Index subclass. This differs\n        from ``_ndarray_values``, which always returns an ndarray.\n\n        Both ``_values`` and ``_ndarray_values`` are consistent between\n        ``Series`` and ``Index``.\n\n        It may differ from the public '.values' method.\n\n        index             | values          | _values       | _ndarray_values |\n        ----------------- | --------------- | ------------- | --------------- |\n        Index             | ndarray         | ndarray       | ndarray         |\n        CategoricalIndex  | Categorical     | Categorical   | ndarray[int]    |\n        DatetimeIndex     | ndarray[M8ns]   | ndarray[M8ns] | ndarray[M8ns]   |\n        DatetimeIndex[tz] | ndarray[M8ns]   | DTI[tz]       | ndarray[M8ns]   |\n        PeriodIndex       | ndarray[object] | PeriodArray   | ndarray[int]    |\n        IntervalIndex     | IntervalArray   | IntervalArray | ndarray[object] |\n\n        See Also\n        --------\n        values\n        _ndarray_values\n        \"\"\"\n        return self._data\n\n    def get_values(self):\n        \"\"\"\n        Return `Index` data as an `numpy.ndarray`.\n\n        .. deprecated:: 0.25.0\n            Use :meth:`Index.to_numpy` or :attr:`Index.array` instead.\n\n        Returns\n        -------\n        numpy.ndarray\n            A one-dimensional numpy array of the `Index` values.\n\n        See Also\n        --------\n        Index.values : The attribute that get_values wraps.\n\n        Examples\n        --------\n        Getting the `Index` values of a `DataFrame`:\n\n        >>> df = pd.DataFrame([[1, 2, 3], [4, 5, 6], [7, 8, 9]],\n        ...                    index=['a', 'b', 'c'], columns=['A', 'B', 'C'])\n        >>> df\n           A  B  C\n        a  1  2  3\n        b  4  5  6\n        c  7  8  9\n        >>> df.index.get_values()\n        array(['a', 'b', 'c'], dtype=object)\n\n        Standalone `Index` values:\n\n        >>> idx = pd.Index(['1', '2', '3'])\n        >>> idx.get_values()\n        array(['1', '2', '3'], dtype=object)\n\n        `MultiIndex` arrays also have only one dimension:\n\n        >>> midx = pd.MultiIndex.from_arrays([[1, 2, 3], ['a', 'b', 'c']],\n        ...                                  names=('number', 'letter'))\n        >>> midx.get_values()\n        array([(1, 'a'), (2, 'b'), (3, 'c')], dtype=object)\n        >>> midx.get_values().ndim\n        1\n        \"\"\"\n        warnings.warn(\n            \"The 'get_values' method is deprecated and will be removed in a \"\n            \"future version. Use '.to_numpy()' or '.array' instead.\",\n            FutureWarning,\n            stacklevel=2,\n        )\n        return self._internal_get_values()\n\n    def _internal_get_values(self):\n        return self.values\n\n    @Appender(IndexOpsMixin.memory_usage.__doc__)\n    def memory_usage(self, deep=False):\n        result = super().memory_usage(deep=deep)\n\n        # include our engine hashtable\n        result += self._engine.sizeof(deep=deep)\n        return result\n\n    _index_shared_docs[\n        \"where\"\n    ] = \"\"\"\n        Return an Index of same shape as self and whose corresponding\n        entries are from self where cond is True and otherwise are from\n        other.\n\n        Parameters\n        ----------\n        cond : boolean array-like with the same length as self\n        other : scalar, or array-like\n\n        Returns\n        -------\n        Index\n        \"\"\"\n\n    @Appender(_index_shared_docs[\"where\"])\n    def where(self, cond, other=None):\n        if other is None:\n            other = self._na_value\n\n        dtype = self.dtype\n        values = self.values\n\n        if is_bool(other) or is_bool_dtype(other):\n\n            # bools force casting\n            values = values.astype(object)\n            dtype = None\n\n        values = np.where(cond, values, other)\n\n        if self._is_numeric_dtype and np.any(isna(values)):\n            # We can't coerce to the numeric dtype of \"self\" (unless\n            # it's float) if there are NaN values in our output.\n            dtype = None\n\n        return self._shallow_copy_with_infer(values, dtype=dtype)\n\n    # construction helpers\n    @classmethod\n    def _try_convert_to_int_index(cls, data, copy, name, dtype):\n        \"\"\"\n        Attempt to convert an array of data into an integer index.\n\n        Parameters\n        ----------\n        data : The data to convert.\n        copy : Whether to copy the data or not.\n        name : The name of the index returned.\n\n        Returns\n        -------\n        int_index : data converted to either an Int64Index or a\n                    UInt64Index\n\n        Raises\n        ------\n        ValueError if the conversion was not successful.\n        \"\"\"\n\n        from .numeric import Int64Index, UInt64Index\n\n        if not is_unsigned_integer_dtype(dtype):\n            # skip int64 conversion attempt if uint-like dtype is passed, as\n            # this could return Int64Index when UInt64Index is what's desired\n            try:\n                res = data.astype(\"i8\", copy=False)\n                if (res == data).all():\n                    return Int64Index(res, copy=copy, name=name)\n            except (OverflowError, TypeError, ValueError):\n                pass\n\n        # Conversion to int64 failed (possibly due to overflow) or was skipped,\n        # so let's try now with uint64.\n        try:\n            res = data.astype(\"u8\", copy=False)\n            if (res == data).all():\n                return UInt64Index(res, copy=copy, name=name)\n        except (OverflowError, TypeError, ValueError):\n            pass\n\n        raise ValueError\n\n    @classmethod\n    def _scalar_data_error(cls, data):\n        # We return the TypeError so that we can raise it from the constructor\n        #  in order to keep mypy happy\n        return TypeError(\n            \"{0}(...) must be called with a collection of some \"\n            \"kind, {1} was passed\".format(cls.__name__, repr(data))\n        )\n\n    @classmethod\n    def _string_data_error(cls, data):\n        raise TypeError(\n            \"String dtype not supported, you may need \"\n            \"to explicitly cast to a numeric type\"\n        )\n\n    @classmethod\n    def _coerce_to_ndarray(cls, data):\n        \"\"\"\n        Coerces data to ndarray.\n\n        Converts other iterables to list first and then to array.\n        Does not touch ndarrays.\n\n        Raises\n        ------\n        TypeError\n            When the data passed in is a scalar.\n        \"\"\"\n\n        if not isinstance(data, (np.ndarray, Index)):\n            if data is None or is_scalar(data):\n                raise cls._scalar_data_error(data)\n\n            # other iterable of some kind\n            if not isinstance(data, (ABCSeries, list, tuple)):\n                data = list(data)\n            data = np.asarray(data)\n        return data\n\n    def _coerce_scalar_to_index(self, item):\n        \"\"\"\n        We need to coerce a scalar to a compat for our index type.\n\n        Parameters\n        ----------\n        item : scalar item to coerce\n        \"\"\"\n        dtype = self.dtype\n\n        if self._is_numeric_dtype and isna(item):\n            # We can't coerce to the numeric dtype of \"self\" (unless\n            # it's float) if there are NaN values in our output.\n            dtype = None\n\n        return Index([item], dtype=dtype, **self._get_attributes_dict())\n\n    def _to_safe_for_reshape(self):\n        \"\"\"\n        Convert to object if we are a categorical.\n        \"\"\"\n        return self\n\n    def _convert_for_op(self, value):\n        \"\"\"\n        Convert value to be insertable to ndarray.\n        \"\"\"\n        return value\n\n    def _assert_can_do_op(self, value):\n        \"\"\"\n        Check value is valid for scalar op.\n        \"\"\"\n        if not is_scalar(value):\n            msg = \"'value' must be a scalar, passed: {0}\"\n            raise TypeError(msg.format(type(value).__name__))\n\n    @property\n    def _has_complex_internals(self):\n        # to disable groupby tricks in MultiIndex\n        return False\n\n    def _is_memory_usage_qualified(self):\n        \"\"\"\n        Return a boolean if we need a qualified .info display.\n        \"\"\"\n        return self.is_object()\n\n    def is_type_compatible(self, kind):\n        \"\"\"\n        Whether the index type is compatible with the provided type.\n        \"\"\"\n        return kind == self.inferred_type\n\n    _index_shared_docs[\n        \"contains\"\n    ] = \"\"\"\n        Return a boolean indicating whether the provided key is in the index.\n\n        Parameters\n        ----------\n        key : label\n            The key to check if it is present in the index.\n\n        Returns\n        -------\n        bool\n            Whether the key search is in the index.\n\n        See Also\n        --------\n        Index.isin : Returns an ndarray of boolean dtype indicating whether the\n            list-like key is in the index.\n\n        Examples\n        --------\n        >>> idx = pd.Index([1, 2, 3, 4])\n        >>> idx\n        Int64Index([1, 2, 3, 4], dtype='int64')\n\n        >>> 2 in idx\n        True\n        >>> 6 in idx\n        False\n        \"\"\"\n\n    @Appender(_index_shared_docs[\"contains\"] % _index_doc_kwargs)\n    def __contains__(self, key):\n        hash(key)\n        try:\n            return key in self._engine\n        except (OverflowError, TypeError, ValueError):\n            return False\n\n    def contains(self, key):\n        \"\"\"\n        Return a boolean indicating whether the provided key is in the index.\n\n        .. deprecated:: 0.25.0\n            Use ``key in index`` instead of ``index.contains(key)``.\n\n        Returns\n        -------\n        bool\n        \"\"\"\n        warnings.warn(\n            \"The 'contains' method is deprecated and will be removed in a \"\n            \"future version. Use 'key in index' instead of \"\n            \"'index.contains(key)'\",\n            FutureWarning,\n            stacklevel=2,\n        )\n        return key in self\n\n    def __hash__(self):\n        raise TypeError(\"unhashable type: %r\" % type(self).__name__)\n\n    def __setitem__(self, key, value):\n        raise TypeError(\"Index does not support mutable operations\")\n\n    def __getitem__(self, key):\n        \"\"\"\n        Override numpy.ndarray's __getitem__ method to work as desired.\n\n        This function adds lists and Series as valid boolean indexers\n        (ndarrays only supports ndarray with dtype=bool).\n\n        If resulting ndim != 1, plain ndarray is returned instead of\n        corresponding `Index` subclass.\n\n        \"\"\"\n        # There's no custom logic to be implemented in __getslice__, so it's\n        # not overloaded intentionally.\n        getitem = self._data.__getitem__\n        promote = self._shallow_copy\n\n        if is_scalar(key):\n            key = com.cast_scalar_indexer(key)\n            return getitem(key)\n\n        if isinstance(key, slice):\n            # This case is separated from the conditional above to avoid\n            # pessimization of basic indexing.\n            return promote(getitem(key))\n\n        if com.is_bool_indexer(key):\n            key = np.asarray(key, dtype=bool)\n\n        key = com.values_from_object(key)\n        result = getitem(key)\n        if not is_scalar(result):\n            return promote(result)\n        else:\n            return result\n\n    def _can_hold_identifiers_and_holds_name(self, name):\n        \"\"\"\n        Faster check for ``name in self`` when we know `name` is a Python\n        identifier (e.g. in NDFrame.__getattr__, which hits this to support\n        . key lookup). For indexes that can't hold identifiers (everything\n        but object & categorical) we just return False.\n\n        https://github.com/pandas-dev/pandas/issues/19764\n        \"\"\"\n        if self.is_object() or self.is_categorical():\n            return name in self\n        return False\n\n    def append(self, other):\n        \"\"\"\n        Append a collection of Index options together.\n\n        Parameters\n        ----------\n        other : Index or list/tuple of indices\n\n        Returns\n        -------\n        appended : Index\n        \"\"\"\n\n        to_concat = [self]\n\n        if isinstance(other, (list, tuple)):\n            to_concat = to_concat + list(other)\n        else:\n            to_concat.append(other)\n\n        for obj in to_concat:\n            if not isinstance(obj, Index):\n                raise TypeError(\"all inputs must be Index\")\n\n        names = {obj.name for obj in to_concat}\n        name = None if len(names) > 1 else self.name\n\n        return self._concat(to_concat, name)\n\n    def _concat(self, to_concat, name):\n\n        typs = _concat.get_dtype_kinds(to_concat)\n\n        if len(typs) == 1:\n            return self._concat_same_dtype(to_concat, name=name)\n        return Index._concat_same_dtype(self, to_concat, name=name)\n\n    def _concat_same_dtype(self, to_concat, name):\n        \"\"\"\n        Concatenate to_concat which has the same class.\n        \"\"\"\n        # must be overridden in specific classes\n        klasses = (ABCDatetimeIndex, ABCTimedeltaIndex, ABCPeriodIndex, ExtensionArray)\n        to_concat = [\n            x.astype(object) if isinstance(x, klasses) else x for x in to_concat\n        ]\n\n        self = to_concat[0]\n        attribs = self._get_attributes_dict()\n        attribs[\"name\"] = name\n\n        to_concat = [x._values if isinstance(x, Index) else x for x in to_concat]\n\n        return self._shallow_copy_with_infer(np.concatenate(to_concat), **attribs)\n\n    def putmask(self, mask, value):\n        \"\"\"\n        Return a new Index of the values set with the mask.\n\n        Returns\n        -------\n        Index\n\n        See Also\n        --------\n        numpy.ndarray.putmask\n        \"\"\"\n        values = self.values.copy()\n        try:\n            np.putmask(values, mask, self._convert_for_op(value))\n            return self._shallow_copy(values)\n        except (ValueError, TypeError) as err:\n            if is_object_dtype(self):\n                raise err\n\n            # coerces to object\n            return self.astype(object).putmask(mask, value)\n\n    def equals(self, other):\n        \"\"\"\n        Determine if two Index objects contain the same elements.\n\n        Returns\n        -------\n        bool\n            True if \"other\" is an Index and it has the same elements as calling\n            index; False otherwise.\n        \"\"\"\n        if self.is_(other):\n            return True\n\n        if not isinstance(other, Index):\n            return False\n\n        if is_object_dtype(self) and not is_object_dtype(other):\n            # if other is not object, use other's logic for coercion\n            return other.equals(self)\n\n        try:\n            return array_equivalent(\n                com.values_from_object(self), com.values_from_object(other)\n            )\n        except Exception:\n            return False\n\n    def identical(self, other):\n        \"\"\"\n        Similar to equals, but check that other comparable attributes are\n        also equal.\n\n        Returns\n        -------\n        bool\n            If two Index objects have equal elements and same type True,\n            otherwise False.\n        \"\"\"\n        return (\n            self.equals(other)\n            and all(\n                (\n                    getattr(self, c, None) == getattr(other, c, None)\n                    for c in self._comparables\n                )\n            )\n            and type(self) == type(other)\n        )\n\n    def asof(self, label):\n        \"\"\"\n        Return the label from the index, or, if not present, the previous one.\n\n        Assuming that the index is sorted, return the passed index label if it\n        is in the index, or return the previous index label if the passed one\n        is not in the index.\n\n        Parameters\n        ----------\n        label : object\n            The label up to which the method returns the latest index label.\n\n        Returns\n        -------\n        object\n            The passed label if it is in the index. The previous label if the\n            passed label is not in the sorted index or `NaN` if there is no\n            such label.\n\n        See Also\n        --------\n        Series.asof : Return the latest value in a Series up to the\n            passed index.\n        merge_asof : Perform an asof merge (similar to left join but it\n            matches on nearest key rather than equal key).\n        Index.get_loc : An `asof` is a thin wrapper around `get_loc`\n            with method='pad'.\n\n        Examples\n        --------\n        `Index.asof` returns the latest index label up to the passed label.\n\n        >>> idx = pd.Index(['2013-12-31', '2014-01-02', '2014-01-03'])\n        >>> idx.asof('2014-01-01')\n        '2013-12-31'\n\n        If the label is in the index, the method returns the passed label.\n\n        >>> idx.asof('2014-01-02')\n        '2014-01-02'\n\n        If all of the labels in the index are later than the passed label,\n        NaN is returned.\n\n        >>> idx.asof('1999-01-02')\n        nan\n\n        If the index is not sorted, an error is raised.\n\n        >>> idx_not_sorted = pd.Index(['2013-12-31', '2015-01-02',\n        ...                            '2014-01-03'])\n        >>> idx_not_sorted.asof('2013-12-31')\n        Traceback (most recent call last):\n        ValueError: index must be monotonic increasing or decreasing\n        \"\"\"\n        try:\n            loc = self.get_loc(label, method=\"pad\")\n        except KeyError:\n            return self._na_value\n        else:\n            if isinstance(loc, slice):\n                loc = loc.indices(len(self))[-1]\n            return self[loc]\n\n    def asof_locs(self, where, mask):\n        \"\"\"\n        Find the locations (indices) of the labels from the index for\n        every entry in the `where` argument.\n\n        As in the `asof` function, if the label (a particular entry in\n        `where`) is not in the index, the latest index label upto the\n        passed label is chosen and its index returned.\n\n        If all of the labels in the index are later than a label in `where`,\n        -1 is returned.\n\n        `mask` is used to ignore NA values in the index during calculation.\n\n        Parameters\n        ----------\n        where : Index\n            An Index consisting of an array of timestamps.\n        mask : array-like\n            Array of booleans denoting where values in the original\n            data are not NA.\n\n        Returns\n        -------\n        numpy.ndarray\n            An array of locations (indices) of the labels from the Index\n            which correspond to the return values of the `asof` function\n            for every element in `where`.\n        \"\"\"\n        locs = self.values[mask].searchsorted(where.values, side=\"right\")\n        locs = np.where(locs > 0, locs - 1, 0)\n\n        result = np.arange(len(self))[mask].take(locs)\n\n        first = mask.argmax()\n        result[(locs == 0) & (where.values < self.values[first])] = -1\n\n        return result\n\n    def sort_values(self, return_indexer=False, ascending=True):\n        \"\"\"\n        Return a sorted copy of the index.\n\n        Return a sorted copy of the index, and optionally return the indices\n        that sorted the index itself.\n\n        Parameters\n        ----------\n        return_indexer : bool, default False\n            Should the indices that would sort the index be returned.\n        ascending : bool, default True\n            Should the index values be sorted in an ascending order.\n\n        Returns\n        -------\n        sorted_index : pandas.Index\n            Sorted copy of the index.\n        indexer : numpy.ndarray, optional\n            The indices that the index itself was sorted by.\n\n        See Also\n        --------\n        Series.sort_values : Sort values of a Series.\n        DataFrame.sort_values : Sort values in a DataFrame.\n\n        Examples\n        --------\n        >>> idx = pd.Index([10, 100, 1, 1000])\n        >>> idx\n        Int64Index([10, 100, 1, 1000], dtype='int64')\n\n        Sort values in ascending order (default behavior).\n\n        >>> idx.sort_values()\n        Int64Index([1, 10, 100, 1000], dtype='int64')\n\n        Sort values in descending order, and also get the indices `idx` was\n        sorted by.\n\n        >>> idx.sort_values(ascending=False, return_indexer=True)\n        (Int64Index([1000, 100, 10, 1], dtype='int64'), array([3, 1, 0, 2]))\n        \"\"\"\n        _as = self.argsort()\n        if not ascending:\n            _as = _as[::-1]\n\n        sorted_index = self.take(_as)\n\n        if return_indexer:\n            return sorted_index, _as\n        else:\n            return sorted_index\n\n    def sort(self, *args, **kwargs):\n        \"\"\"\n        Use sort_values instead.\n        \"\"\"\n        raise TypeError(\"cannot sort an Index object in-place, use sort_values instead\")\n\n    def shift(self, periods=1, freq=None):\n        \"\"\"\n        Shift index by desired number of time frequency increments.\n\n        This method is for shifting the values of datetime-like indexes\n        by a specified time increment a given number of times.\n\n        Parameters\n        ----------\n        periods : int, default 1\n            Number of periods (or increments) to shift by,\n            can be positive or negative.\n        freq : pandas.DateOffset, pandas.Timedelta or string, optional\n            Frequency increment to shift by.\n            If None, the index is shifted by its own `freq` attribute.\n            Offset aliases are valid strings, e.g., 'D', 'W', 'M' etc.\n\n        Returns\n        -------\n        pandas.Index\n            Shifted index.\n\n        See Also\n        --------\n        Series.shift : Shift values of Series.\n\n        Notes\n        -----\n        This method is only implemented for datetime-like index classes,\n        i.e., DatetimeIndex, PeriodIndex and TimedeltaIndex.\n\n        Examples\n        --------\n        Put the first 5 month starts of 2011 into an index.\n\n        >>> month_starts = pd.date_range('1/1/2011', periods=5, freq='MS')\n        >>> month_starts\n        DatetimeIndex(['2011-01-01', '2011-02-01', '2011-03-01', '2011-04-01',\n                       '2011-05-01'],\n                      dtype='datetime64[ns]', freq='MS')\n\n        Shift the index by 10 days.\n\n        >>> month_starts.shift(10, freq='D')\n        DatetimeIndex(['2011-01-11', '2011-02-11', '2011-03-11', '2011-04-11',\n                       '2011-05-11'],\n                      dtype='datetime64[ns]', freq=None)\n\n        The default value of `freq` is the `freq` attribute of the index,\n        which is 'MS' (month start) in this example.\n\n        >>> month_starts.shift(10)\n        DatetimeIndex(['2011-11-01', '2011-12-01', '2012-01-01', '2012-02-01',\n                       '2012-03-01'],\n                      dtype='datetime64[ns]', freq='MS')\n        \"\"\"\n        raise NotImplementedError(\"Not supported for type %s\" % type(self).__name__)\n\n    def argsort(self, *args, **kwargs):\n        \"\"\"\n        Return the integer indices that would sort the index.\n\n        Parameters\n        ----------\n        *args\n            Passed to `numpy.ndarray.argsort`.\n        **kwargs\n            Passed to `numpy.ndarray.argsort`.\n\n        Returns\n        -------\n        numpy.ndarray\n            Integer indices that would sort the index if used as\n            an indexer.\n\n        See Also\n        --------\n        numpy.argsort : Similar method for NumPy arrays.\n        Index.sort_values : Return sorted copy of Index.\n\n        Examples\n        --------\n        >>> idx = pd.Index(['b', 'a', 'd', 'c'])\n        >>> idx\n        Index(['b', 'a', 'd', 'c'], dtype='object')\n\n        >>> order = idx.argsort()\n        >>> order\n        array([1, 0, 3, 2])\n\n        >>> idx[order]\n        Index(['a', 'b', 'c', 'd'], dtype='object')\n        \"\"\"\n        result = self.asi8\n        if result is None:\n            result = np.array(self)\n        return result.argsort(*args, **kwargs)\n\n    _index_shared_docs[\n        \"get_value\"\n    ] = \"\"\"\n        Fast lookup of value from 1-dimensional ndarray. Only use this if you\n        know what you're doing.\n\n        Returns\n        -------\n        scalar\n            A value in the Series with the index of the key value in self.\n        \"\"\"\n\n    @Appender(_index_shared_docs[\"get_value\"] % _index_doc_kwargs)\n    def get_value(self, series, key):\n\n        # if we have something that is Index-like, then\n        # use this, e.g. DatetimeIndex\n        # Things like `Series._get_value` (via .at) pass the EA directly here.\n        s = getattr(series, \"_values\", series)\n        if isinstance(s, (ExtensionArray, Index)) and is_scalar(key):\n            # GH 20882, 21257\n            # Unify Index and ExtensionArray treatment\n            # First try to convert the key to a location\n            # If that fails, raise a KeyError if an integer\n            # index, otherwise, see if key is an integer, and\n            # try that\n            try:\n                iloc = self.get_loc(key)\n                return s[iloc]\n            except KeyError:\n                if len(self) > 0 and (self.holds_integer() or self.is_boolean()):\n                    raise\n                elif is_integer(key):\n                    return s[key]\n\n        s = com.values_from_object(series)\n        k = com.values_from_object(key)\n\n        k = self._convert_scalar_indexer(k, kind=\"getitem\")\n        try:\n            return self._engine.get_value(s, k, tz=getattr(series.dtype, \"tz\", None))\n        except KeyError as e1:\n            if len(self) > 0 and (self.holds_integer() or self.is_boolean()):\n                raise\n\n            try:\n                return libindex.get_value_at(s, key)\n            except IndexError:\n                raise\n            except TypeError:\n                # generator/iterator-like\n                if is_iterator(key):\n                    raise InvalidIndexError(key)\n                else:\n                    raise e1\n            except Exception:  # pragma: no cover\n                raise e1\n        except TypeError:\n            # python 3\n            if is_scalar(key):  # pragma: no cover\n                raise IndexError(key)\n            raise InvalidIndexError(key)\n\n    def set_value(self, arr, key, value):\n        \"\"\"\n        Fast lookup of value from 1-dimensional ndarray.\n\n        Notes\n        -----\n        Only use this if you know what you're doing.\n        \"\"\"\n        self._engine.set_value(\n            com.values_from_object(arr), com.values_from_object(key), value\n        )\n\n    _index_shared_docs[\n        \"get_indexer_non_unique\"\n    ] = \"\"\"\n        Compute indexer and mask for new index given the current index. The\n        indexer should be then used as an input to ndarray.take to align the\n        current data to the new index.\n\n        Parameters\n        ----------\n        target : %(target_klass)s\n\n        Returns\n        -------\n        indexer : ndarray of int\n            Integers from 0 to n - 1 indicating that the index at these\n            positions matches the corresponding target values. Missing values\n            in the target are marked by -1.\n        missing : ndarray of int\n            An indexer into the target of the values not found.\n            These correspond to the -1 in the indexer array.\n        \"\"\"\n\n    @Appender(_index_shared_docs[\"get_indexer_non_unique\"] % _index_doc_kwargs)\n    def get_indexer_non_unique(self, target):\n        target = ensure_index(target)\n        if is_categorical(target):\n            target = target.astype(target.dtype.categories.dtype)\n        pself, ptarget = self._maybe_promote(target)\n        if pself is not self or ptarget is not target:\n            return pself.get_indexer_non_unique(ptarget)\n\n        if self.is_all_dates:\n            tgt_values = target.asi8\n        else:\n            tgt_values = target._ndarray_values\n\n        indexer, missing = self._engine.get_indexer_non_unique(tgt_values)\n        return ensure_platform_int(indexer), missing\n\n    def get_indexer_for(self, target, **kwargs):\n        \"\"\"\n        Guaranteed return of an indexer even when non-unique.\n\n        This dispatches to get_indexer or get_indexer_nonunique\n        as appropriate.\n\n        Returns\n        -------\n        numpy.ndarray\n            List of indices.\n        \"\"\"\n        if self.is_unique:\n            return self.get_indexer(target, **kwargs)\n        indexer, _ = self.get_indexer_non_unique(target, **kwargs)\n        return indexer\n\n    def _maybe_promote(self, other):\n        # A hack, but it works\n        from pandas import DatetimeIndex\n\n        if self.inferred_type == \"date\" and isinstance(other, DatetimeIndex):\n            return DatetimeIndex(self), other\n        elif self.inferred_type == \"boolean\":\n            if not is_object_dtype(self.dtype):\n                return self.astype(\"object\"), other.astype(\"object\")\n        return self, other\n\n    def groupby(self, values):\n        \"\"\"\n        Group the index labels by a given array of values.\n\n        Parameters\n        ----------\n        values : array\n            Values used to determine the groups.\n\n        Returns\n        -------\n        groups : dict\n            {group name -> group labels}\n        \"\"\"\n\n        # TODO: if we are a MultiIndex, we can do better\n        # that converting to tuples\n        if isinstance(values, ABCMultiIndex):\n            values = values.values\n        values = ensure_categorical(values)\n        result = values._reverse_indexer()\n\n        # map to the label\n        result = {k: self.take(v) for k, v in result.items()}\n\n        return result\n\n    def map(self, mapper, na_action=None):\n        \"\"\"\n        Map values using input correspondence (a dict, Series, or function).\n\n        Parameters\n        ----------\n        mapper : function, dict, or Series\n            Mapping correspondence.\n        na_action : {None, 'ignore'}\n            If 'ignore', propagate NA values, without passing them to the\n            mapping correspondence.\n\n        Returns\n        -------\n        applied : Union[Index, MultiIndex], inferred\n            The output of the mapping function applied to the index.\n            If the function returns a tuple with more than one element\n            a MultiIndex will be returned.\n        \"\"\"\n\n        from .multi import MultiIndex\n\n        new_values = super()._map_values(mapper, na_action=na_action)\n\n        attributes = self._get_attributes_dict()\n\n        # we can return a MultiIndex\n        if new_values.size and isinstance(new_values[0], tuple):\n            if isinstance(self, MultiIndex):\n                names = self.names\n            elif attributes.get(\"name\"):\n                names = [attributes.get(\"name\")] * len(new_values[0])\n            else:\n                names = None\n            return MultiIndex.from_tuples(new_values, names=names)\n\n        attributes[\"copy\"] = False\n        if not new_values.size:\n            # empty\n            attributes[\"dtype\"] = self.dtype\n\n        return Index(new_values, **attributes)\n\n    def isin(self, values, level=None):\n        \"\"\"\n        Return a boolean array where the index values are in `values`.\n\n        Compute boolean array of whether each index value is found in the\n        passed set of values. The length of the returned boolean array matches\n        the length of the index.\n\n        Parameters\n        ----------\n        values : set or list-like\n            Sought values.\n        level : str or int, optional\n            Name or position of the index level to use (if the index is a\n            `MultiIndex`).\n\n        Returns\n        -------\n        is_contained : ndarray\n            NumPy array of boolean values.\n\n        See Also\n        --------\n        Series.isin : Same for Series.\n        DataFrame.isin : Same method for DataFrames.\n\n        Notes\n        -----\n        In the case of `MultiIndex` you must either specify `values` as a\n        list-like object containing tuples that are the same length as the\n        number of levels, or specify `level`. Otherwise it will raise a\n        ``ValueError``.\n\n        If `level` is specified:\n\n        - if it is the name of one *and only one* index level, use that level;\n        - otherwise it should be a number indicating level position.\n\n        Examples\n        --------\n        >>> idx = pd.Index([1,2,3])\n        >>> idx\n        Int64Index([1, 2, 3], dtype='int64')\n\n        Check whether each index value in a list of values.\n        >>> idx.isin([1, 4])\n        array([ True, False, False])\n\n        >>> midx = pd.MultiIndex.from_arrays([[1,2,3],\n        ...                                  ['red', 'blue', 'green']],\n        ...                                  names=('number', 'color'))\n        >>> midx\n        MultiIndex(levels=[[1, 2, 3], ['blue', 'green', 'red']],\n                   codes=[[0, 1, 2], [2, 0, 1]],\n                   names=['number', 'color'])\n\n        Check whether the strings in the 'color' level of the MultiIndex\n        are in a list of colors.\n\n        >>> midx.isin(['red', 'orange', 'yellow'], level='color')\n        array([ True, False, False])\n\n        To check across the levels of a MultiIndex, pass a list of tuples:\n\n        >>> midx.isin([(1, 'red'), (3, 'red')])\n        array([ True, False, False])\n\n        For a DatetimeIndex, string values in `values` are converted to\n        Timestamps.\n\n        >>> dates = ['2000-03-11', '2000-03-12', '2000-03-13']\n        >>> dti = pd.to_datetime(dates)\n        >>> dti\n        DatetimeIndex(['2000-03-11', '2000-03-12', '2000-03-13'],\n        dtype='datetime64[ns]', freq=None)\n\n        >>> dti.isin(['2000-03-11'])\n        array([ True, False, False])\n        \"\"\"\n        if level is not None:\n            self._validate_index_level(level)\n        return algos.isin(self, values)\n\n    def _get_string_slice(self, key, use_lhs=True, use_rhs=True):\n        # this is for partial string indexing,\n        # overridden in DatetimeIndex, TimedeltaIndex and PeriodIndex\n        raise NotImplementedError\n\n    def slice_indexer(self, start=None, end=None, step=None, kind=None):\n        \"\"\"\n        For an ordered or unique index, compute the slice indexer for input\n        labels and step.\n\n        Parameters\n        ----------\n        start : label, default None\n            If None, defaults to the beginning\n        end : label, default None\n            If None, defaults to the end\n        step : int, default None\n        kind : string, default None\n\n        Returns\n        -------\n        indexer : slice\n\n        Raises\n        ------\n        KeyError : If key does not exist, or key is not unique and index is\n            not ordered.\n\n        Notes\n        -----\n        This function assumes that the data is sorted, so use at your own peril\n\n        Examples\n        --------\n        This is a method on all index types. For example you can do:\n\n        >>> idx = pd.Index(list('abcd'))\n        >>> idx.slice_indexer(start='b', end='c')\n        slice(1, 3)\n\n        >>> idx = pd.MultiIndex.from_arrays([list('abcd'), list('efgh')])\n        >>> idx.slice_indexer(start='b', end=('c', 'g'))\n        slice(1, 3)\n        \"\"\"\n        start_slice, end_slice = self.slice_locs(start, end, step=step, kind=kind)\n\n        # return a slice\n        if not is_scalar(start_slice):\n            raise AssertionError(\"Start slice bound is non-scalar\")\n        if not is_scalar(end_slice):\n            raise AssertionError(\"End slice bound is non-scalar\")\n\n        return slice(start_slice, end_slice, step)\n\n    def _maybe_cast_indexer(self, key):\n        \"\"\"\n        If we have a float key and are not a floating index, then try to cast\n        to an int if equivalent.\n        \"\"\"\n\n        if is_float(key) and not self.is_floating():\n            try:\n                ckey = int(key)\n                if ckey == key:\n                    key = ckey\n            except (OverflowError, ValueError, TypeError):\n                pass\n        return key\n\n    def _validate_indexer(self, form, key, kind):\n        \"\"\"\n        If we are positional indexer, validate that we have appropriate\n        typed bounds must be an integer.\n        \"\"\"\n        assert kind in [\"ix\", \"loc\", \"getitem\", \"iloc\"]\n\n        if key is None:\n            pass\n        elif is_integer(key):\n            pass\n        elif kind in [\"iloc\", \"getitem\"]:\n            self._invalid_indexer(form, key)\n        return key\n\n    _index_shared_docs[\n        \"_maybe_cast_slice_bound\"\n    ] = \"\"\"\n        This function should be overloaded in subclasses that allow non-trivial\n        casting on label-slice bounds, e.g. datetime-like indices allowing\n        strings containing formatted datetimes.\n\n        Parameters\n        ----------\n        label : object\n        side : {'left', 'right'}\n        kind : {'ix', 'loc', 'getitem'}\n\n        Returns\n        -------\n        label :  object\n\n        Notes\n        -----\n        Value of `side` parameter should be validated in caller.\n\n        \"\"\"\n\n    @Appender(_index_shared_docs[\"_maybe_cast_slice_bound\"])\n    def _maybe_cast_slice_bound(self, label, side, kind):\n        assert kind in [\"ix\", \"loc\", \"getitem\", None]\n\n        # We are a plain index here (sub-class override this method if they\n        # wish to have special treatment for floats/ints, e.g. Float64Index and\n        # datetimelike Indexes\n        # reject them\n        if is_float(label):\n            if not (kind in [\"ix\"] and (self.holds_integer() or self.is_floating())):\n                self._invalid_indexer(\"slice\", label)\n\n        # we are trying to find integer bounds on a non-integer based index\n        # this is rejected (generally .loc gets you here)\n        elif is_integer(label):\n            self._invalid_indexer(\"slice\", label)\n\n        return label\n\n    def _searchsorted_monotonic(self, label, side=\"left\"):\n        if self.is_monotonic_increasing:\n            return self.searchsorted(label, side=side)\n        elif self.is_monotonic_decreasing:\n            # np.searchsorted expects ascending sort order, have to reverse\n            # everything for it to work (element ordering, search side and\n            # resulting value).\n            pos = self[::-1].searchsorted(\n                label, side=\"right\" if side == \"left\" else \"left\"\n            )\n            return len(self) - pos\n\n        raise ValueError(\"index must be monotonic increasing or decreasing\")\n\n    def get_slice_bound(self, label, side, kind):\n        \"\"\"\n        Calculate slice bound that corresponds to given label.\n\n        Returns leftmost (one-past-the-rightmost if ``side=='right'``) position\n        of given label.\n\n        Parameters\n        ----------\n        label : object\n        side : {'left', 'right'}\n        kind : {'ix', 'loc', 'getitem'}\n\n        Returns\n        -------\n        int\n            Index of label.\n        \"\"\"\n        assert kind in [\"ix\", \"loc\", \"getitem\", None]\n\n        if side not in (\"left\", \"right\"):\n            raise ValueError(\n                \"Invalid value for side kwarg,\"\n                \" must be either 'left' or 'right': %s\" % (side,)\n            )\n\n        original_label = label\n\n        # For datetime indices label may be a string that has to be converted\n        # to datetime boundary according to its resolution.\n        label = self._maybe_cast_slice_bound(label, side, kind)\n\n        # we need to look up the label\n        try:\n            slc = self.get_loc(label)\n        except KeyError as err:\n            try:\n                return self._searchsorted_monotonic(label, side)\n            except ValueError:\n                # raise the original KeyError\n                raise err\n\n        if isinstance(slc, np.ndarray):\n            # get_loc may return a boolean array or an array of indices, which\n            # is OK as long as they are representable by a slice.\n            if is_bool_dtype(slc):\n                slc = lib.maybe_booleans_to_slice(slc.view(\"u1\"))\n            else:\n                slc = lib.maybe_indices_to_slice(slc.astype(\"i8\"), len(self))\n            if isinstance(slc, np.ndarray):\n                raise KeyError(\n                    \"Cannot get %s slice bound for non-unique \"\n                    \"label: %r\" % (side, original_label)\n                )\n\n        if isinstance(slc, slice):\n            if side == \"left\":\n                return slc.start\n            else:\n                return slc.stop\n        else:\n            if side == \"right\":\n                return slc + 1\n            else:\n                return slc\n\n    def slice_locs(self, start=None, end=None, step=None, kind=None):\n        \"\"\"\n        Compute slice locations for input labels.\n\n        Parameters\n        ----------\n        start : label, default None\n            If None, defaults to the beginning\n        end : label, default None\n            If None, defaults to the end\n        step : int, defaults None\n            If None, defaults to 1\n        kind : {'ix', 'loc', 'getitem'} or None\n\n        Returns\n        -------\n        start, end : int\n\n        See Also\n        --------\n        Index.get_loc : Get location for a single label.\n\n        Notes\n        -----\n        This method only works if the index is monotonic or unique.\n\n        Examples\n        --------\n        >>> idx = pd.Index(list('abcd'))\n        >>> idx.slice_locs(start='b', end='c')\n        (1, 3)\n        \"\"\"\n        inc = step is None or step >= 0\n\n        if not inc:\n            # If it's a reverse slice, temporarily swap bounds.\n            start, end = end, start\n\n        # GH 16785: If start and end happen to be date strings with UTC offsets\n        # attempt to parse and check that the offsets are the same\n        if isinstance(start, (str, datetime)) and isinstance(end, (str, datetime)):\n            try:\n                ts_start = Timestamp(start)\n                ts_end = Timestamp(end)\n            except (ValueError, TypeError):\n                pass\n            else:\n                if not tz_compare(ts_start.tzinfo, ts_end.tzinfo):\n                    raise ValueError(\"Both dates must have the same UTC offset\")\n\n        start_slice = None\n        if start is not None:\n            start_slice = self.get_slice_bound(start, \"left\", kind)\n        if start_slice is None:\n            start_slice = 0\n\n        end_slice = None\n        if end is not None:\n            end_slice = self.get_slice_bound(end, \"right\", kind)\n        if end_slice is None:\n            end_slice = len(self)\n\n        if not inc:\n            # Bounds at this moment are swapped, swap them back and shift by 1.\n            #\n            # slice_locs('B', 'A', step=-1): s='B', e='A'\n            #\n            #              s='A'                 e='B'\n            # AFTER SWAP:    |                     |\n            #                v ------------------> V\n            #           -----------------------------------\n            #           | | |A|A|A|A| | | | | |B|B| | | | |\n            #           -----------------------------------\n            #              ^ <------------------ ^\n            # SHOULD BE:   |                     |\n            #           end=s-1              start=e-1\n            #\n            end_slice, start_slice = start_slice - 1, end_slice - 1\n\n            # i == -1 triggers ``len(self) + i`` selection that points to the\n            # last element, not before-the-first one, subtracting len(self)\n            # compensates that.\n            if end_slice == -1:\n                end_slice -= len(self)\n            if start_slice == -1:\n                start_slice -= len(self)\n\n        return start_slice, end_slice\n\n    def delete(self, loc):\n        \"\"\"\n        Make new Index with passed location(-s) deleted.\n\n        Returns\n        -------\n        new_index : Index\n        \"\"\"\n        return self._shallow_copy(np.delete(self._data, loc))\n\n    def insert(self, loc, item):\n        \"\"\"\n        Make new Index inserting new item at location.\n\n        Follows Python list.append semantics for negative values.\n\n        Parameters\n        ----------\n        loc : int\n        item : object\n\n        Returns\n        -------\n        new_index : Index\n        \"\"\"\n        _self = np.asarray(self)\n        item = self._coerce_scalar_to_index(item)._ndarray_values\n        idx = np.concatenate((_self[:loc], item, _self[loc:]))\n        return self._shallow_copy_with_infer(idx)\n\n    def drop(self, labels, errors=\"raise\"):\n        \"\"\"\n        Make new Index with passed list of labels deleted.\n\n        Parameters\n        ----------\n        labels : array-like\n        errors : {'ignore', 'raise'}, default 'raise'\n            If 'ignore', suppress error and existing labels are dropped.\n\n        Returns\n        -------\n        dropped : Index\n\n        Raises\n        ------\n        KeyError\n            If not all of the labels are found in the selected axis\n        \"\"\"\n        arr_dtype = \"object\" if self.dtype == \"object\" else None\n        labels = com.index_labels_to_array(labels, dtype=arr_dtype)\n        indexer = self.get_indexer(labels)\n        mask = indexer == -1\n        if mask.any():\n            if errors != \"ignore\":\n                raise KeyError(\"{} not found in axis\".format(labels[mask]))\n            indexer = indexer[~mask]\n        return self.delete(indexer)\n\n    # --------------------------------------------------------------------\n    # Generated Arithmetic, Comparison, and Unary Methods\n\n    @classmethod\n    def _add_comparison_methods(cls):\n        \"\"\"\n        Add in comparison methods.\n        \"\"\"\n        cls.__eq__ = _make_comparison_op(operator.eq, cls)\n        cls.__ne__ = _make_comparison_op(operator.ne, cls)\n        cls.__lt__ = _make_comparison_op(operator.lt, cls)\n        cls.__gt__ = _make_comparison_op(operator.gt, cls)\n        cls.__le__ = _make_comparison_op(operator.le, cls)\n        cls.__ge__ = _make_comparison_op(operator.ge, cls)\n\n    @classmethod\n    def _add_numeric_methods_add_sub_disabled(cls):\n        \"\"\"\n        Add in the numeric add/sub methods to disable.\n        \"\"\"\n        cls.__add__ = make_invalid_op(\"__add__\")\n        cls.__radd__ = make_invalid_op(\"__radd__\")\n        cls.__iadd__ = make_invalid_op(\"__iadd__\")\n        cls.__sub__ = make_invalid_op(\"__sub__\")\n        cls.__rsub__ = make_invalid_op(\"__rsub__\")\n        cls.__isub__ = make_invalid_op(\"__isub__\")\n\n    @classmethod\n    def _add_numeric_methods_disabled(cls):\n        \"\"\"\n        Add in numeric methods to disable other than add/sub.\n        \"\"\"\n        cls.__pow__ = make_invalid_op(\"__pow__\")\n        cls.__rpow__ = make_invalid_op(\"__rpow__\")\n        cls.__mul__ = make_invalid_op(\"__mul__\")\n        cls.__rmul__ = make_invalid_op(\"__rmul__\")\n        cls.__floordiv__ = make_invalid_op(\"__floordiv__\")\n        cls.__rfloordiv__ = make_invalid_op(\"__rfloordiv__\")\n        cls.__truediv__ = make_invalid_op(\"__truediv__\")\n        cls.__rtruediv__ = make_invalid_op(\"__rtruediv__\")\n        cls.__mod__ = make_invalid_op(\"__mod__\")\n        cls.__divmod__ = make_invalid_op(\"__divmod__\")\n        cls.__neg__ = make_invalid_op(\"__neg__\")\n        cls.__pos__ = make_invalid_op(\"__pos__\")\n        cls.__abs__ = make_invalid_op(\"__abs__\")\n        cls.__inv__ = make_invalid_op(\"__inv__\")\n\n    @classmethod\n    def _add_numeric_methods_binary(cls):\n        \"\"\"\n        Add in numeric methods.\n        \"\"\"\n        cls.__add__ = _make_arithmetic_op(operator.add, cls)\n        cls.__radd__ = _make_arithmetic_op(ops.radd, cls)\n        cls.__sub__ = _make_arithmetic_op(operator.sub, cls)\n        cls.__rsub__ = _make_arithmetic_op(ops.rsub, cls)\n        cls.__rpow__ = _make_arithmetic_op(ops.rpow, cls)\n        cls.__pow__ = _make_arithmetic_op(operator.pow, cls)\n\n        cls.__truediv__ = _make_arithmetic_op(operator.truediv, cls)\n        cls.__rtruediv__ = _make_arithmetic_op(ops.rtruediv, cls)\n\n        # TODO: rmod? rdivmod?\n        cls.__mod__ = _make_arithmetic_op(operator.mod, cls)\n        cls.__floordiv__ = _make_arithmetic_op(operator.floordiv, cls)\n        cls.__rfloordiv__ = _make_arithmetic_op(ops.rfloordiv, cls)\n        cls.__divmod__ = _make_arithmetic_op(divmod, cls)\n        cls.__mul__ = _make_arithmetic_op(operator.mul, cls)\n        cls.__rmul__ = _make_arithmetic_op(ops.rmul, cls)\n\n    @classmethod\n    def _add_numeric_methods_unary(cls):\n        \"\"\"\n        Add in numeric unary methods.\n        \"\"\"\n\n        def _make_evaluate_unary(op, opstr):\n            def _evaluate_numeric_unary(self):\n\n                attrs = self._get_attributes_dict()\n                return Index(op(self.values), **attrs)\n\n            _evaluate_numeric_unary.__name__ = opstr\n            return _evaluate_numeric_unary\n\n        cls.__neg__ = _make_evaluate_unary(operator.neg, \"__neg__\")\n        cls.__pos__ = _make_evaluate_unary(operator.pos, \"__pos__\")\n        cls.__abs__ = _make_evaluate_unary(np.abs, \"__abs__\")\n        cls.__inv__ = _make_evaluate_unary(lambda x: -x, \"__inv__\")\n\n    @classmethod\n    def _add_numeric_methods(cls):\n        cls._add_numeric_methods_unary()\n        cls._add_numeric_methods_binary()\n\n    @classmethod\n    def _add_logical_methods(cls):\n        \"\"\"\n        Add in logical methods.\n        \"\"\"\n        _doc = \"\"\"\n        %(desc)s\n\n        Parameters\n        ----------\n        *args\n            These parameters will be passed to numpy.%(outname)s.\n        **kwargs\n            These parameters will be passed to numpy.%(outname)s.\n\n        Returns\n        -------\n        %(outname)s : bool or array_like (if axis is specified)\n            A single element array_like may be converted to bool.\"\"\"\n\n        _index_shared_docs[\"index_all\"] = dedent(\n            \"\"\"\n\n        See Also\n        --------\n        Index.any : Return whether any element in an Index is True.\n        Series.any : Return whether any element in a Series is True.\n        Series.all : Return whether all elements in a Series are True.\n\n        Notes\n        -----\n        Not a Number (NaN), positive infinity and negative infinity\n        evaluate to True because these are not equal to zero.\n\n        Examples\n        --------\n        **all**\n\n        True, because nonzero integers are considered True.\n\n        >>> pd.Index([1, 2, 3]).all()\n        True\n\n        False, because ``0`` is considered False.\n\n        >>> pd.Index([0, 1, 2]).all()\n        False\n\n        **any**\n\n        True, because ``1`` is considered True.\n\n        >>> pd.Index([0, 0, 1]).any()\n        True\n\n        False, because ``0`` is considered False.\n\n        >>> pd.Index([0, 0, 0]).any()\n        False\n        \"\"\"\n        )\n\n        _index_shared_docs[\"index_any\"] = dedent(\n            \"\"\"\n\n        See Also\n        --------\n        Index.all : Return whether all elements are True.\n        Series.all : Return whether all elements are True.\n\n        Notes\n        -----\n        Not a Number (NaN), positive infinity and negative infinity\n        evaluate to True because these are not equal to zero.\n\n        Examples\n        --------\n        >>> index = pd.Index([0, 1, 2])\n        >>> index.any()\n        True\n\n        >>> index = pd.Index([0, 0, 0])\n        >>> index.any()\n        False\n        \"\"\"\n        )\n\n        def _make_logical_function(name, desc, f):\n            @Substitution(outname=name, desc=desc)\n            @Appender(_index_shared_docs[\"index_\" + name])\n            @Appender(_doc)\n            def logical_func(self, *args, **kwargs):\n                result = f(self.values)\n                if (\n                    isinstance(result, (np.ndarray, ABCSeries, Index))\n                    and result.ndim == 0\n                ):\n                    # return NumPy type\n                    return result.dtype.type(result.item())\n                else:  # pragma: no cover\n                    return result\n\n            logical_func.__name__ = name\n            return logical_func\n\n        cls.all = _make_logical_function(\n            \"all\", \"Return whether all elements are True.\", np.all\n        )\n        cls.any = _make_logical_function(\n            \"any\", \"Return whether any element is True.\", np.any\n        )\n\n    @classmethod\n    def _add_logical_methods_disabled(cls):\n        \"\"\"\n        Add in logical methods to disable.\n        \"\"\"\n        cls.all = make_invalid_op(\"all\")\n        cls.any = make_invalid_op(\"any\")\n\n    @property\n    def shape(self):\n        \"\"\"\n        Return a tuple of the shape of the underlying data.\n        \"\"\"\n        # not using \"(len(self), )\" to return \"correct\" shape if the values\n        # consists of a >1 D array (see GH-27775)\n        # overridden in MultiIndex.shape to avoid materializing the values\n        return self._values.shape\n\n\nIndex._add_numeric_methods_disabled()\nIndex._add_logical_methods()\nIndex._add_comparison_methods()\n\n\ndef ensure_index_from_sequences(sequences, names=None):\n    \"\"\"\n    Construct an index from sequences of data.\n\n    A single sequence returns an Index. Many sequences returns a\n    MultiIndex.\n\n    Parameters\n    ----------\n    sequences : sequence of sequences\n    names : sequence of str\n\n    Returns\n    -------\n    index : Index or MultiIndex\n\n    Examples\n    --------\n    >>> ensure_index_from_sequences([[1, 2, 3]], names=['name'])\n    Int64Index([1, 2, 3], dtype='int64', name='name')\n\n    >>> ensure_index_from_sequences([['a', 'a'], ['a', 'b']],\n                                    names=['L1', 'L2'])\n    MultiIndex([('a', 'a'),\n                ('a', 'b')],\n               names=['L1', 'L2'])\n\n    See Also\n    --------\n    ensure_index\n    \"\"\"\n    from .multi import MultiIndex\n\n    if len(sequences) == 1:\n        if names is not None:\n            names = names[0]\n        return Index(sequences[0], name=names)\n    else:\n        return MultiIndex.from_arrays(sequences, names=names)\n\n\ndef ensure_index(index_like, copy=False):\n    \"\"\"\n    Ensure that we have an index from some index-like object.\n\n    Parameters\n    ----------\n    index : sequence\n        An Index or other sequence\n    copy : bool\n\n    Returns\n    -------\n    index : Index or MultiIndex\n\n    Examples\n    --------\n    >>> ensure_index(['a', 'b'])\n    Index(['a', 'b'], dtype='object')\n\n    >>> ensure_index([('a', 'a'),  ('b', 'c')])\n    Index([('a', 'a'), ('b', 'c')], dtype='object')\n\n    >>> ensure_index([['a', 'a'], ['b', 'c']])\n    MultiIndex([('a', 'b'),\n                ('a', 'c')],\n               dtype='object')\n               )\n\n    See Also\n    --------\n    ensure_index_from_sequences\n    \"\"\"\n    if isinstance(index_like, Index):\n        if copy:\n            index_like = index_like.copy()\n        return index_like\n    if hasattr(index_like, \"name\"):\n        return Index(index_like, name=index_like.name, copy=copy)\n\n    if is_iterator(index_like):\n        index_like = list(index_like)\n\n    # must check for exactly list here because of strict type\n    # check in clean_index_list\n    if isinstance(index_like, list):\n        if type(index_like) != list:\n            index_like = list(index_like)\n\n        converted, all_arrays = lib.clean_index_list(index_like)\n\n        if len(converted) > 0 and all_arrays:\n            from .multi import MultiIndex\n\n            return MultiIndex.from_arrays(converted)\n        else:\n            index_like = converted\n    else:\n        # clean_index_list does the equivalent of copying\n        # so only need to do this if not list instance\n        if copy:\n            from copy import copy\n\n            index_like = copy(index_like)\n\n    return Index(index_like)\n\n\ndef _ensure_has_len(seq):\n    \"\"\"\n    If seq is an iterator, put its values into a list.\n    \"\"\"\n    try:\n        len(seq)\n    except TypeError:\n        return list(seq)\n    else:\n        return seq\n\n\ndef _trim_front(strings):\n    \"\"\"\n    Trims zeros and decimal points.\n    \"\"\"\n    trimmed = strings\n    while len(strings) > 0 and all(x[0] == \" \" for x in trimmed):\n        trimmed = [x[1:] for x in trimmed]\n    return trimmed\n\n\ndef _validate_join_method(method):\n    if method not in [\"left\", \"right\", \"inner\", \"outer\"]:\n        raise ValueError(\"do not recognize join method %s\" % method)\n\n\ndef default_index(n):\n    from pandas.core.index import RangeIndex\n\n    return RangeIndex(0, n, name=None)\n",
      "file_after": "from datetime import datetime\nimport operator\nfrom textwrap import dedent\nfrom typing import Union\nimport warnings\n\nimport numpy as np\n\nfrom pandas._libs import algos as libalgos, index as libindex, lib\nimport pandas._libs.join as libjoin\nfrom pandas._libs.lib import is_datetime_array\nfrom pandas._libs.tslibs import OutOfBoundsDatetime, Timestamp\nfrom pandas._libs.tslibs.period import IncompatibleFrequency\nfrom pandas._libs.tslibs.timezones import tz_compare\nfrom pandas.compat import set_function_name\nfrom pandas.compat.numpy import function as nv\nfrom pandas.util._decorators import Appender, Substitution, cache_readonly\n\nfrom pandas.core.dtypes import concat as _concat\nfrom pandas.core.dtypes.cast import maybe_cast_to_integer_array\nfrom pandas.core.dtypes.common import (\n    ensure_categorical,\n    ensure_int64,\n    ensure_object,\n    ensure_platform_int,\n    is_bool,\n    is_bool_dtype,\n    is_categorical,\n    is_categorical_dtype,\n    is_datetime64_any_dtype,\n    is_datetime64tz_dtype,\n    is_dtype_equal,\n    is_extension_array_dtype,\n    is_float,\n    is_float_dtype,\n    is_hashable,\n    is_integer,\n    is_integer_dtype,\n    is_interval_dtype,\n    is_iterator,\n    is_list_like,\n    is_object_dtype,\n    is_period_dtype,\n    is_scalar,\n    is_signed_integer_dtype,\n    is_timedelta64_dtype,\n    is_unsigned_integer_dtype,\n    pandas_dtype,\n)\nfrom pandas.core.dtypes.concat import concat_compat\nfrom pandas.core.dtypes.generic import (\n    ABCCategorical,\n    ABCDataFrame,\n    ABCDatetimeArray,\n    ABCDatetimeIndex,\n    ABCIndexClass,\n    ABCMultiIndex,\n    ABCPandasArray,\n    ABCPeriodIndex,\n    ABCSeries,\n    ABCTimedeltaIndex,\n)\nfrom pandas.core.dtypes.missing import array_equivalent, isna\n\nfrom pandas.core import ops\nfrom pandas.core.accessor import CachedAccessor, DirNamesMixin\nimport pandas.core.algorithms as algos\nfrom pandas.core.arrays import ExtensionArray\nfrom pandas.core.base import IndexOpsMixin, PandasObject\nimport pandas.core.common as com\nfrom pandas.core.indexers import maybe_convert_indices\nfrom pandas.core.indexes.frozen import FrozenList\nimport pandas.core.missing as missing\nfrom pandas.core.ops import get_op_result_name\nfrom pandas.core.ops.invalid import make_invalid_op\nimport pandas.core.sorting as sorting\nfrom pandas.core.strings import StringMethods\n\nfrom pandas.io.formats.printing import (\n    default_pprint,\n    format_object_attrs,\n    format_object_summary,\n    pprint_thing,\n)\n\n__all__ = [\"Index\"]\n\n_unsortable_types = frozenset((\"mixed\", \"mixed-integer\"))\n\n_index_doc_kwargs = dict(\n    klass=\"Index\",\n    inplace=\"\",\n    target_klass=\"Index\",\n    raises_section=\"\",\n    unique=\"Index\",\n    duplicated=\"np.ndarray\",\n)\n_index_shared_docs = dict()\n\n\ndef _make_comparison_op(op, cls):\n    def cmp_method(self, other):\n        if isinstance(other, (np.ndarray, Index, ABCSeries, ExtensionArray)):\n            if other.ndim > 0 and len(self) != len(other):\n                raise ValueError(\"Lengths must match to compare\")\n\n        if is_object_dtype(self) and isinstance(other, ABCCategorical):\n            left = type(other)(self._values, dtype=other.dtype)\n            return op(left, other)\n        elif is_object_dtype(self) and not isinstance(self, ABCMultiIndex):\n            # don't pass MultiIndex\n            with np.errstate(all=\"ignore\"):\n                result = ops.comp_method_OBJECT_ARRAY(op, self.values, other)\n\n        else:\n            with np.errstate(all=\"ignore\"):\n                result = op(self.values, np.asarray(other))\n\n        if is_bool_dtype(result):\n            return result\n        return ops.invalid_comparison(self, other, op)\n\n    name = \"__{name}__\".format(name=op.__name__)\n    return set_function_name(cmp_method, name, cls)\n\n\ndef _make_arithmetic_op(op, cls):\n    def index_arithmetic_method(self, other):\n        if isinstance(other, (ABCSeries, ABCDataFrame, ABCTimedeltaIndex)):\n            return NotImplemented\n\n        from pandas import Series\n\n        result = op(Series(self), other)\n        if isinstance(result, tuple):\n            return (Index(result[0]), Index(result[1]))\n        return Index(result)\n\n    name = \"__{name}__\".format(name=op.__name__)\n    # TODO: docstring?\n    return set_function_name(index_arithmetic_method, name, cls)\n\n\nclass InvalidIndexError(Exception):\n    pass\n\n\n_o_dtype = np.dtype(object)\n_Identity = object\n\n\ndef _new_Index(cls, d):\n    \"\"\"\n    This is called upon unpickling, rather than the default which doesn't\n    have arguments and breaks __new__.\n    \"\"\"\n    # required for backward compat, because PI can't be instantiated with\n    # ordinals through __new__ GH #13277\n    if issubclass(cls, ABCPeriodIndex):\n        from pandas.core.indexes.period import _new_PeriodIndex\n\n        return _new_PeriodIndex(cls, **d)\n    return cls.__new__(cls, **d)\n\n\nclass Index(IndexOpsMixin, PandasObject):\n    \"\"\"\n    Immutable ndarray implementing an ordered, sliceable set. The basic object\n    storing axis labels for all pandas objects.\n\n    Parameters\n    ----------\n    data : array-like (1-dimensional)\n    dtype : NumPy dtype (default: object)\n        If dtype is None, we find the dtype that best fits the data.\n        If an actual dtype is provided, we coerce to that dtype if it's safe.\n        Otherwise, an error will be raised.\n    copy : bool\n        Make a copy of input ndarray\n    name : object\n        Name to be stored in the index\n    tupleize_cols : bool (default: True)\n        When True, attempt to create a MultiIndex if possible\n\n    See Also\n    --------\n    RangeIndex : Index implementing a monotonic integer range.\n    CategoricalIndex : Index of :class:`Categorical` s.\n    MultiIndex : A multi-level, or hierarchical, Index.\n    IntervalIndex : An Index of :class:`Interval` s.\n    DatetimeIndex, TimedeltaIndex, PeriodIndex\n    Int64Index, UInt64Index,  Float64Index\n\n    Notes\n    -----\n    An Index instance can **only** contain hashable objects\n\n    Examples\n    --------\n    >>> pd.Index([1, 2, 3])\n    Int64Index([1, 2, 3], dtype='int64')\n\n    >>> pd.Index(list('abc'))\n    Index(['a', 'b', 'c'], dtype='object')\n    \"\"\"\n\n    # tolist is not actually deprecated, just suppressed in the __dir__\n    _deprecations = DirNamesMixin._deprecations | frozenset([\"tolist\"])\n\n    # To hand over control to subclasses\n    _join_precedence = 1\n\n    # Cython methods; see github.com/cython/cython/issues/2647\n    #  for why we need to wrap these instead of making them class attributes\n    # Moreover, cython will choose the appropriate-dtyped sub-function\n    #  given the dtypes of the passed arguments\n    def _left_indexer_unique(self, left, right):\n        return libjoin.left_join_indexer_unique(left, right)\n\n    def _left_indexer(self, left, right):\n        return libjoin.left_join_indexer(left, right)\n\n    def _inner_indexer(self, left, right):\n        return libjoin.inner_join_indexer(left, right)\n\n    def _outer_indexer(self, left, right):\n        return libjoin.outer_join_indexer(left, right)\n\n    _typ = \"index\"\n    _data = None\n    _id = None\n    name = None\n    _comparables = [\"name\"]\n    _attributes = [\"name\"]\n    _is_numeric_dtype = False\n    _can_hold_na = True\n\n    # would we like our indexing holder to defer to us\n    _defer_to_indexing = False\n\n    # prioritize current class for _shallow_copy_with_infer,\n    # used to infer integers as datetime-likes\n    _infer_as_myclass = False\n\n    _engine_type = libindex.ObjectEngine\n    # whether we support partial string indexing. Overridden\n    # in DatetimeIndex and PeriodIndex\n    _supports_partial_string_indexing = False\n\n    _accessors = {\"str\"}\n\n    str = CachedAccessor(\"str\", StringMethods)\n\n    # --------------------------------------------------------------------\n    # Constructors\n\n    def __new__(\n        cls,\n        data=None,\n        dtype=None,\n        copy=False,\n        name=None,\n        fastpath=None,\n        tupleize_cols=True,\n        **kwargs\n    ) -> \"Index\":\n\n        from .range import RangeIndex\n        from pandas import PeriodIndex, DatetimeIndex, TimedeltaIndex\n        from .numeric import Float64Index, Int64Index, UInt64Index\n        from .interval import IntervalIndex\n        from .category import CategoricalIndex\n\n        if name is None and hasattr(data, \"name\"):\n            name = data.name\n\n        if fastpath is not None:\n            warnings.warn(\n                \"The 'fastpath' keyword is deprecated, and will be \"\n                \"removed in a future version.\",\n                FutureWarning,\n                stacklevel=2,\n            )\n            if fastpath:\n                return cls._simple_new(data, name)\n\n        if isinstance(data, ABCPandasArray):\n            # ensure users don't accidentally put a PandasArray in an index.\n            data = data.to_numpy()\n\n        # range\n        if isinstance(data, RangeIndex):\n            return RangeIndex(start=data, copy=copy, dtype=dtype, name=name)\n        elif isinstance(data, range):\n            return RangeIndex.from_range(data, dtype=dtype, name=name)\n\n        # categorical\n        elif is_categorical_dtype(data) or is_categorical_dtype(dtype):\n            return CategoricalIndex(data, dtype=dtype, copy=copy, name=name, **kwargs)\n\n        # interval\n        elif (\n            is_interval_dtype(data) or is_interval_dtype(dtype)\n        ) and not is_object_dtype(dtype):\n            closed = kwargs.get(\"closed\", None)\n            return IntervalIndex(data, dtype=dtype, name=name, copy=copy, closed=closed)\n\n        elif (\n            is_datetime64_any_dtype(data)\n            or is_datetime64_any_dtype(dtype)\n            or \"tz\" in kwargs\n        ):\n            if is_dtype_equal(_o_dtype, dtype):\n                # GH#23524 passing `dtype=object` to DatetimeIndex is invalid,\n                #  will raise in the where `data` is already tz-aware.  So\n                #  we leave it out of this step and cast to object-dtype after\n                #  the DatetimeIndex construction.\n                # Note we can pass copy=False because the .astype below\n                #  will always make a copy\n                result = DatetimeIndex(\n                    data, copy=False, name=name, **kwargs\n                )  # type: \"Index\"\n                return result.astype(object)\n            else:\n                return DatetimeIndex(data, copy=copy, name=name, dtype=dtype, **kwargs)\n\n        elif is_timedelta64_dtype(data) or is_timedelta64_dtype(dtype):\n            if is_dtype_equal(_o_dtype, dtype):\n                # Note we can pass copy=False because the .astype below\n                #  will always make a copy\n                result = TimedeltaIndex(data, copy=False, name=name, **kwargs)\n                return result.astype(object)\n            else:\n                return TimedeltaIndex(data, copy=copy, name=name, dtype=dtype, **kwargs)\n\n        elif is_period_dtype(data) and not is_object_dtype(dtype):\n            return PeriodIndex(data, copy=copy, name=name, **kwargs)\n\n        # extension dtype\n        elif is_extension_array_dtype(data) or is_extension_array_dtype(dtype):\n            data = np.asarray(data)\n            if not (dtype is None or is_object_dtype(dtype)):\n                # coerce to the provided dtype\n                ea_cls = dtype.construct_array_type()\n                data = ea_cls._from_sequence(data, dtype=dtype, copy=False)\n\n            # coerce to the object dtype\n            data = data.astype(object)\n            return Index(data, dtype=object, copy=copy, name=name, **kwargs)\n\n        # index-like\n        elif isinstance(data, (np.ndarray, Index, ABCSeries)):\n            if dtype is not None:\n                # we need to avoid having numpy coerce\n                # things that look like ints/floats to ints unless\n                # they are actually ints, e.g. '0' and 0.0\n                # should not be coerced\n                # GH 11836\n                if is_integer_dtype(dtype):\n                    inferred = lib.infer_dtype(data, skipna=False)\n                    if inferred == \"integer\":\n                        data = maybe_cast_to_integer_array(data, dtype, copy=copy)\n                    elif inferred in [\"floating\", \"mixed-integer-float\"]:\n                        if isna(data).any():\n                            raise ValueError(\"cannot convert float NaN to integer\")\n\n                        if inferred == \"mixed-integer-float\":\n                            data = maybe_cast_to_integer_array(data, dtype)\n\n                        # If we are actually all equal to integers,\n                        # then coerce to integer.\n                        try:\n                            return cls._try_convert_to_int_index(\n                                data, copy, name, dtype\n                            )\n                        except ValueError:\n                            pass\n\n                        # Return an actual float index.\n                        return Float64Index(data, copy=copy, dtype=dtype, name=name)\n\n                    elif inferred == \"string\":\n                        pass\n                    else:\n                        data = data.astype(dtype)\n                elif is_float_dtype(dtype):\n                    inferred = lib.infer_dtype(data, skipna=False)\n                    if inferred == \"string\":\n                        pass\n                    else:\n                        data = data.astype(dtype)\n                else:\n                    data = np.array(data, dtype=dtype, copy=copy)\n\n            # maybe coerce to a sub-class\n            if is_signed_integer_dtype(data.dtype):\n                return Int64Index(data, copy=copy, dtype=dtype, name=name)\n            elif is_unsigned_integer_dtype(data.dtype):\n                return UInt64Index(data, copy=copy, dtype=dtype, name=name)\n            elif is_float_dtype(data.dtype):\n                return Float64Index(data, copy=copy, dtype=dtype, name=name)\n            elif issubclass(data.dtype.type, np.bool) or is_bool_dtype(data):\n                subarr = data.astype(\"object\")\n            else:\n                subarr = com.asarray_tuplesafe(data, dtype=object)\n\n            # asarray_tuplesafe does not always copy underlying data,\n            # so need to make sure that this happens\n            if copy:\n                subarr = subarr.copy()\n\n            if dtype is None:\n                inferred = lib.infer_dtype(subarr, skipna=False)\n                if inferred == \"integer\":\n                    try:\n                        return cls._try_convert_to_int_index(subarr, copy, name, dtype)\n                    except ValueError:\n                        pass\n\n                    return Index(subarr, copy=copy, dtype=object, name=name)\n                elif inferred in [\"floating\", \"mixed-integer-float\", \"integer-na\"]:\n                    # TODO: Returns IntegerArray for integer-na case in the future\n                    return Float64Index(subarr, copy=copy, name=name)\n                elif inferred == \"interval\":\n                    try:\n                        return IntervalIndex(subarr, name=name, copy=copy)\n                    except ValueError:\n                        # GH27172: mixed closed Intervals --> object dtype\n                        pass\n                elif inferred == \"boolean\":\n                    # don't support boolean explicitly ATM\n                    pass\n                elif inferred != \"string\":\n                    if inferred.startswith(\"datetime\"):\n                        try:\n                            return DatetimeIndex(subarr, copy=copy, name=name, **kwargs)\n                        except (ValueError, OutOfBoundsDatetime):\n                            # GH 27011\n                            # If we have mixed timezones, just send it\n                            # down the base constructor\n                            pass\n\n                    elif inferred.startswith(\"timedelta\"):\n                        return TimedeltaIndex(subarr, copy=copy, name=name, **kwargs)\n                    elif inferred == \"period\":\n                        try:\n                            return PeriodIndex(subarr, name=name, **kwargs)\n                        except IncompatibleFrequency:\n                            pass\n            return cls._simple_new(subarr, name)\n\n        elif hasattr(data, \"__array__\"):\n            return Index(np.asarray(data), dtype=dtype, copy=copy, name=name, **kwargs)\n        elif data is None or is_scalar(data):\n            raise cls._scalar_data_error(data)\n        else:\n            if tupleize_cols and is_list_like(data):\n                # GH21470: convert iterable to list before determining if empty\n                if is_iterator(data):\n                    data = list(data)\n\n                if data and all(isinstance(e, tuple) for e in data):\n                    # we must be all tuples, otherwise don't construct\n                    # 10697\n                    from .multi import MultiIndex\n\n                    return MultiIndex.from_tuples(\n                        data, names=name or kwargs.get(\"names\")\n                    )\n            # other iterable of some kind\n            subarr = com.asarray_tuplesafe(data, dtype=object)\n            return Index(subarr, dtype=dtype, copy=copy, name=name, **kwargs)\n\n    \"\"\"\n    NOTE for new Index creation:\n\n    - _simple_new: It returns new Index with the same type as the caller.\n      All metadata (such as name) must be provided by caller's responsibility.\n      Using _shallow_copy is recommended because it fills these metadata\n      otherwise specified.\n\n    - _shallow_copy: It returns new Index with the same type (using\n      _simple_new), but fills caller's metadata otherwise specified. Passed\n      kwargs will overwrite corresponding metadata.\n\n    - _shallow_copy_with_infer: It returns new Index inferring its type\n      from passed values. It fills caller's metadata otherwise specified as the\n      same as _shallow_copy.\n\n    See each method's docstring.\n    \"\"\"\n\n    @property\n    def asi8(self):\n        \"\"\"\n        Integer representation of the values.\n\n        Returns\n        -------\n        ndarray\n            An ndarray with int64 dtype.\n        \"\"\"\n        return None\n\n    @classmethod\n    def _simple_new(cls, values, name=None, dtype=None, **kwargs):\n        \"\"\"\n        We require that we have a dtype compat for the values. If we are passed\n        a non-dtype compat, then coerce using the constructor.\n\n        Must be careful not to recurse.\n        \"\"\"\n        if isinstance(values, (ABCSeries, ABCIndexClass)):\n            # Index._data must always be an ndarray.\n            # This is no-copy for when _values is an ndarray,\n            # which should be always at this point.\n            values = np.asarray(values._values)\n\n        result = object.__new__(cls)\n        result._data = values\n        # _index_data is a (temporary?) fix to ensure that the direct data\n        # manipulation we do in `_libs/reduction.pyx` continues to work.\n        # We need access to the actual ndarray, since we're messing with\n        # data buffers and strides. We don't re-use `_ndarray_values`, since\n        # we actually set this value too.\n        result._index_data = values\n        result.name = name\n        for k, v in kwargs.items():\n            setattr(result, k, v)\n        return result._reset_identity()\n\n    @cache_readonly\n    def _constructor(self):\n        return type(self)\n\n    # --------------------------------------------------------------------\n    # Index Internals Methods\n\n    def _get_attributes_dict(self):\n        \"\"\"\n        Return an attributes dict for my class.\n        \"\"\"\n        return {k: getattr(self, k, None) for k in self._attributes}\n\n    _index_shared_docs[\n        \"_shallow_copy\"\n    ] = \"\"\"\n        Create a new Index with the same class as the caller, don't copy the\n        data, use the same object attributes with passed in attributes taking\n        precedence.\n\n        *this is an internal non-public method*\n\n        Parameters\n        ----------\n        values : the values to create the new Index, optional\n        kwargs : updates the default attributes for this Index\n        \"\"\"\n\n    @Appender(_index_shared_docs[\"_shallow_copy\"])\n    def _shallow_copy(self, values=None, **kwargs):\n        if values is None:\n            values = self.values\n        attributes = self._get_attributes_dict()\n        attributes.update(kwargs)\n        if not len(values) and \"dtype\" not in kwargs:\n            attributes[\"dtype\"] = self.dtype\n\n        # _simple_new expects an the type of self._data\n        values = getattr(values, \"_values\", values)\n        if isinstance(values, ABCDatetimeArray):\n            # `self.values` returns `self` for tz-aware, so we need to unwrap\n            #  more specifically\n            values = values.asi8\n\n        return self._simple_new(values, **attributes)\n\n    def _shallow_copy_with_infer(self, values, **kwargs):\n        \"\"\"\n        Create a new Index inferring the class with passed value, don't copy\n        the data, use the same object attributes with passed in attributes\n        taking precedence.\n\n        *this is an internal non-public method*\n\n        Parameters\n        ----------\n        values : the values to create the new Index, optional\n        kwargs : updates the default attributes for this Index\n        \"\"\"\n        attributes = self._get_attributes_dict()\n        attributes.update(kwargs)\n        attributes[\"copy\"] = False\n        if not len(values) and \"dtype\" not in kwargs:\n            attributes[\"dtype\"] = self.dtype\n        if self._infer_as_myclass:\n            try:\n                return self._constructor(values, **attributes)\n            except (TypeError, ValueError):\n                pass\n        return Index(values, **attributes)\n\n    def _update_inplace(self, result, **kwargs):\n        # guard when called from IndexOpsMixin\n        raise TypeError(\"Index can't be updated inplace\")\n\n    def is_(self, other):\n        \"\"\"\n        More flexible, faster check like ``is`` but that works through views.\n\n        Note: this is *not* the same as ``Index.identical()``, which checks\n        that metadata is also the same.\n\n        Parameters\n        ----------\n        other : object\n            other object to compare against.\n\n        Returns\n        -------\n        True if both have same underlying data, False otherwise : bool\n        \"\"\"\n        # use something other than None to be clearer\n        return self._id is getattr(other, \"_id\", Ellipsis) and self._id is not None\n\n    def _reset_identity(self):\n        \"\"\"\n        Initializes or resets ``_id`` attribute with new object.\n        \"\"\"\n        self._id = _Identity()\n        return self\n\n    def _cleanup(self):\n        self._engine.clear_mapping()\n\n    @cache_readonly\n    def _engine(self):\n        # property, for now, slow to look up\n\n        # to avoid a reference cycle, bind `_ndarray_values` to a local variable, so\n        # `self` is not passed into the lambda.\n        _ndarray_values = self._ndarray_values\n        return self._engine_type(lambda: _ndarray_values, len(self))\n\n    # --------------------------------------------------------------------\n    # Array-Like Methods\n\n    # ndarray compat\n    def __len__(self):\n        \"\"\"\n        Return the length of the Index.\n        \"\"\"\n        return len(self._data)\n\n    def __array__(self, dtype=None):\n        \"\"\"\n        The array interface, return my values.\n        \"\"\"\n        return np.asarray(self._data, dtype=dtype)\n\n    def __array_wrap__(self, result, context=None):\n        \"\"\"\n        Gets called after a ufunc.\n        \"\"\"\n        result = lib.item_from_zerodim(result)\n        if is_bool_dtype(result) or lib.is_scalar(result):\n            return result\n\n        attrs = self._get_attributes_dict()\n        return Index(result, **attrs)\n\n    @cache_readonly\n    def dtype(self):\n        \"\"\"\n        Return the dtype object of the underlying data.\n        \"\"\"\n        return self._data.dtype\n\n    @property\n    def dtype_str(self):\n        \"\"\"\n        Return the dtype str of the underlying data.\n\n        .. deprecated:: 0.25.0\n        \"\"\"\n        warnings.warn(\n            \"`dtype_str` has been deprecated. Call `str` on the \"\n            \"dtype attribute instead.\",\n            FutureWarning,\n            stacklevel=2,\n        )\n        return str(self.dtype)\n\n    def ravel(self, order=\"C\"):\n        \"\"\"\n        Return an ndarray of the flattened values of the underlying data.\n\n        Returns\n        -------\n        numpy.ndarray\n            Flattened array.\n\n        See Also\n        --------\n        numpy.ndarray.ravel\n        \"\"\"\n        return self._ndarray_values.ravel(order=order)\n\n    def view(self, cls=None):\n\n        # we need to see if we are subclassing an\n        # index type here\n        if cls is not None and not hasattr(cls, \"_typ\"):\n            result = self._data.view(cls)\n        else:\n            result = self._shallow_copy()\n        if isinstance(result, Index):\n            result._id = self._id\n        return result\n\n    _index_shared_docs[\n        \"astype\"\n    ] = \"\"\"\n        Create an Index with values cast to dtypes. The class of a new Index\n        is determined by dtype. When conversion is impossible, a ValueError\n        exception is raised.\n\n        Parameters\n        ----------\n        dtype : numpy dtype or pandas type\n            Note that any signed integer `dtype` is treated as ``'int64'``,\n            and any unsigned integer `dtype` is treated as ``'uint64'``,\n            regardless of the size.\n        copy : bool, default True\n            By default, astype always returns a newly allocated object.\n            If copy is set to False and internal requirements on dtype are\n            satisfied, the original data is used to create a new Index\n            or the original Index is returned.\n\n        Returns\n        -------\n        Index\n            Index with values cast to specified dtype.\n        \"\"\"\n\n    @Appender(_index_shared_docs[\"astype\"])\n    def astype(self, dtype, copy=True):\n        if is_dtype_equal(self.dtype, dtype):\n            return self.copy() if copy else self\n\n        elif is_categorical_dtype(dtype):\n            from .category import CategoricalIndex\n\n            return CategoricalIndex(self.values, name=self.name, dtype=dtype, copy=copy)\n        elif is_datetime64tz_dtype(dtype):\n            # TODO(GH-24559): Remove this block, use the following elif.\n            # avoid FutureWarning from DatetimeIndex constructor.\n            from pandas import DatetimeIndex\n\n            tz = pandas_dtype(dtype).tz\n            return DatetimeIndex(np.asarray(self)).tz_localize(\"UTC\").tz_convert(tz)\n\n        elif is_extension_array_dtype(dtype):\n            return Index(np.asarray(self), dtype=dtype, copy=copy)\n\n        try:\n            if is_datetime64tz_dtype(dtype):\n                from pandas import DatetimeIndex\n\n                return DatetimeIndex(\n                    self.values, name=self.name, dtype=dtype, copy=copy\n                )\n            return Index(\n                self.values.astype(dtype, copy=copy), name=self.name, dtype=dtype\n            )\n        except (TypeError, ValueError):\n            msg = \"Cannot cast {name} to dtype {dtype}\"\n            raise TypeError(msg.format(name=type(self).__name__, dtype=dtype))\n\n    _index_shared_docs[\n        \"take\"\n    ] = \"\"\"\n        Return a new %(klass)s of the values selected by the indices.\n\n        For internal compatibility with numpy arrays.\n\n        Parameters\n        ----------\n        indices : list\n            Indices to be taken\n        axis : int, optional\n            The axis over which to select values, always 0.\n        allow_fill : bool, default True\n        fill_value : bool, default None\n            If allow_fill=True and fill_value is not None, indices specified by\n            -1 is regarded as NA. If Index doesn't hold NA, raise ValueError\n\n        Returns\n        -------\n        numpy.ndarray\n            Elements of given indices.\n\n        See Also\n        --------\n        numpy.ndarray.take\n        \"\"\"\n\n    @Appender(_index_shared_docs[\"take\"] % _index_doc_kwargs)\n    def take(self, indices, axis=0, allow_fill=True, fill_value=None, **kwargs):\n        if kwargs:\n            nv.validate_take(tuple(), kwargs)\n        indices = ensure_platform_int(indices)\n        if self._can_hold_na:\n            taken = self._assert_take_fillable(\n                self.values,\n                indices,\n                allow_fill=allow_fill,\n                fill_value=fill_value,\n                na_value=self._na_value,\n            )\n        else:\n            if allow_fill and fill_value is not None:\n                msg = \"Unable to fill values because {0} cannot contain NA\"\n                raise ValueError(msg.format(self.__class__.__name__))\n            taken = self.values.take(indices)\n        return self._shallow_copy(taken)\n\n    def _assert_take_fillable(\n        self, values, indices, allow_fill=True, fill_value=None, na_value=np.nan\n    ):\n        \"\"\"\n        Internal method to handle NA filling of take.\n        \"\"\"\n        indices = ensure_platform_int(indices)\n\n        # only fill if we are passing a non-None fill_value\n        if allow_fill and fill_value is not None:\n            if (indices < -1).any():\n                msg = (\n                    \"When allow_fill=True and fill_value is not None, \"\n                    \"all indices must be >= -1\"\n                )\n                raise ValueError(msg)\n            taken = algos.take(\n                values, indices, allow_fill=allow_fill, fill_value=na_value\n            )\n        else:\n            taken = values.take(indices)\n        return taken\n\n    _index_shared_docs[\n        \"repeat\"\n    ] = \"\"\"\n        Repeat elements of a %(klass)s.\n\n        Returns a new %(klass)s where each element of the current %(klass)s\n        is repeated consecutively a given number of times.\n\n        Parameters\n        ----------\n        repeats : int or array of ints\n            The number of repetitions for each element. This should be a\n            non-negative integer. Repeating 0 times will return an empty\n            %(klass)s.\n        axis : None\n            Must be ``None``. Has no effect but is accepted for compatibility\n            with numpy.\n\n        Returns\n        -------\n        repeated_index : %(klass)s\n            Newly created %(klass)s with repeated elements.\n\n        See Also\n        --------\n        Series.repeat : Equivalent function for Series.\n        numpy.repeat : Similar method for :class:`numpy.ndarray`.\n\n        Examples\n        --------\n        >>> idx = pd.Index(['a', 'b', 'c'])\n        >>> idx\n        Index(['a', 'b', 'c'], dtype='object')\n        >>> idx.repeat(2)\n        Index(['a', 'a', 'b', 'b', 'c', 'c'], dtype='object')\n        >>> idx.repeat([1, 2, 3])\n        Index(['a', 'b', 'b', 'c', 'c', 'c'], dtype='object')\n        \"\"\"\n\n    @Appender(_index_shared_docs[\"repeat\"] % _index_doc_kwargs)\n    def repeat(self, repeats, axis=None):\n        repeats = ensure_platform_int(repeats)\n        nv.validate_repeat(tuple(), dict(axis=axis))\n        return self._shallow_copy(self._values.repeat(repeats))\n\n    # --------------------------------------------------------------------\n    # Copying Methods\n\n    _index_shared_docs[\n        \"copy\"\n    ] = \"\"\"\n        Make a copy of this object.  Name and dtype sets those attributes on\n        the new object.\n\n        Parameters\n        ----------\n        name : string, optional\n        deep : boolean, default False\n        dtype : numpy dtype or pandas type\n\n        Returns\n        -------\n        copy : Index\n\n        Notes\n        -----\n        In most cases, there should be no functional difference from using\n        ``deep``, but if ``deep`` is passed it will attempt to deepcopy.\n        \"\"\"\n\n    @Appender(_index_shared_docs[\"copy\"])\n    def copy(self, name=None, deep=False, dtype=None, **kwargs):\n        if deep:\n            new_index = self._shallow_copy(self._data.copy())\n        else:\n            new_index = self._shallow_copy()\n\n        names = kwargs.get(\"names\")\n        names = self._validate_names(name=name, names=names, deep=deep)\n        new_index = new_index.set_names(names)\n\n        if dtype:\n            new_index = new_index.astype(dtype)\n        return new_index\n\n    def __copy__(self, **kwargs):\n        return self.copy(**kwargs)\n\n    def __deepcopy__(self, memo=None):\n        \"\"\"\n        Parameters\n        ----------\n        memo, default None\n            Standard signature. Unused\n        \"\"\"\n        if memo is None:\n            memo = {}\n        return self.copy(deep=True)\n\n    # --------------------------------------------------------------------\n    # Rendering Methods\n\n    def __repr__(self):\n        \"\"\"\n        Return a string representation for this object.\n        \"\"\"\n        klass = self.__class__.__name__\n        data = self._format_data()\n        attrs = self._format_attrs()\n        space = self._format_space()\n\n        prepr = (\",%s\" % space).join(\"%s=%s\" % (k, v) for k, v in attrs)\n\n        # no data provided, just attributes\n        if data is None:\n            data = \"\"\n\n        res = \"%s(%s%s)\" % (klass, data, prepr)\n\n        return res\n\n    def _format_space(self):\n\n        # using space here controls if the attributes\n        # are line separated or not (the default)\n\n        # max_seq_items = get_option('display.max_seq_items')\n        # if len(self) > max_seq_items:\n        #    space = \"\\n%s\" % (' ' * (len(klass) + 1))\n        return \" \"\n\n    @property\n    def _formatter_func(self):\n        \"\"\"\n        Return the formatter function.\n        \"\"\"\n        return default_pprint\n\n    def _format_data(self, name=None):\n        \"\"\"\n        Return the formatted data as a unicode string.\n        \"\"\"\n\n        # do we want to justify (only do so for non-objects)\n        is_justify = not (\n            self.inferred_type in (\"string\", \"unicode\")\n            or (\n                self.inferred_type == \"categorical\" and is_object_dtype(self.categories)\n            )\n        )\n\n        return format_object_summary(\n            self, self._formatter_func, is_justify=is_justify, name=name\n        )\n\n    def _format_attrs(self):\n        \"\"\"\n        Return a list of tuples of the (attr,formatted_value).\n        \"\"\"\n        return format_object_attrs(self)\n\n    def _mpl_repr(self):\n        # how to represent ourselves to matplotlib\n        return self.values\n\n    def format(self, name=False, formatter=None, **kwargs):\n        \"\"\"\n        Render a string representation of the Index.\n        \"\"\"\n        header = []\n        if name:\n            header.append(\n                pprint_thing(self.name, escape_chars=(\"\\t\", \"\\r\", \"\\n\"))\n                if self.name is not None\n                else \"\"\n            )\n\n        if formatter is not None:\n            return header + list(self.map(formatter))\n\n        return self._format_with_header(header, **kwargs)\n\n    def _format_with_header(self, header, na_rep=\"NaN\", **kwargs):\n        values = self.values\n\n        from pandas.io.formats.format import format_array\n\n        if is_categorical_dtype(values.dtype):\n            values = np.array(values)\n\n        elif is_object_dtype(values.dtype):\n            values = lib.maybe_convert_objects(values, safe=1)\n\n        if is_object_dtype(values.dtype):\n            result = [pprint_thing(x, escape_chars=(\"\\t\", \"\\r\", \"\\n\")) for x in values]\n\n            # could have nans\n            mask = isna(values)\n            if mask.any():\n                result = np.array(result)\n                result[mask] = na_rep\n                result = result.tolist()\n\n        else:\n            result = _trim_front(format_array(values, None, justify=\"left\"))\n        return header + result\n\n    def to_native_types(self, slicer=None, **kwargs):\n        \"\"\"\n        Format specified values of `self` and return them.\n\n        Parameters\n        ----------\n        slicer : int, array-like\n            An indexer into `self` that specifies which values\n            are used in the formatting process.\n        kwargs : dict\n            Options for specifying how the values should be formatted.\n            These options include the following:\n\n            1) na_rep : str\n                The value that serves as a placeholder for NULL values\n            2) quoting : bool or None\n                Whether or not there are quoted values in `self`\n            3) date_format : str\n                The format used to represent date-like values\n\n        Returns\n        -------\n        numpy.ndarray\n            Formatted values.\n        \"\"\"\n\n        values = self\n        if slicer is not None:\n            values = values[slicer]\n        return values._format_native_types(**kwargs)\n\n    def _format_native_types(self, na_rep=\"\", quoting=None, **kwargs):\n        \"\"\"\n        Actually format specific types of the index.\n        \"\"\"\n        mask = isna(self)\n        if not self.is_object() and not quoting:\n            values = np.asarray(self).astype(str)\n        else:\n            values = np.array(self, dtype=object, copy=True)\n\n        values[mask] = na_rep\n        return values\n\n    def _summary(self, name=None):\n        \"\"\"\n        Return a summarized representation.\n\n        Parameters\n        ----------\n        name : str\n            name to use in the summary representation\n\n        Returns\n        -------\n        String with a summarized representation of the index\n        \"\"\"\n        if len(self) > 0:\n            head = self[0]\n            if hasattr(head, \"format\") and not isinstance(head, str):\n                head = head.format()\n            tail = self[-1]\n            if hasattr(tail, \"format\") and not isinstance(tail, str):\n                tail = tail.format()\n            index_summary = \", %s to %s\" % (pprint_thing(head), pprint_thing(tail))\n        else:\n            index_summary = \"\"\n\n        if name is None:\n            name = type(self).__name__\n        return \"%s: %s entries%s\" % (name, len(self), index_summary)\n\n    def summary(self, name=None):\n        \"\"\"\n        Return a summarized representation.\n\n        .. deprecated:: 0.23.0\n        \"\"\"\n        warnings.warn(\n            \"'summary' is deprecated and will be removed in a future version.\",\n            FutureWarning,\n            stacklevel=2,\n        )\n        return self._summary(name)\n\n    # --------------------------------------------------------------------\n    # Conversion Methods\n\n    def to_flat_index(self):\n        \"\"\"\n        Identity method.\n\n        .. versionadded:: 0.24.0\n\n        This is implemented for compatibility with subclass implementations\n        when chaining.\n\n        Returns\n        -------\n        pd.Index\n            Caller.\n\n        See Also\n        --------\n        MultiIndex.to_flat_index : Subclass implementation.\n        \"\"\"\n        return self\n\n    def to_series(self, index=None, name=None):\n        \"\"\"\n        Create a Series with both index and values equal to the index keys\n        useful with map for returning an indexer based on an index.\n\n        Parameters\n        ----------\n        index : Index, optional\n            index of resulting Series. If None, defaults to original index\n        name : string, optional\n            name of resulting Series. If None, defaults to name of original\n            index\n\n        Returns\n        -------\n        Series : dtype will be based on the type of the Index values.\n        \"\"\"\n\n        from pandas import Series\n\n        if index is None:\n            index = self._shallow_copy()\n        if name is None:\n            name = self.name\n\n        return Series(self.values.copy(), index=index, name=name)\n\n    def to_frame(self, index=True, name=None):\n        \"\"\"\n        Create a DataFrame with a column containing the Index.\n\n        .. versionadded:: 0.24.0\n\n        Parameters\n        ----------\n        index : boolean, default True\n            Set the index of the returned DataFrame as the original Index.\n\n        name : object, default None\n            The passed name should substitute for the index name (if it has\n            one).\n\n        Returns\n        -------\n        DataFrame\n            DataFrame containing the original Index data.\n\n        See Also\n        --------\n        Index.to_series : Convert an Index to a Series.\n        Series.to_frame : Convert Series to DataFrame.\n\n        Examples\n        --------\n        >>> idx = pd.Index(['Ant', 'Bear', 'Cow'], name='animal')\n        >>> idx.to_frame()\n               animal\n        animal\n        Ant       Ant\n        Bear     Bear\n        Cow       Cow\n\n        By default, the original Index is reused. To enforce a new Index:\n\n        >>> idx.to_frame(index=False)\n            animal\n        0   Ant\n        1  Bear\n        2   Cow\n\n        To override the name of the resulting column, specify `name`:\n\n        >>> idx.to_frame(index=False, name='zoo')\n            zoo\n        0   Ant\n        1  Bear\n        2   Cow\n        \"\"\"\n\n        from pandas import DataFrame\n\n        if name is None:\n            name = self.name or 0\n        result = DataFrame({name: self._values.copy()})\n\n        if index:\n            result.index = self\n        return result\n\n    # --------------------------------------------------------------------\n    # Name-Centric Methods\n\n    def _validate_names(self, name=None, names=None, deep=False):\n        \"\"\"\n        Handles the quirks of having a singular 'name' parameter for general\n        Index and plural 'names' parameter for MultiIndex.\n        \"\"\"\n        from copy import deepcopy\n\n        if names is not None and name is not None:\n            raise TypeError(\"Can only provide one of `names` and `name`\")\n        elif names is None and name is None:\n            return deepcopy(self.names) if deep else self.names\n        elif names is not None:\n            if not is_list_like(names):\n                raise TypeError(\"Must pass list-like as `names`.\")\n            return names\n        else:\n            if not is_list_like(name):\n                return [name]\n            return name\n\n    def _get_names(self):\n        return FrozenList((self.name,))\n\n    def _set_names(self, values, level=None):\n        \"\"\"\n        Set new names on index. Each name has to be a hashable type.\n\n        Parameters\n        ----------\n        values : str or sequence\n            name(s) to set\n        level : int, level name, or sequence of int/level names (default None)\n            If the index is a MultiIndex (hierarchical), level(s) to set (None\n            for all levels).  Otherwise level must be None\n\n        Raises\n        ------\n        TypeError if each name is not hashable.\n        \"\"\"\n        if not is_list_like(values):\n            raise ValueError(\"Names must be a list-like\")\n        if len(values) != 1:\n            raise ValueError(\"Length of new names must be 1, got %d\" % len(values))\n\n        # GH 20527\n        # All items in 'name' need to be hashable:\n        for name in values:\n            if not is_hashable(name):\n                raise TypeError(\n                    \"{}.name must be a hashable type\".format(self.__class__.__name__)\n                )\n        self.name = values[0]\n\n    names = property(fset=_set_names, fget=_get_names)\n\n    def set_names(self, names, level=None, inplace=False):\n        \"\"\"\n        Set Index or MultiIndex name.\n\n        Able to set new names partially and by level.\n\n        Parameters\n        ----------\n        names : label or list of label\n            Name(s) to set.\n        level : int, label or list of int or label, optional\n            If the index is a MultiIndex, level(s) to set (None for all\n            levels). Otherwise level must be None.\n        inplace : bool, default False\n            Modifies the object directly, instead of creating a new Index or\n            MultiIndex.\n\n        Returns\n        -------\n        Index\n            The same type as the caller or None if inplace is True.\n\n        See Also\n        --------\n        Index.rename : Able to set new names without level.\n\n        Examples\n        --------\n        >>> idx = pd.Index([1, 2, 3, 4])\n        >>> idx\n        Int64Index([1, 2, 3, 4], dtype='int64')\n        >>> idx.set_names('quarter')\n        Int64Index([1, 2, 3, 4], dtype='int64', name='quarter')\n\n        >>> idx = pd.MultiIndex.from_product([['python', 'cobra'],\n        ...                                   [2018, 2019]])\n        >>> idx\n        MultiIndex([('python', 2018),\n                    ('python', 2019),\n                    ( 'cobra', 2018),\n                    ( 'cobra', 2019)],\n                   )\n        >>> idx.set_names(['kind', 'year'], inplace=True)\n        >>> idx\n        MultiIndex([('python', 2018),\n                    ('python', 2019),\n                    ( 'cobra', 2018),\n                    ( 'cobra', 2019)],\n                   names=['kind', 'year'])\n        >>> idx.set_names('species', level=0)\n        MultiIndex([('python', 2018),\n                    ('python', 2019),\n                    ( 'cobra', 2018),\n                    ( 'cobra', 2019)],\n                   names=['species', 'year'])\n        \"\"\"\n\n        if level is not None and not isinstance(self, ABCMultiIndex):\n            raise ValueError(\"Level must be None for non-MultiIndex\")\n\n        if level is not None and not is_list_like(level) and is_list_like(names):\n            msg = \"Names must be a string when a single level is provided.\"\n            raise TypeError(msg)\n\n        if not is_list_like(names) and level is None and self.nlevels > 1:\n            raise TypeError(\"Must pass list-like as `names`.\")\n\n        if not is_list_like(names):\n            names = [names]\n        if level is not None and not is_list_like(level):\n            level = [level]\n\n        if inplace:\n            idx = self\n        else:\n            idx = self._shallow_copy()\n        idx._set_names(names, level=level)\n        if not inplace:\n            return idx\n\n    def rename(self, name, inplace=False):\n        \"\"\"\n        Alter Index or MultiIndex name.\n\n        Able to set new names without level. Defaults to returning new index.\n        Length of names must match number of levels in MultiIndex.\n\n        Parameters\n        ----------\n        name : label or list of labels\n            Name(s) to set.\n        inplace : boolean, default False\n            Modifies the object directly, instead of creating a new Index or\n            MultiIndex.\n\n        Returns\n        -------\n        Index\n            The same type as the caller or None if inplace is True.\n\n        See Also\n        --------\n        Index.set_names : Able to set new names partially and by level.\n\n        Examples\n        --------\n        >>> idx = pd.Index(['A', 'C', 'A', 'B'], name='score')\n        >>> idx.rename('grade')\n        Index(['A', 'C', 'A', 'B'], dtype='object', name='grade')\n\n        >>> idx = pd.MultiIndex.from_product([['python', 'cobra'],\n        ...                                   [2018, 2019]],\n        ...                                   names=['kind', 'year'])\n        >>> idx\n        MultiIndex([('python', 2018),\n                    ('python', 2019),\n                    ( 'cobra', 2018),\n                    ( 'cobra', 2019)],\n                   names=['kind', 'year'])\n        >>> idx.rename(['species', 'year'])\n        MultiIndex([('python', 2018),\n                    ('python', 2019),\n                    ( 'cobra', 2018),\n                    ( 'cobra', 2019)],\n                   names=['species', 'year'])\n        >>> idx.rename('species')\n        Traceback (most recent call last):\n        TypeError: Must pass list-like as `names`.\n        \"\"\"\n        return self.set_names([name], inplace=inplace)\n\n    # --------------------------------------------------------------------\n    # Level-Centric Methods\n\n    @property\n    def nlevels(self):\n        \"\"\"\n        Number of levels.\n        \"\"\"\n        return 1\n\n    def _sort_levels_monotonic(self):\n        \"\"\"\n        Compat with MultiIndex.\n        \"\"\"\n        return self\n\n    def _validate_index_level(self, level):\n        \"\"\"\n        Validate index level.\n\n        For single-level Index getting level number is a no-op, but some\n        verification must be done like in MultiIndex.\n\n        \"\"\"\n        if isinstance(level, int):\n            if level < 0 and level != -1:\n                raise IndexError(\n                    \"Too many levels: Index has only 1 level,\"\n                    \" %d is not a valid level number\" % (level,)\n                )\n            elif level > 0:\n                raise IndexError(\n                    \"Too many levels: Index has only 1 level, not %d\" % (level + 1)\n                )\n        elif level != self.name:\n            raise KeyError(\n                \"Requested level ({}) does not match index name ({})\".format(\n                    level, self.name\n                )\n            )\n\n    def _get_level_number(self, level):\n        self._validate_index_level(level)\n        return 0\n\n    def sortlevel(self, level=None, ascending=True, sort_remaining=None):\n        \"\"\"\n        For internal compatibility with with the Index API.\n\n        Sort the Index. This is for compat with MultiIndex\n\n        Parameters\n        ----------\n        ascending : boolean, default True\n            False to sort in descending order\n\n        level, sort_remaining are compat parameters\n\n        Returns\n        -------\n        Index\n        \"\"\"\n        return self.sort_values(return_indexer=True, ascending=ascending)\n\n    def _get_level_values(self, level):\n        \"\"\"\n        Return an Index of values for requested level.\n\n        This is primarily useful to get an individual level of values from a\n        MultiIndex, but is provided on Index as well for compatibility.\n\n        Parameters\n        ----------\n        level : int or str\n            It is either the integer position or the name of the level.\n\n        Returns\n        -------\n        Index\n            Calling object, as there is only one level in the Index.\n\n        See Also\n        --------\n        MultiIndex.get_level_values : Get values for a level of a MultiIndex.\n\n        Notes\n        -----\n        For Index, level should be 0, since there are no multiple levels.\n\n        Examples\n        --------\n\n        >>> idx = pd.Index(list('abc'))\n        >>> idx\n        Index(['a', 'b', 'c'], dtype='object')\n\n        Get level values by supplying `level` as integer:\n\n        >>> idx.get_level_values(0)\n        Index(['a', 'b', 'c'], dtype='object')\n        \"\"\"\n        self._validate_index_level(level)\n        return self\n\n    get_level_values = _get_level_values\n\n    def droplevel(self, level=0):\n        \"\"\"\n        Return index with requested level(s) removed.\n\n        If resulting index has only 1 level left, the result will be\n        of Index type, not MultiIndex.\n\n        .. versionadded:: 0.23.1 (support for non-MultiIndex)\n\n        Parameters\n        ----------\n        level : int, str, or list-like, default 0\n            If a string is given, must be the name of a level\n            If list-like, elements must be names or indexes of levels.\n\n        Returns\n        -------\n        Index or MultiIndex\n        \"\"\"\n        if not isinstance(level, (tuple, list)):\n            level = [level]\n\n        levnums = sorted(self._get_level_number(lev) for lev in level)[::-1]\n\n        if len(level) == 0:\n            return self\n        if len(level) >= self.nlevels:\n            raise ValueError(\n                \"Cannot remove {} levels from an index with {} \"\n                \"levels: at least one level must be \"\n                \"left.\".format(len(level), self.nlevels)\n            )\n        # The two checks above guarantee that here self is a MultiIndex\n\n        new_levels = list(self.levels)\n        new_codes = list(self.codes)\n        new_names = list(self.names)\n\n        for i in levnums:\n            new_levels.pop(i)\n            new_codes.pop(i)\n            new_names.pop(i)\n\n        if len(new_levels) == 1:\n\n            # set nan if needed\n            mask = new_codes[0] == -1\n            result = new_levels[0].take(new_codes[0])\n            if mask.any():\n                result = result.putmask(mask, np.nan)\n\n            result.name = new_names[0]\n            return result\n        else:\n            from .multi import MultiIndex\n\n            return MultiIndex(\n                levels=new_levels,\n                codes=new_codes,\n                names=new_names,\n                verify_integrity=False,\n            )\n\n    _index_shared_docs[\n        \"_get_grouper_for_level\"\n    ] = \"\"\"\n        Get index grouper corresponding to an index level\n\n        Parameters\n        ----------\n        mapper: Group mapping function or None\n            Function mapping index values to groups\n        level : int or None\n            Index level\n\n        Returns\n        -------\n        grouper : Index\n            Index of values to group on.\n        labels : ndarray of int or None\n            Array of locations in level_index.\n        uniques : Index or None\n            Index of unique values for level.\n        \"\"\"\n\n    @Appender(_index_shared_docs[\"_get_grouper_for_level\"])\n    def _get_grouper_for_level(self, mapper, level=None):\n        assert level is None or level == 0\n        if mapper is None:\n            grouper = self\n        else:\n            grouper = self.map(mapper)\n\n        return grouper, None, None\n\n    # --------------------------------------------------------------------\n    # Introspection Methods\n\n    @property\n    def is_monotonic(self):\n        \"\"\"\n        Alias for is_monotonic_increasing.\n        \"\"\"\n        return self.is_monotonic_increasing\n\n    @property\n    def is_monotonic_increasing(self):\n        \"\"\"\n        Return if the index is monotonic increasing (only equal or\n        increasing) values.\n\n        Examples\n        --------\n        >>> Index([1, 2, 3]).is_monotonic_increasing\n        True\n        >>> Index([1, 2, 2]).is_monotonic_increasing\n        True\n        >>> Index([1, 3, 2]).is_monotonic_increasing\n        False\n        \"\"\"\n        return self._engine.is_monotonic_increasing\n\n    @property\n    def is_monotonic_decreasing(self):\n        \"\"\"\n        Return if the index is monotonic decreasing (only equal or\n        decreasing) values.\n\n        Examples\n        --------\n        >>> Index([3, 2, 1]).is_monotonic_decreasing\n        True\n        >>> Index([3, 2, 2]).is_monotonic_decreasing\n        True\n        >>> Index([3, 1, 2]).is_monotonic_decreasing\n        False\n        \"\"\"\n        return self._engine.is_monotonic_decreasing\n\n    @property\n    def _is_strictly_monotonic_increasing(self):\n        \"\"\"\n        Return if the index is strictly monotonic increasing\n        (only increasing) values.\n\n        Examples\n        --------\n        >>> Index([1, 2, 3])._is_strictly_monotonic_increasing\n        True\n        >>> Index([1, 2, 2])._is_strictly_monotonic_increasing\n        False\n        >>> Index([1, 3, 2])._is_strictly_monotonic_increasing\n        False\n        \"\"\"\n        return self.is_unique and self.is_monotonic_increasing\n\n    @property\n    def _is_strictly_monotonic_decreasing(self):\n        \"\"\"\n        Return if the index is strictly monotonic decreasing\n        (only decreasing) values.\n\n        Examples\n        --------\n        >>> Index([3, 2, 1])._is_strictly_monotonic_decreasing\n        True\n        >>> Index([3, 2, 2])._is_strictly_monotonic_decreasing\n        False\n        >>> Index([3, 1, 2])._is_strictly_monotonic_decreasing\n        False\n        \"\"\"\n        return self.is_unique and self.is_monotonic_decreasing\n\n    def is_lexsorted_for_tuple(self, tup):\n        return True\n\n    @cache_readonly\n    def is_unique(self):\n        \"\"\"\n        Return if the index has unique values.\n        \"\"\"\n        return self._engine.is_unique\n\n    @property\n    def has_duplicates(self):\n        return not self.is_unique\n\n    def is_boolean(self):\n        return self.inferred_type in [\"boolean\"]\n\n    def is_integer(self):\n        return self.inferred_type in [\"integer\"]\n\n    def is_floating(self):\n        return self.inferred_type in [\"floating\", \"mixed-integer-float\", \"integer-na\"]\n\n    def is_numeric(self):\n        return self.inferred_type in [\"integer\", \"floating\"]\n\n    def is_object(self):\n        return is_object_dtype(self.dtype)\n\n    def is_categorical(self):\n        \"\"\"\n        Check if the Index holds categorical data.\n\n        Returns\n        -------\n        boolean\n            True if the Index is categorical.\n\n        See Also\n        --------\n        CategoricalIndex : Index for categorical data.\n\n        Examples\n        --------\n        >>> idx = pd.Index([\"Watermelon\", \"Orange\", \"Apple\",\n        ...                 \"Watermelon\"]).astype(\"category\")\n        >>> idx.is_categorical()\n        True\n\n        >>> idx = pd.Index([1, 3, 5, 7])\n        >>> idx.is_categorical()\n        False\n\n        >>> s = pd.Series([\"Peter\", \"Victor\", \"Elisabeth\", \"Mar\"])\n        >>> s\n        0        Peter\n        1       Victor\n        2    Elisabeth\n        3          Mar\n        dtype: object\n        >>> s.index.is_categorical()\n        False\n        \"\"\"\n        return self.inferred_type in [\"categorical\"]\n\n    def is_interval(self):\n        return self.inferred_type in [\"interval\"]\n\n    def is_mixed(self):\n        return self.inferred_type in [\"mixed\"]\n\n    def holds_integer(self):\n        \"\"\"\n        Whether the type is an integer type.\n        \"\"\"\n        return self.inferred_type in [\"integer\", \"mixed-integer\"]\n\n    @cache_readonly\n    def inferred_type(self):\n        \"\"\"\n        Return a string of the type inferred from the values.\n        \"\"\"\n        return lib.infer_dtype(self, skipna=False)\n\n    @cache_readonly\n    def is_all_dates(self):\n        return is_datetime_array(ensure_object(self.values))\n\n    # --------------------------------------------------------------------\n    # Pickle Methods\n\n    def __reduce__(self):\n        d = dict(data=self._data)\n        d.update(self._get_attributes_dict())\n        return _new_Index, (self.__class__, d), None\n\n    def __setstate__(self, state):\n        \"\"\"\n        Necessary for making this object picklable.\n        \"\"\"\n\n        if isinstance(state, dict):\n            self._data = state.pop(\"data\")\n            for k, v in state.items():\n                setattr(self, k, v)\n\n        elif isinstance(state, tuple):\n\n            if len(state) == 2:\n                nd_state, own_state = state\n                data = np.empty(nd_state[1], dtype=nd_state[2])\n                np.ndarray.__setstate__(data, nd_state)\n                self.name = own_state[0]\n\n            else:  # pragma: no cover\n                data = np.empty(state)\n                np.ndarray.__setstate__(data, state)\n\n            self._data = data\n            self._reset_identity()\n        else:\n            raise Exception(\"invalid pickle state\")\n\n    _unpickle_compat = __setstate__\n\n    # --------------------------------------------------------------------\n    # Null Handling Methods\n\n    _na_value = np.nan\n    \"\"\"The expected NA value to use with this index.\"\"\"\n\n    @cache_readonly\n    def _isnan(self):\n        \"\"\"\n        Return if each value is NaN.\n        \"\"\"\n        if self._can_hold_na:\n            return isna(self)\n        else:\n            # shouldn't reach to this condition by checking hasnans beforehand\n            values = np.empty(len(self), dtype=np.bool_)\n            values.fill(False)\n            return values\n\n    @cache_readonly\n    def _nan_idxs(self):\n        if self._can_hold_na:\n            w, = self._isnan.nonzero()\n            return w\n        else:\n            return np.array([], dtype=np.int64)\n\n    @cache_readonly\n    def hasnans(self):\n        \"\"\"\n        Return if I have any nans; enables various perf speedups.\n        \"\"\"\n        if self._can_hold_na:\n            return bool(self._isnan.any())\n        else:\n            return False\n\n    def isna(self):\n        \"\"\"\n        Detect missing values.\n\n        Return a boolean same-sized object indicating if the values are NA.\n        NA values, such as ``None``, :attr:`numpy.NaN` or :attr:`pd.NaT`, get\n        mapped to ``True`` values.\n        Everything else get mapped to ``False`` values. Characters such as\n        empty strings `''` or :attr:`numpy.inf` are not considered NA values\n        (unless you set ``pandas.options.mode.use_inf_as_na = True``).\n\n        .. versionadded:: 0.20.0\n\n        Returns\n        -------\n        numpy.ndarray\n            A boolean array of whether my values are NA.\n\n        See Also\n        --------\n        Index.notna : Boolean inverse of isna.\n        Index.dropna : Omit entries with missing values.\n        isna : Top-level isna.\n        Series.isna : Detect missing values in Series object.\n\n        Examples\n        --------\n        Show which entries in a pandas.Index are NA. The result is an\n        array.\n\n        >>> idx = pd.Index([5.2, 6.0, np.NaN])\n        >>> idx\n        Float64Index([5.2, 6.0, nan], dtype='float64')\n        >>> idx.isna()\n        array([False, False,  True], dtype=bool)\n\n        Empty strings are not considered NA values. None is considered an NA\n        value.\n\n        >>> idx = pd.Index(['black', '', 'red', None])\n        >>> idx\n        Index(['black', '', 'red', None], dtype='object')\n        >>> idx.isna()\n        array([False, False, False,  True], dtype=bool)\n\n        For datetimes, `NaT` (Not a Time) is considered as an NA value.\n\n        >>> idx = pd.DatetimeIndex([pd.Timestamp('1940-04-25'),\n        ...                         pd.Timestamp(''), None, pd.NaT])\n        >>> idx\n        DatetimeIndex(['1940-04-25', 'NaT', 'NaT', 'NaT'],\n                      dtype='datetime64[ns]', freq=None)\n        >>> idx.isna()\n        array([False,  True,  True,  True], dtype=bool)\n        \"\"\"\n        return self._isnan\n\n    isnull = isna\n\n    def notna(self):\n        \"\"\"\n        Detect existing (non-missing) values.\n\n        Return a boolean same-sized object indicating if the values are not NA.\n        Non-missing values get mapped to ``True``. Characters such as empty\n        strings ``''`` or :attr:`numpy.inf` are not considered NA values\n        (unless you set ``pandas.options.mode.use_inf_as_na = True``).\n        NA values, such as None or :attr:`numpy.NaN`, get mapped to ``False``\n        values.\n\n        .. versionadded:: 0.20.0\n\n        Returns\n        -------\n        numpy.ndarray\n            Boolean array to indicate which entries are not NA.\n\n        See Also\n        --------\n        Index.notnull : Alias of notna.\n        Index.isna: Inverse of notna.\n        notna : Top-level notna.\n\n        Examples\n        --------\n        Show which entries in an Index are not NA. The result is an\n        array.\n\n        >>> idx = pd.Index([5.2, 6.0, np.NaN])\n        >>> idx\n        Float64Index([5.2, 6.0, nan], dtype='float64')\n        >>> idx.notna()\n        array([ True,  True, False])\n\n        Empty strings are not considered NA values. None is considered a NA\n        value.\n\n        >>> idx = pd.Index(['black', '', 'red', None])\n        >>> idx\n        Index(['black', '', 'red', None], dtype='object')\n        >>> idx.notna()\n        array([ True,  True,  True, False])\n        \"\"\"\n        return ~self.isna()\n\n    notnull = notna\n\n    _index_shared_docs[\n        \"fillna\"\n    ] = \"\"\"\n        Fill NA/NaN values with the specified value.\n\n        Parameters\n        ----------\n        value : scalar\n            Scalar value to use to fill holes (e.g. 0).\n            This value cannot be a list-likes.\n        downcast : dict, default is None\n            a dict of item->dtype of what to downcast if possible,\n            or the string 'infer' which will try to downcast to an appropriate\n            equal type (e.g. float64 to int64 if possible)\n\n        Returns\n        -------\n        filled : Index\n        \"\"\"\n\n    @Appender(_index_shared_docs[\"fillna\"])\n    def fillna(self, value=None, downcast=None):\n        self._assert_can_do_op(value)\n        if self.hasnans:\n            result = self.putmask(self._isnan, value)\n            if downcast is None:\n                # no need to care metadata other than name\n                # because it can't have freq if\n                return Index(result, name=self.name)\n        return self._shallow_copy()\n\n    _index_shared_docs[\n        \"dropna\"\n    ] = \"\"\"\n        Return Index without NA/NaN values.\n\n        Parameters\n        ----------\n        how :  {'any', 'all'}, default 'any'\n            If the Index is a MultiIndex, drop the value when any or all levels\n            are NaN.\n\n        Returns\n        -------\n        valid : Index\n        \"\"\"\n\n    @Appender(_index_shared_docs[\"dropna\"])\n    def dropna(self, how=\"any\"):\n        if how not in (\"any\", \"all\"):\n            raise ValueError(\"invalid how option: {0}\".format(how))\n\n        if self.hasnans:\n            return self._shallow_copy(self.values[~self._isnan])\n        return self._shallow_copy()\n\n    # --------------------------------------------------------------------\n    # Uniqueness Methods\n\n    _index_shared_docs[\n        \"index_unique\"\n    ] = \"\"\"\n        Return unique values in the index. Uniques are returned in order\n        of appearance, this does NOT sort.\n\n        Parameters\n        ----------\n        level : int or str, optional, default None\n            Only return values from specified level (for MultiIndex)\n\n            .. versionadded:: 0.23.0\n\n        Returns\n        -------\n        Index without duplicates\n\n        See Also\n        --------\n        unique\n        Series.unique\n        \"\"\"\n\n    @Appender(_index_shared_docs[\"index_unique\"] % _index_doc_kwargs)\n    def unique(self, level=None):\n        if level is not None:\n            self._validate_index_level(level)\n        result = super().unique()\n        return self._shallow_copy(result)\n\n    def drop_duplicates(self, keep=\"first\"):\n        \"\"\"\n        Return Index with duplicate values removed.\n\n        Parameters\n        ----------\n        keep : {'first', 'last', ``False``}, default 'first'\n            - 'first' : Drop duplicates except for the first occurrence.\n            - 'last' : Drop duplicates except for the last occurrence.\n            - ``False`` : Drop all duplicates.\n\n        Returns\n        -------\n        deduplicated : Index\n\n        See Also\n        --------\n        Series.drop_duplicates : Equivalent method on Series.\n        DataFrame.drop_duplicates : Equivalent method on DataFrame.\n        Index.duplicated : Related method on Index, indicating duplicate\n            Index values.\n\n        Examples\n        --------\n        Generate an pandas.Index with duplicate values.\n\n        >>> idx = pd.Index(['lama', 'cow', 'lama', 'beetle', 'lama', 'hippo'])\n\n        The `keep` parameter controls  which duplicate values are removed.\n        The value 'first' keeps the first occurrence for each\n        set of duplicated entries. The default value of keep is 'first'.\n\n        >>> idx.drop_duplicates(keep='first')\n        Index(['lama', 'cow', 'beetle', 'hippo'], dtype='object')\n\n        The value 'last' keeps the last occurrence for each set of duplicated\n        entries.\n\n        >>> idx.drop_duplicates(keep='last')\n        Index(['cow', 'beetle', 'lama', 'hippo'], dtype='object')\n\n        The value ``False`` discards all sets of duplicated entries.\n\n        >>> idx.drop_duplicates(keep=False)\n        Index(['cow', 'beetle', 'hippo'], dtype='object')\n        \"\"\"\n        return super().drop_duplicates(keep=keep)\n\n    def duplicated(self, keep=\"first\"):\n        \"\"\"\n        Indicate duplicate index values.\n\n        Duplicated values are indicated as ``True`` values in the resulting\n        array. Either all duplicates, all except the first, or all except the\n        last occurrence of duplicates can be indicated.\n\n        Parameters\n        ----------\n        keep : {'first', 'last', False}, default 'first'\n            The value or values in a set of duplicates to mark as missing.\n\n            - 'first' : Mark duplicates as ``True`` except for the first\n              occurrence.\n            - 'last' : Mark duplicates as ``True`` except for the last\n              occurrence.\n            - ``False`` : Mark all duplicates as ``True``.\n\n        Returns\n        -------\n        numpy.ndarray\n\n        See Also\n        --------\n        Series.duplicated : Equivalent method on pandas.Series.\n        DataFrame.duplicated : Equivalent method on pandas.DataFrame.\n        Index.drop_duplicates : Remove duplicate values from Index.\n\n        Examples\n        --------\n        By default, for each set of duplicated values, the first occurrence is\n        set to False and all others to True:\n\n        >>> idx = pd.Index(['lama', 'cow', 'lama', 'beetle', 'lama'])\n        >>> idx.duplicated()\n        array([False, False,  True, False,  True])\n\n        which is equivalent to\n\n        >>> idx.duplicated(keep='first')\n        array([False, False,  True, False,  True])\n\n        By using 'last', the last occurrence of each set of duplicated values\n        is set on False and all others on True:\n\n        >>> idx.duplicated(keep='last')\n        array([ True, False,  True, False, False])\n\n        By setting keep on ``False``, all duplicates are True:\n\n        >>> idx.duplicated(keep=False)\n        array([ True, False,  True, False,  True])\n        \"\"\"\n        return super().duplicated(keep=keep)\n\n    def get_duplicates(self):\n        \"\"\"\n        Extract duplicated index elements.\n\n        .. deprecated:: 0.23.0\n            Use idx[idx.duplicated()].unique() instead\n\n        Returns a sorted list of index elements which appear more than once in\n        the index.\n\n        Returns\n        -------\n        array-like\n            List of duplicated indexes.\n\n        See Also\n        --------\n        Index.duplicated : Return boolean array denoting duplicates.\n        Index.drop_duplicates : Return Index with duplicates removed.\n\n        Examples\n        --------\n\n        Works on different Index of types.\n\n        >>> pd.Index([1, 2, 2, 3, 3, 3, 4]).get_duplicates()  # doctest: +SKIP\n        [2, 3]\n\n        Note that for a DatetimeIndex, it does not return a list but a new\n        DatetimeIndex:\n\n        >>> dates = pd.to_datetime(['2018-01-01', '2018-01-02', '2018-01-03',\n        ...                         '2018-01-03', '2018-01-04', '2018-01-04'],\n        ...                        format='%Y-%m-%d')\n        >>> pd.Index(dates).get_duplicates()  # doctest: +SKIP\n        DatetimeIndex(['2018-01-03', '2018-01-04'],\n                      dtype='datetime64[ns]', freq=None)\n\n        Sorts duplicated elements even when indexes are unordered.\n\n        >>> pd.Index([1, 2, 3, 2, 3, 4, 3]).get_duplicates()  # doctest: +SKIP\n        [2, 3]\n\n        Return empty array-like structure when all elements are unique.\n\n        >>> pd.Index([1, 2, 3, 4]).get_duplicates()  # doctest: +SKIP\n        []\n        >>> dates = pd.to_datetime(['2018-01-01', '2018-01-02', '2018-01-03'],\n        ...                        format='%Y-%m-%d')\n        >>> pd.Index(dates).get_duplicates()  # doctest: +SKIP\n        DatetimeIndex([], dtype='datetime64[ns]', freq=None)\n        \"\"\"\n        warnings.warn(\n            \"'get_duplicates' is deprecated and will be removed in \"\n            \"a future release. You can use \"\n            \"idx[idx.duplicated()].unique() instead\",\n            FutureWarning,\n            stacklevel=2,\n        )\n\n        return self[self.duplicated()].unique()\n\n    def _get_unique_index(self, dropna=False):\n        \"\"\"\n        Returns an index containing unique values.\n\n        Parameters\n        ----------\n        dropna : bool\n            If True, NaN values are dropped.\n\n        Returns\n        -------\n        uniques : index\n        \"\"\"\n        if self.is_unique and not dropna:\n            return self\n\n        values = self.values\n\n        if not self.is_unique:\n            values = self.unique()\n\n        if dropna:\n            try:\n                if self.hasnans:\n                    values = values[~isna(values)]\n            except NotImplementedError:\n                pass\n\n        return self._shallow_copy(values)\n\n    # --------------------------------------------------------------------\n    # Arithmetic & Logical Methods\n\n    def __add__(self, other):\n        if isinstance(other, (ABCSeries, ABCDataFrame)):\n            return NotImplemented\n        from pandas import Series\n\n        return Index(Series(self) + other)\n\n    def __radd__(self, other):\n        from pandas import Series\n\n        return Index(other + Series(self))\n\n    def __iadd__(self, other):\n        # alias for __add__\n        return self + other\n\n    def __sub__(self, other):\n        return Index(np.array(self) - other)\n\n    def __rsub__(self, other):\n        # wrap Series to ensure we pin name correctly\n        from pandas import Series\n\n        return Index(other - Series(self))\n\n    def __and__(self, other):\n        return self.intersection(other)\n\n    def __or__(self, other):\n        return self.union(other)\n\n    def __xor__(self, other):\n        return self.symmetric_difference(other)\n\n    def __nonzero__(self):\n        raise ValueError(\n            \"The truth value of a {0} is ambiguous. \"\n            \"Use a.empty, a.bool(), a.item(), a.any() or a.all().\".format(\n                self.__class__.__name__\n            )\n        )\n\n    __bool__ = __nonzero__\n\n    # --------------------------------------------------------------------\n    # Set Operation Methods\n\n    def _get_reconciled_name_object(self, other):\n        \"\"\"\n        If the result of a set operation will be self,\n        return self, unless the name changes, in which\n        case make a shallow copy of self.\n        \"\"\"\n        name = get_op_result_name(self, other)\n        if self.name != name:\n            return self._shallow_copy(name=name)\n        return self\n\n    def _union_incompatible_dtypes(self, other, sort):\n        \"\"\"\n        Casts this and other index to object dtype to allow the formation\n        of a union between incompatible types.\n\n        Parameters\n        ----------\n        other : Index or array-like\n        sort : False or None, default False\n            Whether to sort the resulting index.\n\n            * False : do not sort the result.\n            * None : sort the result, except when `self` and `other` are equal\n              or when the values cannot be compared.\n\n        Returns\n        -------\n        Index\n        \"\"\"\n        this = self.astype(object, copy=False)\n        # cast to Index for when `other` is list-like\n        other = Index(other).astype(object, copy=False)\n        return Index.union(this, other, sort=sort).astype(object, copy=False)\n\n    def _is_compatible_with_other(self, other):\n        \"\"\"\n        Check whether this and the other dtype are compatible with each other.\n        Meaning a union can be formed between them without needing to be cast\n        to dtype object.\n\n        Parameters\n        ----------\n        other : Index or array-like\n\n        Returns\n        -------\n        bool\n        \"\"\"\n        return type(self) is type(other) and is_dtype_equal(self.dtype, other.dtype)\n\n    def _validate_sort_keyword(self, sort):\n        if sort not in [None, False]:\n            raise ValueError(\n                \"The 'sort' keyword only takes the values of \"\n                \"None or False; {0} was passed.\".format(sort)\n            )\n\n    def union(self, other, sort=None):\n        \"\"\"\n        Form the union of two Index objects.\n\n        If the Index objects are incompatible, both Index objects will be\n        cast to dtype('object') first.\n\n            .. versionchanged:: 0.25.0\n\n        Parameters\n        ----------\n        other : Index or array-like\n        sort : bool or None, default None\n            Whether to sort the resulting Index.\n\n            * None : Sort the result, except when\n\n              1. `self` and `other` are equal.\n              2. `self` or `other` has length 0.\n              3. Some values in `self` or `other` cannot be compared.\n                 A RuntimeWarning is issued in this case.\n\n            * False : do not sort the result.\n\n            .. versionadded:: 0.24.0\n\n            .. versionchanged:: 0.24.1\n\n               Changed the default value from ``True`` to ``None``\n               (without change in behaviour).\n\n        Returns\n        -------\n        union : Index\n\n        Examples\n        --------\n\n        Union matching dtypes\n\n        >>> idx1 = pd.Index([1, 2, 3, 4])\n        >>> idx2 = pd.Index([3, 4, 5, 6])\n        >>> idx1.union(idx2)\n        Int64Index([1, 2, 3, 4, 5, 6], dtype='int64')\n\n        Union mismatched dtypes\n\n        >>> idx1 = pd.Index(['a', 'b', 'c', 'd'])\n        >>> idx2 = pd.Index([1, 2, 3, 4])\n        >>> idx1.union(idx2)\n        Index(['a', 'b', 'c', 'd', 1, 2, 3, 4], dtype='object')\n        \"\"\"\n        self._validate_sort_keyword(sort)\n        self._assert_can_do_setop(other)\n\n        if not self._is_compatible_with_other(other):\n            return self._union_incompatible_dtypes(other, sort=sort)\n\n        return self._union(other, sort=sort)\n\n    def _union(self, other, sort):\n        \"\"\"\n        Specific union logic should go here. In subclasses, union behavior\n        should be overwritten here rather than in `self.union`.\n\n        Parameters\n        ----------\n        other : Index or array-like\n        sort : False or None, default False\n            Whether to sort the resulting index.\n\n            * False : do not sort the result.\n            * None : sort the result, except when `self` and `other` are equal\n              or when the values cannot be compared.\n\n        Returns\n        -------\n        Index\n        \"\"\"\n\n        if not len(other) or self.equals(other):\n            return self._get_reconciled_name_object(other)\n\n        if not len(self):\n            return other._get_reconciled_name_object(self)\n\n        # TODO(EA): setops-refactor, clean all this up\n        if is_period_dtype(self) or is_datetime64tz_dtype(self):\n            lvals = self._ndarray_values\n        else:\n            lvals = self._values\n        if is_period_dtype(other) or is_datetime64tz_dtype(other):\n            rvals = other._ndarray_values\n        else:\n            rvals = other._values\n\n        if sort is None and self.is_monotonic and other.is_monotonic:\n            try:\n                result = self._outer_indexer(lvals, rvals)[0]\n            except TypeError:\n                # incomparable objects\n                result = list(lvals)\n\n                # worth making this faster? a very unusual case\n                value_set = set(lvals)\n                result.extend([x for x in rvals if x not in value_set])\n        else:\n            indexer = self.get_indexer(other)\n            indexer, = (indexer == -1).nonzero()\n\n            if len(indexer) > 0:\n                other_diff = algos.take_nd(rvals, indexer, allow_fill=False)\n                result = concat_compat((lvals, other_diff))\n\n            else:\n                result = lvals\n\n            if sort is None:\n                try:\n                    result = sorting.safe_sort(result)\n                except TypeError as e:\n                    warnings.warn(\n                        \"{}, sort order is undefined for \"\n                        \"incomparable objects\".format(e),\n                        RuntimeWarning,\n                        stacklevel=3,\n                    )\n\n        # for subclasses\n        return self._wrap_setop_result(other, result)\n\n    def _wrap_setop_result(self, other, result):\n        return self._constructor(result, name=get_op_result_name(self, other))\n\n    _index_shared_docs[\n        \"intersection\"\n    ] = \"\"\"\n        Form the intersection of two Index objects.\n\n        This returns a new Index with elements common to the index and `other`.\n\n        Parameters\n        ----------\n        other : Index or array-like\n        sort : False or None, default False\n            Whether to sort the resulting index.\n\n            * False : do not sort the result.\n            * None : sort the result, except when `self` and `other` are equal\n              or when the values cannot be compared.\n\n            .. versionadded:: 0.24.0\n\n            .. versionchanged:: 0.24.1\n\n               Changed the default from ``True`` to ``False``, to match\n               the behaviour of 0.23.4 and earlier.\n\n        Returns\n        -------\n        intersection : Index\n\n        Examples\n        --------\n\n        >>> idx1 = pd.Index([1, 2, 3, 4])\n        >>> idx2 = pd.Index([3, 4, 5, 6])\n        >>> idx1.intersection(idx2)\n        Int64Index([3, 4], dtype='int64')\n        \"\"\"\n\n    # TODO: standardize return type of non-union setops type(self vs other)\n    @Appender(_index_shared_docs[\"intersection\"])\n    def intersection(self, other, sort=False):\n        self._validate_sort_keyword(sort)\n        self._assert_can_do_setop(other)\n        other = ensure_index(other)\n\n        if self.equals(other):\n            return self._get_reconciled_name_object(other)\n\n        if not is_dtype_equal(self.dtype, other.dtype):\n            this = self.astype(\"O\")\n            other = other.astype(\"O\")\n            return this.intersection(other, sort=sort)\n\n        # TODO(EA): setops-refactor, clean all this up\n        if is_period_dtype(self):\n            lvals = self._ndarray_values\n        else:\n            lvals = self._values\n        if is_period_dtype(other):\n            rvals = other._ndarray_values\n        else:\n            rvals = other._values\n\n        if self.is_monotonic and other.is_monotonic:\n            try:\n                result = self._inner_indexer(lvals, rvals)[0]\n                return self._wrap_setop_result(other, result)\n            except TypeError:\n                pass\n\n        try:\n            indexer = Index(rvals).get_indexer(lvals)\n            indexer = indexer.take((indexer != -1).nonzero()[0])\n        except Exception:\n            # duplicates\n            indexer = algos.unique1d(Index(rvals).get_indexer_non_unique(lvals)[0])\n            indexer = indexer[indexer != -1]\n\n        taken = other.take(indexer)\n\n        if sort is None:\n            taken = sorting.safe_sort(taken.values)\n            if self.name != other.name:\n                name = None\n            else:\n                name = self.name\n            return self._shallow_copy(taken, name=name)\n\n        if self.name != other.name:\n            taken.name = None\n\n        return taken\n\n    def difference(self, other, sort=None):\n        \"\"\"\n        Return a new Index with elements from the index that are not in\n        `other`.\n\n        This is the set difference of two Index objects.\n\n        Parameters\n        ----------\n        other : Index or array-like\n        sort : False or None, default None\n            Whether to sort the resulting index. By default, the\n            values are attempted to be sorted, but any TypeError from\n            incomparable elements is caught by pandas.\n\n            * None : Attempt to sort the result, but catch any TypeErrors\n              from comparing incomparable elements.\n            * False : Do not sort the result.\n\n            .. versionadded:: 0.24.0\n\n            .. versionchanged:: 0.24.1\n\n               Changed the default value from ``True`` to ``None``\n               (without change in behaviour).\n\n        Returns\n        -------\n        difference : Index\n\n        Examples\n        --------\n\n        >>> idx1 = pd.Index([2, 1, 3, 4])\n        >>> idx2 = pd.Index([3, 4, 5, 6])\n        >>> idx1.difference(idx2)\n        Int64Index([1, 2], dtype='int64')\n        >>> idx1.difference(idx2, sort=False)\n        Int64Index([2, 1], dtype='int64')\n        \"\"\"\n        self._validate_sort_keyword(sort)\n        self._assert_can_do_setop(other)\n\n        if self.equals(other):\n            # pass an empty np.ndarray with the appropriate dtype\n            return self._shallow_copy(self._data[:0])\n\n        other, result_name = self._convert_can_do_setop(other)\n\n        this = self._get_unique_index()\n\n        indexer = this.get_indexer(other)\n        indexer = indexer.take((indexer != -1).nonzero()[0])\n\n        label_diff = np.setdiff1d(np.arange(this.size), indexer, assume_unique=True)\n        the_diff = this.values.take(label_diff)\n        if sort is None:\n            try:\n                the_diff = sorting.safe_sort(the_diff)\n            except TypeError:\n                pass\n\n        return this._shallow_copy(the_diff, name=result_name, freq=None)\n\n    def symmetric_difference(self, other, result_name=None, sort=None):\n        \"\"\"\n        Compute the symmetric difference of two Index objects.\n\n        Parameters\n        ----------\n        other : Index or array-like\n        result_name : str\n        sort : False or None, default None\n            Whether to sort the resulting index. By default, the\n            values are attempted to be sorted, but any TypeError from\n            incomparable elements is caught by pandas.\n\n            * None : Attempt to sort the result, but catch any TypeErrors\n              from comparing incomparable elements.\n            * False : Do not sort the result.\n\n            .. versionadded:: 0.24.0\n\n            .. versionchanged:: 0.24.1\n\n               Changed the default value from ``True`` to ``None``\n               (without change in behaviour).\n\n        Returns\n        -------\n        symmetric_difference : Index\n\n        Notes\n        -----\n        ``symmetric_difference`` contains elements that appear in either\n        ``idx1`` or ``idx2`` but not both. Equivalent to the Index created by\n        ``idx1.difference(idx2) | idx2.difference(idx1)`` with duplicates\n        dropped.\n\n        Examples\n        --------\n        >>> idx1 = pd.Index([1, 2, 3, 4])\n        >>> idx2 = pd.Index([2, 3, 4, 5])\n        >>> idx1.symmetric_difference(idx2)\n        Int64Index([1, 5], dtype='int64')\n\n        You can also use the ``^`` operator:\n\n        >>> idx1 ^ idx2\n        Int64Index([1, 5], dtype='int64')\n        \"\"\"\n        self._validate_sort_keyword(sort)\n        self._assert_can_do_setop(other)\n        other, result_name_update = self._convert_can_do_setop(other)\n        if result_name is None:\n            result_name = result_name_update\n\n        this = self._get_unique_index()\n        other = other._get_unique_index()\n        indexer = this.get_indexer(other)\n\n        # {this} minus {other}\n        common_indexer = indexer.take((indexer != -1).nonzero()[0])\n        left_indexer = np.setdiff1d(\n            np.arange(this.size), common_indexer, assume_unique=True\n        )\n        left_diff = this.values.take(left_indexer)\n\n        # {other} minus {this}\n        right_indexer = (indexer == -1).nonzero()[0]\n        right_diff = other.values.take(right_indexer)\n\n        the_diff = concat_compat([left_diff, right_diff])\n        if sort is None:\n            try:\n                the_diff = sorting.safe_sort(the_diff)\n            except TypeError:\n                pass\n\n        attribs = self._get_attributes_dict()\n        attribs[\"name\"] = result_name\n        if \"freq\" in attribs:\n            attribs[\"freq\"] = None\n        return self._shallow_copy_with_infer(the_diff, **attribs)\n\n    def _assert_can_do_setop(self, other):\n        if not is_list_like(other):\n            raise TypeError(\"Input must be Index or array-like\")\n        return True\n\n    def _convert_can_do_setop(self, other):\n        if not isinstance(other, Index):\n            other = Index(other, name=self.name)\n            result_name = self.name\n        else:\n            result_name = get_op_result_name(self, other)\n        return other, result_name\n\n    # --------------------------------------------------------------------\n    # Indexing Methods\n\n    _index_shared_docs[\n        \"get_loc\"\n    ] = \"\"\"\n        Get integer location, slice or boolean mask for requested label.\n\n        Parameters\n        ----------\n        key : label\n        method : {None, 'pad'/'ffill', 'backfill'/'bfill', 'nearest'}, optional\n            * default: exact matches only.\n            * pad / ffill: find the PREVIOUS index value if no exact match.\n            * backfill / bfill: use NEXT index value if no exact match\n            * nearest: use the NEAREST index value if no exact match. Tied\n              distances are broken by preferring the larger index value.\n        tolerance : int or float, optional\n            Maximum distance from index value for inexact matches. The value of\n            the index at the matching location most satisfy the equation\n            ``abs(index[loc] - key) <= tolerance``.\n\n            .. versionadded:: 0.21.0 (list-like tolerance)\n\n        Returns\n        -------\n        loc : int if unique index, slice if monotonic index, else mask\n\n        Examples\n        --------\n        >>> unique_index = pd.Index(list('abc'))\n        >>> unique_index.get_loc('b')\n        1\n\n        >>> monotonic_index = pd.Index(list('abbc'))\n        >>> monotonic_index.get_loc('b')\n        slice(1, 3, None)\n\n        >>> non_monotonic_index = pd.Index(list('abcb'))\n        >>> non_monotonic_index.get_loc('b')\n        array([False,  True, False,  True], dtype=bool)\n        \"\"\"\n\n    @Appender(_index_shared_docs[\"get_loc\"])\n    def get_loc(self, key, method=None, tolerance=None):\n        if method is None:\n            if tolerance is not None:\n                raise ValueError(\n                    \"tolerance argument only valid if using pad, \"\n                    \"backfill or nearest lookups\"\n                )\n            try:\n                return self._engine.get_loc(key)\n            except KeyError:\n                return self._engine.get_loc(self._maybe_cast_indexer(key))\n        indexer = self.get_indexer([key], method=method, tolerance=tolerance)\n        if indexer.ndim > 1 or indexer.size > 1:\n            raise TypeError(\"get_loc requires scalar valued input\")\n        loc = indexer.item()\n        if loc == -1:\n            raise KeyError(key)\n        return loc\n\n    _index_shared_docs[\n        \"get_indexer\"\n    ] = \"\"\"\n        Compute indexer and mask for new index given the current index. The\n        indexer should be then used as an input to ndarray.take to align the\n        current data to the new index.\n\n        Parameters\n        ----------\n        target : %(target_klass)s\n        method : {None, 'pad'/'ffill', 'backfill'/'bfill', 'nearest'}, optional\n            * default: exact matches only.\n            * pad / ffill: find the PREVIOUS index value if no exact match.\n            * backfill / bfill: use NEXT index value if no exact match\n            * nearest: use the NEAREST index value if no exact match. Tied\n              distances are broken by preferring the larger index value.\n        limit : int, optional\n            Maximum number of consecutive labels in ``target`` to match for\n            inexact matches.\n        tolerance : optional\n            Maximum distance between original and new labels for inexact\n            matches. The values of the index at the matching locations most\n            satisfy the equation ``abs(index[indexer] - target) <= tolerance``.\n\n            Tolerance may be a scalar value, which applies the same tolerance\n            to all values, or list-like, which applies variable tolerance per\n            element. List-like includes list, tuple, array, Series, and must be\n            the same size as the index and its dtype must exactly match the\n            index's type.\n\n            .. versionadded:: 0.21.0 (list-like tolerance)\n\n        Returns\n        -------\n        indexer : ndarray of int\n            Integers from 0 to n - 1 indicating that the index at these\n            positions matches the corresponding target values. Missing values\n            in the target are marked by -1.\n        %(raises_section)s\n        Examples\n        --------\n        >>> index = pd.Index(['c', 'a', 'b'])\n        >>> index.get_indexer(['a', 'b', 'x'])\n        array([ 1,  2, -1])\n\n        Notice that the return value is an array of locations in ``index``\n        and ``x`` is marked by -1, as it is not in ``index``.\n        \"\"\"\n\n    @Appender(_index_shared_docs[\"get_indexer\"] % _index_doc_kwargs)\n    def get_indexer(self, target, method=None, limit=None, tolerance=None):\n        method = missing.clean_reindex_fill_method(method)\n        target = ensure_index(target)\n        if tolerance is not None:\n            tolerance = self._convert_tolerance(tolerance, target)\n\n        # Treat boolean labels passed to a numeric index as not found. Without\n        # this fix False and True would be treated as 0 and 1 respectively.\n        # (GH #16877)\n        if target.is_boolean() and self.is_numeric():\n            return ensure_platform_int(np.repeat(-1, target.size))\n\n        pself, ptarget = self._maybe_promote(target)\n        if pself is not self or ptarget is not target:\n            return pself.get_indexer(\n                ptarget, method=method, limit=limit, tolerance=tolerance\n            )\n\n        if not is_dtype_equal(self.dtype, target.dtype):\n            this = self.astype(object)\n            target = target.astype(object)\n            return this.get_indexer(\n                target, method=method, limit=limit, tolerance=tolerance\n            )\n\n        if not self.is_unique:\n            raise InvalidIndexError(\n                \"Reindexing only valid with uniquely valued Index objects\"\n            )\n\n        if method == \"pad\" or method == \"backfill\":\n            indexer = self._get_fill_indexer(target, method, limit, tolerance)\n        elif method == \"nearest\":\n            indexer = self._get_nearest_indexer(target, limit, tolerance)\n        else:\n            if tolerance is not None:\n                raise ValueError(\n                    \"tolerance argument only valid if doing pad, \"\n                    \"backfill or nearest reindexing\"\n                )\n            if limit is not None:\n                raise ValueError(\n                    \"limit argument only valid if doing pad, \"\n                    \"backfill or nearest reindexing\"\n                )\n\n            indexer = self._engine.get_indexer(target._ndarray_values)\n\n        return ensure_platform_int(indexer)\n\n    def _convert_tolerance(self, tolerance, target):\n        # override this method on subclasses\n        tolerance = np.asarray(tolerance)\n        if target.size != tolerance.size and tolerance.size > 1:\n            raise ValueError(\"list-like tolerance size must match target index size\")\n        return tolerance\n\n    def _get_fill_indexer(self, target, method, limit=None, tolerance=None):\n        if self.is_monotonic_increasing and target.is_monotonic_increasing:\n            method = (\n                self._engine.get_pad_indexer\n                if method == \"pad\"\n                else self._engine.get_backfill_indexer\n            )\n            indexer = method(target._ndarray_values, limit)\n        else:\n            indexer = self._get_fill_indexer_searchsorted(target, method, limit)\n        if tolerance is not None:\n            indexer = self._filter_indexer_tolerance(\n                target._ndarray_values, indexer, tolerance\n            )\n        return indexer\n\n    def _get_fill_indexer_searchsorted(self, target, method, limit=None):\n        \"\"\"\n        Fallback pad/backfill get_indexer that works for monotonic decreasing\n        indexes and non-monotonic targets.\n        \"\"\"\n        if limit is not None:\n            raise ValueError(\n                \"limit argument for %r method only well-defined \"\n                \"if index and target are monotonic\" % method\n            )\n\n        side = \"left\" if method == \"pad\" else \"right\"\n\n        # find exact matches first (this simplifies the algorithm)\n        indexer = self.get_indexer(target)\n        nonexact = indexer == -1\n        indexer[nonexact] = self._searchsorted_monotonic(target[nonexact], side)\n        if side == \"left\":\n            # searchsorted returns \"indices into a sorted array such that,\n            # if the corresponding elements in v were inserted before the\n            # indices, the order of a would be preserved\".\n            # Thus, we need to subtract 1 to find values to the left.\n            indexer[nonexact] -= 1\n            # This also mapped not found values (values of 0 from\n            # np.searchsorted) to -1, which conveniently is also our\n            # sentinel for missing values\n        else:\n            # Mark indices to the right of the largest value as not found\n            indexer[indexer == len(self)] = -1\n        return indexer\n\n    def _get_nearest_indexer(self, target, limit, tolerance):\n        \"\"\"\n        Get the indexer for the nearest index labels; requires an index with\n        values that can be subtracted from each other (e.g., not strings or\n        tuples).\n        \"\"\"\n        left_indexer = self.get_indexer(target, \"pad\", limit=limit)\n        right_indexer = self.get_indexer(target, \"backfill\", limit=limit)\n\n        target = np.asarray(target)\n        left_distances = abs(self.values[left_indexer] - target)\n        right_distances = abs(self.values[right_indexer] - target)\n\n        op = operator.lt if self.is_monotonic_increasing else operator.le\n        indexer = np.where(\n            op(left_distances, right_distances) | (right_indexer == -1),\n            left_indexer,\n            right_indexer,\n        )\n        if tolerance is not None:\n            indexer = self._filter_indexer_tolerance(target, indexer, tolerance)\n        return indexer\n\n    def _filter_indexer_tolerance(self, target, indexer, tolerance):\n        distance = abs(self.values[indexer] - target)\n        indexer = np.where(distance <= tolerance, indexer, -1)\n        return indexer\n\n    # --------------------------------------------------------------------\n    # Indexer Conversion Methods\n\n    _index_shared_docs[\n        \"_convert_scalar_indexer\"\n    ] = \"\"\"\n        Convert a scalar indexer.\n\n        Parameters\n        ----------\n        key : label of the slice bound\n        kind : {'ix', 'loc', 'getitem', 'iloc'} or None\n    \"\"\"\n\n    @Appender(_index_shared_docs[\"_convert_scalar_indexer\"])\n    def _convert_scalar_indexer(self, key, kind=None):\n        assert kind in [\"ix\", \"loc\", \"getitem\", \"iloc\", None]\n\n        if kind == \"iloc\":\n            return self._validate_indexer(\"positional\", key, kind)\n\n        if len(self) and not isinstance(self, ABCMultiIndex):\n\n            # we can raise here if we are definitive that this\n            # is positional indexing (eg. .ix on with a float)\n            # or label indexing if we are using a type able\n            # to be represented in the index\n\n            if kind in [\"getitem\", \"ix\"] and is_float(key):\n                if not self.is_floating():\n                    return self._invalid_indexer(\"label\", key)\n\n            elif kind in [\"loc\"] and is_float(key):\n\n                # we want to raise KeyError on string/mixed here\n                # technically we *could* raise a TypeError\n                # on anything but mixed though\n                if self.inferred_type not in [\n                    \"floating\",\n                    \"mixed-integer-float\",\n                    \"integer-na\",\n                    \"string\",\n                    \"unicode\",\n                    \"mixed\",\n                ]:\n                    return self._invalid_indexer(\"label\", key)\n\n            elif kind in [\"loc\"] and is_integer(key):\n                if not self.holds_integer():\n                    return self._invalid_indexer(\"label\", key)\n\n        return key\n\n    _index_shared_docs[\n        \"_convert_slice_indexer\"\n    ] = \"\"\"\n        Convert a slice indexer.\n\n        By definition, these are labels unless 'iloc' is passed in.\n        Floats are not allowed as the start, step, or stop of the slice.\n\n        Parameters\n        ----------\n        key : label of the slice bound\n        kind : {'ix', 'loc', 'getitem', 'iloc'} or None\n    \"\"\"\n\n    @Appender(_index_shared_docs[\"_convert_slice_indexer\"])\n    def _convert_slice_indexer(self, key: slice, kind=None):\n        assert kind in [\"ix\", \"loc\", \"getitem\", \"iloc\", None]\n\n        # validate iloc\n        if kind == \"iloc\":\n            return slice(\n                self._validate_indexer(\"slice\", key.start, kind),\n                self._validate_indexer(\"slice\", key.stop, kind),\n                self._validate_indexer(\"slice\", key.step, kind),\n            )\n\n        # potentially cast the bounds to integers\n        start, stop, step = key.start, key.stop, key.step\n\n        # figure out if this is a positional indexer\n        def is_int(v):\n            return v is None or is_integer(v)\n\n        is_null_slicer = start is None and stop is None\n        is_index_slice = is_int(start) and is_int(stop)\n        is_positional = is_index_slice and not self.is_integer()\n\n        if kind == \"getitem\":\n            \"\"\"\n            called from the getitem slicers, validate that we are in fact\n            integers\n            \"\"\"\n            if self.is_integer() or is_index_slice:\n                return slice(\n                    self._validate_indexer(\"slice\", key.start, kind),\n                    self._validate_indexer(\"slice\", key.stop, kind),\n                    self._validate_indexer(\"slice\", key.step, kind),\n                )\n\n        # convert the slice to an indexer here\n\n        # if we are mixed and have integers\n        try:\n            if is_positional and self.is_mixed():\n                # Validate start & stop\n                if start is not None:\n                    self.get_loc(start)\n                if stop is not None:\n                    self.get_loc(stop)\n                is_positional = False\n        except KeyError:\n            if self.inferred_type in [\"mixed-integer-float\", \"integer-na\"]:\n                raise\n\n        if is_null_slicer:\n            indexer = key\n        elif is_positional:\n            indexer = key\n        else:\n            try:\n                indexer = self.slice_indexer(start, stop, step, kind=kind)\n            except Exception:\n                if is_index_slice:\n                    if self.is_integer():\n                        raise\n                    else:\n                        indexer = key\n                else:\n                    raise\n\n        return indexer\n\n    def _convert_listlike_indexer(self, keyarr, kind=None):\n        \"\"\"\n        Parameters\n        ----------\n        keyarr : list-like\n            Indexer to convert.\n\n        Returns\n        -------\n        indexer : numpy.ndarray or None\n            Return an ndarray or None if cannot convert.\n        keyarr : numpy.ndarray\n            Return tuple-safe keys.\n        \"\"\"\n        if isinstance(keyarr, Index):\n            keyarr = self._convert_index_indexer(keyarr)\n        else:\n            keyarr = self._convert_arr_indexer(keyarr)\n\n        indexer = self._convert_list_indexer(keyarr, kind=kind)\n        return indexer, keyarr\n\n    _index_shared_docs[\n        \"_convert_arr_indexer\"\n    ] = \"\"\"\n        Convert an array-like indexer to the appropriate dtype.\n\n        Parameters\n        ----------\n        keyarr : array-like\n            Indexer to convert.\n\n        Returns\n        -------\n        converted_keyarr : array-like\n    \"\"\"\n\n    @Appender(_index_shared_docs[\"_convert_arr_indexer\"])\n    def _convert_arr_indexer(self, keyarr):\n        keyarr = com.asarray_tuplesafe(keyarr)\n        return keyarr\n\n    _index_shared_docs[\n        \"_convert_index_indexer\"\n    ] = \"\"\"\n        Convert an Index indexer to the appropriate dtype.\n\n        Parameters\n        ----------\n        keyarr : Index (or sub-class)\n            Indexer to convert.\n\n        Returns\n        -------\n        converted_keyarr : Index (or sub-class)\n    \"\"\"\n\n    @Appender(_index_shared_docs[\"_convert_index_indexer\"])\n    def _convert_index_indexer(self, keyarr):\n        return keyarr\n\n    _index_shared_docs[\n        \"_convert_list_indexer\"\n    ] = \"\"\"\n        Convert a list-like indexer to the appropriate dtype.\n\n        Parameters\n        ----------\n        keyarr : Index (or sub-class)\n            Indexer to convert.\n        kind : iloc, ix, loc, optional\n\n        Returns\n        -------\n        positional indexer or None\n    \"\"\"\n\n    @Appender(_index_shared_docs[\"_convert_list_indexer\"])\n    def _convert_list_indexer(self, keyarr, kind=None):\n        if (\n            kind in [None, \"iloc\", \"ix\"]\n            and is_integer_dtype(keyarr)\n            and not self.is_floating()\n            and not isinstance(keyarr, ABCPeriodIndex)\n        ):\n\n            if self.inferred_type == \"mixed-integer\":\n                indexer = self.get_indexer(keyarr)\n                if (indexer >= 0).all():\n                    return indexer\n                # missing values are flagged as -1 by get_indexer and negative\n                # indices are already converted to positive indices in the\n                # above if-statement, so the negative flags are changed to\n                # values outside the range of indices so as to trigger an\n                # IndexError in maybe_convert_indices\n                indexer[indexer < 0] = len(self)\n\n                return maybe_convert_indices(indexer, len(self))\n\n            elif not self.inferred_type == \"integer\":\n                keyarr = np.where(keyarr < 0, len(self) + keyarr, keyarr)\n                return keyarr\n\n        return None\n\n    def _invalid_indexer(self, form, key):\n        \"\"\"\n        Consistent invalid indexer message.\n        \"\"\"\n        raise TypeError(\n            \"cannot do {form} indexing on {klass} with these \"\n            \"indexers [{key}] of {kind}\".format(\n                form=form, klass=type(self), key=key, kind=type(key)\n            )\n        )\n\n    # --------------------------------------------------------------------\n    # Reindex Methods\n\n    def _can_reindex(self, indexer):\n        \"\"\"\n        Check if we are allowing reindexing with this particular indexer.\n\n        Parameters\n        ----------\n        indexer : an integer indexer\n\n        Raises\n        ------\n        ValueError if its a duplicate axis\n        \"\"\"\n\n        # trying to reindex on an axis with duplicates\n        if not self.is_unique and len(indexer):\n            raise ValueError(\"cannot reindex from a duplicate axis\")\n\n    def reindex(self, target, method=None, level=None, limit=None, tolerance=None):\n        \"\"\"\n        Create index with target's values (move/add/delete values\n        as necessary).\n\n        Parameters\n        ----------\n        target : an iterable\n\n        Returns\n        -------\n        new_index : pd.Index\n            Resulting index.\n        indexer : np.ndarray or None\n            Indices of output values in original index.\n        \"\"\"\n        # GH6552: preserve names when reindexing to non-named target\n        # (i.e. neither Index nor Series).\n        preserve_names = not hasattr(target, \"name\")\n\n        # GH7774: preserve dtype/tz if target is empty and not an Index.\n        target = _ensure_has_len(target)  # target may be an iterator\n\n        if not isinstance(target, Index) and len(target) == 0:\n            attrs = self._get_attributes_dict()\n            attrs.pop(\"freq\", None)  # don't preserve freq\n            values = self._data[:0]  # appropriately-dtyped empty array\n            target = self._simple_new(values, dtype=self.dtype, **attrs)\n        else:\n            target = ensure_index(target)\n\n        if level is not None:\n            if method is not None:\n                raise TypeError(\"Fill method not supported if level passed\")\n            _, indexer, _ = self._join_level(\n                target, level, how=\"right\", return_indexers=True\n            )\n        else:\n            if self.equals(target):\n                indexer = None\n            else:\n                # check is_overlapping for IntervalIndex compat\n                if self.is_unique and not getattr(self, \"is_overlapping\", False):\n                    indexer = self.get_indexer(\n                        target, method=method, limit=limit, tolerance=tolerance\n                    )\n                else:\n                    if method is not None or limit is not None:\n                        raise ValueError(\n                            \"cannot reindex a non-unique index \"\n                            \"with a method or limit\"\n                        )\n                    indexer, missing = self.get_indexer_non_unique(target)\n\n        if preserve_names and target.nlevels == 1 and target.name != self.name:\n            target = target.copy()\n            target.name = self.name\n\n        return target, indexer\n\n    def _reindex_non_unique(self, target):\n        \"\"\"\n        Create a new index with target's values (move/add/delete values as\n        necessary) use with non-unique Index and a possibly non-unique target.\n\n        Parameters\n        ----------\n        target : an iterable\n\n        Returns\n        -------\n        new_index : pd.Index\n            Resulting index.\n        indexer : np.ndarray or None\n            Indices of output values in original index.\n\n        \"\"\"\n\n        target = ensure_index(target)\n        indexer, missing = self.get_indexer_non_unique(target)\n        check = indexer != -1\n        new_labels = self.take(indexer[check])\n        new_indexer = None\n\n        if len(missing):\n            length = np.arange(len(indexer))\n\n            missing = ensure_platform_int(missing)\n            missing_labels = target.take(missing)\n            missing_indexer = ensure_int64(length[~check])\n            cur_labels = self.take(indexer[check]).values\n            cur_indexer = ensure_int64(length[check])\n\n            new_labels = np.empty(tuple([len(indexer)]), dtype=object)\n            new_labels[cur_indexer] = cur_labels\n            new_labels[missing_indexer] = missing_labels\n\n            # a unique indexer\n            if target.is_unique:\n\n                # see GH5553, make sure we use the right indexer\n                new_indexer = np.arange(len(indexer))\n                new_indexer[cur_indexer] = np.arange(len(cur_labels))\n                new_indexer[missing_indexer] = -1\n\n            # we have a non_unique selector, need to use the original\n            # indexer here\n            else:\n\n                # need to retake to have the same size as the indexer\n                indexer[~check] = -1\n\n                # reset the new indexer to account for the new size\n                new_indexer = np.arange(len(self.take(indexer)))\n                new_indexer[~check] = -1\n\n        new_index = self._shallow_copy_with_infer(new_labels, freq=None)\n        return new_index, indexer, new_indexer\n\n    # --------------------------------------------------------------------\n    # Join Methods\n\n    _index_shared_docs[\n        \"join\"\n    ] = \"\"\"\n        Compute join_index and indexers to conform data\n        structures to the new index.\n\n        Parameters\n        ----------\n        other : Index\n        how : {'left', 'right', 'inner', 'outer'}\n        level : int or level name, default None\n        return_indexers : boolean, default False\n        sort : boolean, default False\n            Sort the join keys lexicographically in the result Index. If False,\n            the order of the join keys depends on the join type (how keyword)\n\n            .. versionadded:: 0.20.0\n\n        Returns\n        -------\n        join_index, (left_indexer, right_indexer)\n        \"\"\"\n\n    @Appender(_index_shared_docs[\"join\"])\n    def join(self, other, how=\"left\", level=None, return_indexers=False, sort=False):\n        self_is_mi = isinstance(self, ABCMultiIndex)\n        other_is_mi = isinstance(other, ABCMultiIndex)\n\n        # try to figure out the join level\n        # GH3662\n        if level is None and (self_is_mi or other_is_mi):\n\n            # have the same levels/names so a simple join\n            if self.names == other.names:\n                pass\n            else:\n                return self._join_multi(other, how=how, return_indexers=return_indexers)\n\n        # join on the level\n        if level is not None and (self_is_mi or other_is_mi):\n            return self._join_level(\n                other, level, how=how, return_indexers=return_indexers\n            )\n\n        other = ensure_index(other)\n\n        if len(other) == 0 and how in (\"left\", \"outer\"):\n            join_index = self._shallow_copy()\n            if return_indexers:\n                rindexer = np.repeat(-1, len(join_index))\n                return join_index, None, rindexer\n            else:\n                return join_index\n\n        if len(self) == 0 and how in (\"right\", \"outer\"):\n            join_index = other._shallow_copy()\n            if return_indexers:\n                lindexer = np.repeat(-1, len(join_index))\n                return join_index, lindexer, None\n            else:\n                return join_index\n\n        if self._join_precedence < other._join_precedence:\n            how = {\"right\": \"left\", \"left\": \"right\"}.get(how, how)\n            result = other.join(\n                self, how=how, level=level, return_indexers=return_indexers\n            )\n            if return_indexers:\n                x, y, z = result\n                result = x, z, y\n            return result\n\n        if not is_dtype_equal(self.dtype, other.dtype):\n            this = self.astype(\"O\")\n            other = other.astype(\"O\")\n            return this.join(other, how=how, return_indexers=return_indexers)\n\n        _validate_join_method(how)\n\n        if not self.is_unique and not other.is_unique:\n            return self._join_non_unique(\n                other, how=how, return_indexers=return_indexers\n            )\n        elif not self.is_unique or not other.is_unique:\n            if self.is_monotonic and other.is_monotonic:\n                return self._join_monotonic(\n                    other, how=how, return_indexers=return_indexers\n                )\n            else:\n                return self._join_non_unique(\n                    other, how=how, return_indexers=return_indexers\n                )\n        elif self.is_monotonic and other.is_monotonic:\n            try:\n                return self._join_monotonic(\n                    other, how=how, return_indexers=return_indexers\n                )\n            except TypeError:\n                pass\n\n        if how == \"left\":\n            join_index = self\n        elif how == \"right\":\n            join_index = other\n        elif how == \"inner\":\n            # TODO: sort=False here for backwards compat. It may\n            # be better to use the sort parameter passed into join\n            join_index = self.intersection(other, sort=False)\n        elif how == \"outer\":\n            # TODO: sort=True here for backwards compat. It may\n            # be better to use the sort parameter passed into join\n            join_index = self.union(other)\n\n        if sort:\n            join_index = join_index.sort_values()\n\n        if return_indexers:\n            if join_index is self:\n                lindexer = None\n            else:\n                lindexer = self.get_indexer(join_index)\n            if join_index is other:\n                rindexer = None\n            else:\n                rindexer = other.get_indexer(join_index)\n            return join_index, lindexer, rindexer\n        else:\n            return join_index\n\n    def _join_multi(self, other, how, return_indexers=True):\n        from .multi import MultiIndex\n        from pandas.core.reshape.merge import _restore_dropped_levels_multijoin\n\n        # figure out join names\n        self_names = set(com.not_none(*self.names))\n        other_names = set(com.not_none(*other.names))\n        overlap = self_names & other_names\n\n        # need at least 1 in common\n        if not overlap:\n            raise ValueError(\"cannot join with no overlapping index names\")\n\n        self_is_mi = isinstance(self, MultiIndex)\n        other_is_mi = isinstance(other, MultiIndex)\n\n        if self_is_mi and other_is_mi:\n\n            # Drop the non-matching levels from left and right respectively\n            ldrop_names = list(self_names - overlap)\n            rdrop_names = list(other_names - overlap)\n\n            self_jnlevels = self.droplevel(ldrop_names)\n            other_jnlevels = other.droplevel(rdrop_names)\n\n            # Join left and right\n            # Join on same leveled multi-index frames is supported\n            join_idx, lidx, ridx = self_jnlevels.join(\n                other_jnlevels, how, return_indexers=True\n            )\n\n            # Restore the dropped levels\n            # Returned index level order is\n            # common levels, ldrop_names, rdrop_names\n            dropped_names = ldrop_names + rdrop_names\n\n            levels, codes, names = _restore_dropped_levels_multijoin(\n                self, other, dropped_names, join_idx, lidx, ridx\n            )\n\n            # Re-create the multi-index\n            multi_join_idx = MultiIndex(\n                levels=levels, codes=codes, names=names, verify_integrity=False\n            )\n\n            multi_join_idx = multi_join_idx.remove_unused_levels()\n\n            return multi_join_idx, lidx, ridx\n\n        jl = list(overlap)[0]\n\n        # Case where only one index is multi\n        # make the indices into mi's that match\n        flip_order = False\n        if self_is_mi:\n            self, other = other, self\n            flip_order = True\n            # flip if join method is right or left\n            how = {\"right\": \"left\", \"left\": \"right\"}.get(how, how)\n\n        level = other.names.index(jl)\n        result = self._join_level(\n            other, level, how=how, return_indexers=return_indexers\n        )\n\n        if flip_order:\n            if isinstance(result, tuple):\n                return result[0], result[2], result[1]\n        return result\n\n    def _join_non_unique(self, other, how=\"left\", return_indexers=False):\n        from pandas.core.reshape.merge import _get_join_indexers\n\n        left_idx, right_idx = _get_join_indexers(\n            [self._ndarray_values], [other._ndarray_values], how=how, sort=True\n        )\n\n        left_idx = ensure_platform_int(left_idx)\n        right_idx = ensure_platform_int(right_idx)\n\n        join_index = np.asarray(self._ndarray_values.take(left_idx))\n        mask = left_idx == -1\n        np.putmask(join_index, mask, other._ndarray_values.take(right_idx))\n\n        join_index = self._wrap_joined_index(join_index, other)\n\n        if return_indexers:\n            return join_index, left_idx, right_idx\n        else:\n            return join_index\n\n    def _join_level(\n        self, other, level, how=\"left\", return_indexers=False, keep_order=True\n    ):\n        \"\"\"\n        The join method *only* affects the level of the resulting\n        MultiIndex. Otherwise it just exactly aligns the Index data to the\n        labels of the level in the MultiIndex.\n\n        If ```keep_order == True```, the order of the data indexed by the\n        MultiIndex will not be changed; otherwise, it will tie out\n        with `other`.\n        \"\"\"\n        from .multi import MultiIndex\n\n        def _get_leaf_sorter(labels):\n            \"\"\"\n            Returns sorter for the inner most level while preserving the\n            order of higher levels.\n            \"\"\"\n            if labels[0].size == 0:\n                return np.empty(0, dtype=\"int64\")\n\n            if len(labels) == 1:\n                lab = ensure_int64(labels[0])\n                sorter, _ = libalgos.groupsort_indexer(lab, 1 + lab.max())\n                return sorter\n\n            # find indexers of beginning of each set of\n            # same-key labels w.r.t all but last level\n            tic = labels[0][:-1] != labels[0][1:]\n            for lab in labels[1:-1]:\n                tic |= lab[:-1] != lab[1:]\n\n            starts = np.hstack(([True], tic, [True])).nonzero()[0]\n            lab = ensure_int64(labels[-1])\n            return lib.get_level_sorter(lab, ensure_int64(starts))\n\n        if isinstance(self, MultiIndex) and isinstance(other, MultiIndex):\n            raise TypeError(\"Join on level between two MultiIndex objects is ambiguous\")\n\n        left, right = self, other\n\n        flip_order = not isinstance(self, MultiIndex)\n        if flip_order:\n            left, right = right, left\n            how = {\"right\": \"left\", \"left\": \"right\"}.get(how, how)\n\n        level = left._get_level_number(level)\n        old_level = left.levels[level]\n\n        if not right.is_unique:\n            raise NotImplementedError(\n                \"Index._join_level on non-unique index is not implemented\"\n            )\n\n        new_level, left_lev_indexer, right_lev_indexer = old_level.join(\n            right, how=how, return_indexers=True\n        )\n\n        if left_lev_indexer is None:\n            if keep_order or len(left) == 0:\n                left_indexer = None\n                join_index = left\n            else:  # sort the leaves\n                left_indexer = _get_leaf_sorter(left.codes[: level + 1])\n                join_index = left[left_indexer]\n\n        else:\n            left_lev_indexer = ensure_int64(left_lev_indexer)\n            rev_indexer = lib.get_reverse_indexer(left_lev_indexer, len(old_level))\n\n            new_lev_codes = algos.take_nd(\n                rev_indexer, left.codes[level], allow_fill=False\n            )\n\n            new_codes = list(left.codes)\n            new_codes[level] = new_lev_codes\n\n            new_levels = list(left.levels)\n            new_levels[level] = new_level\n\n            if keep_order:  # just drop missing values. o.w. keep order\n                left_indexer = np.arange(len(left), dtype=np.intp)\n                mask = new_lev_codes != -1\n                if not mask.all():\n                    new_codes = [lab[mask] for lab in new_codes]\n                    left_indexer = left_indexer[mask]\n\n            else:  # tie out the order with other\n                if level == 0:  # outer most level, take the fast route\n                    ngroups = 1 + new_lev_codes.max()\n                    left_indexer, counts = libalgos.groupsort_indexer(\n                        new_lev_codes, ngroups\n                    )\n\n                    # missing values are placed first; drop them!\n                    left_indexer = left_indexer[counts[0] :]\n                    new_codes = [lab[left_indexer] for lab in new_codes]\n\n                else:  # sort the leaves\n                    mask = new_lev_codes != -1\n                    mask_all = mask.all()\n                    if not mask_all:\n                        new_codes = [lab[mask] for lab in new_codes]\n\n                    left_indexer = _get_leaf_sorter(new_codes[: level + 1])\n                    new_codes = [lab[left_indexer] for lab in new_codes]\n\n                    # left_indexers are w.r.t masked frame.\n                    # reverse to original frame!\n                    if not mask_all:\n                        left_indexer = mask.nonzero()[0][left_indexer]\n\n            join_index = MultiIndex(\n                levels=new_levels,\n                codes=new_codes,\n                names=left.names,\n                verify_integrity=False,\n            )\n\n        if right_lev_indexer is not None:\n            right_indexer = algos.take_nd(\n                right_lev_indexer, join_index.codes[level], allow_fill=False\n            )\n        else:\n            right_indexer = join_index.codes[level]\n\n        if flip_order:\n            left_indexer, right_indexer = right_indexer, left_indexer\n\n        if return_indexers:\n            left_indexer = (\n                None if left_indexer is None else ensure_platform_int(left_indexer)\n            )\n            right_indexer = (\n                None if right_indexer is None else ensure_platform_int(right_indexer)\n            )\n            return join_index, left_indexer, right_indexer\n        else:\n            return join_index\n\n    def _join_monotonic(self, other, how=\"left\", return_indexers=False):\n        if self.equals(other):\n            ret_index = other if how == \"right\" else self\n            if return_indexers:\n                return ret_index, None, None\n            else:\n                return ret_index\n\n        sv = self._ndarray_values\n        ov = other._ndarray_values\n\n        if self.is_unique and other.is_unique:\n            # We can perform much better than the general case\n            if how == \"left\":\n                join_index = self\n                lidx = None\n                ridx = self._left_indexer_unique(sv, ov)\n            elif how == \"right\":\n                join_index = other\n                lidx = self._left_indexer_unique(ov, sv)\n                ridx = None\n            elif how == \"inner\":\n                join_index, lidx, ridx = self._inner_indexer(sv, ov)\n                join_index = self._wrap_joined_index(join_index, other)\n            elif how == \"outer\":\n                join_index, lidx, ridx = self._outer_indexer(sv, ov)\n                join_index = self._wrap_joined_index(join_index, other)\n        else:\n            if how == \"left\":\n                join_index, lidx, ridx = self._left_indexer(sv, ov)\n            elif how == \"right\":\n                join_index, ridx, lidx = self._left_indexer(ov, sv)\n            elif how == \"inner\":\n                join_index, lidx, ridx = self._inner_indexer(sv, ov)\n            elif how == \"outer\":\n                join_index, lidx, ridx = self._outer_indexer(sv, ov)\n            join_index = self._wrap_joined_index(join_index, other)\n\n        if return_indexers:\n            lidx = None if lidx is None else ensure_platform_int(lidx)\n            ridx = None if ridx is None else ensure_platform_int(ridx)\n            return join_index, lidx, ridx\n        else:\n            return join_index\n\n    def _wrap_joined_index(self, joined, other):\n        name = get_op_result_name(self, other)\n        return Index(joined, name=name)\n\n    # --------------------------------------------------------------------\n    # Uncategorized Methods\n\n    @property\n    def values(self):\n        \"\"\"\n        Return an array representing the data in the Index.\n\n        .. warning::\n\n           We recommend using :attr:`Index.array` or\n           :meth:`Index.to_numpy`, depending on whether you need\n           a reference to the underlying data or a NumPy array.\n\n        Returns\n        -------\n        array: numpy.ndarray or ExtensionArray\n\n        See Also\n        --------\n        Index.array : Reference to the underlying data.\n        Index.to_numpy : A NumPy array representing the underlying data.\n        \"\"\"\n        return self._data.view(np.ndarray)\n\n    @property\n    def _values(self) -> Union[ExtensionArray, ABCIndexClass, np.ndarray]:\n        # TODO(EA): remove index types as they become extension arrays\n        \"\"\"\n        The best array representation.\n\n        This is an ndarray, ExtensionArray, or Index subclass. This differs\n        from ``_ndarray_values``, which always returns an ndarray.\n\n        Both ``_values`` and ``_ndarray_values`` are consistent between\n        ``Series`` and ``Index``.\n\n        It may differ from the public '.values' method.\n\n        index             | values          | _values       | _ndarray_values |\n        ----------------- | --------------- | ------------- | --------------- |\n        Index             | ndarray         | ndarray       | ndarray         |\n        CategoricalIndex  | Categorical     | Categorical   | ndarray[int]    |\n        DatetimeIndex     | ndarray[M8ns]   | ndarray[M8ns] | ndarray[M8ns]   |\n        DatetimeIndex[tz] | ndarray[M8ns]   | DTI[tz]       | ndarray[M8ns]   |\n        PeriodIndex       | ndarray[object] | PeriodArray   | ndarray[int]    |\n        IntervalIndex     | IntervalArray   | IntervalArray | ndarray[object] |\n\n        See Also\n        --------\n        values\n        _ndarray_values\n        \"\"\"\n        return self._data\n\n    def get_values(self):\n        \"\"\"\n        Return `Index` data as an `numpy.ndarray`.\n\n        .. deprecated:: 0.25.0\n            Use :meth:`Index.to_numpy` or :attr:`Index.array` instead.\n\n        Returns\n        -------\n        numpy.ndarray\n            A one-dimensional numpy array of the `Index` values.\n\n        See Also\n        --------\n        Index.values : The attribute that get_values wraps.\n\n        Examples\n        --------\n        Getting the `Index` values of a `DataFrame`:\n\n        >>> df = pd.DataFrame([[1, 2, 3], [4, 5, 6], [7, 8, 9]],\n        ...                    index=['a', 'b', 'c'], columns=['A', 'B', 'C'])\n        >>> df\n           A  B  C\n        a  1  2  3\n        b  4  5  6\n        c  7  8  9\n        >>> df.index.get_values()\n        array(['a', 'b', 'c'], dtype=object)\n\n        Standalone `Index` values:\n\n        >>> idx = pd.Index(['1', '2', '3'])\n        >>> idx.get_values()\n        array(['1', '2', '3'], dtype=object)\n\n        `MultiIndex` arrays also have only one dimension:\n\n        >>> midx = pd.MultiIndex.from_arrays([[1, 2, 3], ['a', 'b', 'c']],\n        ...                                  names=('number', 'letter'))\n        >>> midx.get_values()\n        array([(1, 'a'), (2, 'b'), (3, 'c')], dtype=object)\n        >>> midx.get_values().ndim\n        1\n        \"\"\"\n        warnings.warn(\n            \"The 'get_values' method is deprecated and will be removed in a \"\n            \"future version. Use '.to_numpy()' or '.array' instead.\",\n            FutureWarning,\n            stacklevel=2,\n        )\n        return self._internal_get_values()\n\n    def _internal_get_values(self):\n        return self.values\n\n    @Appender(IndexOpsMixin.memory_usage.__doc__)\n    def memory_usage(self, deep=False):\n        result = super().memory_usage(deep=deep)\n\n        # include our engine hashtable\n        result += self._engine.sizeof(deep=deep)\n        return result\n\n    _index_shared_docs[\n        \"where\"\n    ] = \"\"\"\n        Return an Index of same shape as self and whose corresponding\n        entries are from self where cond is True and otherwise are from\n        other.\n\n        Parameters\n        ----------\n        cond : boolean array-like with the same length as self\n        other : scalar, or array-like\n\n        Returns\n        -------\n        Index\n        \"\"\"\n\n    @Appender(_index_shared_docs[\"where\"])\n    def where(self, cond, other=None):\n        if other is None:\n            other = self._na_value\n\n        dtype = self.dtype\n        values = self.values\n\n        if is_bool(other) or is_bool_dtype(other):\n\n            # bools force casting\n            values = values.astype(object)\n            dtype = None\n\n        values = np.where(cond, values, other)\n\n        if self._is_numeric_dtype and np.any(isna(values)):\n            # We can't coerce to the numeric dtype of \"self\" (unless\n            # it's float) if there are NaN values in our output.\n            dtype = None\n\n        return self._shallow_copy_with_infer(values, dtype=dtype)\n\n    # construction helpers\n    @classmethod\n    def _try_convert_to_int_index(cls, data, copy, name, dtype):\n        \"\"\"\n        Attempt to convert an array of data into an integer index.\n\n        Parameters\n        ----------\n        data : The data to convert.\n        copy : Whether to copy the data or not.\n        name : The name of the index returned.\n\n        Returns\n        -------\n        int_index : data converted to either an Int64Index or a\n                    UInt64Index\n\n        Raises\n        ------\n        ValueError if the conversion was not successful.\n        \"\"\"\n\n        from .numeric import Int64Index, UInt64Index\n\n        if not is_unsigned_integer_dtype(dtype):\n            # skip int64 conversion attempt if uint-like dtype is passed, as\n            # this could return Int64Index when UInt64Index is what's desired\n            try:\n                res = data.astype(\"i8\", copy=False)\n                if (res == data).all():\n                    return Int64Index(res, copy=copy, name=name)\n            except (OverflowError, TypeError, ValueError):\n                pass\n\n        # Conversion to int64 failed (possibly due to overflow) or was skipped,\n        # so let's try now with uint64.\n        try:\n            res = data.astype(\"u8\", copy=False)\n            if (res == data).all():\n                return UInt64Index(res, copy=copy, name=name)\n        except (OverflowError, TypeError, ValueError):\n            pass\n\n        raise ValueError\n\n    @classmethod\n    def _scalar_data_error(cls, data):\n        # We return the TypeError so that we can raise it from the constructor\n        #  in order to keep mypy happy\n        return TypeError(\n            \"{0}(...) must be called with a collection of some \"\n            \"kind, {1} was passed\".format(cls.__name__, repr(data))\n        )\n\n    @classmethod\n    def _string_data_error(cls, data):\n        raise TypeError(\n            \"String dtype not supported, you may need \"\n            \"to explicitly cast to a numeric type\"\n        )\n\n    @classmethod\n    def _coerce_to_ndarray(cls, data):\n        \"\"\"\n        Coerces data to ndarray.\n\n        Converts other iterables to list first and then to array.\n        Does not touch ndarrays.\n\n        Raises\n        ------\n        TypeError\n            When the data passed in is a scalar.\n        \"\"\"\n\n        if not isinstance(data, (np.ndarray, Index)):\n            if data is None or is_scalar(data):\n                raise cls._scalar_data_error(data)\n\n            # other iterable of some kind\n            if not isinstance(data, (ABCSeries, list, tuple)):\n                data = list(data)\n            data = np.asarray(data)\n        return data\n\n    def _coerce_scalar_to_index(self, item):\n        \"\"\"\n        We need to coerce a scalar to a compat for our index type.\n\n        Parameters\n        ----------\n        item : scalar item to coerce\n        \"\"\"\n        dtype = self.dtype\n\n        if self._is_numeric_dtype and isna(item):\n            # We can't coerce to the numeric dtype of \"self\" (unless\n            # it's float) if there are NaN values in our output.\n            dtype = None\n\n        return Index([item], dtype=dtype, **self._get_attributes_dict())\n\n    def _to_safe_for_reshape(self):\n        \"\"\"\n        Convert to object if we are a categorical.\n        \"\"\"\n        return self\n\n    def _convert_for_op(self, value):\n        \"\"\"\n        Convert value to be insertable to ndarray.\n        \"\"\"\n        return value\n\n    def _assert_can_do_op(self, value):\n        \"\"\"\n        Check value is valid for scalar op.\n        \"\"\"\n        if not is_scalar(value):\n            msg = \"'value' must be a scalar, passed: {0}\"\n            raise TypeError(msg.format(type(value).__name__))\n\n    @property\n    def _has_complex_internals(self):\n        # to disable groupby tricks in MultiIndex\n        return False\n\n    def _is_memory_usage_qualified(self):\n        \"\"\"\n        Return a boolean if we need a qualified .info display.\n        \"\"\"\n        return self.is_object()\n\n    def is_type_compatible(self, kind):\n        \"\"\"\n        Whether the index type is compatible with the provided type.\n        \"\"\"\n        return kind == self.inferred_type\n\n    _index_shared_docs[\n        \"contains\"\n    ] = \"\"\"\n        Return a boolean indicating whether the provided key is in the index.\n\n        Parameters\n        ----------\n        key : label\n            The key to check if it is present in the index.\n\n        Returns\n        -------\n        bool\n            Whether the key search is in the index.\n\n        See Also\n        --------\n        Index.isin : Returns an ndarray of boolean dtype indicating whether the\n            list-like key is in the index.\n\n        Examples\n        --------\n        >>> idx = pd.Index([1, 2, 3, 4])\n        >>> idx\n        Int64Index([1, 2, 3, 4], dtype='int64')\n\n        >>> 2 in idx\n        True\n        >>> 6 in idx\n        False\n        \"\"\"\n\n    @Appender(_index_shared_docs[\"contains\"] % _index_doc_kwargs)\n    def __contains__(self, key):\n        hash(key)\n        try:\n            return key in self._engine\n        except (OverflowError, TypeError, ValueError):\n            return False\n\n    def contains(self, key):\n        \"\"\"\n        Return a boolean indicating whether the provided key is in the index.\n\n        .. deprecated:: 0.25.0\n            Use ``key in index`` instead of ``index.contains(key)``.\n\n        Returns\n        -------\n        bool\n        \"\"\"\n        warnings.warn(\n            \"The 'contains' method is deprecated and will be removed in a \"\n            \"future version. Use 'key in index' instead of \"\n            \"'index.contains(key)'\",\n            FutureWarning,\n            stacklevel=2,\n        )\n        return key in self\n\n    def __hash__(self):\n        raise TypeError(\"unhashable type: %r\" % type(self).__name__)\n\n    def __setitem__(self, key, value):\n        raise TypeError(\"Index does not support mutable operations\")\n\n    def __getitem__(self, key):\n        \"\"\"\n        Override numpy.ndarray's __getitem__ method to work as desired.\n\n        This function adds lists and Series as valid boolean indexers\n        (ndarrays only supports ndarray with dtype=bool).\n\n        If resulting ndim != 1, plain ndarray is returned instead of\n        corresponding `Index` subclass.\n\n        \"\"\"\n        # There's no custom logic to be implemented in __getslice__, so it's\n        # not overloaded intentionally.\n        getitem = self._data.__getitem__\n        promote = self._shallow_copy\n\n        if is_scalar(key):\n            key = com.cast_scalar_indexer(key)\n            return getitem(key)\n\n        if isinstance(key, slice):\n            # This case is separated from the conditional above to avoid\n            # pessimization of basic indexing.\n            return promote(getitem(key))\n\n        if com.is_bool_indexer(key):\n            key = np.asarray(key, dtype=bool)\n\n        key = com.values_from_object(key)\n        result = getitem(key)\n        if not is_scalar(result):\n            return promote(result)\n        else:\n            return result\n\n    def _can_hold_identifiers_and_holds_name(self, name):\n        \"\"\"\n        Faster check for ``name in self`` when we know `name` is a Python\n        identifier (e.g. in NDFrame.__getattr__, which hits this to support\n        . key lookup). For indexes that can't hold identifiers (everything\n        but object & categorical) we just return False.\n\n        https://github.com/pandas-dev/pandas/issues/19764\n        \"\"\"\n        if self.is_object() or self.is_categorical():\n            return name in self\n        return False\n\n    def append(self, other):\n        \"\"\"\n        Append a collection of Index options together.\n\n        Parameters\n        ----------\n        other : Index or list/tuple of indices\n\n        Returns\n        -------\n        appended : Index\n        \"\"\"\n\n        to_concat = [self]\n\n        if isinstance(other, (list, tuple)):\n            to_concat = to_concat + list(other)\n        else:\n            to_concat.append(other)\n\n        for obj in to_concat:\n            if not isinstance(obj, Index):\n                raise TypeError(\"all inputs must be Index\")\n\n        names = {obj.name for obj in to_concat}\n        name = None if len(names) > 1 else self.name\n\n        return self._concat(to_concat, name)\n\n    def _concat(self, to_concat, name):\n\n        typs = _concat.get_dtype_kinds(to_concat)\n\n        if len(typs) == 1:\n            return self._concat_same_dtype(to_concat, name=name)\n        return Index._concat_same_dtype(self, to_concat, name=name)\n\n    def _concat_same_dtype(self, to_concat, name):\n        \"\"\"\n        Concatenate to_concat which has the same class.\n        \"\"\"\n        # must be overridden in specific classes\n        klasses = (ABCDatetimeIndex, ABCTimedeltaIndex, ABCPeriodIndex, ExtensionArray)\n        to_concat = [\n            x.astype(object) if isinstance(x, klasses) else x for x in to_concat\n        ]\n\n        self = to_concat[0]\n        attribs = self._get_attributes_dict()\n        attribs[\"name\"] = name\n\n        to_concat = [x._values if isinstance(x, Index) else x for x in to_concat]\n\n        return self._shallow_copy_with_infer(np.concatenate(to_concat), **attribs)\n\n    def putmask(self, mask, value):\n        \"\"\"\n        Return a new Index of the values set with the mask.\n\n        Returns\n        -------\n        Index\n\n        See Also\n        --------\n        numpy.ndarray.putmask\n        \"\"\"\n        values = self.values.copy()\n        try:\n            np.putmask(values, mask, self._convert_for_op(value))\n            return self._shallow_copy(values)\n        except (ValueError, TypeError) as err:\n            if is_object_dtype(self):\n                raise err\n\n            # coerces to object\n            return self.astype(object).putmask(mask, value)\n\n    def equals(self, other):\n        \"\"\"\n        Determine if two Index objects contain the same elements.\n\n        Returns\n        -------\n        bool\n            True if \"other\" is an Index and it has the same elements as calling\n            index; False otherwise.\n        \"\"\"\n        if self.is_(other):\n            return True\n\n        if not isinstance(other, Index):\n            return False\n\n        if is_object_dtype(self) and not is_object_dtype(other):\n            # if other is not object, use other's logic for coercion\n            return other.equals(self)\n\n        try:\n            return array_equivalent(\n                com.values_from_object(self), com.values_from_object(other)\n            )\n        except Exception:\n            return False\n\n    def identical(self, other):\n        \"\"\"\n        Similar to equals, but check that other comparable attributes are\n        also equal.\n\n        Returns\n        -------\n        bool\n            If two Index objects have equal elements and same type True,\n            otherwise False.\n        \"\"\"\n        return (\n            self.equals(other)\n            and all(\n                (\n                    getattr(self, c, None) == getattr(other, c, None)\n                    for c in self._comparables\n                )\n            )\n            and type(self) == type(other)\n        )\n\n    def asof(self, label):\n        \"\"\"\n        Return the label from the index, or, if not present, the previous one.\n\n        Assuming that the index is sorted, return the passed index label if it\n        is in the index, or return the previous index label if the passed one\n        is not in the index.\n\n        Parameters\n        ----------\n        label : object\n            The label up to which the method returns the latest index label.\n\n        Returns\n        -------\n        object\n            The passed label if it is in the index. The previous label if the\n            passed label is not in the sorted index or `NaN` if there is no\n            such label.\n\n        See Also\n        --------\n        Series.asof : Return the latest value in a Series up to the\n            passed index.\n        merge_asof : Perform an asof merge (similar to left join but it\n            matches on nearest key rather than equal key).\n        Index.get_loc : An `asof` is a thin wrapper around `get_loc`\n            with method='pad'.\n\n        Examples\n        --------\n        `Index.asof` returns the latest index label up to the passed label.\n\n        >>> idx = pd.Index(['2013-12-31', '2014-01-02', '2014-01-03'])\n        >>> idx.asof('2014-01-01')\n        '2013-12-31'\n\n        If the label is in the index, the method returns the passed label.\n\n        >>> idx.asof('2014-01-02')\n        '2014-01-02'\n\n        If all of the labels in the index are later than the passed label,\n        NaN is returned.\n\n        >>> idx.asof('1999-01-02')\n        nan\n\n        If the index is not sorted, an error is raised.\n\n        >>> idx_not_sorted = pd.Index(['2013-12-31', '2015-01-02',\n        ...                            '2014-01-03'])\n        >>> idx_not_sorted.asof('2013-12-31')\n        Traceback (most recent call last):\n        ValueError: index must be monotonic increasing or decreasing\n        \"\"\"\n        try:\n            loc = self.get_loc(label, method=\"pad\")\n        except KeyError:\n            return self._na_value\n        else:\n            if isinstance(loc, slice):\n                loc = loc.indices(len(self))[-1]\n            return self[loc]\n\n    def asof_locs(self, where, mask):\n        \"\"\"\n        Find the locations (indices) of the labels from the index for\n        every entry in the `where` argument.\n\n        As in the `asof` function, if the label (a particular entry in\n        `where`) is not in the index, the latest index label upto the\n        passed label is chosen and its index returned.\n\n        If all of the labels in the index are later than a label in `where`,\n        -1 is returned.\n\n        `mask` is used to ignore NA values in the index during calculation.\n\n        Parameters\n        ----------\n        where : Index\n            An Index consisting of an array of timestamps.\n        mask : array-like\n            Array of booleans denoting where values in the original\n            data are not NA.\n\n        Returns\n        -------\n        numpy.ndarray\n            An array of locations (indices) of the labels from the Index\n            which correspond to the return values of the `asof` function\n            for every element in `where`.\n        \"\"\"\n        locs = self.values[mask].searchsorted(where.values, side=\"right\")\n        locs = np.where(locs > 0, locs - 1, 0)\n\n        result = np.arange(len(self))[mask].take(locs)\n\n        first = mask.argmax()\n        result[(locs == 0) & (where.values < self.values[first])] = -1\n\n        return result\n\n    def sort_values(self, return_indexer=False, ascending=True):\n        \"\"\"\n        Return a sorted copy of the index.\n\n        Return a sorted copy of the index, and optionally return the indices\n        that sorted the index itself.\n\n        Parameters\n        ----------\n        return_indexer : bool, default False\n            Should the indices that would sort the index be returned.\n        ascending : bool, default True\n            Should the index values be sorted in an ascending order.\n\n        Returns\n        -------\n        sorted_index : pandas.Index\n            Sorted copy of the index.\n        indexer : numpy.ndarray, optional\n            The indices that the index itself was sorted by.\n\n        See Also\n        --------\n        Series.sort_values : Sort values of a Series.\n        DataFrame.sort_values : Sort values in a DataFrame.\n\n        Examples\n        --------\n        >>> idx = pd.Index([10, 100, 1, 1000])\n        >>> idx\n        Int64Index([10, 100, 1, 1000], dtype='int64')\n\n        Sort values in ascending order (default behavior).\n\n        >>> idx.sort_values()\n        Int64Index([1, 10, 100, 1000], dtype='int64')\n\n        Sort values in descending order, and also get the indices `idx` was\n        sorted by.\n\n        >>> idx.sort_values(ascending=False, return_indexer=True)\n        (Int64Index([1000, 100, 10, 1], dtype='int64'), array([3, 1, 0, 2]))\n        \"\"\"\n        _as = self.argsort()\n        if not ascending:\n            _as = _as[::-1]\n\n        sorted_index = self.take(_as)\n\n        if return_indexer:\n            return sorted_index, _as\n        else:\n            return sorted_index\n\n    def sort(self, *args, **kwargs):\n        \"\"\"\n        Use sort_values instead.\n        \"\"\"\n        raise TypeError(\"cannot sort an Index object in-place, use sort_values instead\")\n\n    def shift(self, periods=1, freq=None):\n        \"\"\"\n        Shift index by desired number of time frequency increments.\n\n        This method is for shifting the values of datetime-like indexes\n        by a specified time increment a given number of times.\n\n        Parameters\n        ----------\n        periods : int, default 1\n            Number of periods (or increments) to shift by,\n            can be positive or negative.\n        freq : pandas.DateOffset, pandas.Timedelta or string, optional\n            Frequency increment to shift by.\n            If None, the index is shifted by its own `freq` attribute.\n            Offset aliases are valid strings, e.g., 'D', 'W', 'M' etc.\n\n        Returns\n        -------\n        pandas.Index\n            Shifted index.\n\n        See Also\n        --------\n        Series.shift : Shift values of Series.\n\n        Notes\n        -----\n        This method is only implemented for datetime-like index classes,\n        i.e., DatetimeIndex, PeriodIndex and TimedeltaIndex.\n\n        Examples\n        --------\n        Put the first 5 month starts of 2011 into an index.\n\n        >>> month_starts = pd.date_range('1/1/2011', periods=5, freq='MS')\n        >>> month_starts\n        DatetimeIndex(['2011-01-01', '2011-02-01', '2011-03-01', '2011-04-01',\n                       '2011-05-01'],\n                      dtype='datetime64[ns]', freq='MS')\n\n        Shift the index by 10 days.\n\n        >>> month_starts.shift(10, freq='D')\n        DatetimeIndex(['2011-01-11', '2011-02-11', '2011-03-11', '2011-04-11',\n                       '2011-05-11'],\n                      dtype='datetime64[ns]', freq=None)\n\n        The default value of `freq` is the `freq` attribute of the index,\n        which is 'MS' (month start) in this example.\n\n        >>> month_starts.shift(10)\n        DatetimeIndex(['2011-11-01', '2011-12-01', '2012-01-01', '2012-02-01',\n                       '2012-03-01'],\n                      dtype='datetime64[ns]', freq='MS')\n        \"\"\"\n        raise NotImplementedError(\"Not supported for type %s\" % type(self).__name__)\n\n    def argsort(self, *args, **kwargs):\n        \"\"\"\n        Return the integer indices that would sort the index.\n\n        Parameters\n        ----------\n        *args\n            Passed to `numpy.ndarray.argsort`.\n        **kwargs\n            Passed to `numpy.ndarray.argsort`.\n\n        Returns\n        -------\n        numpy.ndarray\n            Integer indices that would sort the index if used as\n            an indexer.\n\n        See Also\n        --------\n        numpy.argsort : Similar method for NumPy arrays.\n        Index.sort_values : Return sorted copy of Index.\n\n        Examples\n        --------\n        >>> idx = pd.Index(['b', 'a', 'd', 'c'])\n        >>> idx\n        Index(['b', 'a', 'd', 'c'], dtype='object')\n\n        >>> order = idx.argsort()\n        >>> order\n        array([1, 0, 3, 2])\n\n        >>> idx[order]\n        Index(['a', 'b', 'c', 'd'], dtype='object')\n        \"\"\"\n        result = self.asi8\n        if result is None:\n            result = np.array(self)\n        return result.argsort(*args, **kwargs)\n\n    _index_shared_docs[\n        \"get_value\"\n    ] = \"\"\"\n        Fast lookup of value from 1-dimensional ndarray. Only use this if you\n        know what you're doing.\n\n        Returns\n        -------\n        scalar\n            A value in the Series with the index of the key value in self.\n        \"\"\"\n\n    @Appender(_index_shared_docs[\"get_value\"] % _index_doc_kwargs)\n    def get_value(self, series, key):\n\n        # if we have something that is Index-like, then\n        # use this, e.g. DatetimeIndex\n        # Things like `Series._get_value` (via .at) pass the EA directly here.\n        s = getattr(series, \"_values\", series)\n        if isinstance(s, (ExtensionArray, Index)) and is_scalar(key):\n            # GH 20882, 21257\n            # Unify Index and ExtensionArray treatment\n            # First try to convert the key to a location\n            # If that fails, raise a KeyError if an integer\n            # index, otherwise, see if key is an integer, and\n            # try that\n            try:\n                iloc = self.get_loc(key)\n                return s[iloc]\n            except KeyError:\n                if len(self) > 0 and (self.holds_integer() or self.is_boolean()):\n                    raise\n                elif is_integer(key):\n                    return s[key]\n\n        s = com.values_from_object(series)\n        k = com.values_from_object(key)\n\n        k = self._convert_scalar_indexer(k, kind=\"getitem\")\n        try:\n            return self._engine.get_value(s, k, tz=getattr(series.dtype, \"tz\", None))\n        except KeyError as e1:\n            if len(self) > 0 and (self.holds_integer() or self.is_boolean()):\n                raise\n\n            try:\n                return libindex.get_value_at(s, key)\n            except IndexError:\n                raise\n            except TypeError:\n                # generator/iterator-like\n                if is_iterator(key):\n                    raise InvalidIndexError(key)\n                else:\n                    raise e1\n            except Exception:  # pragma: no cover\n                raise e1\n        except TypeError:\n            # python 3\n            if is_scalar(key):  # pragma: no cover\n                raise IndexError(key)\n            raise InvalidIndexError(key)\n\n    def set_value(self, arr, key, value):\n        \"\"\"\n        Fast lookup of value from 1-dimensional ndarray.\n\n        Notes\n        -----\n        Only use this if you know what you're doing.\n        \"\"\"\n        self._engine.set_value(\n            com.values_from_object(arr), com.values_from_object(key), value\n        )\n\n    _index_shared_docs[\n        \"get_indexer_non_unique\"\n    ] = \"\"\"\n        Compute indexer and mask for new index given the current index. The\n        indexer should be then used as an input to ndarray.take to align the\n        current data to the new index.\n\n        Parameters\n        ----------\n        target : %(target_klass)s\n\n        Returns\n        -------\n        indexer : ndarray of int\n            Integers from 0 to n - 1 indicating that the index at these\n            positions matches the corresponding target values. Missing values\n            in the target are marked by -1.\n        missing : ndarray of int\n            An indexer into the target of the values not found.\n            These correspond to the -1 in the indexer array.\n        \"\"\"\n\n    @Appender(_index_shared_docs[\"get_indexer_non_unique\"] % _index_doc_kwargs)\n    def get_indexer_non_unique(self, target):\n        target = ensure_index(target)\n        pself, ptarget = self._maybe_promote(target)\n        if pself is not self or ptarget is not target:\n            return pself.get_indexer_non_unique(ptarget)\n\n        if is_categorical(target):\n            tgt_values = np.asarray(target)\n        elif self.is_all_dates:\n            tgt_values = target.asi8\n        else:\n            tgt_values = target._ndarray_values\n\n        indexer, missing = self._engine.get_indexer_non_unique(tgt_values)\n        return ensure_platform_int(indexer), missing\n\n    def get_indexer_for(self, target, **kwargs):\n        \"\"\"\n        Guaranteed return of an indexer even when non-unique.\n\n        This dispatches to get_indexer or get_indexer_non_unique\n        as appropriate.\n\n        Returns\n        -------\n        numpy.ndarray\n            List of indices.\n        \"\"\"\n        if self.is_unique:\n            return self.get_indexer(target, **kwargs)\n        indexer, _ = self.get_indexer_non_unique(target, **kwargs)\n        return indexer\n\n    def _maybe_promote(self, other):\n        # A hack, but it works\n        from pandas import DatetimeIndex\n\n        if self.inferred_type == \"date\" and isinstance(other, DatetimeIndex):\n            return DatetimeIndex(self), other\n        elif self.inferred_type == \"boolean\":\n            if not is_object_dtype(self.dtype):\n                return self.astype(\"object\"), other.astype(\"object\")\n        return self, other\n\n    def groupby(self, values):\n        \"\"\"\n        Group the index labels by a given array of values.\n\n        Parameters\n        ----------\n        values : array\n            Values used to determine the groups.\n\n        Returns\n        -------\n        groups : dict\n            {group name -> group labels}\n        \"\"\"\n\n        # TODO: if we are a MultiIndex, we can do better\n        # that converting to tuples\n        if isinstance(values, ABCMultiIndex):\n            values = values.values\n        values = ensure_categorical(values)\n        result = values._reverse_indexer()\n\n        # map to the label\n        result = {k: self.take(v) for k, v in result.items()}\n\n        return result\n\n    def map(self, mapper, na_action=None):\n        \"\"\"\n        Map values using input correspondence (a dict, Series, or function).\n\n        Parameters\n        ----------\n        mapper : function, dict, or Series\n            Mapping correspondence.\n        na_action : {None, 'ignore'}\n            If 'ignore', propagate NA values, without passing them to the\n            mapping correspondence.\n\n        Returns\n        -------\n        applied : Union[Index, MultiIndex], inferred\n            The output of the mapping function applied to the index.\n            If the function returns a tuple with more than one element\n            a MultiIndex will be returned.\n        \"\"\"\n\n        from .multi import MultiIndex\n\n        new_values = super()._map_values(mapper, na_action=na_action)\n\n        attributes = self._get_attributes_dict()\n\n        # we can return a MultiIndex\n        if new_values.size and isinstance(new_values[0], tuple):\n            if isinstance(self, MultiIndex):\n                names = self.names\n            elif attributes.get(\"name\"):\n                names = [attributes.get(\"name\")] * len(new_values[0])\n            else:\n                names = None\n            return MultiIndex.from_tuples(new_values, names=names)\n\n        attributes[\"copy\"] = False\n        if not new_values.size:\n            # empty\n            attributes[\"dtype\"] = self.dtype\n\n        return Index(new_values, **attributes)\n\n    def isin(self, values, level=None):\n        \"\"\"\n        Return a boolean array where the index values are in `values`.\n\n        Compute boolean array of whether each index value is found in the\n        passed set of values. The length of the returned boolean array matches\n        the length of the index.\n\n        Parameters\n        ----------\n        values : set or list-like\n            Sought values.\n        level : str or int, optional\n            Name or position of the index level to use (if the index is a\n            `MultiIndex`).\n\n        Returns\n        -------\n        is_contained : ndarray\n            NumPy array of boolean values.\n\n        See Also\n        --------\n        Series.isin : Same for Series.\n        DataFrame.isin : Same method for DataFrames.\n\n        Notes\n        -----\n        In the case of `MultiIndex` you must either specify `values` as a\n        list-like object containing tuples that are the same length as the\n        number of levels, or specify `level`. Otherwise it will raise a\n        ``ValueError``.\n\n        If `level` is specified:\n\n        - if it is the name of one *and only one* index level, use that level;\n        - otherwise it should be a number indicating level position.\n\n        Examples\n        --------\n        >>> idx = pd.Index([1,2,3])\n        >>> idx\n        Int64Index([1, 2, 3], dtype='int64')\n\n        Check whether each index value in a list of values.\n        >>> idx.isin([1, 4])\n        array([ True, False, False])\n\n        >>> midx = pd.MultiIndex.from_arrays([[1,2,3],\n        ...                                  ['red', 'blue', 'green']],\n        ...                                  names=('number', 'color'))\n        >>> midx\n        MultiIndex(levels=[[1, 2, 3], ['blue', 'green', 'red']],\n                   codes=[[0, 1, 2], [2, 0, 1]],\n                   names=['number', 'color'])\n\n        Check whether the strings in the 'color' level of the MultiIndex\n        are in a list of colors.\n\n        >>> midx.isin(['red', 'orange', 'yellow'], level='color')\n        array([ True, False, False])\n\n        To check across the levels of a MultiIndex, pass a list of tuples:\n\n        >>> midx.isin([(1, 'red'), (3, 'red')])\n        array([ True, False, False])\n\n        For a DatetimeIndex, string values in `values` are converted to\n        Timestamps.\n\n        >>> dates = ['2000-03-11', '2000-03-12', '2000-03-13']\n        >>> dti = pd.to_datetime(dates)\n        >>> dti\n        DatetimeIndex(['2000-03-11', '2000-03-12', '2000-03-13'],\n        dtype='datetime64[ns]', freq=None)\n\n        >>> dti.isin(['2000-03-11'])\n        array([ True, False, False])\n        \"\"\"\n        if level is not None:\n            self._validate_index_level(level)\n        return algos.isin(self, values)\n\n    def _get_string_slice(self, key, use_lhs=True, use_rhs=True):\n        # this is for partial string indexing,\n        # overridden in DatetimeIndex, TimedeltaIndex and PeriodIndex\n        raise NotImplementedError\n\n    def slice_indexer(self, start=None, end=None, step=None, kind=None):\n        \"\"\"\n        For an ordered or unique index, compute the slice indexer for input\n        labels and step.\n\n        Parameters\n        ----------\n        start : label, default None\n            If None, defaults to the beginning\n        end : label, default None\n            If None, defaults to the end\n        step : int, default None\n        kind : string, default None\n\n        Returns\n        -------\n        indexer : slice\n\n        Raises\n        ------\n        KeyError : If key does not exist, or key is not unique and index is\n            not ordered.\n\n        Notes\n        -----\n        This function assumes that the data is sorted, so use at your own peril\n\n        Examples\n        --------\n        This is a method on all index types. For example you can do:\n\n        >>> idx = pd.Index(list('abcd'))\n        >>> idx.slice_indexer(start='b', end='c')\n        slice(1, 3)\n\n        >>> idx = pd.MultiIndex.from_arrays([list('abcd'), list('efgh')])\n        >>> idx.slice_indexer(start='b', end=('c', 'g'))\n        slice(1, 3)\n        \"\"\"\n        start_slice, end_slice = self.slice_locs(start, end, step=step, kind=kind)\n\n        # return a slice\n        if not is_scalar(start_slice):\n            raise AssertionError(\"Start slice bound is non-scalar\")\n        if not is_scalar(end_slice):\n            raise AssertionError(\"End slice bound is non-scalar\")\n\n        return slice(start_slice, end_slice, step)\n\n    def _maybe_cast_indexer(self, key):\n        \"\"\"\n        If we have a float key and are not a floating index, then try to cast\n        to an int if equivalent.\n        \"\"\"\n\n        if is_float(key) and not self.is_floating():\n            try:\n                ckey = int(key)\n                if ckey == key:\n                    key = ckey\n            except (OverflowError, ValueError, TypeError):\n                pass\n        return key\n\n    def _validate_indexer(self, form, key, kind):\n        \"\"\"\n        If we are positional indexer, validate that we have appropriate\n        typed bounds must be an integer.\n        \"\"\"\n        assert kind in [\"ix\", \"loc\", \"getitem\", \"iloc\"]\n\n        if key is None:\n            pass\n        elif is_integer(key):\n            pass\n        elif kind in [\"iloc\", \"getitem\"]:\n            self._invalid_indexer(form, key)\n        return key\n\n    _index_shared_docs[\n        \"_maybe_cast_slice_bound\"\n    ] = \"\"\"\n        This function should be overloaded in subclasses that allow non-trivial\n        casting on label-slice bounds, e.g. datetime-like indices allowing\n        strings containing formatted datetimes.\n\n        Parameters\n        ----------\n        label : object\n        side : {'left', 'right'}\n        kind : {'ix', 'loc', 'getitem'}\n\n        Returns\n        -------\n        label :  object\n\n        Notes\n        -----\n        Value of `side` parameter should be validated in caller.\n\n        \"\"\"\n\n    @Appender(_index_shared_docs[\"_maybe_cast_slice_bound\"])\n    def _maybe_cast_slice_bound(self, label, side, kind):\n        assert kind in [\"ix\", \"loc\", \"getitem\", None]\n\n        # We are a plain index here (sub-class override this method if they\n        # wish to have special treatment for floats/ints, e.g. Float64Index and\n        # datetimelike Indexes\n        # reject them\n        if is_float(label):\n            if not (kind in [\"ix\"] and (self.holds_integer() or self.is_floating())):\n                self._invalid_indexer(\"slice\", label)\n\n        # we are trying to find integer bounds on a non-integer based index\n        # this is rejected (generally .loc gets you here)\n        elif is_integer(label):\n            self._invalid_indexer(\"slice\", label)\n\n        return label\n\n    def _searchsorted_monotonic(self, label, side=\"left\"):\n        if self.is_monotonic_increasing:\n            return self.searchsorted(label, side=side)\n        elif self.is_monotonic_decreasing:\n            # np.searchsorted expects ascending sort order, have to reverse\n            # everything for it to work (element ordering, search side and\n            # resulting value).\n            pos = self[::-1].searchsorted(\n                label, side=\"right\" if side == \"left\" else \"left\"\n            )\n            return len(self) - pos\n\n        raise ValueError(\"index must be monotonic increasing or decreasing\")\n\n    def get_slice_bound(self, label, side, kind):\n        \"\"\"\n        Calculate slice bound that corresponds to given label.\n\n        Returns leftmost (one-past-the-rightmost if ``side=='right'``) position\n        of given label.\n\n        Parameters\n        ----------\n        label : object\n        side : {'left', 'right'}\n        kind : {'ix', 'loc', 'getitem'}\n\n        Returns\n        -------\n        int\n            Index of label.\n        \"\"\"\n        assert kind in [\"ix\", \"loc\", \"getitem\", None]\n\n        if side not in (\"left\", \"right\"):\n            raise ValueError(\n                \"Invalid value for side kwarg,\"\n                \" must be either 'left' or 'right': %s\" % (side,)\n            )\n\n        original_label = label\n\n        # For datetime indices label may be a string that has to be converted\n        # to datetime boundary according to its resolution.\n        label = self._maybe_cast_slice_bound(label, side, kind)\n\n        # we need to look up the label\n        try:\n            slc = self.get_loc(label)\n        except KeyError as err:\n            try:\n                return self._searchsorted_monotonic(label, side)\n            except ValueError:\n                # raise the original KeyError\n                raise err\n\n        if isinstance(slc, np.ndarray):\n            # get_loc may return a boolean array or an array of indices, which\n            # is OK as long as they are representable by a slice.\n            if is_bool_dtype(slc):\n                slc = lib.maybe_booleans_to_slice(slc.view(\"u1\"))\n            else:\n                slc = lib.maybe_indices_to_slice(slc.astype(\"i8\"), len(self))\n            if isinstance(slc, np.ndarray):\n                raise KeyError(\n                    \"Cannot get %s slice bound for non-unique \"\n                    \"label: %r\" % (side, original_label)\n                )\n\n        if isinstance(slc, slice):\n            if side == \"left\":\n                return slc.start\n            else:\n                return slc.stop\n        else:\n            if side == \"right\":\n                return slc + 1\n            else:\n                return slc\n\n    def slice_locs(self, start=None, end=None, step=None, kind=None):\n        \"\"\"\n        Compute slice locations for input labels.\n\n        Parameters\n        ----------\n        start : label, default None\n            If None, defaults to the beginning\n        end : label, default None\n            If None, defaults to the end\n        step : int, defaults None\n            If None, defaults to 1\n        kind : {'ix', 'loc', 'getitem'} or None\n\n        Returns\n        -------\n        start, end : int\n\n        See Also\n        --------\n        Index.get_loc : Get location for a single label.\n\n        Notes\n        -----\n        This method only works if the index is monotonic or unique.\n\n        Examples\n        --------\n        >>> idx = pd.Index(list('abcd'))\n        >>> idx.slice_locs(start='b', end='c')\n        (1, 3)\n        \"\"\"\n        inc = step is None or step >= 0\n\n        if not inc:\n            # If it's a reverse slice, temporarily swap bounds.\n            start, end = end, start\n\n        # GH 16785: If start and end happen to be date strings with UTC offsets\n        # attempt to parse and check that the offsets are the same\n        if isinstance(start, (str, datetime)) and isinstance(end, (str, datetime)):\n            try:\n                ts_start = Timestamp(start)\n                ts_end = Timestamp(end)\n            except (ValueError, TypeError):\n                pass\n            else:\n                if not tz_compare(ts_start.tzinfo, ts_end.tzinfo):\n                    raise ValueError(\"Both dates must have the same UTC offset\")\n\n        start_slice = None\n        if start is not None:\n            start_slice = self.get_slice_bound(start, \"left\", kind)\n        if start_slice is None:\n            start_slice = 0\n\n        end_slice = None\n        if end is not None:\n            end_slice = self.get_slice_bound(end, \"right\", kind)\n        if end_slice is None:\n            end_slice = len(self)\n\n        if not inc:\n            # Bounds at this moment are swapped, swap them back and shift by 1.\n            #\n            # slice_locs('B', 'A', step=-1): s='B', e='A'\n            #\n            #              s='A'                 e='B'\n            # AFTER SWAP:    |                     |\n            #                v ------------------> V\n            #           -----------------------------------\n            #           | | |A|A|A|A| | | | | |B|B| | | | |\n            #           -----------------------------------\n            #              ^ <------------------ ^\n            # SHOULD BE:   |                     |\n            #           end=s-1              start=e-1\n            #\n            end_slice, start_slice = start_slice - 1, end_slice - 1\n\n            # i == -1 triggers ``len(self) + i`` selection that points to the\n            # last element, not before-the-first one, subtracting len(self)\n            # compensates that.\n            if end_slice == -1:\n                end_slice -= len(self)\n            if start_slice == -1:\n                start_slice -= len(self)\n\n        return start_slice, end_slice\n\n    def delete(self, loc):\n        \"\"\"\n        Make new Index with passed location(-s) deleted.\n\n        Returns\n        -------\n        new_index : Index\n        \"\"\"\n        return self._shallow_copy(np.delete(self._data, loc))\n\n    def insert(self, loc, item):\n        \"\"\"\n        Make new Index inserting new item at location.\n\n        Follows Python list.append semantics for negative values.\n\n        Parameters\n        ----------\n        loc : int\n        item : object\n\n        Returns\n        -------\n        new_index : Index\n        \"\"\"\n        _self = np.asarray(self)\n        item = self._coerce_scalar_to_index(item)._ndarray_values\n        idx = np.concatenate((_self[:loc], item, _self[loc:]))\n        return self._shallow_copy_with_infer(idx)\n\n    def drop(self, labels, errors=\"raise\"):\n        \"\"\"\n        Make new Index with passed list of labels deleted.\n\n        Parameters\n        ----------\n        labels : array-like\n        errors : {'ignore', 'raise'}, default 'raise'\n            If 'ignore', suppress error and existing labels are dropped.\n\n        Returns\n        -------\n        dropped : Index\n\n        Raises\n        ------\n        KeyError\n            If not all of the labels are found in the selected axis\n        \"\"\"\n        arr_dtype = \"object\" if self.dtype == \"object\" else None\n        labels = com.index_labels_to_array(labels, dtype=arr_dtype)\n        indexer = self.get_indexer(labels)\n        mask = indexer == -1\n        if mask.any():\n            if errors != \"ignore\":\n                raise KeyError(\"{} not found in axis\".format(labels[mask]))\n            indexer = indexer[~mask]\n        return self.delete(indexer)\n\n    # --------------------------------------------------------------------\n    # Generated Arithmetic, Comparison, and Unary Methods\n\n    @classmethod\n    def _add_comparison_methods(cls):\n        \"\"\"\n        Add in comparison methods.\n        \"\"\"\n        cls.__eq__ = _make_comparison_op(operator.eq, cls)\n        cls.__ne__ = _make_comparison_op(operator.ne, cls)\n        cls.__lt__ = _make_comparison_op(operator.lt, cls)\n        cls.__gt__ = _make_comparison_op(operator.gt, cls)\n        cls.__le__ = _make_comparison_op(operator.le, cls)\n        cls.__ge__ = _make_comparison_op(operator.ge, cls)\n\n    @classmethod\n    def _add_numeric_methods_add_sub_disabled(cls):\n        \"\"\"\n        Add in the numeric add/sub methods to disable.\n        \"\"\"\n        cls.__add__ = make_invalid_op(\"__add__\")\n        cls.__radd__ = make_invalid_op(\"__radd__\")\n        cls.__iadd__ = make_invalid_op(\"__iadd__\")\n        cls.__sub__ = make_invalid_op(\"__sub__\")\n        cls.__rsub__ = make_invalid_op(\"__rsub__\")\n        cls.__isub__ = make_invalid_op(\"__isub__\")\n\n    @classmethod\n    def _add_numeric_methods_disabled(cls):\n        \"\"\"\n        Add in numeric methods to disable other than add/sub.\n        \"\"\"\n        cls.__pow__ = make_invalid_op(\"__pow__\")\n        cls.__rpow__ = make_invalid_op(\"__rpow__\")\n        cls.__mul__ = make_invalid_op(\"__mul__\")\n        cls.__rmul__ = make_invalid_op(\"__rmul__\")\n        cls.__floordiv__ = make_invalid_op(\"__floordiv__\")\n        cls.__rfloordiv__ = make_invalid_op(\"__rfloordiv__\")\n        cls.__truediv__ = make_invalid_op(\"__truediv__\")\n        cls.__rtruediv__ = make_invalid_op(\"__rtruediv__\")\n        cls.__mod__ = make_invalid_op(\"__mod__\")\n        cls.__divmod__ = make_invalid_op(\"__divmod__\")\n        cls.__neg__ = make_invalid_op(\"__neg__\")\n        cls.__pos__ = make_invalid_op(\"__pos__\")\n        cls.__abs__ = make_invalid_op(\"__abs__\")\n        cls.__inv__ = make_invalid_op(\"__inv__\")\n\n    @classmethod\n    def _add_numeric_methods_binary(cls):\n        \"\"\"\n        Add in numeric methods.\n        \"\"\"\n        cls.__add__ = _make_arithmetic_op(operator.add, cls)\n        cls.__radd__ = _make_arithmetic_op(ops.radd, cls)\n        cls.__sub__ = _make_arithmetic_op(operator.sub, cls)\n        cls.__rsub__ = _make_arithmetic_op(ops.rsub, cls)\n        cls.__rpow__ = _make_arithmetic_op(ops.rpow, cls)\n        cls.__pow__ = _make_arithmetic_op(operator.pow, cls)\n\n        cls.__truediv__ = _make_arithmetic_op(operator.truediv, cls)\n        cls.__rtruediv__ = _make_arithmetic_op(ops.rtruediv, cls)\n\n        # TODO: rmod? rdivmod?\n        cls.__mod__ = _make_arithmetic_op(operator.mod, cls)\n        cls.__floordiv__ = _make_arithmetic_op(operator.floordiv, cls)\n        cls.__rfloordiv__ = _make_arithmetic_op(ops.rfloordiv, cls)\n        cls.__divmod__ = _make_arithmetic_op(divmod, cls)\n        cls.__mul__ = _make_arithmetic_op(operator.mul, cls)\n        cls.__rmul__ = _make_arithmetic_op(ops.rmul, cls)\n\n    @classmethod\n    def _add_numeric_methods_unary(cls):\n        \"\"\"\n        Add in numeric unary methods.\n        \"\"\"\n\n        def _make_evaluate_unary(op, opstr):\n            def _evaluate_numeric_unary(self):\n\n                attrs = self._get_attributes_dict()\n                return Index(op(self.values), **attrs)\n\n            _evaluate_numeric_unary.__name__ = opstr\n            return _evaluate_numeric_unary\n\n        cls.__neg__ = _make_evaluate_unary(operator.neg, \"__neg__\")\n        cls.__pos__ = _make_evaluate_unary(operator.pos, \"__pos__\")\n        cls.__abs__ = _make_evaluate_unary(np.abs, \"__abs__\")\n        cls.__inv__ = _make_evaluate_unary(lambda x: -x, \"__inv__\")\n\n    @classmethod\n    def _add_numeric_methods(cls):\n        cls._add_numeric_methods_unary()\n        cls._add_numeric_methods_binary()\n\n    @classmethod\n    def _add_logical_methods(cls):\n        \"\"\"\n        Add in logical methods.\n        \"\"\"\n        _doc = \"\"\"\n        %(desc)s\n\n        Parameters\n        ----------\n        *args\n            These parameters will be passed to numpy.%(outname)s.\n        **kwargs\n            These parameters will be passed to numpy.%(outname)s.\n\n        Returns\n        -------\n        %(outname)s : bool or array_like (if axis is specified)\n            A single element array_like may be converted to bool.\"\"\"\n\n        _index_shared_docs[\"index_all\"] = dedent(\n            \"\"\"\n\n        See Also\n        --------\n        Index.any : Return whether any element in an Index is True.\n        Series.any : Return whether any element in a Series is True.\n        Series.all : Return whether all elements in a Series are True.\n\n        Notes\n        -----\n        Not a Number (NaN), positive infinity and negative infinity\n        evaluate to True because these are not equal to zero.\n\n        Examples\n        --------\n        **all**\n\n        True, because nonzero integers are considered True.\n\n        >>> pd.Index([1, 2, 3]).all()\n        True\n\n        False, because ``0`` is considered False.\n\n        >>> pd.Index([0, 1, 2]).all()\n        False\n\n        **any**\n\n        True, because ``1`` is considered True.\n\n        >>> pd.Index([0, 0, 1]).any()\n        True\n\n        False, because ``0`` is considered False.\n\n        >>> pd.Index([0, 0, 0]).any()\n        False\n        \"\"\"\n        )\n\n        _index_shared_docs[\"index_any\"] = dedent(\n            \"\"\"\n\n        See Also\n        --------\n        Index.all : Return whether all elements are True.\n        Series.all : Return whether all elements are True.\n\n        Notes\n        -----\n        Not a Number (NaN), positive infinity and negative infinity\n        evaluate to True because these are not equal to zero.\n\n        Examples\n        --------\n        >>> index = pd.Index([0, 1, 2])\n        >>> index.any()\n        True\n\n        >>> index = pd.Index([0, 0, 0])\n        >>> index.any()\n        False\n        \"\"\"\n        )\n\n        def _make_logical_function(name, desc, f):\n            @Substitution(outname=name, desc=desc)\n            @Appender(_index_shared_docs[\"index_\" + name])\n            @Appender(_doc)\n            def logical_func(self, *args, **kwargs):\n                result = f(self.values)\n                if (\n                    isinstance(result, (np.ndarray, ABCSeries, Index))\n                    and result.ndim == 0\n                ):\n                    # return NumPy type\n                    return result.dtype.type(result.item())\n                else:  # pragma: no cover\n                    return result\n\n            logical_func.__name__ = name\n            return logical_func\n\n        cls.all = _make_logical_function(\n            \"all\", \"Return whether all elements are True.\", np.all\n        )\n        cls.any = _make_logical_function(\n            \"any\", \"Return whether any element is True.\", np.any\n        )\n\n    @classmethod\n    def _add_logical_methods_disabled(cls):\n        \"\"\"\n        Add in logical methods to disable.\n        \"\"\"\n        cls.all = make_invalid_op(\"all\")\n        cls.any = make_invalid_op(\"any\")\n\n    @property\n    def shape(self):\n        \"\"\"\n        Return a tuple of the shape of the underlying data.\n        \"\"\"\n        # not using \"(len(self), )\" to return \"correct\" shape if the values\n        # consists of a >1 D array (see GH-27775)\n        # overridden in MultiIndex.shape to avoid materializing the values\n        return self._values.shape\n\n\nIndex._add_numeric_methods_disabled()\nIndex._add_logical_methods()\nIndex._add_comparison_methods()\n\n\ndef ensure_index_from_sequences(sequences, names=None):\n    \"\"\"\n    Construct an index from sequences of data.\n\n    A single sequence returns an Index. Many sequences returns a\n    MultiIndex.\n\n    Parameters\n    ----------\n    sequences : sequence of sequences\n    names : sequence of str\n\n    Returns\n    -------\n    index : Index or MultiIndex\n\n    Examples\n    --------\n    >>> ensure_index_from_sequences([[1, 2, 3]], names=['name'])\n    Int64Index([1, 2, 3], dtype='int64', name='name')\n\n    >>> ensure_index_from_sequences([['a', 'a'], ['a', 'b']],\n                                    names=['L1', 'L2'])\n    MultiIndex([('a', 'a'),\n                ('a', 'b')],\n               names=['L1', 'L2'])\n\n    See Also\n    --------\n    ensure_index\n    \"\"\"\n    from .multi import MultiIndex\n\n    if len(sequences) == 1:\n        if names is not None:\n            names = names[0]\n        return Index(sequences[0], name=names)\n    else:\n        return MultiIndex.from_arrays(sequences, names=names)\n\n\ndef ensure_index(index_like, copy=False):\n    \"\"\"\n    Ensure that we have an index from some index-like object.\n\n    Parameters\n    ----------\n    index : sequence\n        An Index or other sequence\n    copy : bool\n\n    Returns\n    -------\n    index : Index or MultiIndex\n\n    Examples\n    --------\n    >>> ensure_index(['a', 'b'])\n    Index(['a', 'b'], dtype='object')\n\n    >>> ensure_index([('a', 'a'),  ('b', 'c')])\n    Index([('a', 'a'), ('b', 'c')], dtype='object')\n\n    >>> ensure_index([['a', 'a'], ['b', 'c']])\n    MultiIndex([('a', 'b'),\n                ('a', 'c')],\n               dtype='object')\n               )\n\n    See Also\n    --------\n    ensure_index_from_sequences\n    \"\"\"\n    if isinstance(index_like, Index):\n        if copy:\n            index_like = index_like.copy()\n        return index_like\n    if hasattr(index_like, \"name\"):\n        return Index(index_like, name=index_like.name, copy=copy)\n\n    if is_iterator(index_like):\n        index_like = list(index_like)\n\n    # must check for exactly list here because of strict type\n    # check in clean_index_list\n    if isinstance(index_like, list):\n        if type(index_like) != list:\n            index_like = list(index_like)\n\n        converted, all_arrays = lib.clean_index_list(index_like)\n\n        if len(converted) > 0 and all_arrays:\n            from .multi import MultiIndex\n\n            return MultiIndex.from_arrays(converted)\n        else:\n            index_like = converted\n    else:\n        # clean_index_list does the equivalent of copying\n        # so only need to do this if not list instance\n        if copy:\n            from copy import copy\n\n            index_like = copy(index_like)\n\n    return Index(index_like)\n\n\ndef _ensure_has_len(seq):\n    \"\"\"\n    If seq is an iterator, put its values into a list.\n    \"\"\"\n    try:\n        len(seq)\n    except TypeError:\n        return list(seq)\n    else:\n        return seq\n\n\ndef _trim_front(strings):\n    \"\"\"\n    Trims zeros and decimal points.\n    \"\"\"\n    trimmed = strings\n    while len(strings) > 0 and all(x[0] == \" \" for x in trimmed):\n        trimmed = [x[1:] for x in trimmed]\n    return trimmed\n\n\ndef _validate_join_method(method):\n    if method not in [\"left\", \"right\", \"inner\", \"outer\"]:\n        raise ValueError(\"do not recognize join method %s\" % method)\n\n\ndef default_index(n):\n    from pandas.core.index import RangeIndex\n\n    return RangeIndex(0, n, name=None)\n",
      "file_patch": "@@ -4715,13 +4715,13 @@ class Index(IndexOpsMixin, PandasObject):\n     @Appender(_index_shared_docs[\"get_indexer_non_unique\"] % _index_doc_kwargs)\n     def get_indexer_non_unique(self, target):\n         target = ensure_index(target)\n-        if is_categorical(target):\n-            target = target.astype(target.dtype.categories.dtype)\n         pself, ptarget = self._maybe_promote(target)\n         if pself is not self or ptarget is not target:\n             return pself.get_indexer_non_unique(ptarget)\n \n-        if self.is_all_dates:\n+        if is_categorical(target):\n+            tgt_values = np.asarray(target)\n+        elif self.is_all_dates:\n             tgt_values = target.asi8\n         else:\n             tgt_values = target._ndarray_values\n@@ -4733,7 +4733,7 @@ class Index(IndexOpsMixin, PandasObject):\n         \"\"\"\n         Guaranteed return of an indexer even when non-unique.\n \n-        This dispatches to get_indexer or get_indexer_nonunique\n+        This dispatches to get_indexer or get_indexer_non_unique\n         as appropriate.\n \n         Returns\n",
      "files_name_in_blame_commit": [
        "base.py",
        "test_categorical.py",
        "categorical.py"
      ]
    }
  },
  "commits_modify_file_before_fix": {
    "size": 428
  },
  "recursive_blame_commits": {
    "recursive_blame_function_lines": {
      "4716": {
        "commit_id": "30c1290610c65d53bcdd1665be3104104ca27eb2",
        "line_code": "    def get_indexer_non_unique(self, target):",
        "commit_date": "2018-11-28 12:45:46",
        "valid": 1
      },
      "4717": {
        "commit_id": "30c1290610c65d53bcdd1665be3104104ca27eb2",
        "line_code": "        target = ensure_index(target)",
        "commit_date": "2018-11-28 12:45:46",
        "valid": 1
      },
      "4718": {
        "commit_id": "30c1290610c65d53bcdd1665be3104104ca27eb2",
        "line_code": "        if is_categorical(target):",
        "commit_date": "2018-11-28 12:45:46",
        "valid": 1
      },
      "4719": {
        "commit_id": "30c1290610c65d53bcdd1665be3104104ca27eb2",
        "line_code": "            target = target.astype(target.dtype.categories.dtype)",
        "commit_date": "2018-11-28 12:45:46",
        "valid": 1
      },
      "4720": {
        "commit_id": "30c1290610c65d53bcdd1665be3104104ca27eb2",
        "line_code": "        pself, ptarget = self._maybe_promote(target)",
        "commit_date": "2018-11-28 12:45:46",
        "valid": 1
      },
      "4721": {
        "commit_id": "30c1290610c65d53bcdd1665be3104104ca27eb2",
        "line_code": "        if pself is not self or ptarget is not target:",
        "commit_date": "2018-11-28 12:45:46",
        "valid": 1
      },
      "4722": {
        "commit_id": "30c1290610c65d53bcdd1665be3104104ca27eb2",
        "line_code": "            return pself.get_indexer_non_unique(ptarget)",
        "commit_date": "2018-11-28 12:45:46",
        "valid": 1
      },
      "4723": {
        "commit_id": "49c0023cd18ca88532b59ce0e8b72fc10b75562b",
        "line_code": "",
        "commit_date": "2016-01-24 17:53:14",
        "valid": 0
      },
      "4724": {
        "commit_id": "30c1290610c65d53bcdd1665be3104104ca27eb2",
        "line_code": "        if self.is_all_dates:",
        "commit_date": "2018-11-28 12:45:46",
        "valid": 1
      },
      "4725": {
        "commit_id": "30c1290610c65d53bcdd1665be3104104ca27eb2",
        "line_code": "            tgt_values = target.asi8",
        "commit_date": "2018-11-28 12:45:46",
        "valid": 1
      },
      "4726": {
        "commit_id": "30c1290610c65d53bcdd1665be3104104ca27eb2",
        "line_code": "        else:",
        "commit_date": "2018-11-28 12:45:46",
        "valid": 1
      },
      "4727": {
        "commit_id": "30c1290610c65d53bcdd1665be3104104ca27eb2",
        "line_code": "            tgt_values = target._ndarray_values",
        "commit_date": "2018-11-28 12:45:46",
        "valid": 1
      },
      "4728": {
        "commit_id": "49c0023cd18ca88532b59ce0e8b72fc10b75562b",
        "line_code": "",
        "commit_date": "2016-01-24 17:53:14",
        "valid": 0
      },
      "4729": {
        "commit_id": "30c1290610c65d53bcdd1665be3104104ca27eb2",
        "line_code": "        indexer, missing = self._engine.get_indexer_non_unique(tgt_values)",
        "commit_date": "2018-11-28 12:45:46",
        "valid": 1
      },
      "4730": {
        "commit_id": "30c1290610c65d53bcdd1665be3104104ca27eb2",
        "line_code": "        return ensure_platform_int(indexer), missing",
        "commit_date": "2018-11-28 12:45:46",
        "valid": 1
      }
    },
    "commits": {
      "30c1290610c65d53bcdd1665be3104104ca27eb2": {
        "commit": {
          "commit_id": "30c1290610c65d53bcdd1665be3104104ca27eb2",
          "commit_message": "Collect Index methods by purpose: rendering, constructors, setops... (#23961)",
          "commit_author": "jbrockmendel",
          "commit_date": "2018-11-28 12:45:46",
          "commit_parent": "2712c8ff8d46b49868ec8704fb915cfbd9cf5c7a"
        },
        "function": {
          "function_name": "get_indexer_non_unique",
          "function_code_before": "@Appender(_index_shared_docs['get_indexer_non_unique'] % _index_doc_kwargs)\ndef get_indexer_non_unique(self, target):\n    target = ensure_index(target)\n    if is_categorical(target):\n        target = target.astype(target.dtype.categories.dtype)\n    (pself, ptarget) = self._maybe_promote(target)\n    if pself is not self or ptarget is not target:\n        return pself.get_indexer_non_unique(ptarget)\n    if self.is_all_dates:\n        self = Index(self.asi8)\n        tgt_values = target.asi8\n    else:\n        tgt_values = target._ndarray_values\n    (indexer, missing) = self._engine.get_indexer_non_unique(tgt_values)\n    return (ensure_platform_int(indexer), missing)",
          "function_code_after": "@Appender(_index_shared_docs['get_indexer_non_unique'] % _index_doc_kwargs)\ndef get_indexer_non_unique(self, target):\n    target = ensure_index(target)\n    if is_categorical(target):\n        target = target.astype(target.dtype.categories.dtype)\n    (pself, ptarget) = self._maybe_promote(target)\n    if pself is not self or ptarget is not target:\n        return pself.get_indexer_non_unique(ptarget)\n    if self.is_all_dates:\n        self = Index(self.asi8)\n        tgt_values = target.asi8\n    else:\n        tgt_values = target._ndarray_values\n    (indexer, missing) = self._engine.get_indexer_non_unique(tgt_values)\n    return (ensure_platform_int(indexer), missing)",
          "function_before_start_line": 3566,
          "function_before_end_line": 3581,
          "function_after_start_line": 4333,
          "function_after_end_line": 4348,
          "function_before_token_count": 105,
          "function_after_token_count": 105,
          "functions_name_modified_file": [
            "_get_fill_indexer_searchsorted",
            "_sort_levels_monotonic",
            "map",
            "to_flat_index",
            "format",
            "__or__",
            "sort_values",
            "is_floating",
            "memory_usage",
            "is_monotonic_decreasing",
            "_add_logical_methods_disabled",
            "_cleanup",
            "is_boolean",
            "_get_names",
            "_add_comparison_methods",
            "get_loc",
            "to_native_types",
            "is_lexsorted_for_tuple",
            "_convert_slice_indexer",
            "_format_space",
            "_has_complex_internals",
            "_reindex_non_unique",
            "_wrap_joined_index",
            "identical",
            "_get_unique_index",
            "_add_numeric_methods_disabled",
            "dtype",
            "is_mixed",
            "__new__",
            "default_index",
            "_can_hold_identifiers_and_holds_name",
            "notna",
            "_engine",
            "_format_native_types",
            "unique",
            "__getitem__",
            "_get_grouper_for_level",
            "__sub__",
            "isin",
            "_is_memory_usage_qualified",
            "_maybe_update_attributes",
            "__setstate__",
            "_string_data_error",
            "_validate_join_method",
            "contains",
            "_outer_indexer",
            "has_duplicates",
            "__rsub__",
            "get_indexer_non_unique",
            "ensure_index",
            "drop_duplicates",
            "is_numeric",
            "_simple_new",
            "_validate_indexer",
            "_isnan",
            "set_names",
            "_values",
            "get_indexer_for",
            "_convert_for_op",
            "_update_inplace",
            "shift",
            "is_",
            "_validate_index_level",
            "_formatter_func",
            "_convert_can_do_setop",
            "_add_numeric_methods_add_sub_disabled",
            "hasnans",
            "_searchsorted_monotonic",
            "_join_multi",
            "nlevels",
            "_trim_front",
            "argsort",
            "_new_Index",
            "__contains__",
            "get_indexer",
            "_get_fill_indexer",
            "drop",
            "asof_locs",
            "_join_level",
            "take",
            "_make_arithmetic_op",
            "_is_strictly_monotonic_increasing",
            "_join_monotonic",
            "__xor__",
            "__reduce__",
            "_try_convert_to_int_index",
            "_add_numeric_methods_unary",
            "_get_level_number",
            "union",
            "_add_numeric_methods_binary",
            "holds_integer",
            "is_all_dates",
            "_invalid_indexer",
            "inferred_type",
            "__len__",
            "__nonzero__",
            "_wrap_setop_result",
            "ravel",
            "slice_indexer",
            "_assert_take_fillable",
            "_evaluate_with_datetime_like",
            "_shallow_copy",
            "_nan_idxs",
            "_get_loc_only_exact_matches",
            "astype",
            "groupby",
            "asof",
            "is_monotonic",
            "_assert_can_do_op",
            "append",
            "_inner_indexer",
            "_deepcopy_if_needed",
            "_summary",
            "_convert_tolerance",
            "insert",
            "_format_with_header",
            "_get_level_values",
            "__hash__",
            "isna",
            "_get_reconciled_name_object",
            "_convert_index_indexer",
            "__radd__",
            "_reset_identity",
            "is_categorical",
            "where",
            "is_integer",
            "_maybe_cast_slice_bound",
            "__array__",
            "dtype_str",
            "__unicode__",
            "__add__",
            "_try_get_item",
            "is_interval",
            "_get_attributes_dict",
            "difference",
            "_maybe_cast_indexer",
            "duplicated",
            "fillna",
            "putmask",
            "_assert_can_do_setop",
            "_concat",
            "__deepcopy__",
            "_can_reindex",
            "sortlevel",
            "__iadd__",
            "copy",
            "is_type_compatible",
            "_validate_for_numeric_unaryop",
            "_mpl_repr",
            "__setitem__",
            "_make_comparison_op",
            "_left_indexer",
            "_format_data",
            "_add_logical_methods",
            "_is_strictly_monotonic_decreasing",
            "get_slice_bound",
            "_join_non_unique",
            "reindex",
            "_coerce_to_ndarray",
            "_format_attrs",
            "_add_numeric_methods",
            "is_monotonic_increasing",
            "set_value",
            "equals",
            "ensure_index_from_sequences",
            "__and__",
            "_get_string_slice",
            "get_duplicates",
            "is_unique",
            "rename",
            "intersection",
            "_convert_scalar_indexer",
            "_convert_list_indexer",
            "_to_safe_for_reshape",
            "_shallow_copy_with_infer",
            "__array_wrap__",
            "_coerce_scalar_to_index",
            "_set_names",
            "repeat",
            "get_value",
            "slice_locs",
            "_constructor",
            "_evaluate_with_timedelta_like",
            "_validate_for_numeric_binop",
            "_ensure_has_len",
            "get_values",
            "summary",
            "droplevel",
            "_get_nearest_indexer",
            "_left_indexer_unique",
            "_filter_indexer_tolerance",
            "_convert_listlike_indexer",
            "_validate_names",
            "is_object",
            "_maybe_promote",
            "delete",
            "values",
            "__copy__",
            "join",
            "to_series",
            "_scalar_data_error",
            "to_frame",
            "_concat_same_dtype",
            "dropna",
            "_convert_arr_indexer",
            "sort",
            "symmetric_difference",
            "view"
          ],
          "functions_name_all_files": [
            "map",
            "format",
            "date_range",
            "freq",
            "_new_IntervalIndex",
            "is_floating",
            "get_loc",
            "_is_homogeneous_type",
            "_maybe_convert_timedelta",
            "identical",
            "ordered",
            "_box_func",
            "is_full",
            "_delegate_property_set",
            "_can_hold_identifiers_and_holds_name",
            "_format_native_types",
            "unique",
            "_apply_meta",
            "mid",
            "to_timestamp",
            "shape",
            "_string_data_error",
            "_extended_gcd",
            "_validate_join_method",
            "from_range",
            "equal_levels",
            "drop_duplicates",
            "timedelta_range",
            "_simple_new",
            "_min_fitting_element",
            "indexer_between_time",
            "__floordiv__",
            "_update_inplace",
            "is_",
            "from_product",
            "_delegate_method",
            "size",
            "_hashed_values",
            "_trim_front",
            "argsort",
            "_new_Index",
            "__contains__",
            "get_indexer",
            "length",
            "drop",
            "_join_level",
            "_make_arithmetic_op",
            "_join_monotonic",
            "_is_dtype_compat",
            "__xor__",
            "_get_level_number",
            "_get_next_label",
            "_invalid_indexer",
            "inferred_type",
            "__nonzero__",
            "_ensure_localized",
            "ravel",
            "_can_fast_union",
            "get_locs",
            "_get_level_values",
            "_get_reconciled_name_object",
            "_maybe_cast_slice_bound",
            "__array__",
            "set_levels",
            "_try_get_item",
            "levshape",
            "duplicated",
            "_time_shift",
            "ceil",
            "__deepcopy__",
            "_validate_for_numeric_unaryop",
            "right",
            "_box_values_as_index",
            "_left_indexer",
            "left",
            "_is_strictly_monotonic_decreasing",
            "_join_non_unique",
            "_check_method",
            "_time_to_micros",
            "wrap_field_accessor",
            "argmin",
            "_nbytes",
            "snap",
            "from_breaks",
            "_find_non_overlapping_monotonic_bounds",
            "repeat",
            "slice_locs",
            "_constructor",
            "droplevel",
            "_get_nearest_indexer",
            "delete",
            "_maybe_promote",
            "tz",
            "to_series",
            "to_frame",
            "symmetric_difference",
            "_set_levels",
            "view",
            "_is_dates_only",
            "sort_values",
            "is_boolean",
            "_get_names",
            "_hashed_indexing_key",
            "_get_unique_index",
            "_add_numeric_methods_disabled",
            "default_index",
            "__getitem__",
            "__sub__",
            "_get_na_rep",
            "get_indexer_non_unique",
            "_verify_integrity",
            "_validate_indexer",
            "_values",
            "_join_multi",
            "_create_categorical",
            "base",
            "_ndarray_values",
            "_get_fill_indexer",
            "remove_unused_levels",
            "_get_data_as_items",
            "_get_time_micros",
            "_setop",
            "_create_comparison_method",
            "from_arrays",
            "_add_numeric_methods_unary",
            "is_all_dates",
            "_is_convertible_to_index",
            "_wrap_setop_result",
            "closed",
            "slice_indexer",
            "_evaluate_with_datetime_like",
            "set_labels",
            "_get_loc_only_exact_matches",
            "_partial_tup_index",
            "wrap_arithmetic_op",
            "is_lexsorted",
            "_format_with_header",
            "_partial_td_slice",
            "__hash__",
            "_max_fitting_element",
            "_validate_dtype",
            "_convert_index_indexer",
            "__radd__",
            "is_categorical",
            "get_loc_level",
            "dtype_str",
            "__add__",
            "_maybe_convert_i8",
            "item",
            "is_interval",
            "_needs_i8_conversion",
            "fillna",
            "_minmax",
            "_concat",
            "is_type_compatible",
            "sortlevel",
            "__iadd__",
            "_maybe_box_as_values",
            "_new_PeriodIndex",
            "__getslice__",
            "__setitem__",
            "_make_comparison_op",
            "_get_level_indexer",
            "_is_type_compatible",
            "from_tuples",
            "_join_i8_wrapper",
            "get_slice_bound",
            "_ensure_datetimelike_to_i8",
            "_format_attrs",
            "_add_numeric_methods",
            "set_value",
            "intersection",
            "_convert_list_indexer",
            "_coerce_scalar_to_index",
            "_validate_for_numeric_binop",
            "_evaluate_with_timedelta_like",
            "summary",
            "_convert_listlike_indexer",
            "values",
            "dropna",
            "_get_fill_indexer_searchsorted",
            "_sort_levels_monotonic",
            "__or__",
            "period_range",
            "swaplevel",
            "memory_usage",
            "is_monotonic_decreasing",
            "_add_logical_methods_disabled",
            "_drop_from_level",
            "_add_comparison_methods",
            "is_non_overlapping_monotonic",
            "is_lexsorted_for_tuple",
            "_convert_slice_indexer",
            "_has_complex_internals",
            "nbytes",
            "tolist",
            "cdate_range",
            "dtype",
            "_evaluate_compare",
            "is_mixed",
            "__new__",
            "searchsorted",
            "_add_datetimelike_methods",
            "_int64index",
            "codes",
            "_get_grouper_for_level",
            "_delegate_property_get",
            "contains",
            "ensure_index",
            "is_numeric",
            "asfreq",
            "truncate",
            "set_names",
            "get_indexer_for",
            "_as_like_interval_index",
            "_convert_for_op",
            "_validate_index_level",
            "get_value_maybe_box",
            "asof_locs",
            "take",
            "_is_strictly_monotonic_increasing",
            "union",
            "__len__",
            "_reverse_indexer",
            "astype",
            "_is_valid_endpoint",
            "append",
            "_summary",
            "_inferred_type_levels",
            "isna",
            "to_hierarchical",
            "_create_from_codes",
            "where",
            "__unicode__",
            "strftime",
            "_maybe_cast_indexer",
            "set_closed",
            "_can_reindex",
            "_mpl_repr",
            "_add_logical_methods",
            "_fast_union",
            "max",
            "lexsort_depth",
            "interval_range",
            "indexer_at_time",
            "argmax",
            "equals",
            "__and__",
            "is_unique",
            "rename",
            "_convert_scalar_indexer",
            "_to_safe_for_reshape",
            "levels",
            "_partial_date_slice",
            "_shallow_copy_with_infer",
            "_set_names",
            "flags",
            "get_value",
            "_ensure_has_len",
            "_filter_indexer_tolerance",
            "is_object",
            "__copy__",
            "_scalar_data_error",
            "_concat_same_dtype",
            "to_flat_index",
            "categories",
            "_get_prev_label",
            "overlaps",
            "_cleanup",
            "from_intervals",
            "is_dtype_equal",
            "to_tuples",
            "to_native_types",
            "_format_space",
            "_reindex_non_unique",
            "_wrap_joined_index",
            "_maybe_cast_indexed",
            "_round",
            "notna",
            "_engine",
            "isin",
            "union_many",
            "_is_memory_usage_qualified",
            "_maybe_update_attributes",
            "__setstate__",
            "_outer_indexer",
            "has_duplicates",
            "__rsub__",
            "_codes_to_ints",
            "_get_labels_for_sorting",
            "_isnan",
            "itemsize",
            "shift",
            "_formatter_func",
            "_convert_can_do_setop",
            "_add_numeric_methods_add_sub_disabled",
            "hasnans",
            "_searchsorted_monotonic",
            "get_level_values",
            "nlevels",
            "_codes_for_groupby",
            "asobject",
            "_get_interval_closed_bounds",
            "_parsed_string_to_bounds",
            "_set_labels",
            "__reduce__",
            "_try_convert_to_int_index",
            "reorder_levels",
            "_get_reindexer",
            "_add_numeric_methods_binary",
            "holds_integer",
            "_assert_take_fillable",
            "_shallow_copy",
            "_nan_idxs",
            "_engine_type",
            "labels",
            "groupby",
            "asof",
            "_maybe_utc_convert",
            "is_monotonic",
            "_assert_can_do_op",
            "_inner_indexer",
            "_deepcopy_if_needed",
            "_convert_tolerance",
            "insert",
            "_have_mixed_levels",
            "asi8",
            "_reset_identity",
            "is_integer",
            "_get_attributes_dict",
            "difference",
            "putmask",
            "wrap_array_method",
            "_assert_can_do_setop",
            "_data",
            "_new_DatetimeIndex",
            "copy",
            "_multiindex",
            "bdate_range",
            "_format_data",
            "reindex",
            "_coerce_to_ndarray",
            "is_monotonic_increasing",
            "ensure_index_from_sequences",
            "_get_string_slice",
            "get_duplicates",
            "round",
            "data",
            "__array_wrap__",
            "get_values",
            "_left_indexer_unique",
            "_validate_names",
            "join",
            "floor",
            "min",
            "_convert_arr_indexer",
            "sort",
            "_sparsify"
          ],
          "functions_name_co_evolved_modified_file": [
            "_get_fill_indexer_searchsorted",
            "_sort_levels_monotonic",
            "map",
            "to_flat_index",
            "format",
            "__or__",
            "sort_values",
            "memory_usage",
            "_cleanup",
            "get_loc",
            "to_native_types",
            "_convert_slice_indexer",
            "_has_complex_internals",
            "_reindex_non_unique",
            "_wrap_joined_index",
            "identical",
            "_get_unique_index",
            "_can_hold_identifiers_and_holds_name",
            "notna",
            "_engine",
            "_format_native_types",
            "unique",
            "__getitem__",
            "_get_grouper_for_level",
            "__sub__",
            "isin",
            "_is_memory_usage_qualified",
            "_string_data_error",
            "__setstate__",
            "contains",
            "__rsub__",
            "drop_duplicates",
            "_isnan",
            "_values",
            "get_indexer_for",
            "_convert_for_op",
            "shift",
            "_validate_index_level",
            "_convert_can_do_setop",
            "hasnans",
            "_join_multi",
            "nlevels",
            "argsort",
            "__contains__",
            "get_indexer",
            "_get_fill_indexer",
            "asof_locs",
            "_join_level",
            "take",
            "_join_monotonic",
            "__xor__",
            "__reduce__",
            "_try_convert_to_int_index",
            "_get_level_number",
            "union",
            "is_all_dates",
            "_invalid_indexer",
            "inferred_type",
            "_wrap_setop_result",
            "ravel",
            "_assert_take_fillable",
            "_nan_idxs",
            "astype",
            "groupby",
            "asof",
            "append",
            "_assert_can_do_op",
            "_summary",
            "_convert_tolerance",
            "_format_with_header",
            "_get_level_values",
            "__hash__",
            "isna",
            "_get_reconciled_name_object",
            "_convert_index_indexer",
            "__radd__",
            "where",
            "__add__",
            "_get_attributes_dict",
            "difference",
            "duplicated",
            "fillna",
            "putmask",
            "_assert_can_do_setop",
            "_concat",
            "_can_reindex",
            "is_type_compatible",
            "sortlevel",
            "__iadd__",
            "_mpl_repr",
            "__setitem__",
            "_join_non_unique",
            "reindex",
            "_coerce_to_ndarray",
            "set_value",
            "equals",
            "__and__",
            "intersection",
            "_convert_scalar_indexer",
            "_convert_list_indexer",
            "_to_safe_for_reshape",
            "_coerce_scalar_to_index",
            "get_value",
            "_constructor",
            "get_values",
            "_convert_listlike_indexer",
            "_get_nearest_indexer",
            "droplevel",
            "_filter_indexer_tolerance",
            "summary",
            "_validate_names",
            "_maybe_promote",
            "values",
            "join",
            "to_series",
            "_concat_same_dtype",
            "_scalar_data_error",
            "to_frame",
            "dropna",
            "_convert_arr_indexer",
            "sort",
            "symmetric_difference",
            "view"
          ],
          "functions_name_co_evolved_all_files": [
            "_get_fill_indexer_searchsorted",
            "_sort_levels_monotonic",
            "map",
            "to_flat_index",
            "format",
            "__or__",
            "sort_values",
            "memory_usage",
            "_cleanup",
            "from_intervals",
            "get_loc",
            "to_native_types",
            "_convert_slice_indexer",
            "_has_complex_internals",
            "_reindex_non_unique",
            "_wrap_joined_index",
            "identical",
            "_get_unique_index",
            "_can_hold_identifiers_and_holds_name",
            "notna",
            "_engine",
            "_format_native_types",
            "unique",
            "__getitem__",
            "_get_grouper_for_level",
            "__sub__",
            "isin",
            "_is_memory_usage_qualified",
            "_string_data_error",
            "__setstate__",
            "contains",
            "__rsub__",
            "drop_duplicates",
            "_isnan",
            "_values",
            "get_indexer_for",
            "_convert_for_op",
            "shift",
            "from_product",
            "_validate_index_level",
            "_formatter_func",
            "_convert_can_do_setop",
            "hasnans",
            "_join_multi",
            "nlevels",
            "argsort",
            "__contains__",
            "get_indexer",
            "_get_fill_indexer",
            "asof_locs",
            "_join_level",
            "take",
            "_join_monotonic",
            "_get_time_micros",
            "__xor__",
            "from_arrays",
            "__reduce__",
            "_try_convert_to_int_index",
            "_get_level_number",
            "union",
            "is_all_dates",
            "_invalid_indexer",
            "inferred_type",
            "_wrap_setop_result",
            "ravel",
            "_assert_take_fillable",
            "_nan_idxs",
            "astype",
            "groupby",
            "asof",
            "_maybe_utc_convert",
            "append",
            "_assert_can_do_op",
            "_summary",
            "_convert_tolerance",
            "_format_with_header",
            "_get_level_values",
            "__hash__",
            "isna",
            "_get_reconciled_name_object",
            "_convert_index_indexer",
            "__radd__",
            "where",
            "__add__",
            "_get_attributes_dict",
            "difference",
            "duplicated",
            "fillna",
            "putmask",
            "_assert_can_do_setop",
            "_concat",
            "_can_reindex",
            "is_type_compatible",
            "sortlevel",
            "__iadd__",
            "_mpl_repr",
            "__setitem__",
            "from_tuples",
            "_join_non_unique",
            "reindex",
            "_coerce_to_ndarray",
            "set_value",
            "equals",
            "__and__",
            "intersection",
            "_convert_scalar_indexer",
            "_convert_list_indexer",
            "_to_safe_for_reshape",
            "snap",
            "from_breaks",
            "_coerce_scalar_to_index",
            "get_value",
            "_constructor",
            "get_values",
            "_convert_listlike_indexer",
            "_get_nearest_indexer",
            "droplevel",
            "_filter_indexer_tolerance",
            "summary",
            "_validate_names",
            "_maybe_promote",
            "values",
            "join",
            "to_series",
            "_concat_same_dtype",
            "_scalar_data_error",
            "to_frame",
            "dropna",
            "_convert_arr_indexer",
            "sort",
            "symmetric_difference",
            "view"
          ]
        },
        "file": {
          "file_name": "base.py",
          "file_nloc": 2663,
          "file_complexity": 787,
          "file_token_count": 16338,
          "file_before": "from datetime import datetime, timedelta\nimport operator\nfrom textwrap import dedent\nimport warnings\n\nimport numpy as np\n\nfrom pandas._libs import (\n    Timedelta, algos as libalgos, index as libindex, join as libjoin, lib,\n    tslibs)\nfrom pandas._libs.lib import is_datetime_array\nimport pandas.compat as compat\nfrom pandas.compat import range, set_function_name, u\nfrom pandas.compat.numpy import function as nv\nfrom pandas.util._decorators import Appender, Substitution, cache_readonly\n\nfrom pandas.core.dtypes.cast import maybe_cast_to_integer_array\nfrom pandas.core.dtypes.common import (\n    ensure_categorical, ensure_int64, ensure_object, ensure_platform_int,\n    is_bool, is_bool_dtype, is_categorical, is_categorical_dtype,\n    is_datetime64_any_dtype, is_datetime64tz_dtype, is_dtype_equal,\n    is_dtype_union_equal, is_extension_array_dtype, is_float, is_float_dtype,\n    is_hashable, is_integer, is_integer_dtype, is_interval_dtype, is_iterator,\n    is_list_like, is_object_dtype, is_period_dtype, is_scalar,\n    is_signed_integer_dtype, is_timedelta64_dtype, is_unsigned_integer_dtype)\nimport pandas.core.dtypes.concat as _concat\nfrom pandas.core.dtypes.generic import (\n    ABCDataFrame, ABCDateOffset, ABCDatetimeIndex, ABCIndexClass,\n    ABCMultiIndex, ABCPeriodIndex, ABCSeries, ABCTimedeltaArray,\n    ABCTimedeltaIndex)\nfrom pandas.core.dtypes.missing import array_equivalent, isna\n\nfrom pandas.core import ops\nfrom pandas.core.accessor import CachedAccessor\nimport pandas.core.algorithms as algos\nfrom pandas.core.arrays import ExtensionArray\nfrom pandas.core.base import IndexOpsMixin, PandasObject\nimport pandas.core.common as com\nfrom pandas.core.indexes.frozen import FrozenList\nimport pandas.core.missing as missing\nfrom pandas.core.ops import get_op_result_name, make_invalid_op\nimport pandas.core.sorting as sorting\nfrom pandas.core.strings import StringMethods\n\nfrom pandas.io.formats.printing import (\n    default_pprint, format_object_attrs, format_object_summary, pprint_thing)\n\n__all__ = ['Index']\n\n_unsortable_types = frozenset(('mixed', 'mixed-integer'))\n\n_index_doc_kwargs = dict(klass='Index', inplace='',\n                         target_klass='Index',\n                         unique='Index', duplicated='np.ndarray')\n_index_shared_docs = dict()\n\n\ndef _try_get_item(x):\n    try:\n        return x.item()\n    except AttributeError:\n        return x\n\n\ndef _make_comparison_op(op, cls):\n    def cmp_method(self, other):\n        if isinstance(other, (np.ndarray, Index, ABCSeries)):\n            if other.ndim > 0 and len(self) != len(other):\n                raise ValueError('Lengths must match to compare')\n\n        from .multi import MultiIndex\n        if is_object_dtype(self) and not isinstance(self, MultiIndex):\n            # don't pass MultiIndex\n            with np.errstate(all='ignore'):\n                result = ops._comp_method_OBJECT_ARRAY(op, self.values, other)\n\n        else:\n\n            # numpy will show a DeprecationWarning on invalid elementwise\n            # comparisons, this will raise in the future\n            with warnings.catch_warnings(record=True):\n                warnings.filterwarnings(\"ignore\", \"elementwise\", FutureWarning)\n                with np.errstate(all='ignore'):\n                    result = op(self.values, np.asarray(other))\n\n        # technically we could support bool dtyped Index\n        # for now just return the indexing array directly\n        if is_bool_dtype(result):\n            return result\n        try:\n            return Index(result)\n        except TypeError:\n            return result\n\n    name = '__{name}__'.format(name=op.__name__)\n    # TODO: docstring?\n    return set_function_name(cmp_method, name, cls)\n\n\ndef _make_arithmetic_op(op, cls):\n    def index_arithmetic_method(self, other):\n        if isinstance(other, (ABCSeries, ABCDataFrame)):\n            return NotImplemented\n        elif isinstance(other, ABCTimedeltaIndex):\n            # Defer to subclass implementation\n            return NotImplemented\n        elif (isinstance(other, (np.ndarray, ABCTimedeltaArray)) and\n              is_timedelta64_dtype(other)):\n            # GH#22390; wrap in Series for op, this will in turn wrap in\n            # TimedeltaIndex, but will correctly raise TypeError instead of\n            # NullFrequencyError for add/sub ops\n            from pandas import Series\n            other = Series(other)\n            out = op(self, other)\n            return Index(out, name=self.name)\n\n        other = self._validate_for_numeric_binop(other, op)\n\n        # handle time-based others\n        if isinstance(other, (ABCDateOffset, np.timedelta64, timedelta)):\n            return self._evaluate_with_timedelta_like(other, op)\n        elif isinstance(other, (datetime, np.datetime64)):\n            return self._evaluate_with_datetime_like(other, op)\n\n        values = self.values\n        with np.errstate(all='ignore'):\n            result = op(values, other)\n\n        result = missing.dispatch_missing(op, values, other, result)\n\n        attrs = self._get_attributes_dict()\n        attrs = self._maybe_update_attributes(attrs)\n        if op is divmod:\n            result = (Index(result[0], **attrs), Index(result[1], **attrs))\n        else:\n            result = Index(result, **attrs)\n        return result\n\n    name = '__{name}__'.format(name=op.__name__)\n    # TODO: docstring?\n    return set_function_name(index_arithmetic_method, name, cls)\n\n\nclass InvalidIndexError(Exception):\n    pass\n\n\n_o_dtype = np.dtype(object)\n_Identity = object\n\n\ndef _new_Index(cls, d):\n    \"\"\"\n    This is called upon unpickling, rather than the default which doesn't\n    have arguments and breaks __new__.\n    \"\"\"\n    # required for backward compat, because PI can't be instantiated with\n    # ordinals through __new__ GH #13277\n    if issubclass(cls, ABCPeriodIndex):\n        from pandas.core.indexes.period import _new_PeriodIndex\n        return _new_PeriodIndex(cls, **d)\n    return cls.__new__(cls, **d)\n\n\nclass Index(IndexOpsMixin, PandasObject):\n    \"\"\"\n    Immutable ndarray implementing an ordered, sliceable set. The basic object\n    storing axis labels for all pandas objects.\n\n    Parameters\n    ----------\n    data : array-like (1-dimensional)\n    dtype : NumPy dtype (default: object)\n        If dtype is None, we find the dtype that best fits the data.\n        If an actual dtype is provided, we coerce to that dtype if it's safe.\n        Otherwise, an error will be raised.\n    copy : bool\n        Make a copy of input ndarray\n    name : object\n        Name to be stored in the index\n    tupleize_cols : bool (default: True)\n        When True, attempt to create a MultiIndex if possible\n\n    Notes\n    -----\n    An Index instance can **only** contain hashable objects\n\n    Examples\n    --------\n    >>> pd.Index([1, 2, 3])\n    Int64Index([1, 2, 3], dtype='int64')\n\n    >>> pd.Index(list('abc'))\n    Index(['a', 'b', 'c'], dtype='object')\n\n    See Also\n    ---------\n    RangeIndex : Index implementing a monotonic integer range.\n    CategoricalIndex : Index of :class:`Categorical` s.\n    MultiIndex : A multi-level, or hierarchical, Index.\n    IntervalIndex : An Index of :class:`Interval` s.\n    DatetimeIndex, TimedeltaIndex, PeriodIndex\n    Int64Index, UInt64Index,  Float64Index\n    \"\"\"\n    # To hand over control to subclasses\n    _join_precedence = 1\n\n    # Cython methods; see github.com/cython/cython/issues/2647\n    #  for why we need to wrap these instead of making them class attributes\n    # Moreover, cython will choose the appropriate-dtyped sub-function\n    #  given the dtypes of the passed arguments\n    def _left_indexer_unique(self, left, right):\n        return libjoin.left_join_indexer_unique(left, right)\n\n    def _left_indexer(self, left, right):\n        return libjoin.left_join_indexer(left, right)\n\n    def _inner_indexer(self, left, right):\n        return libjoin.inner_join_indexer(left, right)\n\n    def _outer_indexer(self, left, right):\n        return libjoin.outer_join_indexer(left, right)\n\n    _typ = 'index'\n    _data = None\n    _id = None\n    name = None\n    asi8 = None\n    _comparables = ['name']\n    _attributes = ['name']\n    _is_numeric_dtype = False\n    _can_hold_na = True\n\n    # would we like our indexing holder to defer to us\n    _defer_to_indexing = False\n\n    # prioritize current class for _shallow_copy_with_infer,\n    # used to infer integers as datetime-likes\n    _infer_as_myclass = False\n\n    _engine_type = libindex.ObjectEngine\n\n    _accessors = {'str'}\n\n    str = CachedAccessor(\"str\", StringMethods)\n\n    def __new__(cls, data=None, dtype=None, copy=False, name=None,\n                fastpath=None, tupleize_cols=True, **kwargs):\n\n        if name is None and hasattr(data, 'name'):\n            name = data.name\n\n        if fastpath is not None:\n            warnings.warn(\"The 'fastpath' keyword is deprecated, and will be \"\n                          \"removed in a future version.\",\n                          FutureWarning, stacklevel=2)\n            if fastpath:\n                return cls._simple_new(data, name)\n\n        from .range import RangeIndex\n\n        # range\n        if isinstance(data, RangeIndex):\n            return RangeIndex(start=data, copy=copy, dtype=dtype, name=name)\n        elif isinstance(data, range):\n            return RangeIndex.from_range(data, copy=copy, dtype=dtype,\n                                         name=name)\n\n        # categorical\n        elif is_categorical_dtype(data) or is_categorical_dtype(dtype):\n            from .category import CategoricalIndex\n            return CategoricalIndex(data, dtype=dtype, copy=copy, name=name,\n                                    **kwargs)\n\n        # interval\n        elif ((is_interval_dtype(data) or is_interval_dtype(dtype)) and\n              not is_object_dtype(dtype)):\n            from .interval import IntervalIndex\n            closed = kwargs.get('closed', None)\n            return IntervalIndex(data, dtype=dtype, name=name, copy=copy,\n                                 closed=closed)\n\n        elif (is_datetime64_any_dtype(data) or\n              (dtype is not None and is_datetime64_any_dtype(dtype)) or\n                'tz' in kwargs):\n            from pandas import DatetimeIndex\n\n            if dtype is not None and is_dtype_equal(_o_dtype, dtype):\n                # GH#23524 passing `dtype=object` to DatetimeIndex is invalid,\n                #  will raise in the where `data` is already tz-aware.  So\n                #  we leave it out of this step and cast to object-dtype after\n                #  the DatetimeIndex construction.\n                # Note we can pass copy=False because the .astype below\n                #  will always make a copy\n                result = DatetimeIndex(data, copy=False, name=name, **kwargs)\n                return result.astype(object)\n            else:\n                result = DatetimeIndex(data, copy=copy, name=name,\n                                       dtype=dtype, **kwargs)\n                return result\n\n        elif (is_timedelta64_dtype(data) or\n              (dtype is not None and is_timedelta64_dtype(dtype))):\n            from pandas import TimedeltaIndex\n            result = TimedeltaIndex(data, copy=copy, name=name, **kwargs)\n            if dtype is not None and _o_dtype == dtype:\n                return Index(result.to_pytimedelta(), dtype=_o_dtype)\n            else:\n                return result\n\n        elif is_period_dtype(data) and not is_object_dtype(dtype):\n            from pandas import PeriodIndex\n            result = PeriodIndex(data, copy=copy, name=name, **kwargs)\n            return result\n\n        # extension dtype\n        elif is_extension_array_dtype(data) or is_extension_array_dtype(dtype):\n            data = np.asarray(data)\n            if not (dtype is None or is_object_dtype(dtype)):\n\n                # coerce to the provided dtype\n                data = dtype.construct_array_type()._from_sequence(\n                    data, dtype=dtype, copy=False)\n\n            # coerce to the object dtype\n            data = data.astype(object)\n            return Index(data, dtype=object, copy=copy, name=name,\n                         **kwargs)\n\n        # index-like\n        elif isinstance(data, (np.ndarray, Index, ABCSeries)):\n            if dtype is not None:\n                try:\n\n                    # we need to avoid having numpy coerce\n                    # things that look like ints/floats to ints unless\n                    # they are actually ints, e.g. '0' and 0.0\n                    # should not be coerced\n                    # GH 11836\n                    if is_integer_dtype(dtype):\n                        inferred = lib.infer_dtype(data)\n                        if inferred == 'integer':\n                            data = maybe_cast_to_integer_array(data, dtype,\n                                                               copy=copy)\n                        elif inferred in ['floating', 'mixed-integer-float']:\n                            if isna(data).any():\n                                raise ValueError('cannot convert float '\n                                                 'NaN to integer')\n\n                            if inferred == \"mixed-integer-float\":\n                                data = maybe_cast_to_integer_array(data, dtype)\n\n                            # If we are actually all equal to integers,\n                            # then coerce to integer.\n                            try:\n                                return cls._try_convert_to_int_index(\n                                    data, copy, name, dtype)\n                            except ValueError:\n                                pass\n\n                            # Return an actual float index.\n                            from .numeric import Float64Index\n                            return Float64Index(data, copy=copy, dtype=dtype,\n                                                name=name)\n\n                        elif inferred == 'string':\n                            pass\n                        else:\n                            data = data.astype(dtype)\n                    elif is_float_dtype(dtype):\n                        inferred = lib.infer_dtype(data)\n                        if inferred == 'string':\n                            pass\n                        else:\n                            data = data.astype(dtype)\n                    else:\n                        data = np.array(data, dtype=dtype, copy=copy)\n\n                except (TypeError, ValueError) as e:\n                    msg = str(e)\n                    if (\"cannot convert float\" in msg or\n                            \"Trying to coerce float values to integer\" in msg):\n                        raise\n\n            # maybe coerce to a sub-class\n            from pandas.core.indexes.period import (\n                PeriodIndex, IncompatibleFrequency)\n\n            if is_signed_integer_dtype(data.dtype):\n                from .numeric import Int64Index\n                return Int64Index(data, copy=copy, dtype=dtype, name=name)\n            elif is_unsigned_integer_dtype(data.dtype):\n                from .numeric import UInt64Index\n                return UInt64Index(data, copy=copy, dtype=dtype, name=name)\n            elif is_float_dtype(data.dtype):\n                from .numeric import Float64Index\n                return Float64Index(data, copy=copy, dtype=dtype, name=name)\n            elif issubclass(data.dtype.type, np.bool) or is_bool_dtype(data):\n                subarr = data.astype('object')\n            else:\n                subarr = com.asarray_tuplesafe(data, dtype=object)\n\n            # asarray_tuplesafe does not always copy underlying data,\n            # so need to make sure that this happens\n            if copy:\n                subarr = subarr.copy()\n\n            if dtype is None:\n                inferred = lib.infer_dtype(subarr)\n                if inferred == 'integer':\n                    try:\n                        return cls._try_convert_to_int_index(\n                            subarr, copy, name, dtype)\n                    except ValueError:\n                        pass\n\n                    return Index(subarr, copy=copy,\n                                 dtype=object, name=name)\n                elif inferred in ['floating', 'mixed-integer-float']:\n                    from .numeric import Float64Index\n                    return Float64Index(subarr, copy=copy, name=name)\n                elif inferred == 'interval':\n                    from .interval import IntervalIndex\n                    return IntervalIndex(subarr, name=name, copy=copy)\n                elif inferred == 'boolean':\n                    # don't support boolean explicitly ATM\n                    pass\n                elif inferred != 'string':\n                    if inferred.startswith('datetime'):\n                        if (lib.is_datetime_with_singletz_array(subarr) or\n                                'tz' in kwargs):\n                            # only when subarr has the same tz\n                            from pandas import DatetimeIndex\n                            try:\n                                return DatetimeIndex(subarr, copy=copy,\n                                                     name=name, **kwargs)\n                            except tslibs.OutOfBoundsDatetime:\n                                pass\n\n                    elif inferred.startswith('timedelta'):\n                        from pandas import TimedeltaIndex\n                        return TimedeltaIndex(subarr, copy=copy, name=name,\n                                              **kwargs)\n                    elif inferred == 'period':\n                        try:\n                            return PeriodIndex(subarr, name=name, **kwargs)\n                        except IncompatibleFrequency:\n                            pass\n            return cls._simple_new(subarr, name)\n\n        elif hasattr(data, '__array__'):\n            return Index(np.asarray(data), dtype=dtype, copy=copy, name=name,\n                         **kwargs)\n        elif data is None or is_scalar(data):\n            cls._scalar_data_error(data)\n        else:\n            if tupleize_cols and is_list_like(data):\n                # GH21470: convert iterable to list before determining if empty\n                if is_iterator(data):\n                    data = list(data)\n\n                if data and all(isinstance(e, tuple) for e in data):\n                    # we must be all tuples, otherwise don't construct\n                    # 10697\n                    from .multi import MultiIndex\n                    return MultiIndex.from_tuples(\n                        data, names=name or kwargs.get('names'))\n            # other iterable of some kind\n            subarr = com.asarray_tuplesafe(data, dtype=object)\n            return Index(subarr, dtype=dtype, copy=copy, name=name, **kwargs)\n\n    \"\"\"\n    NOTE for new Index creation:\n\n    - _simple_new: It returns new Index with the same type as the caller.\n      All metadata (such as name) must be provided by caller's responsibility.\n      Using _shallow_copy is recommended because it fills these metadata\n      otherwise specified.\n\n    - _shallow_copy: It returns new Index with the same type (using\n      _simple_new), but fills caller's metadata otherwise specified. Passed\n      kwargs will overwrite corresponding metadata.\n\n    - _shallow_copy_with_infer: It returns new Index inferring its type\n      from passed values. It fills caller's metadata otherwise specified as the\n      same as _shallow_copy.\n\n    See each method's docstring.\n    \"\"\"\n\n    @classmethod\n    def _simple_new(cls, values, name=None, dtype=None, **kwargs):\n        \"\"\"\n        We require that we have a dtype compat for the values. If we are passed\n        a non-dtype compat, then coerce using the constructor.\n\n        Must be careful not to recurse.\n        \"\"\"\n        if not hasattr(values, 'dtype'):\n            if (values is None or not len(values)) and dtype is not None:\n                values = np.empty(0, dtype=dtype)\n            else:\n                values = np.array(values, copy=False)\n                if is_object_dtype(values):\n                    values = cls(values, name=name, dtype=dtype,\n                                 **kwargs)._ndarray_values\n\n        if isinstance(values, (ABCSeries, ABCIndexClass)):\n            # Index._data must always be an ndarray.\n            # This is no-copy for when _values is an ndarray,\n            # which should be always at this point.\n            values = np.asarray(values._values)\n\n        result = object.__new__(cls)\n        result._data = values\n        result.name = name\n        for k, v in compat.iteritems(kwargs):\n            setattr(result, k, v)\n        return result._reset_identity()\n\n    _index_shared_docs['_shallow_copy'] = \"\"\"\n        Create a new Index with the same class as the caller, don't copy the\n        data, use the same object attributes with passed in attributes taking\n        precedence.\n\n        *this is an internal non-public method*\n\n        Parameters\n        ----------\n        values : the values to create the new Index, optional\n        kwargs : updates the default attributes for this Index\n        \"\"\"\n\n    @Appender(_index_shared_docs['_shallow_copy'])\n    def _shallow_copy(self, values=None, **kwargs):\n        if values is None:\n            values = self.values\n        attributes = self._get_attributes_dict()\n        attributes.update(kwargs)\n        if not len(values) and 'dtype' not in kwargs:\n            attributes['dtype'] = self.dtype\n\n        # _simple_new expects an ndarray\n        values = getattr(values, 'values', values)\n        if isinstance(values, ABCDatetimeIndex):\n            # `self.values` returns `self` for tz-aware, so we need to unwrap\n            #  more specifically\n            values = values.asi8\n\n        return self._simple_new(values, **attributes)\n\n    def _shallow_copy_with_infer(self, values, **kwargs):\n        \"\"\"\n        Create a new Index inferring the class with passed value, don't copy\n        the data, use the same object attributes with passed in attributes\n        taking precedence.\n\n        *this is an internal non-public method*\n\n        Parameters\n        ----------\n        values : the values to create the new Index, optional\n        kwargs : updates the default attributes for this Index\n        \"\"\"\n        attributes = self._get_attributes_dict()\n        attributes.update(kwargs)\n        attributes['copy'] = False\n        if not len(values) and 'dtype' not in kwargs:\n            attributes['dtype'] = self.dtype\n        if self._infer_as_myclass:\n            try:\n                return self._constructor(values, **attributes)\n            except (TypeError, ValueError):\n                pass\n        return Index(values, **attributes)\n\n    def _deepcopy_if_needed(self, orig, copy=False):\n        \"\"\"\n        Make a copy of self if data coincides (in memory) with orig.\n        Subclasses should override this if self._base is not an ndarray.\n\n        .. versionadded:: 0.19.0\n\n        Parameters\n        ----------\n        orig : ndarray\n            other ndarray to compare self._data against\n        copy : boolean, default False\n            when False, do not run any check, just return self\n\n        Returns\n        -------\n        A copy of self if needed, otherwise self : Index\n        \"\"\"\n        if copy:\n            # Retrieve the \"base objects\", i.e. the original memory allocations\n            if not isinstance(orig, np.ndarray):\n                # orig is a DatetimeIndex\n                orig = orig.values\n            orig = orig if orig.base is None else orig.base\n            new = self._data if self._data.base is None else self._data.base\n            if orig is new:\n                return self.copy(deep=True)\n\n        return self\n\n    def _update_inplace(self, result, **kwargs):\n        # guard when called from IndexOpsMixin\n        raise TypeError(\"Index can't be updated inplace\")\n\n    def _sort_levels_monotonic(self):\n        \"\"\"\n        Compat with MultiIndex.\n        \"\"\"\n        return self\n\n    _index_shared_docs['_get_grouper_for_level'] = \"\"\"\n        Get index grouper corresponding to an index level\n\n        Parameters\n        ----------\n        mapper: Group mapping function or None\n            Function mapping index values to groups\n        level : int or None\n            Index level\n\n        Returns\n        -------\n        grouper : Index\n            Index of values to group on\n        labels : ndarray of int or None\n            Array of locations in level_index\n        uniques : Index or None\n            Index of unique values for level\n        \"\"\"\n\n    @Appender(_index_shared_docs['_get_grouper_for_level'])\n    def _get_grouper_for_level(self, mapper, level=None):\n        assert level is None or level == 0\n        if mapper is None:\n            grouper = self\n        else:\n            grouper = self.map(mapper)\n\n        return grouper, None, None\n\n    def is_(self, other):\n        \"\"\"\n        More flexible, faster check like ``is`` but that works through views.\n\n        Note: this is *not* the same as ``Index.identical()``, which checks\n        that metadata is also the same.\n\n        Parameters\n        ----------\n        other : object\n            other object to compare against.\n\n        Returns\n        -------\n        True if both have same underlying data, False otherwise : bool\n        \"\"\"\n        # use something other than None to be clearer\n        return self._id is getattr(\n            other, '_id', Ellipsis) and self._id is not None\n\n    def _reset_identity(self):\n        \"\"\"\n        Initializes or resets ``_id`` attribute with new object.\n        \"\"\"\n        self._id = _Identity()\n        return self\n\n    # ndarray compat\n    def __len__(self):\n        \"\"\"\n        Return the length of the Index.\n        \"\"\"\n        return len(self._data)\n\n    def __array__(self, dtype=None):\n        \"\"\"\n        The array interface, return my values.\n        \"\"\"\n        return self._data.view(np.ndarray)\n\n    def __array_wrap__(self, result, context=None):\n        \"\"\"\n        Gets called after a ufunc.\n        \"\"\"\n        if is_bool_dtype(result):\n            return result\n\n        attrs = self._get_attributes_dict()\n        attrs = self._maybe_update_attributes(attrs)\n        return Index(result, **attrs)\n\n    @cache_readonly\n    def dtype(self):\n        \"\"\"\n        Return the dtype object of the underlying data.\n        \"\"\"\n        return self._data.dtype\n\n    @cache_readonly\n    def dtype_str(self):\n        \"\"\"\n        Return the dtype str of the underlying data.\n        \"\"\"\n        return str(self.dtype)\n\n    @property\n    def values(self):\n        \"\"\"\n        Return the underlying data as an ndarray.\n        \"\"\"\n        return self._data.view(np.ndarray)\n\n    @property\n    def _values(self):\n        # type: () -> Union[ExtensionArray, Index, np.ndarray]\n        # TODO(EA): remove index types as they become extension arrays\n        \"\"\"\n        The best array representation.\n\n        This is an ndarray, ExtensionArray, or Index subclass. This differs\n        from ``_ndarray_values``, which always returns an ndarray.\n\n        Both ``_values`` and ``_ndarray_values`` are consistent between\n        ``Series`` and ``Index``.\n\n        It may differ from the public '.values' method.\n\n        index             | values          | _values       | _ndarray_values |\n        ----------------- | --------------- | ------------- | --------------- |\n        Index             | ndarray         | ndarray       | ndarray         |\n        CategoricalIndex  | Categorical     | Categorical   | ndarray[int]    |\n        DatetimeIndex     | ndarray[M8ns]   | ndarray[M8ns] | ndarray[M8ns]   |\n        DatetimeIndex[tz] | ndarray[M8ns]   | DTI[tz]       | ndarray[M8ns]   |\n        PeriodIndex       | ndarray[object] | PeriodArray   | ndarray[int]    |\n        IntervalIndex     | IntervalArray   | IntervalArray | ndarray[object] |\n\n        See Also\n        --------\n        values\n        _ndarray_values\n        \"\"\"\n        return self.values\n\n    def get_values(self):\n        \"\"\"\n        Return `Index` data as an `numpy.ndarray`.\n\n        Returns\n        -------\n        numpy.ndarray\n            A one-dimensional numpy array of the `Index` values.\n\n        See Also\n        --------\n        Index.values : The attribute that get_values wraps.\n\n        Examples\n        --------\n        Getting the `Index` values of a `DataFrame`:\n\n        >>> df = pd.DataFrame([[1, 2, 3], [4, 5, 6], [7, 8, 9]],\n        ...                    index=['a', 'b', 'c'], columns=['A', 'B', 'C'])\n        >>> df\n           A  B  C\n        a  1  2  3\n        b  4  5  6\n        c  7  8  9\n        >>> df.index.get_values()\n        array(['a', 'b', 'c'], dtype=object)\n\n        Standalone `Index` values:\n\n        >>> idx = pd.Index(['1', '2', '3'])\n        >>> idx.get_values()\n        array(['1', '2', '3'], dtype=object)\n\n        `MultiIndex` arrays also have only one dimension:\n\n        >>> midx = pd.MultiIndex.from_arrays([[1, 2, 3], ['a', 'b', 'c']],\n        ...                                  names=('number', 'letter'))\n        >>> midx.get_values()\n        array([(1, 'a'), (2, 'b'), (3, 'c')], dtype=object)\n        >>> midx.get_values().ndim\n        1\n        \"\"\"\n        return self.values\n\n    @Appender(IndexOpsMixin.memory_usage.__doc__)\n    def memory_usage(self, deep=False):\n        result = super(Index, self).memory_usage(deep=deep)\n\n        # include our engine hashtable\n        result += self._engine.sizeof(deep=deep)\n        return result\n\n    # ops compat\n    def repeat(self, repeats, *args, **kwargs):\n        \"\"\"\n        Repeat elements of an Index.\n\n        Returns a new index where each element of the current index\n        is repeated consecutively a given number of times.\n\n        Parameters\n        ----------\n        repeats : int\n            The number of repetitions for each element.\n        **kwargs\n            Additional keywords have no effect but might be accepted for\n            compatibility with numpy.\n\n        Returns\n        -------\n        pandas.Index\n            Newly created Index with repeated elements.\n\n        See Also\n        --------\n        Series.repeat : Equivalent function for Series.\n        numpy.repeat : Underlying implementation.\n\n        Examples\n        --------\n        >>> idx = pd.Index([1, 2, 3])\n        >>> idx\n        Int64Index([1, 2, 3], dtype='int64')\n        >>> idx.repeat(2)\n        Int64Index([1, 1, 2, 2, 3, 3], dtype='int64')\n        >>> idx.repeat(3)\n        Int64Index([1, 1, 1, 2, 2, 2, 3, 3, 3], dtype='int64')\n        \"\"\"\n        nv.validate_repeat(args, kwargs)\n        return self._shallow_copy(self._values.repeat(repeats))\n\n    _index_shared_docs['where'] = \"\"\"\n        Return an Index of same shape as self and whose corresponding\n        entries are from self where cond is True and otherwise are from\n        other.\n\n        .. versionadded:: 0.19.0\n\n        Parameters\n        ----------\n        cond : boolean array-like with the same length as self\n        other : scalar, or array-like\n        \"\"\"\n\n    @Appender(_index_shared_docs['where'])\n    def where(self, cond, other=None):\n        if other is None:\n            other = self._na_value\n\n        dtype = self.dtype\n        values = self.values\n\n        if is_bool(other) or is_bool_dtype(other):\n\n            # bools force casting\n            values = values.astype(object)\n            dtype = None\n\n        values = np.where(cond, values, other)\n\n        if self._is_numeric_dtype and np.any(isna(values)):\n            # We can't coerce to the numeric dtype of \"self\" (unless\n            # it's float) if there are NaN values in our output.\n            dtype = None\n\n        return self._shallow_copy_with_infer(values, dtype=dtype)\n\n    def ravel(self, order='C'):\n        \"\"\"\n        Return an ndarray of the flattened values of the underlying data.\n\n        See Also\n        --------\n        numpy.ndarray.ravel\n        \"\"\"\n        return self._ndarray_values.ravel(order=order)\n\n    # construction helpers\n    @classmethod\n    def _try_convert_to_int_index(cls, data, copy, name, dtype):\n        \"\"\"\n        Attempt to convert an array of data into an integer index.\n\n        Parameters\n        ----------\n        data : The data to convert.\n        copy : Whether to copy the data or not.\n        name : The name of the index returned.\n\n        Returns\n        -------\n        int_index : data converted to either an Int64Index or a\n                    UInt64Index\n\n        Raises\n        ------\n        ValueError if the conversion was not successful.\n        \"\"\"\n\n        from .numeric import Int64Index, UInt64Index\n        if not is_unsigned_integer_dtype(dtype):\n            # skip int64 conversion attempt if uint-like dtype is passed, as\n            # this could return Int64Index when UInt64Index is what's desrired\n            try:\n                res = data.astype('i8', copy=False)\n                if (res == data).all():\n                    return Int64Index(res, copy=copy, name=name)\n            except (OverflowError, TypeError, ValueError):\n                pass\n\n        # Conversion to int64 failed (possibly due to overflow) or was skipped,\n        # so let's try now with uint64.\n        try:\n            res = data.astype('u8', copy=False)\n            if (res == data).all():\n                return UInt64Index(res, copy=copy, name=name)\n        except (OverflowError, TypeError, ValueError):\n            pass\n\n        raise ValueError\n\n    @classmethod\n    def _scalar_data_error(cls, data):\n        raise TypeError('{0}(...) must be called with a collection of some '\n                        'kind, {1} was passed'.format(cls.__name__,\n                                                      repr(data)))\n\n    @classmethod\n    def _string_data_error(cls, data):\n        raise TypeError('String dtype not supported, you may need '\n                        'to explicitly cast to a numeric type')\n\n    @classmethod\n    def _coerce_to_ndarray(cls, data):\n        \"\"\"\n        Coerces data to ndarray.\n\n        Converts other iterables to list first and then to array.\n        Does not touch ndarrays.\n\n        Raises\n        ------\n        TypeError\n            When the data passed in is a scalar.\n        \"\"\"\n\n        if not isinstance(data, (np.ndarray, Index)):\n            if data is None or is_scalar(data):\n                cls._scalar_data_error(data)\n\n            # other iterable of some kind\n            if not isinstance(data, (ABCSeries, list, tuple)):\n                data = list(data)\n            data = np.asarray(data)\n        return data\n\n    def _get_attributes_dict(self):\n        \"\"\"\n        Return an attributes dict for my class.\n        \"\"\"\n        return {k: getattr(self, k, None) for k in self._attributes}\n\n    def view(self, cls=None):\n\n        # we need to see if we are subclassing an\n        # index type here\n        if cls is not None and not hasattr(cls, '_typ'):\n            result = self._data.view(cls)\n        else:\n            result = self._shallow_copy()\n        if isinstance(result, Index):\n            result._id = self._id\n        return result\n\n    def _coerce_scalar_to_index(self, item):\n        \"\"\"\n        We need to coerce a scalar to a compat for our index type.\n\n        Parameters\n        ----------\n        item : scalar item to coerce\n        \"\"\"\n        dtype = self.dtype\n\n        if self._is_numeric_dtype and isna(item):\n            # We can't coerce to the numeric dtype of \"self\" (unless\n            # it's float) if there are NaN values in our output.\n            dtype = None\n\n        return Index([item], dtype=dtype, **self._get_attributes_dict())\n\n    _index_shared_docs['copy'] = \"\"\"\n        Make a copy of this object.  Name and dtype sets those attributes on\n        the new object.\n\n        Parameters\n        ----------\n        name : string, optional\n        deep : boolean, default False\n        dtype : numpy dtype or pandas type\n\n        Returns\n        -------\n        copy : Index\n\n        Notes\n        -----\n        In most cases, there should be no functional difference from using\n        ``deep``, but if ``deep`` is passed it will attempt to deepcopy.\n        \"\"\"\n\n    @Appender(_index_shared_docs['copy'])\n    def copy(self, name=None, deep=False, dtype=None, **kwargs):\n        if deep:\n            new_index = self._shallow_copy(self._data.copy())\n        else:\n            new_index = self._shallow_copy()\n\n        names = kwargs.get('names')\n        names = self._validate_names(name=name, names=names, deep=deep)\n        new_index = new_index.set_names(names)\n\n        if dtype:\n            new_index = new_index.astype(dtype)\n        return new_index\n\n    def __copy__(self, **kwargs):\n        return self.copy(**kwargs)\n\n    def __deepcopy__(self, memo=None):\n        \"\"\"\n        Parameters\n        ----------\n        memo, default None\n            Standard signature. Unused\n        \"\"\"\n        if memo is None:\n            memo = {}\n        return self.copy(deep=True)\n\n    def _validate_names(self, name=None, names=None, deep=False):\n        \"\"\"\n        Handles the quirks of having a singular 'name' parameter for general\n        Index and plural 'names' parameter for MultiIndex.\n        \"\"\"\n        from copy import deepcopy\n        if names is not None and name is not None:\n            raise TypeError(\"Can only provide one of `names` and `name`\")\n        elif names is None and name is None:\n            return deepcopy(self.names) if deep else self.names\n        elif names is not None:\n            if not is_list_like(names):\n                raise TypeError(\"Must pass list-like as `names`.\")\n            return names\n        else:\n            if not is_list_like(name):\n                return [name]\n            return name\n\n    def __unicode__(self):\n        \"\"\"\n        Return a string representation for this object.\n\n        Invoked by unicode(df) in py2 only. Yields a Unicode String in both\n        py2/py3.\n        \"\"\"\n        klass = self.__class__.__name__\n        data = self._format_data()\n        attrs = self._format_attrs()\n        space = self._format_space()\n\n        prepr = (u(\",%s\") %\n                 space).join(u(\"%s=%s\") % (k, v) for k, v in attrs)\n\n        # no data provided, just attributes\n        if data is None:\n            data = ''\n\n        res = u(\"%s(%s%s)\") % (klass, data, prepr)\n\n        return res\n\n    def _format_space(self):\n\n        # using space here controls if the attributes\n        # are line separated or not (the default)\n\n        # max_seq_items = get_option('display.max_seq_items')\n        # if len(self) > max_seq_items:\n        #    space = \"\\n%s\" % (' ' * (len(klass) + 1))\n        return \" \"\n\n    @property\n    def _formatter_func(self):\n        \"\"\"\n        Return the formatter function.\n        \"\"\"\n        return default_pprint\n\n    def _format_data(self, name=None):\n        \"\"\"\n        Return the formatted data as a unicode string.\n        \"\"\"\n\n        # do we want to justify (only do so for non-objects)\n        is_justify = not (self.inferred_type in ('string', 'unicode') or\n                          (self.inferred_type == 'categorical' and\n                           is_object_dtype(self.categories)))\n\n        return format_object_summary(self, self._formatter_func,\n                                     is_justify=is_justify, name=name)\n\n    def _format_attrs(self):\n        \"\"\"\n        Return a list of tuples of the (attr,formatted_value).\n        \"\"\"\n        return format_object_attrs(self)\n\n    def to_flat_index(self):\n        \"\"\"\n        Identity method.\n\n        .. versionadded:: 0.24.0\n\n        This is implemented for compatability with subclass implementations\n        when chaining.\n\n        Returns\n        -------\n        pd.Index\n            Caller.\n\n        See Also\n        --------\n        MultiIndex.to_flat_index : Subclass implementation.\n        \"\"\"\n        return self\n\n    def to_series(self, index=None, name=None):\n        \"\"\"\n        Create a Series with both index and values equal to the index keys\n        useful with map for returning an indexer based on an index.\n\n        Parameters\n        ----------\n        index : Index, optional\n            index of resulting Series. If None, defaults to original index\n        name : string, optional\n            name of resulting Series. If None, defaults to name of original\n            index\n\n        Returns\n        -------\n        Series : dtype will be based on the type of the Index values.\n        \"\"\"\n\n        from pandas import Series\n\n        if index is None:\n            index = self._shallow_copy()\n        if name is None:\n            name = self.name\n\n        return Series(self.values.copy(), index=index, name=name)\n\n    def to_frame(self, index=True, name=None):\n        \"\"\"\n        Create a DataFrame with a column containing the Index.\n\n        .. versionadded:: 0.24.0\n\n        Parameters\n        ----------\n        index : boolean, default True\n            Set the index of the returned DataFrame as the original Index.\n\n        name : object, default None\n            The passed name should substitute for the index name (if it has\n            one).\n\n        Returns\n        -------\n        DataFrame\n            DataFrame containing the original Index data.\n\n        See Also\n        --------\n        Index.to_series : Convert an Index to a Series.\n        Series.to_frame : Convert Series to DataFrame.\n\n        Examples\n        --------\n        >>> idx = pd.Index(['Ant', 'Bear', 'Cow'], name='animal')\n        >>> idx.to_frame()\n               animal\n        animal\n        Ant       Ant\n        Bear     Bear\n        Cow       Cow\n\n        By default, the original Index is reused. To enforce a new Index:\n\n        >>> idx.to_frame(index=False)\n            animal\n        0   Ant\n        1  Bear\n        2   Cow\n\n        To override the name of the resulting column, specify `name`:\n\n        >>> idx.to_frame(index=False, name='zoo')\n            zoo\n        0   Ant\n        1  Bear\n        2   Cow\n        \"\"\"\n\n        from pandas import DataFrame\n        if name is None:\n            name = self.name or 0\n        result = DataFrame({name: self.values.copy()})\n\n        if index:\n            result.index = self\n        return result\n\n    _index_shared_docs['astype'] = \"\"\"\n        Create an Index with values cast to dtypes. The class of a new Index\n        is determined by dtype. When conversion is impossible, a ValueError\n        exception is raised.\n\n        Parameters\n        ----------\n        dtype : numpy dtype or pandas type\n        copy : bool, default True\n            By default, astype always returns a newly allocated object.\n            If copy is set to False and internal requirements on dtype are\n            satisfied, the original data is used to create a new Index\n            or the original Index is returned.\n\n            .. versionadded:: 0.19.0\n        \"\"\"\n\n    @Appender(_index_shared_docs['astype'])\n    def astype(self, dtype, copy=True):\n        if is_dtype_equal(self.dtype, dtype):\n            return self.copy() if copy else self\n\n        elif is_categorical_dtype(dtype):\n            from .category import CategoricalIndex\n            return CategoricalIndex(self.values, name=self.name, dtype=dtype,\n                                    copy=copy)\n\n        elif is_extension_array_dtype(dtype):\n            return Index(np.asarray(self), dtype=dtype, copy=copy)\n\n        try:\n            if is_datetime64tz_dtype(dtype):\n                from pandas import DatetimeIndex\n                return DatetimeIndex(self.values, name=self.name, dtype=dtype,\n                                     copy=copy)\n            return Index(self.values.astype(dtype, copy=copy), name=self.name,\n                         dtype=dtype)\n        except (TypeError, ValueError):\n            msg = 'Cannot cast {name} to dtype {dtype}'\n            raise TypeError(msg.format(name=type(self).__name__, dtype=dtype))\n\n    def _to_safe_for_reshape(self):\n        \"\"\"\n        Convert to object if we are a categorical.\n        \"\"\"\n        return self\n\n    def _assert_can_do_setop(self, other):\n        if not is_list_like(other):\n            raise TypeError('Input must be Index or array-like')\n        return True\n\n    def _convert_can_do_setop(self, other):\n        if not isinstance(other, Index):\n            other = Index(other, name=self.name)\n            result_name = self.name\n        else:\n            result_name = get_op_result_name(self, other)\n        return other, result_name\n\n    def _convert_for_op(self, value):\n        \"\"\"\n        Convert value to be insertable to ndarray.\n        \"\"\"\n        return value\n\n    def _assert_can_do_op(self, value):\n        \"\"\"\n        Check value is valid for scalar op.\n        \"\"\"\n        if not is_scalar(value):\n            msg = \"'value' must be a scalar, passed: {0}\"\n            raise TypeError(msg.format(type(value).__name__))\n\n    @property\n    def nlevels(self):\n        return 1\n\n    def _get_names(self):\n        return FrozenList((self.name, ))\n\n    def _set_names(self, values, level=None):\n        \"\"\"\n        Set new names on index. Each name has to be a hashable type.\n\n        Parameters\n        ----------\n        values : str or sequence\n            name(s) to set\n        level : int, level name, or sequence of int/level names (default None)\n            If the index is a MultiIndex (hierarchical), level(s) to set (None\n            for all levels).  Otherwise level must be None\n\n        Raises\n        ------\n        TypeError if each name is not hashable.\n        \"\"\"\n        if not is_list_like(values):\n            raise ValueError('Names must be a list-like')\n        if len(values) != 1:\n            raise ValueError('Length of new names must be 1, got %d' %\n                             len(values))\n\n        # GH 20527\n        # All items in 'name' need to be hashable:\n        for name in values:\n            if not is_hashable(name):\n                raise TypeError('{}.name must be a hashable type'\n                                .format(self.__class__.__name__))\n        self.name = values[0]\n\n    names = property(fset=_set_names, fget=_get_names)\n\n    def set_names(self, names, level=None, inplace=False):\n        \"\"\"\n        Set Index or MultiIndex name.\n\n        Able to set new names partially and by level.\n\n        Parameters\n        ----------\n        names : label or list of label\n            Name(s) to set.\n        level : int, label or list of int or label, optional\n            If the index is a MultiIndex, level(s) to set (None for all\n            levels). Otherwise level must be None.\n        inplace : bool, default False\n            Modifies the object directly, instead of creating a new Index or\n            MultiIndex.\n\n        Returns\n        -------\n        Index\n            The same type as the caller or None if inplace is True.\n\n        See Also\n        --------\n        Index.rename : Able to set new names without level.\n\n        Examples\n        --------\n        >>> idx = pd.Index([1, 2, 3, 4])\n        >>> idx\n        Int64Index([1, 2, 3, 4], dtype='int64')\n        >>> idx.set_names('quarter')\n        Int64Index([1, 2, 3, 4], dtype='int64', name='quarter')\n\n        >>> idx = pd.MultiIndex.from_product([['python', 'cobra'],\n        ...                                   [2018, 2019]])\n        >>> idx\n        MultiIndex(levels=[['cobra', 'python'], [2018, 2019]],\n                   labels=[[1, 1, 0, 0], [0, 1, 0, 1]])\n        >>> idx.set_names(['kind', 'year'], inplace=True)\n        >>> idx\n        MultiIndex(levels=[['cobra', 'python'], [2018, 2019]],\n                   labels=[[1, 1, 0, 0], [0, 1, 0, 1]],\n                   names=['kind', 'year'])\n        >>> idx.set_names('species', level=0)\n        MultiIndex(levels=[['cobra', 'python'], [2018, 2019]],\n                   labels=[[1, 1, 0, 0], [0, 1, 0, 1]],\n                   names=['species', 'year'])\n        \"\"\"\n\n        from .multi import MultiIndex\n        if level is not None and not isinstance(self, MultiIndex):\n            raise ValueError('Level must be None for non-MultiIndex')\n\n        if level is not None and not is_list_like(level) and is_list_like(\n                names):\n            msg = \"Names must be a string when a single level is provided.\"\n            raise TypeError(msg)\n\n        if not is_list_like(names) and level is None and self.nlevels > 1:\n            raise TypeError(\"Must pass list-like as `names`.\")\n\n        if not is_list_like(names):\n            names = [names]\n        if level is not None and not is_list_like(level):\n            level = [level]\n\n        if inplace:\n            idx = self\n        else:\n            idx = self._shallow_copy()\n        idx._set_names(names, level=level)\n        if not inplace:\n            return idx\n\n    def rename(self, name, inplace=False):\n        \"\"\"\n        Alter Index or MultiIndex name.\n\n        Able to set new names without level. Defaults to returning new index.\n        Length of names must match number of levels in MultiIndex.\n\n        Parameters\n        ----------\n        name : label or list of labels\n            Name(s) to set.\n        inplace : boolean, default False\n            Modifies the object directly, instead of creating a new Index or\n            MultiIndex.\n\n        Returns\n        -------\n        Index\n            The same type as the caller or None if inplace is True.\n\n        See Also\n        --------\n        Index.set_names : Able to set new names partially and by level.\n\n        Examples\n        --------\n        >>> idx = pd.Index(['A', 'C', 'A', 'B'], name='score')\n        >>> idx.rename('grade')\n        Index(['A', 'C', 'A', 'B'], dtype='object', name='grade')\n\n        >>> idx = pd.MultiIndex.from_product([['python', 'cobra'],\n        ...                                   [2018, 2019]],\n        ...                                   names=['kind', 'year'])\n        >>> idx\n        MultiIndex(levels=[['cobra', 'python'], [2018, 2019]],\n                   labels=[[1, 1, 0, 0], [0, 1, 0, 1]],\n                   names=['kind', 'year'])\n        >>> idx.rename(['species', 'year'])\n        MultiIndex(levels=[['cobra', 'python'], [2018, 2019]],\n                   labels=[[1, 1, 0, 0], [0, 1, 0, 1]],\n                   names=['species', 'year'])\n        >>> idx.rename('species')\n        Traceback (most recent call last):\n        TypeError: Must pass list-like as `names`.\n        \"\"\"\n        return self.set_names([name], inplace=inplace)\n\n    @property\n    def _has_complex_internals(self):\n        # to disable groupby tricks in MultiIndex\n        return False\n\n    def _summary(self, name=None):\n        \"\"\"\n        Return a summarized representation.\n\n        Parameters\n        ----------\n        name : str\n            name to use in the summary representation\n\n        Returns\n        -------\n        String with a summarized representation of the index\n        \"\"\"\n        if len(self) > 0:\n            head = self[0]\n            if (hasattr(head, 'format') and\n                    not isinstance(head, compat.string_types)):\n                head = head.format()\n            tail = self[-1]\n            if (hasattr(tail, 'format') and\n                    not isinstance(tail, compat.string_types)):\n                tail = tail.format()\n            index_summary = ', %s to %s' % (pprint_thing(head),\n                                            pprint_thing(tail))\n        else:\n            index_summary = ''\n\n        if name is None:\n            name = type(self).__name__\n        return '%s: %s entries%s' % (name, len(self), index_summary)\n\n    def summary(self, name=None):\n        \"\"\"\n        Return a summarized representation.\n\n        .. deprecated:: 0.23.0\n        \"\"\"\n        warnings.warn(\"'summary' is deprecated and will be removed in a \"\n                      \"future version.\", FutureWarning, stacklevel=2)\n        return self._summary(name)\n\n    def _mpl_repr(self):\n        # how to represent ourselves to matplotlib\n        return self.values\n\n    _na_value = np.nan\n    \"\"\"The expected NA value to use with this index.\"\"\"\n\n    # introspection\n    @property\n    def is_monotonic(self):\n        \"\"\"\n        Alias for is_monotonic_increasing.\n        \"\"\"\n        return self.is_monotonic_increasing\n\n    @property\n    def is_monotonic_increasing(self):\n        \"\"\"\n        Return if the index is monotonic increasing (only equal or\n        increasing) values.\n\n        Examples\n        --------\n        >>> Index([1, 2, 3]).is_monotonic_increasing\n        True\n        >>> Index([1, 2, 2]).is_monotonic_increasing\n        True\n        >>> Index([1, 3, 2]).is_monotonic_increasing\n        False\n        \"\"\"\n        return self._engine.is_monotonic_increasing\n\n    @property\n    def is_monotonic_decreasing(self):\n        \"\"\"\n        Return if the index is monotonic decreasing (only equal or\n        decreasing) values.\n\n        Examples\n        --------\n        >>> Index([3, 2, 1]).is_monotonic_decreasing\n        True\n        >>> Index([3, 2, 2]).is_monotonic_decreasing\n        True\n        >>> Index([3, 1, 2]).is_monotonic_decreasing\n        False\n        \"\"\"\n        return self._engine.is_monotonic_decreasing\n\n    @property\n    def _is_strictly_monotonic_increasing(self):\n        \"\"\"\n        Return if the index is strictly monotonic increasing\n        (only increasing) values.\n\n        Examples\n        --------\n        >>> Index([1, 2, 3])._is_strictly_monotonic_increasing\n        True\n        >>> Index([1, 2, 2])._is_strictly_monotonic_increasing\n        False\n        >>> Index([1, 3, 2])._is_strictly_monotonic_increasing\n        False\n        \"\"\"\n        return self.is_unique and self.is_monotonic_increasing\n\n    @property\n    def _is_strictly_monotonic_decreasing(self):\n        \"\"\"\n        Return if the index is strictly monotonic decreasing\n        (only decreasing) values.\n\n        Examples\n        --------\n        >>> Index([3, 2, 1])._is_strictly_monotonic_decreasing\n        True\n        >>> Index([3, 2, 2])._is_strictly_monotonic_decreasing\n        False\n        >>> Index([3, 1, 2])._is_strictly_monotonic_decreasing\n        False\n        \"\"\"\n        return self.is_unique and self.is_monotonic_decreasing\n\n    def is_lexsorted_for_tuple(self, tup):\n        return True\n\n    @cache_readonly\n    def is_unique(self):\n        \"\"\"\n        Return if the index has unique values.\n        \"\"\"\n        return self._engine.is_unique\n\n    @property\n    def has_duplicates(self):\n        return not self.is_unique\n\n    def is_boolean(self):\n        return self.inferred_type in ['boolean']\n\n    def is_integer(self):\n        return self.inferred_type in ['integer']\n\n    def is_floating(self):\n        return self.inferred_type in ['floating', 'mixed-integer-float']\n\n    def is_numeric(self):\n        return self.inferred_type in ['integer', 'floating']\n\n    def is_object(self):\n        return is_object_dtype(self.dtype)\n\n    def is_categorical(self):\n        \"\"\"\n        Check if the Index holds categorical data.\n\n        Returns\n        -------\n        boolean\n            True if the Index is categorical.\n\n        See Also\n        --------\n        CategoricalIndex : Index for categorical data.\n\n        Examples\n        --------\n        >>> idx = pd.Index([\"Watermelon\", \"Orange\", \"Apple\",\n        ...                 \"Watermelon\"]).astype(\"category\")\n        >>> idx.is_categorical()\n        True\n\n        >>> idx = pd.Index([1, 3, 5, 7])\n        >>> idx.is_categorical()\n        False\n\n        >>> s = pd.Series([\"Peter\", \"Victor\", \"Elisabeth\", \"Mar\"])\n        >>> s\n        0        Peter\n        1       Victor\n        2    Elisabeth\n        3          Mar\n        dtype: object\n        >>> s.index.is_categorical()\n        False\n        \"\"\"\n        return self.inferred_type in ['categorical']\n\n    def is_interval(self):\n        return self.inferred_type in ['interval']\n\n    def is_mixed(self):\n        return self.inferred_type in ['mixed']\n\n    def holds_integer(self):\n        return self.inferred_type in ['integer', 'mixed-integer']\n\n    _index_shared_docs['_convert_scalar_indexer'] = \"\"\"\n        Convert a scalar indexer.\n\n        Parameters\n        ----------\n        key : label of the slice bound\n        kind : {'ix', 'loc', 'getitem', 'iloc'} or None\n    \"\"\"\n\n    @Appender(_index_shared_docs['_convert_scalar_indexer'])\n    def _convert_scalar_indexer(self, key, kind=None):\n        assert kind in ['ix', 'loc', 'getitem', 'iloc', None]\n\n        if kind == 'iloc':\n            return self._validate_indexer('positional', key, kind)\n\n        if len(self) and not isinstance(self, ABCMultiIndex,):\n\n            # we can raise here if we are definitive that this\n            # is positional indexing (eg. .ix on with a float)\n            # or label indexing if we are using a type able\n            # to be represented in the index\n\n            if kind in ['getitem', 'ix'] and is_float(key):\n                if not self.is_floating():\n                    return self._invalid_indexer('label', key)\n\n            elif kind in ['loc'] and is_float(key):\n\n                # we want to raise KeyError on string/mixed here\n                # technically we *could* raise a TypeError\n                # on anything but mixed though\n                if self.inferred_type not in ['floating',\n                                              'mixed-integer-float',\n                                              'string',\n                                              'unicode',\n                                              'mixed']:\n                    return self._invalid_indexer('label', key)\n\n            elif kind in ['loc'] and is_integer(key):\n                if not self.holds_integer():\n                    return self._invalid_indexer('label', key)\n\n        return key\n\n    _index_shared_docs['_convert_slice_indexer'] = \"\"\"\n        Convert a slice indexer.\n\n        By definition, these are labels unless 'iloc' is passed in.\n        Floats are not allowed as the start, step, or stop of the slice.\n\n        Parameters\n        ----------\n        key : label of the slice bound\n        kind : {'ix', 'loc', 'getitem', 'iloc'} or None\n    \"\"\"\n\n    @Appender(_index_shared_docs['_convert_slice_indexer'])\n    def _convert_slice_indexer(self, key, kind=None):\n        assert kind in ['ix', 'loc', 'getitem', 'iloc', None]\n\n        # if we are not a slice, then we are done\n        if not isinstance(key, slice):\n            return key\n\n        # validate iloc\n        if kind == 'iloc':\n            return slice(self._validate_indexer('slice', key.start, kind),\n                         self._validate_indexer('slice', key.stop, kind),\n                         self._validate_indexer('slice', key.step, kind))\n\n        # potentially cast the bounds to integers\n        start, stop, step = key.start, key.stop, key.step\n\n        # figure out if this is a positional indexer\n        def is_int(v):\n            return v is None or is_integer(v)\n\n        is_null_slicer = start is None and stop is None\n        is_index_slice = is_int(start) and is_int(stop)\n        is_positional = is_index_slice and not self.is_integer()\n\n        if kind == 'getitem':\n            \"\"\"\n            called from the getitem slicers, validate that we are in fact\n            integers\n            \"\"\"\n            if self.is_integer() or is_index_slice:\n                return slice(self._validate_indexer('slice', key.start, kind),\n                             self._validate_indexer('slice', key.stop, kind),\n                             self._validate_indexer('slice', key.step, kind))\n\n        # convert the slice to an indexer here\n\n        # if we are mixed and have integers\n        try:\n            if is_positional and self.is_mixed():\n                # Validate start & stop\n                if start is not None:\n                    self.get_loc(start)\n                if stop is not None:\n                    self.get_loc(stop)\n                is_positional = False\n        except KeyError:\n            if self.inferred_type == 'mixed-integer-float':\n                raise\n\n        if is_null_slicer:\n            indexer = key\n        elif is_positional:\n            indexer = key\n        else:\n            try:\n                indexer = self.slice_indexer(start, stop, step, kind=kind)\n            except Exception:\n                if is_index_slice:\n                    if self.is_integer():\n                        raise\n                    else:\n                        indexer = key\n                else:\n                    raise\n\n        return indexer\n\n    def _convert_listlike_indexer(self, keyarr, kind=None):\n        \"\"\"\n        Parameters\n        ----------\n        keyarr : list-like\n            Indexer to convert.\n\n        Returns\n        -------\n        tuple (indexer, keyarr)\n            indexer is an ndarray or None if cannot convert\n            keyarr are tuple-safe keys\n        \"\"\"\n        if isinstance(keyarr, Index):\n            keyarr = self._convert_index_indexer(keyarr)\n        else:\n            keyarr = self._convert_arr_indexer(keyarr)\n\n        indexer = self._convert_list_indexer(keyarr, kind=kind)\n        return indexer, keyarr\n\n    _index_shared_docs['_convert_arr_indexer'] = \"\"\"\n        Convert an array-like indexer to the appropriate dtype.\n\n        Parameters\n        ----------\n        keyarr : array-like\n            Indexer to convert.\n\n        Returns\n        -------\n        converted_keyarr : array-like\n    \"\"\"\n\n    @Appender(_index_shared_docs['_convert_arr_indexer'])\n    def _convert_arr_indexer(self, keyarr):\n        keyarr = com.asarray_tuplesafe(keyarr)\n        return keyarr\n\n    _index_shared_docs['_convert_index_indexer'] = \"\"\"\n        Convert an Index indexer to the appropriate dtype.\n\n        Parameters\n        ----------\n        keyarr : Index (or sub-class)\n            Indexer to convert.\n\n        Returns\n        -------\n        converted_keyarr : Index (or sub-class)\n    \"\"\"\n\n    @Appender(_index_shared_docs['_convert_index_indexer'])\n    def _convert_index_indexer(self, keyarr):\n        return keyarr\n\n    _index_shared_docs['_convert_list_indexer'] = \"\"\"\n        Convert a list-like indexer to the appropriate dtype.\n\n        Parameters\n        ----------\n        keyarr : Index (or sub-class)\n            Indexer to convert.\n        kind : iloc, ix, loc, optional\n\n        Returns\n        -------\n        positional indexer or None\n    \"\"\"\n\n    @Appender(_index_shared_docs['_convert_list_indexer'])\n    def _convert_list_indexer(self, keyarr, kind=None):\n        if (kind in [None, 'iloc', 'ix'] and\n                is_integer_dtype(keyarr) and not self.is_floating() and\n                not isinstance(keyarr, ABCPeriodIndex)):\n\n            if self.inferred_type == 'mixed-integer':\n                indexer = self.get_indexer(keyarr)\n                if (indexer >= 0).all():\n                    return indexer\n                # missing values are flagged as -1 by get_indexer and negative\n                # indices are already converted to positive indices in the\n                # above if-statement, so the negative flags are changed to\n                # values outside the range of indices so as to trigger an\n                # IndexError in maybe_convert_indices\n                indexer[indexer < 0] = len(self)\n                from pandas.core.indexing import maybe_convert_indices\n                return maybe_convert_indices(indexer, len(self))\n\n            elif not self.inferred_type == 'integer':\n                keyarr = np.where(keyarr < 0, len(self) + keyarr, keyarr)\n                return keyarr\n\n        return None\n\n    def _invalid_indexer(self, form, key):\n        \"\"\"\n        Consistent invalid indexer message.\n        \"\"\"\n        raise TypeError(\"cannot do {form} indexing on {klass} with these \"\n                        \"indexers [{key}] of {kind}\".format(\n                            form=form, klass=type(self), key=key,\n                            kind=type(key)))\n\n    def get_duplicates(self):\n        \"\"\"\n        Extract duplicated index elements.\n\n        Returns a sorted list of index elements which appear more than once in\n        the index.\n\n        .. deprecated:: 0.23.0\n            Use idx[idx.duplicated()].unique() instead\n\n        Returns\n        -------\n        array-like\n            List of duplicated indexes.\n\n        See Also\n        --------\n        Index.duplicated : Return boolean array denoting duplicates.\n        Index.drop_duplicates : Return Index with duplicates removed.\n\n        Examples\n        --------\n\n        Works on different Index of types.\n\n        >>> pd.Index([1, 2, 2, 3, 3, 3, 4]).get_duplicates()  # doctest: +SKIP\n        [2, 3]\n\n        Note that for a DatetimeIndex, it does not return a list but a new\n        DatetimeIndex:\n\n        >>> dates = pd.to_datetime(['2018-01-01', '2018-01-02', '2018-01-03',\n        ...                         '2018-01-03', '2018-01-04', '2018-01-04'],\n        ...                        format='%Y-%m-%d')\n        >>> pd.Index(dates).get_duplicates()  # doctest: +SKIP\n        DatetimeIndex(['2018-01-03', '2018-01-04'],\n                      dtype='datetime64[ns]', freq=None)\n\n        Sorts duplicated elements even when indexes are unordered.\n\n        >>> pd.Index([1, 2, 3, 2, 3, 4, 3]).get_duplicates()  # doctest: +SKIP\n        [2, 3]\n\n        Return empty array-like structure when all elements are unique.\n\n        >>> pd.Index([1, 2, 3, 4]).get_duplicates()  # doctest: +SKIP\n        []\n        >>> dates = pd.to_datetime(['2018-01-01', '2018-01-02', '2018-01-03'],\n        ...                        format='%Y-%m-%d')\n        >>> pd.Index(dates).get_duplicates()  # doctest: +SKIP\n        DatetimeIndex([], dtype='datetime64[ns]', freq=None)\n        \"\"\"\n        warnings.warn(\"'get_duplicates' is deprecated and will be removed in \"\n                      \"a future release. You can use \"\n                      \"idx[idx.duplicated()].unique() instead\",\n                      FutureWarning, stacklevel=2)\n\n        return self[self.duplicated()].unique()\n\n    def _cleanup(self):\n        self._engine.clear_mapping()\n\n    @cache_readonly\n    def _constructor(self):\n        return type(self)\n\n    @cache_readonly\n    def _engine(self):\n        # property, for now, slow to look up\n        return self._engine_type(lambda: self._ndarray_values, len(self))\n\n    def _validate_index_level(self, level):\n        \"\"\"\n        Validate index level.\n\n        For single-level Index getting level number is a no-op, but some\n        verification must be done like in MultiIndex.\n\n        \"\"\"\n        if isinstance(level, int):\n            if level < 0 and level != -1:\n                raise IndexError(\"Too many levels: Index has only 1 level,\"\n                                 \" %d is not a valid level number\" % (level, ))\n            elif level > 0:\n                raise IndexError(\"Too many levels:\"\n                                 \" Index has only 1 level, not %d\" %\n                                 (level + 1))\n        elif level != self.name:\n            raise KeyError('Level %s must be same as name (%s)' %\n                           (level, self.name))\n\n    def _get_level_number(self, level):\n        self._validate_index_level(level)\n        return 0\n\n    @cache_readonly\n    def inferred_type(self):\n        \"\"\"\n        Return a string of the type inferred from the values.\n        \"\"\"\n        return lib.infer_dtype(self)\n\n    def _is_memory_usage_qualified(self):\n        \"\"\"\n        Return a boolean if we need a qualified .info display.\n        \"\"\"\n        return self.is_object()\n\n    def is_type_compatible(self, kind):\n        return kind == self.inferred_type\n\n    @cache_readonly\n    def is_all_dates(self):\n        if self._data is None:\n            return False\n        return is_datetime_array(ensure_object(self.values))\n\n    def __reduce__(self):\n        d = dict(data=self._data)\n        d.update(self._get_attributes_dict())\n        return _new_Index, (self.__class__, d), None\n\n    def __setstate__(self, state):\n        \"\"\"\n        Necessary for making this object picklable.\n        \"\"\"\n\n        if isinstance(state, dict):\n            self._data = state.pop('data')\n            for k, v in compat.iteritems(state):\n                setattr(self, k, v)\n\n        elif isinstance(state, tuple):\n\n            if len(state) == 2:\n                nd_state, own_state = state\n                data = np.empty(nd_state[1], dtype=nd_state[2])\n                np.ndarray.__setstate__(data, nd_state)\n                self.name = own_state[0]\n\n            else:  # pragma: no cover\n                data = np.empty(state)\n                np.ndarray.__setstate__(data, state)\n\n            self._data = data\n            self._reset_identity()\n        else:\n            raise Exception(\"invalid pickle state\")\n\n    _unpickle_compat = __setstate__\n\n    def __nonzero__(self):\n        raise ValueError(\"The truth value of a {0} is ambiguous. \"\n                         \"Use a.empty, a.bool(), a.item(), a.any() or a.all().\"\n                         .format(self.__class__.__name__))\n\n    __bool__ = __nonzero__\n\n    _index_shared_docs['__contains__'] = \"\"\"\n        Return a boolean if this key is IN the index.\n\n        Parameters\n        ----------\n        key : object\n\n        Returns\n        -------\n        boolean\n        \"\"\"\n\n    @Appender(_index_shared_docs['__contains__'] % _index_doc_kwargs)\n    def __contains__(self, key):\n        hash(key)\n        try:\n            return key in self._engine\n        except (OverflowError, TypeError, ValueError):\n            return False\n\n    _index_shared_docs['contains'] = \"\"\"\n        Return a boolean if this key is IN the index.\n\n        Parameters\n        ----------\n        key : object\n\n        Returns\n        -------\n        boolean\n        \"\"\"\n\n    @Appender(_index_shared_docs['contains'] % _index_doc_kwargs)\n    def contains(self, key):\n        hash(key)\n        try:\n            return key in self._engine\n        except (TypeError, ValueError):\n            return False\n\n    def __hash__(self):\n        raise TypeError(\"unhashable type: %r\" % type(self).__name__)\n\n    def __setitem__(self, key, value):\n        raise TypeError(\"Index does not support mutable operations\")\n\n    def __getitem__(self, key):\n        \"\"\"\n        Override numpy.ndarray's __getitem__ method to work as desired.\n\n        This function adds lists and Series as valid boolean indexers\n        (ndarrays only supports ndarray with dtype=bool).\n\n        If resulting ndim != 1, plain ndarray is returned instead of\n        corresponding `Index` subclass.\n\n        \"\"\"\n        # There's no custom logic to be implemented in __getslice__, so it's\n        # not overloaded intentionally.\n        getitem = self._data.__getitem__\n        promote = self._shallow_copy\n\n        if is_scalar(key):\n            key = com.cast_scalar_indexer(key)\n            return getitem(key)\n\n        if isinstance(key, slice):\n            # This case is separated from the conditional above to avoid\n            # pessimization of basic indexing.\n            return promote(getitem(key))\n\n        if com.is_bool_indexer(key):\n            key = np.asarray(key, dtype=bool)\n\n        key = com.values_from_object(key)\n        result = getitem(key)\n        if not is_scalar(result):\n            return promote(result)\n        else:\n            return result\n\n    def _can_hold_identifiers_and_holds_name(self, name):\n        \"\"\"\n        Faster check for ``name in self`` when we know `name` is a Python\n        identifier (e.g. in NDFrame.__getattr__, which hits this to support\n        . key lookup). For indexes that can't hold identifiers (everything\n        but object & categorical) we just return False.\n\n        https://github.com/pandas-dev/pandas/issues/19764\n        \"\"\"\n        if self.is_object() or self.is_categorical():\n            return name in self\n        return False\n\n    def append(self, other):\n        \"\"\"\n        Append a collection of Index options together.\n\n        Parameters\n        ----------\n        other : Index or list/tuple of indices\n\n        Returns\n        -------\n        appended : Index\n        \"\"\"\n\n        to_concat = [self]\n\n        if isinstance(other, (list, tuple)):\n            to_concat = to_concat + list(other)\n        else:\n            to_concat.append(other)\n\n        for obj in to_concat:\n            if not isinstance(obj, Index):\n                raise TypeError('all inputs must be Index')\n\n        names = {obj.name for obj in to_concat}\n        name = None if len(names) > 1 else self.name\n\n        return self._concat(to_concat, name)\n\n    def _concat(self, to_concat, name):\n\n        typs = _concat.get_dtype_kinds(to_concat)\n\n        if len(typs) == 1:\n            return self._concat_same_dtype(to_concat, name=name)\n        return _concat._concat_index_asobject(to_concat, name=name)\n\n    def _concat_same_dtype(self, to_concat, name):\n        \"\"\"\n        Concatenate to_concat which has the same class.\n        \"\"\"\n        # must be overridden in specific classes\n        return _concat._concat_index_asobject(to_concat, name)\n\n    _index_shared_docs['take'] = \"\"\"\n        Return a new %(klass)s of the values selected by the indices.\n\n        For internal compatibility with numpy arrays.\n\n        Parameters\n        ----------\n        indices : list\n            Indices to be taken\n        axis : int, optional\n            The axis over which to select values, always 0.\n        allow_fill : bool, default True\n        fill_value : bool, default None\n            If allow_fill=True and fill_value is not None, indices specified by\n            -1 is regarded as NA. If Index doesn't hold NA, raise ValueError\n\n        See Also\n        --------\n        numpy.ndarray.take\n        \"\"\"\n\n    @Appender(_index_shared_docs['take'] % _index_doc_kwargs)\n    def take(self, indices, axis=0, allow_fill=True,\n             fill_value=None, **kwargs):\n        if kwargs:\n            nv.validate_take(tuple(), kwargs)\n        indices = ensure_platform_int(indices)\n        if self._can_hold_na:\n            taken = self._assert_take_fillable(self.values, indices,\n                                               allow_fill=allow_fill,\n                                               fill_value=fill_value,\n                                               na_value=self._na_value)\n        else:\n            if allow_fill and fill_value is not None:\n                msg = 'Unable to fill values because {0} cannot contain NA'\n                raise ValueError(msg.format(self.__class__.__name__))\n            taken = self.values.take(indices)\n        return self._shallow_copy(taken)\n\n    def _assert_take_fillable(self, values, indices, allow_fill=True,\n                              fill_value=None, na_value=np.nan):\n        \"\"\"\n        Internal method to handle NA filling of take.\n        \"\"\"\n        indices = ensure_platform_int(indices)\n\n        # only fill if we are passing a non-None fill_value\n        if allow_fill and fill_value is not None:\n            if (indices < -1).any():\n                msg = ('When allow_fill=True and fill_value is not None, '\n                       'all indices must be >= -1')\n                raise ValueError(msg)\n            taken = algos.take(values,\n                               indices,\n                               allow_fill=allow_fill,\n                               fill_value=na_value)\n        else:\n            taken = values.take(indices)\n        return taken\n\n    @cache_readonly\n    def _isnan(self):\n        \"\"\"\n        Return if each value is NaN.\n        \"\"\"\n        if self._can_hold_na:\n            return isna(self)\n        else:\n            # shouldn't reach to this condition by checking hasnans beforehand\n            values = np.empty(len(self), dtype=np.bool_)\n            values.fill(False)\n            return values\n\n    @cache_readonly\n    def _nan_idxs(self):\n        if self._can_hold_na:\n            w, = self._isnan.nonzero()\n            return w\n        else:\n            return np.array([], dtype=np.int64)\n\n    @cache_readonly\n    def hasnans(self):\n        \"\"\"\n        Return if I have any nans; enables various perf speedups.\n        \"\"\"\n        if self._can_hold_na:\n            return bool(self._isnan.any())\n        else:\n            return False\n\n    def isna(self):\n        \"\"\"\n        Detect missing values.\n\n        Return a boolean same-sized object indicating if the values are NA.\n        NA values, such as ``None``, :attr:`numpy.NaN` or :attr:`pd.NaT`, get\n        mapped to ``True`` values.\n        Everything else get mapped to ``False`` values. Characters such as\n        empty strings `''` or :attr:`numpy.inf` are not considered NA values\n        (unless you set ``pandas.options.mode.use_inf_as_na = True``).\n\n        .. versionadded:: 0.20.0\n\n        Returns\n        -------\n        numpy.ndarray\n            A boolean array of whether my values are NA\n\n        See Also\n        --------\n        pandas.Index.notna : Boolean inverse of isna.\n        pandas.Index.dropna : Omit entries with missing values.\n        pandas.isna : Top-level isna.\n        Series.isna : Detect missing values in Series object.\n\n        Examples\n        --------\n        Show which entries in a pandas.Index are NA. The result is an\n        array.\n\n        >>> idx = pd.Index([5.2, 6.0, np.NaN])\n        >>> idx\n        Float64Index([5.2, 6.0, nan], dtype='float64')\n        >>> idx.isna()\n        array([False, False,  True], dtype=bool)\n\n        Empty strings are not considered NA values. None is considered an NA\n        value.\n\n        >>> idx = pd.Index(['black', '', 'red', None])\n        >>> idx\n        Index(['black', '', 'red', None], dtype='object')\n        >>> idx.isna()\n        array([False, False, False,  True], dtype=bool)\n\n        For datetimes, `NaT` (Not a Time) is considered as an NA value.\n\n        >>> idx = pd.DatetimeIndex([pd.Timestamp('1940-04-25'),\n        ...                         pd.Timestamp(''), None, pd.NaT])\n        >>> idx\n        DatetimeIndex(['1940-04-25', 'NaT', 'NaT', 'NaT'],\n                      dtype='datetime64[ns]', freq=None)\n        >>> idx.isna()\n        array([False,  True,  True,  True], dtype=bool)\n        \"\"\"\n        return self._isnan\n    isnull = isna\n\n    def notna(self):\n        \"\"\"\n        Detect existing (non-missing) values.\n\n        Return a boolean same-sized object indicating if the values are not NA.\n        Non-missing values get mapped to ``True``. Characters such as empty\n        strings ``''`` or :attr:`numpy.inf` are not considered NA values\n        (unless you set ``pandas.options.mode.use_inf_as_na = True``).\n        NA values, such as None or :attr:`numpy.NaN`, get mapped to ``False``\n        values.\n\n        .. versionadded:: 0.20.0\n\n        Returns\n        -------\n        numpy.ndarray\n            Boolean array to indicate which entries are not NA.\n\n        See Also\n        --------\n        Index.notnull : Alias of notna.\n        Index.isna: Inverse of notna.\n        pandas.notna : Top-level notna.\n\n        Examples\n        --------\n        Show which entries in an Index are not NA. The result is an\n        array.\n\n        >>> idx = pd.Index([5.2, 6.0, np.NaN])\n        >>> idx\n        Float64Index([5.2, 6.0, nan], dtype='float64')\n        >>> idx.notna()\n        array([ True,  True, False])\n\n        Empty strings are not considered NA values. None is considered a NA\n        value.\n\n        >>> idx = pd.Index(['black', '', 'red', None])\n        >>> idx\n        Index(['black', '', 'red', None], dtype='object')\n        >>> idx.notna()\n        array([ True,  True,  True, False])\n        \"\"\"\n        return ~self.isna()\n    notnull = notna\n\n    def putmask(self, mask, value):\n        \"\"\"\n        Return a new Index of the values set with the mask.\n\n        See Also\n        --------\n        numpy.ndarray.putmask\n        \"\"\"\n        values = self.values.copy()\n        try:\n            np.putmask(values, mask, self._convert_for_op(value))\n            return self._shallow_copy(values)\n        except (ValueError, TypeError) as err:\n            if is_object_dtype(self):\n                raise err\n\n            # coerces to object\n            return self.astype(object).putmask(mask, value)\n\n    def format(self, name=False, formatter=None, **kwargs):\n        \"\"\"\n        Render a string representation of the Index.\n        \"\"\"\n        header = []\n        if name:\n            header.append(pprint_thing(self.name,\n                                       escape_chars=('\\t', '\\r', '\\n')) if\n                          self.name is not None else '')\n\n        if formatter is not None:\n            return header + list(self.map(formatter))\n\n        return self._format_with_header(header, **kwargs)\n\n    def _format_with_header(self, header, na_rep='NaN', **kwargs):\n        values = self.values\n\n        from pandas.io.formats.format import format_array\n\n        if is_categorical_dtype(values.dtype):\n            values = np.array(values)\n\n        elif is_object_dtype(values.dtype):\n            values = lib.maybe_convert_objects(values, safe=1)\n\n        if is_object_dtype(values.dtype):\n            result = [pprint_thing(x, escape_chars=('\\t', '\\r', '\\n'))\n                      for x in values]\n\n            # could have nans\n            mask = isna(values)\n            if mask.any():\n                result = np.array(result)\n                result[mask] = na_rep\n                result = result.tolist()\n\n        else:\n            result = _trim_front(format_array(values, None, justify='left'))\n        return header + result\n\n    def to_native_types(self, slicer=None, **kwargs):\n        \"\"\"\n        Format specified values of `self` and return them.\n\n        Parameters\n        ----------\n        slicer : int, array-like\n            An indexer into `self` that specifies which values\n            are used in the formatting process.\n        kwargs : dict\n            Options for specifying how the values should be formatted.\n            These options include the following:\n\n            1) na_rep : str\n                The value that serves as a placeholder for NULL values\n            2) quoting : bool or None\n                Whether or not there are quoted values in `self`\n            3) date_format : str\n                The format used to represent date-like values\n        \"\"\"\n\n        values = self\n        if slicer is not None:\n            values = values[slicer]\n        return values._format_native_types(**kwargs)\n\n    def _format_native_types(self, na_rep='', quoting=None, **kwargs):\n        \"\"\"\n        Actually format specific types of the index.\n        \"\"\"\n        mask = isna(self)\n        if not self.is_object() and not quoting:\n            values = np.asarray(self).astype(str)\n        else:\n            values = np.array(self, dtype=object, copy=True)\n\n        values[mask] = na_rep\n        return values\n\n    def equals(self, other):\n        \"\"\"\n        Determines if two Index objects contain the same elements.\n        \"\"\"\n        if self.is_(other):\n            return True\n\n        if not isinstance(other, Index):\n            return False\n\n        if is_object_dtype(self) and not is_object_dtype(other):\n            # if other is not object, use other's logic for coercion\n            return other.equals(self)\n\n        try:\n            return array_equivalent(com.values_from_object(self),\n                                    com.values_from_object(other))\n        except Exception:\n            return False\n\n    def identical(self, other):\n        \"\"\"\n        Similar to equals, but check that other comparable attributes are\n        also equal.\n        \"\"\"\n        return (self.equals(other) and\n                all((getattr(self, c, None) == getattr(other, c, None)\n                     for c in self._comparables)) and\n                type(self) == type(other))\n\n    def asof(self, label):\n        \"\"\"\n        Return the label from the index, or, if not present, the previous one.\n\n        Assuming that the index is sorted, return the passed index label if it\n        is in the index, or return the previous index label if the passed one\n        is not in the index.\n\n        Parameters\n        ----------\n        label : object\n            The label up to which the method returns the latest index label.\n\n        Returns\n        -------\n        object\n            The passed label if it is in the index. The previous label if the\n            passed label is not in the sorted index or `NaN` if there is no\n            such label.\n\n        See Also\n        --------\n        Series.asof : Return the latest value in a Series up to the\n            passed index.\n        merge_asof : Perform an asof merge (similar to left join but it\n            matches on nearest key rather than equal key).\n        Index.get_loc : An `asof` is a thin wrapper around `get_loc`\n            with method='pad'.\n\n        Examples\n        --------\n        `Index.asof` returns the latest index label up to the passed label.\n\n        >>> idx = pd.Index(['2013-12-31', '2014-01-02', '2014-01-03'])\n        >>> idx.asof('2014-01-01')\n        '2013-12-31'\n\n        If the label is in the index, the method returns the passed label.\n\n        >>> idx.asof('2014-01-02')\n        '2014-01-02'\n\n        If all of the labels in the index are later than the passed label,\n        NaN is returned.\n\n        >>> idx.asof('1999-01-02')\n        nan\n\n        If the index is not sorted, an error is raised.\n\n        >>> idx_not_sorted = pd.Index(['2013-12-31', '2015-01-02',\n        ...                            '2014-01-03'])\n        >>> idx_not_sorted.asof('2013-12-31')\n        Traceback (most recent call last):\n        ValueError: index must be monotonic increasing or decreasing\n        \"\"\"\n        try:\n            loc = self.get_loc(label, method='pad')\n        except KeyError:\n            return self._na_value\n        else:\n            if isinstance(loc, slice):\n                loc = loc.indices(len(self))[-1]\n            return self[loc]\n\n    def asof_locs(self, where, mask):\n        \"\"\"\n        Finds the locations (indices) of the labels from the index for\n        every entry in the `where` argument.\n\n        As in the `asof` function, if the label (a particular entry in\n        `where`) is not in the index, the latest index label upto the\n        passed label is chosen and its index returned.\n\n        If all of the labels in the index are later than a label in `where`,\n        -1 is returned.\n\n        `mask` is used to ignore NA values in the index during calculation.\n\n        Parameters\n        ----------\n        where : Index\n            An Index consisting of an array of timestamps.\n        mask : array-like\n            Array of booleans denoting where values in the original\n            data are not NA.\n\n        Returns\n        -------\n        numpy.ndarray\n            An array of locations (indices) of the labels from the Index\n            which correspond to the return values of the `asof` function\n            for every element in `where`.\n        \"\"\"\n        locs = self.values[mask].searchsorted(where.values, side='right')\n        locs = np.where(locs > 0, locs - 1, 0)\n\n        result = np.arange(len(self))[mask].take(locs)\n\n        first = mask.argmax()\n        result[(locs == 0) & (where.values < self.values[first])] = -1\n\n        return result\n\n    def sort_values(self, return_indexer=False, ascending=True):\n        \"\"\"\n        Return a sorted copy of the index.\n\n        Return a sorted copy of the index, and optionally return the indices\n        that sorted the index itself.\n\n        Parameters\n        ----------\n        return_indexer : bool, default False\n            Should the indices that would sort the index be returned.\n        ascending : bool, default True\n            Should the index values be sorted in an ascending order.\n\n        Returns\n        -------\n        sorted_index : pandas.Index\n            Sorted copy of the index.\n        indexer : numpy.ndarray, optional\n            The indices that the index itself was sorted by.\n\n        See Also\n        --------\n        pandas.Series.sort_values : Sort values of a Series.\n        pandas.DataFrame.sort_values : Sort values in a DataFrame.\n\n        Examples\n        --------\n        >>> idx = pd.Index([10, 100, 1, 1000])\n        >>> idx\n        Int64Index([10, 100, 1, 1000], dtype='int64')\n\n        Sort values in ascending order (default behavior).\n\n        >>> idx.sort_values()\n        Int64Index([1, 10, 100, 1000], dtype='int64')\n\n        Sort values in descending order, and also get the indices `idx` was\n        sorted by.\n\n        >>> idx.sort_values(ascending=False, return_indexer=True)\n        (Int64Index([1000, 100, 10, 1], dtype='int64'), array([3, 1, 0, 2]))\n        \"\"\"\n        _as = self.argsort()\n        if not ascending:\n            _as = _as[::-1]\n\n        sorted_index = self.take(_as)\n\n        if return_indexer:\n            return sorted_index, _as\n        else:\n            return sorted_index\n\n    def sort(self, *args, **kwargs):\n        raise TypeError(\"cannot sort an Index object in-place, use \"\n                        \"sort_values instead\")\n\n    def sortlevel(self, level=None, ascending=True, sort_remaining=None):\n        \"\"\"\n        For internal compatibility with with the Index API.\n\n        Sort the Index. This is for compat with MultiIndex\n\n        Parameters\n        ----------\n        ascending : boolean, default True\n            False to sort in descending order\n\n        level, sort_remaining are compat parameters\n\n        Returns\n        -------\n        sorted_index : Index\n        \"\"\"\n        return self.sort_values(return_indexer=True, ascending=ascending)\n\n    def shift(self, periods=1, freq=None):\n        \"\"\"\n        Shift index by desired number of time frequency increments.\n\n        This method is for shifting the values of datetime-like indexes\n        by a specified time increment a given number of times.\n\n        Parameters\n        ----------\n        periods : int, default 1\n            Number of periods (or increments) to shift by,\n            can be positive or negative.\n        freq : pandas.DateOffset, pandas.Timedelta or string, optional\n            Frequency increment to shift by.\n            If None, the index is shifted by its own `freq` attribute.\n            Offset aliases are valid strings, e.g., 'D', 'W', 'M' etc.\n\n        Returns\n        -------\n        pandas.Index\n            shifted index\n\n        See Also\n        --------\n        Series.shift : Shift values of Series.\n\n        Examples\n        --------\n        Put the first 5 month starts of 2011 into an index.\n\n        >>> month_starts = pd.date_range('1/1/2011', periods=5, freq='MS')\n        >>> month_starts\n        DatetimeIndex(['2011-01-01', '2011-02-01', '2011-03-01', '2011-04-01',\n                       '2011-05-01'],\n                      dtype='datetime64[ns]', freq='MS')\n\n        Shift the index by 10 days.\n\n        >>> month_starts.shift(10, freq='D')\n        DatetimeIndex(['2011-01-11', '2011-02-11', '2011-03-11', '2011-04-11',\n                       '2011-05-11'],\n                      dtype='datetime64[ns]', freq=None)\n\n        The default value of `freq` is the `freq` attribute of the index,\n        which is 'MS' (month start) in this example.\n\n        >>> month_starts.shift(10)\n        DatetimeIndex(['2011-11-01', '2011-12-01', '2012-01-01', '2012-02-01',\n                       '2012-03-01'],\n                      dtype='datetime64[ns]', freq='MS')\n\n        Notes\n        -----\n        This method is only implemented for datetime-like index classes,\n        i.e., DatetimeIndex, PeriodIndex and TimedeltaIndex.\n        \"\"\"\n        raise NotImplementedError(\"Not supported for type %s\" %\n                                  type(self).__name__)\n\n    def argsort(self, *args, **kwargs):\n        \"\"\"\n        Return the integer indices that would sort the index.\n\n        Parameters\n        ----------\n        *args\n            Passed to `numpy.ndarray.argsort`.\n        **kwargs\n            Passed to `numpy.ndarray.argsort`.\n\n        Returns\n        -------\n        numpy.ndarray\n            Integer indices that would sort the index if used as\n            an indexer.\n\n        See Also\n        --------\n        numpy.argsort : Similar method for NumPy arrays.\n        Index.sort_values : Return sorted copy of Index.\n\n        Examples\n        --------\n        >>> idx = pd.Index(['b', 'a', 'd', 'c'])\n        >>> idx\n        Index(['b', 'a', 'd', 'c'], dtype='object')\n\n        >>> order = idx.argsort()\n        >>> order\n        array([1, 0, 3, 2])\n\n        >>> idx[order]\n        Index(['a', 'b', 'c', 'd'], dtype='object')\n        \"\"\"\n        result = self.asi8\n        if result is None:\n            result = np.array(self)\n        return result.argsort(*args, **kwargs)\n\n    def __add__(self, other):\n        if isinstance(other, (ABCSeries, ABCDataFrame)):\n            return NotImplemented\n        return Index(np.array(self) + other)\n\n    def __radd__(self, other):\n        return Index(other + np.array(self))\n\n    def __iadd__(self, other):\n        # alias for __add__\n        return self + other\n\n    def __sub__(self, other):\n        return Index(np.array(self) - other)\n\n    def __rsub__(self, other):\n        return Index(other - np.array(self))\n\n    def __and__(self, other):\n        return self.intersection(other)\n\n    def __or__(self, other):\n        return self.union(other)\n\n    def __xor__(self, other):\n        return self.symmetric_difference(other)\n\n    def _get_reconciled_name_object(self, other):\n        \"\"\"\n        If the result of a set operation will be self,\n        return self, unless the name changes, in which\n        case make a shallow copy of self.\n        \"\"\"\n        name = get_op_result_name(self, other)\n        if self.name != name:\n            return self._shallow_copy(name=name)\n        return self\n\n    def union(self, other):\n        \"\"\"\n        Form the union of two Index objects and sorts if possible.\n\n        Parameters\n        ----------\n        other : Index or array-like\n\n        Returns\n        -------\n        union : Index\n\n        Examples\n        --------\n\n        >>> idx1 = pd.Index([1, 2, 3, 4])\n        >>> idx2 = pd.Index([3, 4, 5, 6])\n        >>> idx1.union(idx2)\n        Int64Index([1, 2, 3, 4, 5, 6], dtype='int64')\n        \"\"\"\n        self._assert_can_do_setop(other)\n        other = ensure_index(other)\n\n        if len(other) == 0 or self.equals(other):\n            return self._get_reconciled_name_object(other)\n\n        if len(self) == 0:\n            return other._get_reconciled_name_object(self)\n\n        # TODO: is_dtype_union_equal is a hack around\n        # 1. buggy set ops with duplicates (GH #13432)\n        # 2. CategoricalIndex lacking setops (GH #10186)\n        # Once those are fixed, this workaround can be removed\n        if not is_dtype_union_equal(self.dtype, other.dtype):\n            this = self.astype('O')\n            other = other.astype('O')\n            return this.union(other)\n\n        # TODO(EA): setops-refactor, clean all this up\n        if is_period_dtype(self) or is_datetime64tz_dtype(self):\n            lvals = self._ndarray_values\n        else:\n            lvals = self._values\n        if is_period_dtype(other) or is_datetime64tz_dtype(other):\n            rvals = other._ndarray_values\n        else:\n            rvals = other._values\n\n        if self.is_monotonic and other.is_monotonic:\n            try:\n                result = self._outer_indexer(lvals, rvals)[0]\n            except TypeError:\n                # incomparable objects\n                result = list(lvals)\n\n                # worth making this faster? a very unusual case\n                value_set = set(lvals)\n                result.extend([x for x in rvals if x not in value_set])\n        else:\n            indexer = self.get_indexer(other)\n            indexer, = (indexer == -1).nonzero()\n\n            if len(indexer) > 0:\n                other_diff = algos.take_nd(rvals, indexer,\n                                           allow_fill=False)\n                result = _concat._concat_compat((lvals, other_diff))\n\n                try:\n                    lvals[0] < other_diff[0]\n                except TypeError as e:\n                    warnings.warn(\"%s, sort order is undefined for \"\n                                  \"incomparable objects\" % e, RuntimeWarning,\n                                  stacklevel=3)\n                else:\n                    types = frozenset((self.inferred_type,\n                                       other.inferred_type))\n                    if not types & _unsortable_types:\n                        result.sort()\n\n            else:\n                result = lvals\n\n                try:\n                    result = np.sort(result)\n                except TypeError as e:\n                    warnings.warn(\"%s, sort order is undefined for \"\n                                  \"incomparable objects\" % e, RuntimeWarning,\n                                  stacklevel=3)\n\n        # for subclasses\n        return self._wrap_setop_result(other, result)\n\n    def _wrap_setop_result(self, other, result):\n        return self._constructor(result, name=get_op_result_name(self, other))\n\n    def intersection(self, other):\n        \"\"\"\n        Form the intersection of two Index objects.\n\n        This returns a new Index with elements common to the index and `other`,\n        preserving the order of the calling index.\n\n        Parameters\n        ----------\n        other : Index or array-like\n\n        Returns\n        -------\n        intersection : Index\n\n        Examples\n        --------\n\n        >>> idx1 = pd.Index([1, 2, 3, 4])\n        >>> idx2 = pd.Index([3, 4, 5, 6])\n        >>> idx1.intersection(idx2)\n        Int64Index([3, 4], dtype='int64')\n        \"\"\"\n        self._assert_can_do_setop(other)\n        other = ensure_index(other)\n\n        if self.equals(other):\n            return self._get_reconciled_name_object(other)\n\n        if not is_dtype_equal(self.dtype, other.dtype):\n            this = self.astype('O')\n            other = other.astype('O')\n            return this.intersection(other)\n\n        # TODO(EA): setops-refactor, clean all this up\n        if is_period_dtype(self):\n            lvals = self._ndarray_values\n        else:\n            lvals = self._values\n        if is_period_dtype(other):\n            rvals = other._ndarray_values\n        else:\n            rvals = other._values\n\n        if self.is_monotonic and other.is_monotonic:\n            try:\n                result = self._inner_indexer(lvals, rvals)[0]\n                return self._wrap_setop_result(other, result)\n            except TypeError:\n                pass\n\n        try:\n            indexer = Index(rvals).get_indexer(lvals)\n            indexer = indexer.take((indexer != -1).nonzero()[0])\n        except Exception:\n            # duplicates\n            indexer = algos.unique1d(\n                Index(rvals).get_indexer_non_unique(lvals)[0])\n            indexer = indexer[indexer != -1]\n\n        taken = other.take(indexer)\n        if self.name != other.name:\n            taken.name = None\n        return taken\n\n    def difference(self, other, sort=True):\n        \"\"\"\n        Return a new Index with elements from the index that are not in\n        `other`.\n\n        This is the set difference of two Index objects.\n\n        Parameters\n        ----------\n        other : Index or array-like\n        sort : bool, default True\n            Sort the resulting index if possible\n\n            .. versionadded:: 0.24.0\n\n        Returns\n        -------\n        difference : Index\n\n        Examples\n        --------\n\n        >>> idx1 = pd.Index([2, 1, 3, 4])\n        >>> idx2 = pd.Index([3, 4, 5, 6])\n        >>> idx1.difference(idx2)\n        Int64Index([1, 2], dtype='int64')\n        >>> idx1.difference(idx2, sort=False)\n        Int64Index([2, 1], dtype='int64')\n        \"\"\"\n        self._assert_can_do_setop(other)\n\n        if self.equals(other):\n            # pass an empty np.ndarray with the appropriate dtype\n            return self._shallow_copy(self._data[:0])\n\n        other, result_name = self._convert_can_do_setop(other)\n\n        this = self._get_unique_index()\n\n        indexer = this.get_indexer(other)\n        indexer = indexer.take((indexer != -1).nonzero()[0])\n\n        label_diff = np.setdiff1d(np.arange(this.size), indexer,\n                                  assume_unique=True)\n        the_diff = this.values.take(label_diff)\n        if sort:\n            try:\n                the_diff = sorting.safe_sort(the_diff)\n            except TypeError:\n                pass\n\n        return this._shallow_copy(the_diff, name=result_name, freq=None)\n\n    def symmetric_difference(self, other, result_name=None):\n        \"\"\"\n        Compute the symmetric difference of two Index objects.\n\n        It's sorted if sorting is possible.\n\n        Parameters\n        ----------\n        other : Index or array-like\n        result_name : str\n\n        Returns\n        -------\n        symmetric_difference : Index\n\n        Notes\n        -----\n        ``symmetric_difference`` contains elements that appear in either\n        ``idx1`` or ``idx2`` but not both. Equivalent to the Index created by\n        ``idx1.difference(idx2) | idx2.difference(idx1)`` with duplicates\n        dropped.\n\n        Examples\n        --------\n        >>> idx1 = pd.Index([1, 2, 3, 4])\n        >>> idx2 = pd.Index([2, 3, 4, 5])\n        >>> idx1.symmetric_difference(idx2)\n        Int64Index([1, 5], dtype='int64')\n\n        You can also use the ``^`` operator:\n\n        >>> idx1 ^ idx2\n        Int64Index([1, 5], dtype='int64')\n        \"\"\"\n        self._assert_can_do_setop(other)\n        other, result_name_update = self._convert_can_do_setop(other)\n        if result_name is None:\n            result_name = result_name_update\n\n        this = self._get_unique_index()\n        other = other._get_unique_index()\n        indexer = this.get_indexer(other)\n\n        # {this} minus {other}\n        common_indexer = indexer.take((indexer != -1).nonzero()[0])\n        left_indexer = np.setdiff1d(np.arange(this.size), common_indexer,\n                                    assume_unique=True)\n        left_diff = this.values.take(left_indexer)\n\n        # {other} minus {this}\n        right_indexer = (indexer == -1).nonzero()[0]\n        right_diff = other.values.take(right_indexer)\n\n        the_diff = _concat._concat_compat([left_diff, right_diff])\n        try:\n            the_diff = sorting.safe_sort(the_diff)\n        except TypeError:\n            pass\n\n        attribs = self._get_attributes_dict()\n        attribs['name'] = result_name\n        if 'freq' in attribs:\n            attribs['freq'] = None\n        return self._shallow_copy_with_infer(the_diff, **attribs)\n\n    def _get_unique_index(self, dropna=False):\n        \"\"\"\n        Returns an index containing unique values.\n\n        Parameters\n        ----------\n        dropna : bool\n            If True, NaN values are dropped.\n\n        Returns\n        -------\n        uniques : index\n        \"\"\"\n        if self.is_unique and not dropna:\n            return self\n\n        values = self.values\n\n        if not self.is_unique:\n            values = self.unique()\n\n        if dropna:\n            try:\n                if self.hasnans:\n                    values = values[~isna(values)]\n            except NotImplementedError:\n                pass\n\n        return self._shallow_copy(values)\n\n    _index_shared_docs['get_loc'] = \"\"\"\n        Get integer location, slice or boolean mask for requested label.\n\n        Parameters\n        ----------\n        key : label\n        method : {None, 'pad'/'ffill', 'backfill'/'bfill', 'nearest'}, optional\n            * default: exact matches only.\n            * pad / ffill: find the PREVIOUS index value if no exact match.\n            * backfill / bfill: use NEXT index value if no exact match\n            * nearest: use the NEAREST index value if no exact match. Tied\n              distances are broken by preferring the larger index value.\n        tolerance : optional\n            Maximum distance from index value for inexact matches. The value of\n            the index at the matching location most satisfy the equation\n            ``abs(index[loc] - key) <= tolerance``.\n\n            Tolerance may be a scalar\n            value, which applies the same tolerance to all values, or\n            list-like, which applies variable tolerance per element. List-like\n            includes list, tuple, array, Series, and must be the same size as\n            the index and its dtype must exactly match the index's type.\n\n            .. versionadded:: 0.21.0 (list-like tolerance)\n\n        Returns\n        -------\n        loc : int if unique index, slice if monotonic index, else mask\n\n        Examples\n        ---------\n        >>> unique_index = pd.Index(list('abc'))\n        >>> unique_index.get_loc('b')\n        1\n\n        >>> monotonic_index = pd.Index(list('abbc'))\n        >>> monotonic_index.get_loc('b')\n        slice(1, 3, None)\n\n        >>> non_monotonic_index = pd.Index(list('abcb'))\n        >>> non_monotonic_index.get_loc('b')\n        array([False,  True, False,  True], dtype=bool)\n        \"\"\"\n\n    @Appender(_index_shared_docs['get_loc'])\n    def get_loc(self, key, method=None, tolerance=None):\n        if method is None:\n            if tolerance is not None:\n                raise ValueError('tolerance argument only valid if using pad, '\n                                 'backfill or nearest lookups')\n            try:\n                return self._engine.get_loc(key)\n            except KeyError:\n                return self._engine.get_loc(self._maybe_cast_indexer(key))\n        indexer = self.get_indexer([key], method=method, tolerance=tolerance)\n        if indexer.ndim > 1 or indexer.size > 1:\n            raise TypeError('get_loc requires scalar valued input')\n        loc = indexer.item()\n        if loc == -1:\n            raise KeyError(key)\n        return loc\n\n    def get_value(self, series, key):\n        \"\"\"\n        Fast lookup of value from 1-dimensional ndarray. Only use this if you\n        know what you're doing.\n        \"\"\"\n\n        # if we have something that is Index-like, then\n        # use this, e.g. DatetimeIndex\n        s = getattr(series, '_values', None)\n        if isinstance(s, (ExtensionArray, Index)) and is_scalar(key):\n            # GH 20882, 21257\n            # Unify Index and ExtensionArray treatment\n            # First try to convert the key to a location\n            # If that fails, raise a KeyError if an integer\n            # index, otherwise, see if key is an integer, and\n            # try that\n            try:\n                iloc = self.get_loc(key)\n                return s[iloc]\n            except KeyError:\n                if (len(self) > 0 and\n                        (self.holds_integer() or self.is_boolean())):\n                    raise\n                elif is_integer(key):\n                    return s[key]\n\n        s = com.values_from_object(series)\n        k = com.values_from_object(key)\n\n        k = self._convert_scalar_indexer(k, kind='getitem')\n        try:\n            return self._engine.get_value(s, k,\n                                          tz=getattr(series.dtype, 'tz', None))\n        except KeyError as e1:\n            if len(self) > 0 and (self.holds_integer() or self.is_boolean()):\n                raise\n\n            try:\n                return libindex.get_value_box(s, key)\n            except IndexError:\n                raise\n            except TypeError:\n                # generator/iterator-like\n                if is_iterator(key):\n                    raise InvalidIndexError(key)\n                else:\n                    raise e1\n            except Exception:  # pragma: no cover\n                raise e1\n        except TypeError:\n            # python 3\n            if is_scalar(key):  # pragma: no cover\n                raise IndexError(key)\n            raise InvalidIndexError(key)\n\n    def set_value(self, arr, key, value):\n        \"\"\"\n        Fast lookup of value from 1-dimensional ndarray.\n\n        Notes\n        -----\n        Only use this if you know what you're doing.\n        \"\"\"\n        self._engine.set_value(com.values_from_object(arr),\n                               com.values_from_object(key), value)\n\n    def _get_level_values(self, level):\n        \"\"\"\n        Return an Index of values for requested level.\n\n        This is primarily useful to get an individual level of values from a\n        MultiIndex, but is provided on Index as well for compatability.\n\n        Parameters\n        ----------\n        level : int or str\n            It is either the integer position or the name of the level.\n\n        Returns\n        -------\n        values : Index\n            Calling object, as there is only one level in the Index.\n\n        See Also\n        --------\n        MultiIndex.get_level_values : Get values for a level of a MultiIndex.\n\n        Notes\n        -----\n        For Index, level should be 0, since there are no multiple levels.\n\n        Examples\n        --------\n\n        >>> idx = pd.Index(list('abc'))\n        >>> idx\n        Index(['a', 'b', 'c'], dtype='object')\n\n        Get level values by supplying `level` as integer:\n\n        >>> idx.get_level_values(0)\n        Index(['a', 'b', 'c'], dtype='object')\n        \"\"\"\n        self._validate_index_level(level)\n        return self\n\n    get_level_values = _get_level_values\n\n    def droplevel(self, level=0):\n        \"\"\"\n        Return index with requested level(s) removed.\n\n        If resulting index has only 1 level left, the result will be\n        of Index type, not MultiIndex.\n\n        .. versionadded:: 0.23.1 (support for non-MultiIndex)\n\n        Parameters\n        ----------\n        level : int, str, or list-like, default 0\n            If a string is given, must be the name of a level\n            If list-like, elements must be names or indexes of levels.\n\n        Returns\n        -------\n        index : Index or MultiIndex\n        \"\"\"\n        if not isinstance(level, (tuple, list)):\n            level = [level]\n\n        levnums = sorted(self._get_level_number(lev) for lev in level)[::-1]\n\n        if len(level) == 0:\n            return self\n        if len(level) >= self.nlevels:\n            raise ValueError(\"Cannot remove {} levels from an index with {} \"\n                             \"levels: at least one level must be \"\n                             \"left.\".format(len(level), self.nlevels))\n        # The two checks above guarantee that here self is a MultiIndex\n\n        new_levels = list(self.levels)\n        new_labels = list(self.labels)\n        new_names = list(self.names)\n\n        for i in levnums:\n            new_levels.pop(i)\n            new_labels.pop(i)\n            new_names.pop(i)\n\n        if len(new_levels) == 1:\n\n            # set nan if needed\n            mask = new_labels[0] == -1\n            result = new_levels[0].take(new_labels[0])\n            if mask.any():\n                result = result.putmask(mask, np.nan)\n\n            result.name = new_names[0]\n            return result\n        else:\n            from .multi import MultiIndex\n            return MultiIndex(levels=new_levels, labels=new_labels,\n                              names=new_names, verify_integrity=False)\n\n    _index_shared_docs['get_indexer'] = \"\"\"\n        Compute indexer and mask for new index given the current index. The\n        indexer should be then used as an input to ndarray.take to align the\n        current data to the new index.\n\n        Parameters\n        ----------\n        target : %(target_klass)s\n        method : {None, 'pad'/'ffill', 'backfill'/'bfill', 'nearest'}, optional\n            * default: exact matches only.\n            * pad / ffill: find the PREVIOUS index value if no exact match.\n            * backfill / bfill: use NEXT index value if no exact match\n            * nearest: use the NEAREST index value if no exact match. Tied\n              distances are broken by preferring the larger index value.\n        limit : int, optional\n            Maximum number of consecutive labels in ``target`` to match for\n            inexact matches.\n        tolerance : optional\n            Maximum distance between original and new labels for inexact\n            matches. The values of the index at the matching locations most\n            satisfy the equation ``abs(index[indexer] - target) <= tolerance``.\n\n            Tolerance may be a scalar value, which applies the same tolerance\n            to all values, or list-like, which applies variable tolerance per\n            element. List-like includes list, tuple, array, Series, and must be\n            the same size as the index and its dtype must exactly match the\n            index's type.\n\n            .. versionadded:: 0.21.0 (list-like tolerance)\n\n        Returns\n        -------\n        indexer : ndarray of int\n            Integers from 0 to n - 1 indicating that the index at these\n            positions matches the corresponding target values. Missing values\n            in the target are marked by -1.\n\n        Examples\n        --------\n        >>> index = pd.Index(['c', 'a', 'b'])\n        >>> index.get_indexer(['a', 'b', 'x'])\n        array([ 1,  2, -1])\n\n        Notice that the return value is an array of locations in ``index``\n        and ``x`` is marked by -1, as it is not in ``index``.\n        \"\"\"\n\n    @Appender(_index_shared_docs['get_indexer'] % _index_doc_kwargs)\n    def get_indexer(self, target, method=None, limit=None, tolerance=None):\n        method = missing.clean_reindex_fill_method(method)\n        target = ensure_index(target)\n        if tolerance is not None:\n            tolerance = self._convert_tolerance(tolerance, target)\n\n        # Treat boolean labels passed to a numeric index as not found. Without\n        # this fix False and True would be treated as 0 and 1 respectively.\n        # (GH #16877)\n        if target.is_boolean() and self.is_numeric():\n            return ensure_platform_int(np.repeat(-1, target.size))\n\n        pself, ptarget = self._maybe_promote(target)\n        if pself is not self or ptarget is not target:\n            return pself.get_indexer(ptarget, method=method, limit=limit,\n                                     tolerance=tolerance)\n\n        if not is_dtype_equal(self.dtype, target.dtype):\n            this = self.astype(object)\n            target = target.astype(object)\n            return this.get_indexer(target, method=method, limit=limit,\n                                    tolerance=tolerance)\n\n        if not self.is_unique:\n            raise InvalidIndexError('Reindexing only valid with uniquely'\n                                    ' valued Index objects')\n\n        if method == 'pad' or method == 'backfill':\n            indexer = self._get_fill_indexer(target, method, limit, tolerance)\n        elif method == 'nearest':\n            indexer = self._get_nearest_indexer(target, limit, tolerance)\n        else:\n            if tolerance is not None:\n                raise ValueError('tolerance argument only valid if doing pad, '\n                                 'backfill or nearest reindexing')\n            if limit is not None:\n                raise ValueError('limit argument only valid if doing pad, '\n                                 'backfill or nearest reindexing')\n\n            indexer = self._engine.get_indexer(target._ndarray_values)\n\n        return ensure_platform_int(indexer)\n\n    def _convert_tolerance(self, tolerance, target):\n        # override this method on subclasses\n        tolerance = np.asarray(tolerance)\n        if target.size != tolerance.size and tolerance.size > 1:\n            raise ValueError('list-like tolerance size must match '\n                             'target index size')\n        return tolerance\n\n    def _get_fill_indexer(self, target, method, limit=None, tolerance=None):\n        if self.is_monotonic_increasing and target.is_monotonic_increasing:\n            method = (self._engine.get_pad_indexer if method == 'pad' else\n                      self._engine.get_backfill_indexer)\n            indexer = method(target._ndarray_values, limit)\n        else:\n            indexer = self._get_fill_indexer_searchsorted(target, method,\n                                                          limit)\n        if tolerance is not None:\n            indexer = self._filter_indexer_tolerance(target._ndarray_values,\n                                                     indexer,\n                                                     tolerance)\n        return indexer\n\n    def _get_fill_indexer_searchsorted(self, target, method, limit=None):\n        \"\"\"\n        Fallback pad/backfill get_indexer that works for monotonic decreasing\n        indexes and non-monotonic targets.\n        \"\"\"\n        if limit is not None:\n            raise ValueError('limit argument for %r method only well-defined '\n                             'if index and target are monotonic' % method)\n\n        side = 'left' if method == 'pad' else 'right'\n\n        # find exact matches first (this simplifies the algorithm)\n        indexer = self.get_indexer(target)\n        nonexact = (indexer == -1)\n        indexer[nonexact] = self._searchsorted_monotonic(target[nonexact],\n                                                         side)\n        if side == 'left':\n            # searchsorted returns \"indices into a sorted array such that,\n            # if the corresponding elements in v were inserted before the\n            # indices, the order of a would be preserved\".\n            # Thus, we need to subtract 1 to find values to the left.\n            indexer[nonexact] -= 1\n            # This also mapped not found values (values of 0 from\n            # np.searchsorted) to -1, which conveniently is also our\n            # sentinel for missing values\n        else:\n            # Mark indices to the right of the largest value as not found\n            indexer[indexer == len(self)] = -1\n        return indexer\n\n    def _get_nearest_indexer(self, target, limit, tolerance):\n        \"\"\"\n        Get the indexer for the nearest index labels; requires an index with\n        values that can be subtracted from each other (e.g., not strings or\n        tuples).\n        \"\"\"\n        left_indexer = self.get_indexer(target, 'pad', limit=limit)\n        right_indexer = self.get_indexer(target, 'backfill', limit=limit)\n\n        target = np.asarray(target)\n        left_distances = abs(self.values[left_indexer] - target)\n        right_distances = abs(self.values[right_indexer] - target)\n\n        op = operator.lt if self.is_monotonic_increasing else operator.le\n        indexer = np.where(op(left_distances, right_distances) |\n                           (right_indexer == -1), left_indexer, right_indexer)\n        if tolerance is not None:\n            indexer = self._filter_indexer_tolerance(target, indexer,\n                                                     tolerance)\n        return indexer\n\n    def _filter_indexer_tolerance(self, target, indexer, tolerance):\n        distance = abs(self.values[indexer] - target)\n        indexer = np.where(distance <= tolerance, indexer, -1)\n        return indexer\n\n    _index_shared_docs['get_indexer_non_unique'] = \"\"\"\n        Compute indexer and mask for new index given the current index. The\n        indexer should be then used as an input to ndarray.take to align the\n        current data to the new index.\n\n        Parameters\n        ----------\n        target : %(target_klass)s\n\n        Returns\n        -------\n        indexer : ndarray of int\n            Integers from 0 to n - 1 indicating that the index at these\n            positions matches the corresponding target values. Missing values\n            in the target are marked by -1.\n        missing : ndarray of int\n            An indexer into the target of the values not found.\n            These correspond to the -1 in the indexer array\n        \"\"\"\n\n    @Appender(_index_shared_docs['get_indexer_non_unique'] % _index_doc_kwargs)\n    def get_indexer_non_unique(self, target):\n        target = ensure_index(target)\n        if is_categorical(target):\n            target = target.astype(target.dtype.categories.dtype)\n        pself, ptarget = self._maybe_promote(target)\n        if pself is not self or ptarget is not target:\n            return pself.get_indexer_non_unique(ptarget)\n\n        if self.is_all_dates:\n            self = Index(self.asi8)\n            tgt_values = target.asi8\n        else:\n            tgt_values = target._ndarray_values\n\n        indexer, missing = self._engine.get_indexer_non_unique(tgt_values)\n        return ensure_platform_int(indexer), missing\n\n    def get_indexer_for(self, target, **kwargs):\n        \"\"\"\n        Guaranteed return of an indexer even when non-unique.\n\n        This dispatches to get_indexer or get_indexer_nonunique\n        as appropriate.\n        \"\"\"\n        if self.is_unique:\n            return self.get_indexer(target, **kwargs)\n        indexer, _ = self.get_indexer_non_unique(target, **kwargs)\n        return indexer\n\n    def _maybe_promote(self, other):\n        # A hack, but it works\n        from pandas import DatetimeIndex\n        if self.inferred_type == 'date' and isinstance(other, DatetimeIndex):\n            return DatetimeIndex(self), other\n        elif self.inferred_type == 'boolean':\n            if not is_object_dtype(self.dtype):\n                return self.astype('object'), other.astype('object')\n        return self, other\n\n    def groupby(self, values):\n        \"\"\"\n        Group the index labels by a given array of values.\n\n        Parameters\n        ----------\n        values : array\n            Values used to determine the groups.\n\n        Returns\n        -------\n        groups : dict\n            {group name -> group labels}\n        \"\"\"\n\n        # TODO: if we are a MultiIndex, we can do better\n        # that converting to tuples\n        from .multi import MultiIndex\n        if isinstance(values, MultiIndex):\n            values = values.values\n        values = ensure_categorical(values)\n        result = values._reverse_indexer()\n\n        # map to the label\n        result = {k: self.take(v) for k, v in compat.iteritems(result)}\n\n        return result\n\n    def map(self, mapper, na_action=None):\n        \"\"\"\n        Map values using input correspondence (a dict, Series, or function).\n\n        Parameters\n        ----------\n        mapper : function, dict, or Series\n            Mapping correspondence.\n        na_action : {None, 'ignore'}\n            If 'ignore', propagate NA values, without passing them to the\n            mapping correspondence.\n\n        Returns\n        -------\n        applied : Union[Index, MultiIndex], inferred\n            The output of the mapping function applied to the index.\n            If the function returns a tuple with more than one element\n            a MultiIndex will be returned.\n        \"\"\"\n\n        from .multi import MultiIndex\n        new_values = super(Index, self)._map_values(\n            mapper, na_action=na_action)\n\n        attributes = self._get_attributes_dict()\n\n        # we can return a MultiIndex\n        if new_values.size and isinstance(new_values[0], tuple):\n            if isinstance(self, MultiIndex):\n                names = self.names\n            elif attributes.get('name'):\n                names = [attributes.get('name')] * len(new_values[0])\n            else:\n                names = None\n            return MultiIndex.from_tuples(new_values,\n                                          names=names)\n\n        attributes['copy'] = False\n        if not new_values.size:\n            # empty\n            attributes['dtype'] = self.dtype\n\n        return Index(new_values, **attributes)\n\n    def isin(self, values, level=None):\n        \"\"\"\n        Return a boolean array where the index values are in `values`.\n\n        Compute boolean array of whether each index value is found in the\n        passed set of values. The length of the returned boolean array matches\n        the length of the index.\n\n        Parameters\n        ----------\n        values : set or list-like\n            Sought values.\n\n            .. versionadded:: 0.18.1\n\n               Support for values as a set.\n\n        level : str or int, optional\n            Name or position of the index level to use (if the index is a\n            `MultiIndex`).\n\n        Returns\n        -------\n        is_contained : ndarray\n            NumPy array of boolean values.\n\n        See Also\n        --------\n        Series.isin : Same for Series.\n        DataFrame.isin : Same method for DataFrames.\n\n        Notes\n        -----\n        In the case of `MultiIndex` you must either specify `values` as a\n        list-like object containing tuples that are the same length as the\n        number of levels, or specify `level`. Otherwise it will raise a\n        ``ValueError``.\n\n        If `level` is specified:\n\n        - if it is the name of one *and only one* index level, use that level;\n        - otherwise it should be a number indicating level position.\n\n        Examples\n        --------\n        >>> idx = pd.Index([1,2,3])\n        >>> idx\n        Int64Index([1, 2, 3], dtype='int64')\n\n        Check whether each index value in a list of values.\n        >>> idx.isin([1, 4])\n        array([ True, False, False])\n\n        >>> midx = pd.MultiIndex.from_arrays([[1,2,3],\n        ...                                  ['red', 'blue', 'green']],\n        ...                                  names=('number', 'color'))\n        >>> midx\n        MultiIndex(levels=[[1, 2, 3], ['blue', 'green', 'red']],\n                   labels=[[0, 1, 2], [2, 0, 1]],\n                   names=['number', 'color'])\n\n        Check whether the strings in the 'color' level of the MultiIndex\n        are in a list of colors.\n\n        >>> midx.isin(['red', 'orange', 'yellow'], level='color')\n        array([ True, False, False])\n\n        To check across the levels of a MultiIndex, pass a list of tuples:\n\n        >>> midx.isin([(1, 'red'), (3, 'red')])\n        array([ True, False, False])\n\n        For a DatetimeIndex, string values in `values` are converted to\n        Timestamps.\n\n        >>> dates = ['2000-03-11', '2000-03-12', '2000-03-13']\n        >>> dti = pd.to_datetime(dates)\n        >>> dti\n        DatetimeIndex(['2000-03-11', '2000-03-12', '2000-03-13'],\n        dtype='datetime64[ns]', freq=None)\n\n        >>> dti.isin(['2000-03-11'])\n        array([ True, False, False])\n        \"\"\"\n        if level is not None:\n            self._validate_index_level(level)\n        return algos.isin(self, values)\n\n    def _can_reindex(self, indexer):\n        \"\"\"\n        Check if we are allowing reindexing with this particular indexer.\n\n        Parameters\n        ----------\n        indexer : an integer indexer\n\n        Raises\n        ------\n        ValueError if its a duplicate axis\n        \"\"\"\n\n        # trying to reindex on an axis with duplicates\n        if not self.is_unique and len(indexer):\n            raise ValueError(\"cannot reindex from a duplicate axis\")\n\n    def reindex(self, target, method=None, level=None, limit=None,\n                tolerance=None):\n        \"\"\"\n        Create index with target's values (move/add/delete values\n        as necessary).\n\n        Parameters\n        ----------\n        target : an iterable\n\n        Returns\n        -------\n        new_index : pd.Index\n            Resulting index\n        indexer : np.ndarray or None\n            Indices of output values in original index\n\n        \"\"\"\n        # GH6552: preserve names when reindexing to non-named target\n        # (i.e. neither Index nor Series).\n        preserve_names = not hasattr(target, 'name')\n\n        # GH7774: preserve dtype/tz if target is empty and not an Index.\n        target = _ensure_has_len(target)  # target may be an iterator\n\n        if not isinstance(target, Index) and len(target) == 0:\n            attrs = self._get_attributes_dict()\n            attrs.pop('freq', None)  # don't preserve freq\n            values = self._data[:0]  # appropriately-dtyped empty array\n            target = self._simple_new(values, dtype=self.dtype, **attrs)\n        else:\n            target = ensure_index(target)\n\n        if level is not None:\n            if method is not None:\n                raise TypeError('Fill method not supported if level passed')\n            _, indexer, _ = self._join_level(target, level, how='right',\n                                             return_indexers=True)\n        else:\n            if self.equals(target):\n                indexer = None\n            else:\n\n                if self.is_unique:\n                    indexer = self.get_indexer(target, method=method,\n                                               limit=limit,\n                                               tolerance=tolerance)\n                else:\n                    if method is not None or limit is not None:\n                        raise ValueError(\"cannot reindex a non-unique index \"\n                                         \"with a method or limit\")\n                    indexer, missing = self.get_indexer_non_unique(target)\n\n        if preserve_names and target.nlevels == 1 and target.name != self.name:\n            target = target.copy()\n            target.name = self.name\n\n        return target, indexer\n\n    def _reindex_non_unique(self, target):\n        \"\"\"\n        Create a new index with target's values (move/add/delete values as\n        necessary) use with non-unique Index and a possibly non-unique target.\n\n        Parameters\n        ----------\n        target : an iterable\n\n        Returns\n        -------\n        new_index : pd.Index\n            Resulting index\n        indexer : np.ndarray or None\n            Indices of output values in original index\n\n        \"\"\"\n\n        target = ensure_index(target)\n        indexer, missing = self.get_indexer_non_unique(target)\n        check = indexer != -1\n        new_labels = self.take(indexer[check])\n        new_indexer = None\n\n        if len(missing):\n            length = np.arange(len(indexer))\n\n            missing = ensure_platform_int(missing)\n            missing_labels = target.take(missing)\n            missing_indexer = ensure_int64(length[~check])\n            cur_labels = self.take(indexer[check]).values\n            cur_indexer = ensure_int64(length[check])\n\n            new_labels = np.empty(tuple([len(indexer)]), dtype=object)\n            new_labels[cur_indexer] = cur_labels\n            new_labels[missing_indexer] = missing_labels\n\n            # a unique indexer\n            if target.is_unique:\n\n                # see GH5553, make sure we use the right indexer\n                new_indexer = np.arange(len(indexer))\n                new_indexer[cur_indexer] = np.arange(len(cur_labels))\n                new_indexer[missing_indexer] = -1\n\n            # we have a non_unique selector, need to use the original\n            # indexer here\n            else:\n\n                # need to retake to have the same size as the indexer\n                indexer[~check] = -1\n\n                # reset the new indexer to account for the new size\n                new_indexer = np.arange(len(self.take(indexer)))\n                new_indexer[~check] = -1\n\n        new_index = self._shallow_copy_with_infer(new_labels, freq=None)\n        return new_index, indexer, new_indexer\n\n    _index_shared_docs['join'] = \"\"\"\n        Compute join_index and indexers to conform data\n        structures to the new index.\n\n        Parameters\n        ----------\n        other : Index\n        how : {'left', 'right', 'inner', 'outer'}\n        level : int or level name, default None\n        return_indexers : boolean, default False\n        sort : boolean, default False\n            Sort the join keys lexicographically in the result Index. If False,\n            the order of the join keys depends on the join type (how keyword)\n\n            .. versionadded:: 0.20.0\n\n        Returns\n        -------\n        join_index, (left_indexer, right_indexer)\n        \"\"\"\n\n    @Appender(_index_shared_docs['join'])\n    def join(self, other, how='left', level=None, return_indexers=False,\n             sort=False):\n        from .multi import MultiIndex\n        self_is_mi = isinstance(self, MultiIndex)\n        other_is_mi = isinstance(other, MultiIndex)\n\n        # try to figure out the join level\n        # GH3662\n        if level is None and (self_is_mi or other_is_mi):\n\n            # have the same levels/names so a simple join\n            if self.names == other.names:\n                pass\n            else:\n                return self._join_multi(other, how=how,\n                                        return_indexers=return_indexers)\n\n        # join on the level\n        if level is not None and (self_is_mi or other_is_mi):\n            return self._join_level(other, level, how=how,\n                                    return_indexers=return_indexers)\n\n        other = ensure_index(other)\n\n        if len(other) == 0 and how in ('left', 'outer'):\n            join_index = self._shallow_copy()\n            if return_indexers:\n                rindexer = np.repeat(-1, len(join_index))\n                return join_index, None, rindexer\n            else:\n                return join_index\n\n        if len(self) == 0 and how in ('right', 'outer'):\n            join_index = other._shallow_copy()\n            if return_indexers:\n                lindexer = np.repeat(-1, len(join_index))\n                return join_index, lindexer, None\n            else:\n                return join_index\n\n        if self._join_precedence < other._join_precedence:\n            how = {'right': 'left', 'left': 'right'}.get(how, how)\n            result = other.join(self, how=how, level=level,\n                                return_indexers=return_indexers)\n            if return_indexers:\n                x, y, z = result\n                result = x, z, y\n            return result\n\n        if not is_dtype_equal(self.dtype, other.dtype):\n            this = self.astype('O')\n            other = other.astype('O')\n            return this.join(other, how=how, return_indexers=return_indexers)\n\n        _validate_join_method(how)\n\n        if not self.is_unique and not other.is_unique:\n            return self._join_non_unique(other, how=how,\n                                         return_indexers=return_indexers)\n        elif not self.is_unique or not other.is_unique:\n            if self.is_monotonic and other.is_monotonic:\n                return self._join_monotonic(other, how=how,\n                                            return_indexers=return_indexers)\n            else:\n                return self._join_non_unique(other, how=how,\n                                             return_indexers=return_indexers)\n        elif self.is_monotonic and other.is_monotonic:\n            try:\n                return self._join_monotonic(other, how=how,\n                                            return_indexers=return_indexers)\n            except TypeError:\n                pass\n\n        if how == 'left':\n            join_index = self\n        elif how == 'right':\n            join_index = other\n        elif how == 'inner':\n            join_index = self.intersection(other)\n        elif how == 'outer':\n            join_index = self.union(other)\n\n        if sort:\n            join_index = join_index.sort_values()\n\n        if return_indexers:\n            if join_index is self:\n                lindexer = None\n            else:\n                lindexer = self.get_indexer(join_index)\n            if join_index is other:\n                rindexer = None\n            else:\n                rindexer = other.get_indexer(join_index)\n            return join_index, lindexer, rindexer\n        else:\n            return join_index\n\n    def _join_multi(self, other, how, return_indexers=True):\n        from .multi import MultiIndex\n        from pandas.core.reshape.merge import _restore_dropped_levels_multijoin\n\n        # figure out join names\n        self_names = set(com._not_none(*self.names))\n        other_names = set(com._not_none(*other.names))\n        overlap = self_names & other_names\n\n        # need at least 1 in common\n        if not overlap:\n            raise ValueError(\"cannot join with no overlapping index names\")\n\n        self_is_mi = isinstance(self, MultiIndex)\n        other_is_mi = isinstance(other, MultiIndex)\n\n        if self_is_mi and other_is_mi:\n\n            # Drop the non-matching levels from left and right respectively\n            ldrop_names = list(self_names - overlap)\n            rdrop_names = list(other_names - overlap)\n\n            self_jnlevels = self.droplevel(ldrop_names)\n            other_jnlevels = other.droplevel(rdrop_names)\n\n            # Join left and right\n            # Join on same leveled multi-index frames is supported\n            join_idx, lidx, ridx = self_jnlevels.join(other_jnlevels, how,\n                                                      return_indexers=True)\n\n            # Restore the dropped levels\n            # Returned index level order is\n            # common levels, ldrop_names, rdrop_names\n            dropped_names = ldrop_names + rdrop_names\n\n            levels, labels, names = (\n                _restore_dropped_levels_multijoin(self, other,\n                                                  dropped_names,\n                                                  join_idx,\n                                                  lidx, ridx))\n\n            # Re-create the multi-index\n            multi_join_idx = MultiIndex(levels=levels, labels=labels,\n                                        names=names, verify_integrity=False)\n\n            multi_join_idx = multi_join_idx.remove_unused_levels()\n\n            return multi_join_idx, lidx, ridx\n\n        jl = list(overlap)[0]\n\n        # Case where only one index is multi\n        # make the indices into mi's that match\n        flip_order = False\n        if self_is_mi:\n            self, other = other, self\n            flip_order = True\n            # flip if join method is right or left\n            how = {'right': 'left', 'left': 'right'}.get(how, how)\n\n        level = other.names.index(jl)\n        result = self._join_level(other, level, how=how,\n                                  return_indexers=return_indexers)\n\n        if flip_order:\n            if isinstance(result, tuple):\n                return result[0], result[2], result[1]\n        return result\n\n    def _join_non_unique(self, other, how='left', return_indexers=False):\n        from pandas.core.reshape.merge import _get_join_indexers\n\n        left_idx, right_idx = _get_join_indexers([self._ndarray_values],\n                                                 [other._ndarray_values],\n                                                 how=how,\n                                                 sort=True)\n\n        left_idx = ensure_platform_int(left_idx)\n        right_idx = ensure_platform_int(right_idx)\n\n        join_index = np.asarray(self._ndarray_values.take(left_idx))\n        mask = left_idx == -1\n        np.putmask(join_index, mask, other._ndarray_values.take(right_idx))\n\n        join_index = self._wrap_joined_index(join_index, other)\n\n        if return_indexers:\n            return join_index, left_idx, right_idx\n        else:\n            return join_index\n\n    def _join_level(self, other, level, how='left', return_indexers=False,\n                    keep_order=True):\n        \"\"\"\n        The join method *only* affects the level of the resulting\n        MultiIndex. Otherwise it just exactly aligns the Index data to the\n        labels of the level in the MultiIndex.\n\n        If ```keep_order == True```, the order of the data indexed by the\n        MultiIndex will not be changed; otherwise, it will tie out\n        with `other`.\n        \"\"\"\n        from .multi import MultiIndex\n\n        def _get_leaf_sorter(labels):\n            \"\"\"\n            Returns sorter for the inner most level while preserving the\n            order of higher levels.\n            \"\"\"\n            if labels[0].size == 0:\n                return np.empty(0, dtype='int64')\n\n            if len(labels) == 1:\n                lab = ensure_int64(labels[0])\n                sorter, _ = libalgos.groupsort_indexer(lab, 1 + lab.max())\n                return sorter\n\n            # find indexers of beginning of each set of\n            # same-key labels w.r.t all but last level\n            tic = labels[0][:-1] != labels[0][1:]\n            for lab in labels[1:-1]:\n                tic |= lab[:-1] != lab[1:]\n\n            starts = np.hstack(([True], tic, [True])).nonzero()[0]\n            lab = ensure_int64(labels[-1])\n            return lib.get_level_sorter(lab, ensure_int64(starts))\n\n        if isinstance(self, MultiIndex) and isinstance(other, MultiIndex):\n            raise TypeError('Join on level between two MultiIndex objects '\n                            'is ambiguous')\n\n        left, right = self, other\n\n        flip_order = not isinstance(self, MultiIndex)\n        if flip_order:\n            left, right = right, left\n            how = {'right': 'left', 'left': 'right'}.get(how, how)\n\n        level = left._get_level_number(level)\n        old_level = left.levels[level]\n\n        if not right.is_unique:\n            raise NotImplementedError('Index._join_level on non-unique index '\n                                      'is not implemented')\n\n        new_level, left_lev_indexer, right_lev_indexer = \\\n            old_level.join(right, how=how, return_indexers=True)\n\n        if left_lev_indexer is None:\n            if keep_order or len(left) == 0:\n                left_indexer = None\n                join_index = left\n            else:  # sort the leaves\n                left_indexer = _get_leaf_sorter(left.labels[:level + 1])\n                join_index = left[left_indexer]\n\n        else:\n            left_lev_indexer = ensure_int64(left_lev_indexer)\n            rev_indexer = lib.get_reverse_indexer(left_lev_indexer,\n                                                  len(old_level))\n\n            new_lev_labels = algos.take_nd(rev_indexer, left.labels[level],\n                                           allow_fill=False)\n\n            new_labels = list(left.labels)\n            new_labels[level] = new_lev_labels\n\n            new_levels = list(left.levels)\n            new_levels[level] = new_level\n\n            if keep_order:  # just drop missing values. o.w. keep order\n                left_indexer = np.arange(len(left), dtype=np.intp)\n                mask = new_lev_labels != -1\n                if not mask.all():\n                    new_labels = [lab[mask] for lab in new_labels]\n                    left_indexer = left_indexer[mask]\n\n            else:  # tie out the order with other\n                if level == 0:  # outer most level, take the fast route\n                    ngroups = 1 + new_lev_labels.max()\n                    left_indexer, counts = libalgos.groupsort_indexer(\n                        new_lev_labels, ngroups)\n\n                    # missing values are placed first; drop them!\n                    left_indexer = left_indexer[counts[0]:]\n                    new_labels = [lab[left_indexer] for lab in new_labels]\n\n                else:  # sort the leaves\n                    mask = new_lev_labels != -1\n                    mask_all = mask.all()\n                    if not mask_all:\n                        new_labels = [lab[mask] for lab in new_labels]\n\n                    left_indexer = _get_leaf_sorter(new_labels[:level + 1])\n                    new_labels = [lab[left_indexer] for lab in new_labels]\n\n                    # left_indexers are w.r.t masked frame.\n                    # reverse to original frame!\n                    if not mask_all:\n                        left_indexer = mask.nonzero()[0][left_indexer]\n\n            join_index = MultiIndex(levels=new_levels, labels=new_labels,\n                                    names=left.names, verify_integrity=False)\n\n        if right_lev_indexer is not None:\n            right_indexer = algos.take_nd(right_lev_indexer,\n                                          join_index.labels[level],\n                                          allow_fill=False)\n        else:\n            right_indexer = join_index.labels[level]\n\n        if flip_order:\n            left_indexer, right_indexer = right_indexer, left_indexer\n\n        if return_indexers:\n            left_indexer = (None if left_indexer is None\n                            else ensure_platform_int(left_indexer))\n            right_indexer = (None if right_indexer is None\n                             else ensure_platform_int(right_indexer))\n            return join_index, left_indexer, right_indexer\n        else:\n            return join_index\n\n    def _join_monotonic(self, other, how='left', return_indexers=False):\n        if self.equals(other):\n            ret_index = other if how == 'right' else self\n            if return_indexers:\n                return ret_index, None, None\n            else:\n                return ret_index\n\n        sv = self._ndarray_values\n        ov = other._ndarray_values\n\n        if self.is_unique and other.is_unique:\n            # We can perform much better than the general case\n            if how == 'left':\n                join_index = self\n                lidx = None\n                ridx = self._left_indexer_unique(sv, ov)\n            elif how == 'right':\n                join_index = other\n                lidx = self._left_indexer_unique(ov, sv)\n                ridx = None\n            elif how == 'inner':\n                join_index, lidx, ridx = self._inner_indexer(sv, ov)\n                join_index = self._wrap_joined_index(join_index, other)\n            elif how == 'outer':\n                join_index, lidx, ridx = self._outer_indexer(sv, ov)\n                join_index = self._wrap_joined_index(join_index, other)\n        else:\n            if how == 'left':\n                join_index, lidx, ridx = self._left_indexer(sv, ov)\n            elif how == 'right':\n                join_index, ridx, lidx = self._left_indexer(ov, sv)\n            elif how == 'inner':\n                join_index, lidx, ridx = self._inner_indexer(sv, ov)\n            elif how == 'outer':\n                join_index, lidx, ridx = self._outer_indexer(sv, ov)\n            join_index = self._wrap_joined_index(join_index, other)\n\n        if return_indexers:\n            lidx = None if lidx is None else ensure_platform_int(lidx)\n            ridx = None if ridx is None else ensure_platform_int(ridx)\n            return join_index, lidx, ridx\n        else:\n            return join_index\n\n    def _wrap_joined_index(self, joined, other):\n        name = get_op_result_name(self, other)\n        return Index(joined, name=name)\n\n    def _get_string_slice(self, key, use_lhs=True, use_rhs=True):\n        # this is for partial string indexing,\n        # overridden in DatetimeIndex, TimedeltaIndex and PeriodIndex\n        raise NotImplementedError\n\n    def slice_indexer(self, start=None, end=None, step=None, kind=None):\n        \"\"\"\n        For an ordered or unique index, compute the slice indexer for input\n        labels and step.\n\n        Parameters\n        ----------\n        start : label, default None\n            If None, defaults to the beginning\n        end : label, default None\n            If None, defaults to the end\n        step : int, default None\n        kind : string, default None\n\n        Returns\n        -------\n        indexer : slice\n\n        Raises\n        ------\n        KeyError : If key does not exist, or key is not unique and index is\n            not ordered.\n\n        Notes\n        -----\n        This function assumes that the data is sorted, so use at your own peril\n\n        Examples\n        ---------\n        This is a method on all index types. For example you can do:\n\n        >>> idx = pd.Index(list('abcd'))\n        >>> idx.slice_indexer(start='b', end='c')\n        slice(1, 3)\n\n        >>> idx = pd.MultiIndex.from_arrays([list('abcd'), list('efgh')])\n        >>> idx.slice_indexer(start='b', end=('c', 'g'))\n        slice(1, 3)\n        \"\"\"\n        start_slice, end_slice = self.slice_locs(start, end, step=step,\n                                                 kind=kind)\n\n        # return a slice\n        if not is_scalar(start_slice):\n            raise AssertionError(\"Start slice bound is non-scalar\")\n        if not is_scalar(end_slice):\n            raise AssertionError(\"End slice bound is non-scalar\")\n\n        return slice(start_slice, end_slice, step)\n\n    def _maybe_cast_indexer(self, key):\n        \"\"\"\n        If we have a float key and are not a floating index, then try to cast\n        to an int if equivalent.\n        \"\"\"\n\n        if is_float(key) and not self.is_floating():\n            try:\n                ckey = int(key)\n                if ckey == key:\n                    key = ckey\n            except (OverflowError, ValueError, TypeError):\n                pass\n        return key\n\n    def _validate_indexer(self, form, key, kind):\n        \"\"\"\n        If we are positional indexer, validate that we have appropriate\n        typed bounds must be an integer.\n        \"\"\"\n        assert kind in ['ix', 'loc', 'getitem', 'iloc']\n\n        if key is None:\n            pass\n        elif is_integer(key):\n            pass\n        elif kind in ['iloc', 'getitem']:\n            self._invalid_indexer(form, key)\n        return key\n\n    _index_shared_docs['_maybe_cast_slice_bound'] = \"\"\"\n        This function should be overloaded in subclasses that allow non-trivial\n        casting on label-slice bounds, e.g. datetime-like indices allowing\n        strings containing formatted datetimes.\n\n        Parameters\n        ----------\n        label : object\n        side : {'left', 'right'}\n        kind : {'ix', 'loc', 'getitem'}\n\n        Returns\n        -------\n        label :  object\n\n        Notes\n        -----\n        Value of `side` parameter should be validated in caller.\n\n        \"\"\"\n\n    @Appender(_index_shared_docs['_maybe_cast_slice_bound'])\n    def _maybe_cast_slice_bound(self, label, side, kind):\n        assert kind in ['ix', 'loc', 'getitem', None]\n\n        # We are a plain index here (sub-class override this method if they\n        # wish to have special treatment for floats/ints, e.g. Float64Index and\n        # datetimelike Indexes\n        # reject them\n        if is_float(label):\n            if not (kind in ['ix'] and (self.holds_integer() or\n                                        self.is_floating())):\n                self._invalid_indexer('slice', label)\n\n        # we are trying to find integer bounds on a non-integer based index\n        # this is rejected (generally .loc gets you here)\n        elif is_integer(label):\n            self._invalid_indexer('slice', label)\n\n        return label\n\n    def _searchsorted_monotonic(self, label, side='left'):\n        if self.is_monotonic_increasing:\n            return self.searchsorted(label, side=side)\n        elif self.is_monotonic_decreasing:\n            # np.searchsorted expects ascending sort order, have to reverse\n            # everything for it to work (element ordering, search side and\n            # resulting value).\n            pos = self[::-1].searchsorted(label, side='right' if side == 'left'\n                                          else 'left')\n            return len(self) - pos\n\n        raise ValueError('index must be monotonic increasing or decreasing')\n\n    def _get_loc_only_exact_matches(self, key):\n        \"\"\"\n        This is overridden on subclasses (namely, IntervalIndex) to control\n        get_slice_bound.\n        \"\"\"\n        return self.get_loc(key)\n\n    def get_slice_bound(self, label, side, kind):\n        \"\"\"\n        Calculate slice bound that corresponds to given label.\n\n        Returns leftmost (one-past-the-rightmost if ``side=='right'``) position\n        of given label.\n\n        Parameters\n        ----------\n        label : object\n        side : {'left', 'right'}\n        kind : {'ix', 'loc', 'getitem'}\n        \"\"\"\n        assert kind in ['ix', 'loc', 'getitem', None]\n\n        if side not in ('left', 'right'):\n            raise ValueError(\"Invalid value for side kwarg,\"\n                             \" must be either 'left' or 'right': %s\" %\n                             (side, ))\n\n        original_label = label\n\n        # For datetime indices label may be a string that has to be converted\n        # to datetime boundary according to its resolution.\n        label = self._maybe_cast_slice_bound(label, side, kind)\n\n        # we need to look up the label\n        try:\n            slc = self._get_loc_only_exact_matches(label)\n        except KeyError as err:\n            try:\n                return self._searchsorted_monotonic(label, side)\n            except ValueError:\n                # raise the original KeyError\n                raise err\n\n        if isinstance(slc, np.ndarray):\n            # get_loc may return a boolean array or an array of indices, which\n            # is OK as long as they are representable by a slice.\n            if is_bool_dtype(slc):\n                slc = lib.maybe_booleans_to_slice(slc.view('u1'))\n            else:\n                slc = lib.maybe_indices_to_slice(slc.astype('i8'), len(self))\n            if isinstance(slc, np.ndarray):\n                raise KeyError(\"Cannot get %s slice bound for non-unique \"\n                               \"label: %r\" % (side, original_label))\n\n        if isinstance(slc, slice):\n            if side == 'left':\n                return slc.start\n            else:\n                return slc.stop\n        else:\n            if side == 'right':\n                return slc + 1\n            else:\n                return slc\n\n    def slice_locs(self, start=None, end=None, step=None, kind=None):\n        \"\"\"\n        Compute slice locations for input labels.\n\n        Parameters\n        ----------\n        start : label, default None\n            If None, defaults to the beginning\n        end : label, default None\n            If None, defaults to the end\n        step : int, defaults None\n            If None, defaults to 1\n        kind : {'ix', 'loc', 'getitem'} or None\n\n        Returns\n        -------\n        start, end : int\n\n        Notes\n        -----\n        This method only works if the index is monotonic or unique.\n\n        Examples\n        ---------\n        >>> idx = pd.Index(list('abcd'))\n        >>> idx.slice_locs(start='b', end='c')\n        (1, 3)\n\n        See Also\n        --------\n        Index.get_loc : Get location for a single label.\n        \"\"\"\n        inc = (step is None or step >= 0)\n\n        if not inc:\n            # If it's a reverse slice, temporarily swap bounds.\n            start, end = end, start\n\n        start_slice = None\n        if start is not None:\n            start_slice = self.get_slice_bound(start, 'left', kind)\n        if start_slice is None:\n            start_slice = 0\n\n        end_slice = None\n        if end is not None:\n            end_slice = self.get_slice_bound(end, 'right', kind)\n        if end_slice is None:\n            end_slice = len(self)\n\n        if not inc:\n            # Bounds at this moment are swapped, swap them back and shift by 1.\n            #\n            # slice_locs('B', 'A', step=-1): s='B', e='A'\n            #\n            #              s='A'                 e='B'\n            # AFTER SWAP:    |                     |\n            #                v ------------------> V\n            #           -----------------------------------\n            #           | | |A|A|A|A| | | | | |B|B| | | | |\n            #           -----------------------------------\n            #              ^ <------------------ ^\n            # SHOULD BE:   |                     |\n            #           end=s-1              start=e-1\n            #\n            end_slice, start_slice = start_slice - 1, end_slice - 1\n\n            # i == -1 triggers ``len(self) + i`` selection that points to the\n            # last element, not before-the-first one, subtracting len(self)\n            # compensates that.\n            if end_slice == -1:\n                end_slice -= len(self)\n            if start_slice == -1:\n                start_slice -= len(self)\n\n        return start_slice, end_slice\n\n    def delete(self, loc):\n        \"\"\"\n        Make new Index with passed location(-s) deleted.\n\n        Returns\n        -------\n        new_index : Index\n        \"\"\"\n        return self._shallow_copy(np.delete(self._data, loc))\n\n    def insert(self, loc, item):\n        \"\"\"\n        Make new Index inserting new item at location.\n\n        Follows Python list.append semantics for negative values.\n\n        Parameters\n        ----------\n        loc : int\n        item : object\n\n        Returns\n        -------\n        new_index : Index\n        \"\"\"\n        _self = np.asarray(self)\n        item = self._coerce_scalar_to_index(item)._ndarray_values\n        idx = np.concatenate((_self[:loc], item, _self[loc:]))\n        return self._shallow_copy_with_infer(idx)\n\n    def drop(self, labels, errors='raise'):\n        \"\"\"\n        Make new Index with passed list of labels deleted.\n\n        Parameters\n        ----------\n        labels : array-like\n        errors : {'ignore', 'raise'}, default 'raise'\n            If 'ignore', suppress error and existing labels are dropped.\n\n        Returns\n        -------\n        dropped : Index\n\n        Raises\n        ------\n        KeyError\n            If not all of the labels are found in the selected axis\n        \"\"\"\n        arr_dtype = 'object' if self.dtype == 'object' else None\n        labels = com.index_labels_to_array(labels, dtype=arr_dtype)\n        indexer = self.get_indexer(labels)\n        mask = indexer == -1\n        if mask.any():\n            if errors != 'ignore':\n                raise KeyError(\n                    '{} not found in axis'.format(labels[mask]))\n            indexer = indexer[~mask]\n        return self.delete(indexer)\n\n    _index_shared_docs['index_unique'] = (\n        \"\"\"\n        Return unique values in the index. Uniques are returned in order\n        of appearance, this does NOT sort.\n\n        Parameters\n        ----------\n        level : int or str, optional, default None\n            Only return values from specified level (for MultiIndex)\n\n            .. versionadded:: 0.23.0\n\n        Returns\n        -------\n        Index without duplicates\n\n        See Also\n        --------\n        unique\n        Series.unique\n        \"\"\")\n\n    @Appender(_index_shared_docs['index_unique'] % _index_doc_kwargs)\n    def unique(self, level=None):\n        if level is not None:\n            self._validate_index_level(level)\n        result = super(Index, self).unique()\n        return self._shallow_copy(result)\n\n    def drop_duplicates(self, keep='first'):\n        \"\"\"\n        Return Index with duplicate values removed.\n\n        Parameters\n        ----------\n        keep : {'first', 'last', ``False``}, default 'first'\n            - 'first' : Drop duplicates except for the first occurrence.\n            - 'last' : Drop duplicates except for the last occurrence.\n            - ``False`` : Drop all duplicates.\n\n        Returns\n        -------\n        deduplicated : Index\n\n        See Also\n        --------\n        Series.drop_duplicates : Equivalent method on Series.\n        DataFrame.drop_duplicates : Equivalent method on DataFrame.\n        Index.duplicated : Related method on Index, indicating duplicate\n            Index values.\n\n        Examples\n        --------\n        Generate an pandas.Index with duplicate values.\n\n        >>> idx = pd.Index(['lama', 'cow', 'lama', 'beetle', 'lama', 'hippo'])\n\n        The `keep` parameter controls  which duplicate values are removed.\n        The value 'first' keeps the first occurrence for each\n        set of duplicated entries. The default value of keep is 'first'.\n\n        >>> idx.drop_duplicates(keep='first')\n        Index(['lama', 'cow', 'beetle', 'hippo'], dtype='object')\n\n        The value 'last' keeps the last occurrence for each set of duplicated\n        entries.\n\n        >>> idx.drop_duplicates(keep='last')\n        Index(['cow', 'beetle', 'lama', 'hippo'], dtype='object')\n\n        The value ``False`` discards all sets of duplicated entries.\n\n        >>> idx.drop_duplicates(keep=False)\n        Index(['cow', 'beetle', 'hippo'], dtype='object')\n        \"\"\"\n        return super(Index, self).drop_duplicates(keep=keep)\n\n    def duplicated(self, keep='first'):\n        \"\"\"\n        Indicate duplicate index values.\n\n        Duplicated values are indicated as ``True`` values in the resulting\n        array. Either all duplicates, all except the first, or all except the\n        last occurrence of duplicates can be indicated.\n\n        Parameters\n        ----------\n        keep : {'first', 'last', False}, default 'first'\n            The value or values in a set of duplicates to mark as missing.\n\n            - 'first' : Mark duplicates as ``True`` except for the first\n              occurrence.\n            - 'last' : Mark duplicates as ``True`` except for the last\n              occurrence.\n            - ``False`` : Mark all duplicates as ``True``.\n\n        Examples\n        --------\n        By default, for each set of duplicated values, the first occurrence is\n        set to False and all others to True:\n\n        >>> idx = pd.Index(['lama', 'cow', 'lama', 'beetle', 'lama'])\n        >>> idx.duplicated()\n        array([False, False,  True, False,  True])\n\n        which is equivalent to\n\n        >>> idx.duplicated(keep='first')\n        array([False, False,  True, False,  True])\n\n        By using 'last', the last occurrence of each set of duplicated values\n        is set on False and all others on True:\n\n        >>> idx.duplicated(keep='last')\n        array([ True, False,  True, False, False])\n\n        By setting keep on ``False``, all duplicates are True:\n\n        >>> idx.duplicated(keep=False)\n        array([ True, False,  True, False,  True])\n\n        Returns\n        -------\n        numpy.ndarray\n\n        See Also\n        --------\n        pandas.Series.duplicated : Equivalent method on pandas.Series.\n        pandas.DataFrame.duplicated : Equivalent method on pandas.DataFrame.\n        pandas.Index.drop_duplicates : Remove duplicate values from Index.\n        \"\"\"\n        return super(Index, self).duplicated(keep=keep)\n\n    _index_shared_docs['fillna'] = \"\"\"\n        Fill NA/NaN values with the specified value\n\n        Parameters\n        ----------\n        value : scalar\n            Scalar value to use to fill holes (e.g. 0).\n            This value cannot be a list-likes.\n        downcast : dict, default is None\n            a dict of item->dtype of what to downcast if possible,\n            or the string 'infer' which will try to downcast to an appropriate\n            equal type (e.g. float64 to int64 if possible)\n\n        Returns\n        -------\n        filled : %(klass)s\n        \"\"\"\n\n    @Appender(_index_shared_docs['fillna'])\n    def fillna(self, value=None, downcast=None):\n        self._assert_can_do_op(value)\n        if self.hasnans:\n            result = self.putmask(self._isnan, value)\n            if downcast is None:\n                # no need to care metadata other than name\n                # because it can't have freq if\n                return Index(result, name=self.name)\n        return self._shallow_copy()\n\n    _index_shared_docs['dropna'] = \"\"\"\n        Return Index without NA/NaN values\n\n        Parameters\n        ----------\n        how :  {'any', 'all'}, default 'any'\n            If the Index is a MultiIndex, drop the value when any or all levels\n            are NaN.\n\n        Returns\n        -------\n        valid : Index\n        \"\"\"\n\n    @Appender(_index_shared_docs['dropna'])\n    def dropna(self, how='any'):\n        if how not in ('any', 'all'):\n            raise ValueError(\"invalid how option: {0}\".format(how))\n\n        if self.hasnans:\n            return self._shallow_copy(self.values[~self._isnan])\n        return self._shallow_copy()\n\n    def _evaluate_with_timedelta_like(self, other, op):\n        # Timedelta knows how to operate with np.array, so dispatch to that\n        # operation and then wrap the results\n        if self._is_numeric_dtype and op.__name__ in ['add', 'sub',\n                                                      'radd', 'rsub']:\n            raise TypeError(\"Operation {opname} between {cls} and {other} \"\n                            \"is invalid\".format(opname=op.__name__,\n                                                cls=self.dtype,\n                                                other=type(other).__name__))\n\n        other = Timedelta(other)\n        values = self.values\n\n        with np.errstate(all='ignore'):\n            result = op(values, other)\n\n        attrs = self._get_attributes_dict()\n        attrs = self._maybe_update_attributes(attrs)\n        if op == divmod:\n            return Index(result[0], **attrs), Index(result[1], **attrs)\n        return Index(result, **attrs)\n\n    def _evaluate_with_datetime_like(self, other, op):\n        raise TypeError(\"can only perform ops with datetime like values\")\n\n    @classmethod\n    def _add_comparison_methods(cls):\n        \"\"\"\n        Add in comparison methods.\n        \"\"\"\n        cls.__eq__ = _make_comparison_op(operator.eq, cls)\n        cls.__ne__ = _make_comparison_op(operator.ne, cls)\n        cls.__lt__ = _make_comparison_op(operator.lt, cls)\n        cls.__gt__ = _make_comparison_op(operator.gt, cls)\n        cls.__le__ = _make_comparison_op(operator.le, cls)\n        cls.__ge__ = _make_comparison_op(operator.ge, cls)\n\n    @classmethod\n    def _add_numeric_methods_add_sub_disabled(cls):\n        \"\"\"\n        Add in the numeric add/sub methods to disable.\n        \"\"\"\n        cls.__add__ = make_invalid_op('__add__')\n        cls.__radd__ = make_invalid_op('__radd__')\n        cls.__iadd__ = make_invalid_op('__iadd__')\n        cls.__sub__ = make_invalid_op('__sub__')\n        cls.__rsub__ = make_invalid_op('__rsub__')\n        cls.__isub__ = make_invalid_op('__isub__')\n\n    @classmethod\n    def _add_numeric_methods_disabled(cls):\n        \"\"\"\n        Add in numeric methods to disable other than add/sub.\n        \"\"\"\n        cls.__pow__ = make_invalid_op('__pow__')\n        cls.__rpow__ = make_invalid_op('__rpow__')\n        cls.__mul__ = make_invalid_op('__mul__')\n        cls.__rmul__ = make_invalid_op('__rmul__')\n        cls.__floordiv__ = make_invalid_op('__floordiv__')\n        cls.__rfloordiv__ = make_invalid_op('__rfloordiv__')\n        cls.__truediv__ = make_invalid_op('__truediv__')\n        cls.__rtruediv__ = make_invalid_op('__rtruediv__')\n        if not compat.PY3:\n            cls.__div__ = make_invalid_op('__div__')\n            cls.__rdiv__ = make_invalid_op('__rdiv__')\n        cls.__mod__ = make_invalid_op('__mod__')\n        cls.__divmod__ = make_invalid_op('__divmod__')\n        cls.__neg__ = make_invalid_op('__neg__')\n        cls.__pos__ = make_invalid_op('__pos__')\n        cls.__abs__ = make_invalid_op('__abs__')\n        cls.__inv__ = make_invalid_op('__inv__')\n\n    def _maybe_update_attributes(self, attrs):\n        \"\"\"\n        Update Index attributes (e.g. freq) depending on op.\n        \"\"\"\n        return attrs\n\n    def _validate_for_numeric_unaryop(self, op, opstr):\n        \"\"\"\n        Validate if we can perform a numeric unary operation.\n        \"\"\"\n        if not self._is_numeric_dtype:\n            raise TypeError(\"cannot evaluate a numeric op \"\n                            \"{opstr} for type: {typ}\"\n                            .format(opstr=opstr, typ=type(self).__name__))\n\n    def _validate_for_numeric_binop(self, other, op):\n        \"\"\"\n        Return valid other; evaluate or raise TypeError if we are not of\n        the appropriate type.\n\n        Notes\n        -----\n        This is an internal method called by ops.\n        \"\"\"\n        opstr = '__{opname}__'.format(opname=op.__name__)\n        # if we are an inheritor of numeric,\n        # but not actually numeric (e.g. DatetimeIndex/PeriodIndex)\n        if not self._is_numeric_dtype:\n            raise TypeError(\"cannot evaluate a numeric op {opstr} \"\n                            \"for type: {typ}\"\n                            .format(opstr=opstr, typ=type(self).__name__))\n\n        if isinstance(other, Index):\n            if not other._is_numeric_dtype:\n                raise TypeError(\"cannot evaluate a numeric op \"\n                                \"{opstr} with type: {typ}\"\n                                .format(opstr=opstr, typ=type(other)))\n        elif isinstance(other, np.ndarray) and not other.ndim:\n            other = other.item()\n\n        if isinstance(other, (Index, ABCSeries, np.ndarray)):\n            if len(self) != len(other):\n                raise ValueError(\"cannot evaluate a numeric op with \"\n                                 \"unequal lengths\")\n            other = com.values_from_object(other)\n            if other.dtype.kind not in ['f', 'i', 'u']:\n                raise TypeError(\"cannot evaluate a numeric op \"\n                                \"with a non-numeric dtype\")\n        elif isinstance(other, (ABCDateOffset, np.timedelta64, timedelta)):\n            # higher up to handle\n            pass\n        elif isinstance(other, (datetime, np.datetime64)):\n            # higher up to handle\n            pass\n        else:\n            if not (is_float(other) or is_integer(other)):\n                raise TypeError(\"can only perform ops with scalar values\")\n\n        return other\n\n    @classmethod\n    def _add_numeric_methods_binary(cls):\n        \"\"\"\n        Add in numeric methods.\n        \"\"\"\n        cls.__add__ = _make_arithmetic_op(operator.add, cls)\n        cls.__radd__ = _make_arithmetic_op(ops.radd, cls)\n        cls.__sub__ = _make_arithmetic_op(operator.sub, cls)\n        cls.__rsub__ = _make_arithmetic_op(ops.rsub, cls)\n        cls.__mul__ = _make_arithmetic_op(operator.mul, cls)\n        cls.__rmul__ = _make_arithmetic_op(ops.rmul, cls)\n        cls.__rpow__ = _make_arithmetic_op(ops.rpow, cls)\n        cls.__pow__ = _make_arithmetic_op(operator.pow, cls)\n        cls.__mod__ = _make_arithmetic_op(operator.mod, cls)\n        cls.__floordiv__ = _make_arithmetic_op(operator.floordiv, cls)\n        cls.__rfloordiv__ = _make_arithmetic_op(ops.rfloordiv, cls)\n        cls.__truediv__ = _make_arithmetic_op(operator.truediv, cls)\n        cls.__rtruediv__ = _make_arithmetic_op(ops.rtruediv, cls)\n        if not compat.PY3:\n            cls.__div__ = _make_arithmetic_op(operator.div, cls)\n            cls.__rdiv__ = _make_arithmetic_op(ops.rdiv, cls)\n\n        cls.__divmod__ = _make_arithmetic_op(divmod, cls)\n\n    @classmethod\n    def _add_numeric_methods_unary(cls):\n        \"\"\"\n        Add in numeric unary methods.\n        \"\"\"\n        def _make_evaluate_unary(op, opstr):\n\n            def _evaluate_numeric_unary(self):\n\n                self._validate_for_numeric_unaryop(op, opstr)\n                attrs = self._get_attributes_dict()\n                attrs = self._maybe_update_attributes(attrs)\n                return Index(op(self.values), **attrs)\n\n            return _evaluate_numeric_unary\n\n        cls.__neg__ = _make_evaluate_unary(operator.neg, '__neg__')\n        cls.__pos__ = _make_evaluate_unary(operator.pos, '__pos__')\n        cls.__abs__ = _make_evaluate_unary(np.abs, '__abs__')\n        cls.__inv__ = _make_evaluate_unary(lambda x: -x, '__inv__')\n\n    @classmethod\n    def _add_numeric_methods(cls):\n        cls._add_numeric_methods_unary()\n        cls._add_numeric_methods_binary()\n\n    @classmethod\n    def _add_logical_methods(cls):\n        \"\"\"\n        Add in logical methods.\n        \"\"\"\n        _doc = \"\"\"\n        %(desc)s\n\n        Parameters\n        ----------\n        *args\n            These parameters will be passed to numpy.%(outname)s.\n        **kwargs\n            These parameters will be passed to numpy.%(outname)s.\n\n        Returns\n        -------\n        %(outname)s : bool or array_like (if axis is specified)\n            A single element array_like may be converted to bool.\"\"\"\n\n        _index_shared_docs['index_all'] = dedent(\"\"\"\n\n        See Also\n        --------\n        pandas.Index.any : Return whether any element in an Index is True.\n        pandas.Series.any : Return whether any element in a Series is True.\n        pandas.Series.all : Return whether all elements in a Series are True.\n\n        Notes\n        -----\n        Not a Number (NaN), positive infinity and negative infinity\n        evaluate to True because these are not equal to zero.\n\n        Examples\n        --------\n        **all**\n\n        True, because nonzero integers are considered True.\n\n        >>> pd.Index([1, 2, 3]).all()\n        True\n\n        False, because ``0`` is considered False.\n\n        >>> pd.Index([0, 1, 2]).all()\n        False\n\n        **any**\n\n        True, because ``1`` is considered True.\n\n        >>> pd.Index([0, 0, 1]).any()\n        True\n\n        False, because ``0`` is considered False.\n\n        >>> pd.Index([0, 0, 0]).any()\n        False\n        \"\"\")\n\n        _index_shared_docs['index_any'] = dedent(\"\"\"\n\n        See Also\n        --------\n        pandas.Index.all : Return whether all elements are True.\n        pandas.Series.all : Return whether all elements are True.\n\n        Notes\n        -----\n        Not a Number (NaN), positive infinity and negative infinity\n        evaluate to True because these are not equal to zero.\n\n        Examples\n        --------\n        >>> index = pd.Index([0, 1, 2])\n        >>> index.any()\n        True\n\n        >>> index = pd.Index([0, 0, 0])\n        >>> index.any()\n        False\n        \"\"\")\n\n        def _make_logical_function(name, desc, f):\n            @Substitution(outname=name, desc=desc)\n            @Appender(_index_shared_docs['index_' + name])\n            @Appender(_doc)\n            def logical_func(self, *args, **kwargs):\n                result = f(self.values)\n                if (isinstance(result, (np.ndarray, ABCSeries, Index)) and\n                        result.ndim == 0):\n                    # return NumPy type\n                    return result.dtype.type(result.item())\n                else:  # pragma: no cover\n                    return result\n\n            logical_func.__name__ = name\n            return logical_func\n\n        cls.all = _make_logical_function('all', 'Return whether all elements '\n                                                'are True.',\n                                         np.all)\n        cls.any = _make_logical_function('any',\n                                         'Return whether any element is True.',\n                                         np.any)\n\n    @classmethod\n    def _add_logical_methods_disabled(cls):\n        \"\"\"\n        Add in logical methods to disable.\n        \"\"\"\n        cls.all = make_invalid_op('all')\n        cls.any = make_invalid_op('any')\n\n\nIndex._add_numeric_methods_disabled()\nIndex._add_logical_methods()\nIndex._add_comparison_methods()\n\n\ndef ensure_index_from_sequences(sequences, names=None):\n    \"\"\"\n    Construct an index from sequences of data.\n\n    A single sequence returns an Index. Many sequences returns a\n    MultiIndex.\n\n    Parameters\n    ----------\n    sequences : sequence of sequences\n    names : sequence of str\n\n    Returns\n    -------\n    index : Index or MultiIndex\n\n    Examples\n    --------\n    >>> ensure_index_from_sequences([[1, 2, 3]], names=['name'])\n    Int64Index([1, 2, 3], dtype='int64', name='name')\n\n    >>> ensure_index_from_sequences([['a', 'a'], ['a', 'b']],\n                                    names=['L1', 'L2'])\n    MultiIndex(levels=[['a'], ['a', 'b']],\n               labels=[[0, 0], [0, 1]],\n               names=['L1', 'L2'])\n\n    See Also\n    --------\n    ensure_index\n    \"\"\"\n    from .multi import MultiIndex\n\n    if len(sequences) == 1:\n        if names is not None:\n            names = names[0]\n        return Index(sequences[0], name=names)\n    else:\n        return MultiIndex.from_arrays(sequences, names=names)\n\n\ndef ensure_index(index_like, copy=False):\n    \"\"\"\n    Ensure that we have an index from some index-like object.\n\n    Parameters\n    ----------\n    index : sequence\n        An Index or other sequence\n    copy : bool\n\n    Returns\n    -------\n    index : Index or MultiIndex\n\n    Examples\n    --------\n    >>> ensure_index(['a', 'b'])\n    Index(['a', 'b'], dtype='object')\n\n    >>> ensure_index([('a', 'a'),  ('b', 'c')])\n    Index([('a', 'a'), ('b', 'c')], dtype='object')\n\n    >>> ensure_index([['a', 'a'], ['b', 'c']])\n    MultiIndex(levels=[['a'], ['b', 'c']],\n               labels=[[0, 0], [0, 1]])\n\n    See Also\n    --------\n    ensure_index_from_sequences\n    \"\"\"\n    if isinstance(index_like, Index):\n        if copy:\n            index_like = index_like.copy()\n        return index_like\n    if hasattr(index_like, 'name'):\n        return Index(index_like, name=index_like.name, copy=copy)\n\n    if is_iterator(index_like):\n        index_like = list(index_like)\n\n    # must check for exactly list here because of strict type\n    # check in clean_index_list\n    if isinstance(index_like, list):\n        if type(index_like) != list:\n            index_like = list(index_like)\n\n        converted, all_arrays = lib.clean_index_list(index_like)\n\n        if len(converted) > 0 and all_arrays:\n            from .multi import MultiIndex\n            return MultiIndex.from_arrays(converted)\n        else:\n            index_like = converted\n    else:\n        # clean_index_list does the equivalent of copying\n        # so only need to do this if not list instance\n        if copy:\n            from copy import copy\n            index_like = copy(index_like)\n\n    return Index(index_like)\n\n\ndef _ensure_has_len(seq):\n    \"\"\"\n    If seq is an iterator, put its values into a list.\n    \"\"\"\n    try:\n        len(seq)\n    except TypeError:\n        return list(seq)\n    else:\n        return seq\n\n\ndef _trim_front(strings):\n    \"\"\"\n    Trims zeros and decimal points.\n    \"\"\"\n    trimmed = strings\n    while len(strings) > 0 and all(x[0] == ' ' for x in trimmed):\n        trimmed = [x[1:] for x in trimmed]\n    return trimmed\n\n\ndef _validate_join_method(method):\n    if method not in ['left', 'right', 'inner', 'outer']:\n        raise ValueError('do not recognize join method %s' % method)\n\n\ndef default_index(n):\n    from pandas.core.index import RangeIndex\n    return RangeIndex(0, n, name=None)\n",
          "file_after": "from datetime import datetime, timedelta\nimport operator\nfrom textwrap import dedent\nimport warnings\n\nimport numpy as np\n\nfrom pandas._libs import (\n    Timedelta, algos as libalgos, index as libindex, join as libjoin, lib,\n    tslibs)\nfrom pandas._libs.lib import is_datetime_array\nimport pandas.compat as compat\nfrom pandas.compat import range, set_function_name, u\nfrom pandas.compat.numpy import function as nv\nfrom pandas.util._decorators import Appender, Substitution, cache_readonly\n\nfrom pandas.core.dtypes.cast import maybe_cast_to_integer_array\nfrom pandas.core.dtypes.common import (\n    ensure_categorical, ensure_int64, ensure_object, ensure_platform_int,\n    is_bool, is_bool_dtype, is_categorical, is_categorical_dtype,\n    is_datetime64_any_dtype, is_datetime64tz_dtype, is_dtype_equal,\n    is_dtype_union_equal, is_extension_array_dtype, is_float, is_float_dtype,\n    is_hashable, is_integer, is_integer_dtype, is_interval_dtype, is_iterator,\n    is_list_like, is_object_dtype, is_period_dtype, is_scalar,\n    is_signed_integer_dtype, is_timedelta64_dtype, is_unsigned_integer_dtype)\nimport pandas.core.dtypes.concat as _concat\nfrom pandas.core.dtypes.generic import (\n    ABCDataFrame, ABCDateOffset, ABCDatetimeIndex, ABCIndexClass,\n    ABCMultiIndex, ABCPeriodIndex, ABCSeries, ABCTimedeltaArray,\n    ABCTimedeltaIndex)\nfrom pandas.core.dtypes.missing import array_equivalent, isna\n\nfrom pandas.core import ops\nfrom pandas.core.accessor import CachedAccessor\nimport pandas.core.algorithms as algos\nfrom pandas.core.arrays import ExtensionArray\nfrom pandas.core.base import IndexOpsMixin, PandasObject\nimport pandas.core.common as com\nfrom pandas.core.indexes.frozen import FrozenList\nimport pandas.core.missing as missing\nfrom pandas.core.ops import get_op_result_name, make_invalid_op\nimport pandas.core.sorting as sorting\nfrom pandas.core.strings import StringMethods\n\nfrom pandas.io.formats.printing import (\n    default_pprint, format_object_attrs, format_object_summary, pprint_thing)\n\n__all__ = ['Index']\n\n_unsortable_types = frozenset(('mixed', 'mixed-integer'))\n\n_index_doc_kwargs = dict(klass='Index', inplace='',\n                         target_klass='Index',\n                         unique='Index', duplicated='np.ndarray')\n_index_shared_docs = dict()\n\n\ndef _try_get_item(x):\n    try:\n        return x.item()\n    except AttributeError:\n        return x\n\n\ndef _make_comparison_op(op, cls):\n    def cmp_method(self, other):\n        if isinstance(other, (np.ndarray, Index, ABCSeries)):\n            if other.ndim > 0 and len(self) != len(other):\n                raise ValueError('Lengths must match to compare')\n\n        from .multi import MultiIndex\n        if is_object_dtype(self) and not isinstance(self, MultiIndex):\n            # don't pass MultiIndex\n            with np.errstate(all='ignore'):\n                result = ops._comp_method_OBJECT_ARRAY(op, self.values, other)\n\n        else:\n\n            # numpy will show a DeprecationWarning on invalid elementwise\n            # comparisons, this will raise in the future\n            with warnings.catch_warnings(record=True):\n                warnings.filterwarnings(\"ignore\", \"elementwise\", FutureWarning)\n                with np.errstate(all='ignore'):\n                    result = op(self.values, np.asarray(other))\n\n        # technically we could support bool dtyped Index\n        # for now just return the indexing array directly\n        if is_bool_dtype(result):\n            return result\n        try:\n            return Index(result)\n        except TypeError:\n            return result\n\n    name = '__{name}__'.format(name=op.__name__)\n    # TODO: docstring?\n    return set_function_name(cmp_method, name, cls)\n\n\ndef _make_arithmetic_op(op, cls):\n    def index_arithmetic_method(self, other):\n        if isinstance(other, (ABCSeries, ABCDataFrame)):\n            return NotImplemented\n        elif isinstance(other, ABCTimedeltaIndex):\n            # Defer to subclass implementation\n            return NotImplemented\n        elif (isinstance(other, (np.ndarray, ABCTimedeltaArray)) and\n              is_timedelta64_dtype(other)):\n            # GH#22390; wrap in Series for op, this will in turn wrap in\n            # TimedeltaIndex, but will correctly raise TypeError instead of\n            # NullFrequencyError for add/sub ops\n            from pandas import Series\n            other = Series(other)\n            out = op(self, other)\n            return Index(out, name=self.name)\n\n        other = self._validate_for_numeric_binop(other, op)\n\n        # handle time-based others\n        if isinstance(other, (ABCDateOffset, np.timedelta64, timedelta)):\n            return self._evaluate_with_timedelta_like(other, op)\n        elif isinstance(other, (datetime, np.datetime64)):\n            return self._evaluate_with_datetime_like(other, op)\n\n        values = self.values\n        with np.errstate(all='ignore'):\n            result = op(values, other)\n\n        result = missing.dispatch_missing(op, values, other, result)\n\n        attrs = self._get_attributes_dict()\n        attrs = self._maybe_update_attributes(attrs)\n        if op is divmod:\n            result = (Index(result[0], **attrs), Index(result[1], **attrs))\n        else:\n            result = Index(result, **attrs)\n        return result\n\n    name = '__{name}__'.format(name=op.__name__)\n    # TODO: docstring?\n    return set_function_name(index_arithmetic_method, name, cls)\n\n\nclass InvalidIndexError(Exception):\n    pass\n\n\n_o_dtype = np.dtype(object)\n_Identity = object\n\n\ndef _new_Index(cls, d):\n    \"\"\"\n    This is called upon unpickling, rather than the default which doesn't\n    have arguments and breaks __new__.\n    \"\"\"\n    # required for backward compat, because PI can't be instantiated with\n    # ordinals through __new__ GH #13277\n    if issubclass(cls, ABCPeriodIndex):\n        from pandas.core.indexes.period import _new_PeriodIndex\n        return _new_PeriodIndex(cls, **d)\n    return cls.__new__(cls, **d)\n\n\nclass Index(IndexOpsMixin, PandasObject):\n    \"\"\"\n    Immutable ndarray implementing an ordered, sliceable set. The basic object\n    storing axis labels for all pandas objects.\n\n    Parameters\n    ----------\n    data : array-like (1-dimensional)\n    dtype : NumPy dtype (default: object)\n        If dtype is None, we find the dtype that best fits the data.\n        If an actual dtype is provided, we coerce to that dtype if it's safe.\n        Otherwise, an error will be raised.\n    copy : bool\n        Make a copy of input ndarray\n    name : object\n        Name to be stored in the index\n    tupleize_cols : bool (default: True)\n        When True, attempt to create a MultiIndex if possible\n\n    Notes\n    -----\n    An Index instance can **only** contain hashable objects\n\n    Examples\n    --------\n    >>> pd.Index([1, 2, 3])\n    Int64Index([1, 2, 3], dtype='int64')\n\n    >>> pd.Index(list('abc'))\n    Index(['a', 'b', 'c'], dtype='object')\n\n    See Also\n    ---------\n    RangeIndex : Index implementing a monotonic integer range.\n    CategoricalIndex : Index of :class:`Categorical` s.\n    MultiIndex : A multi-level, or hierarchical, Index.\n    IntervalIndex : An Index of :class:`Interval` s.\n    DatetimeIndex, TimedeltaIndex, PeriodIndex\n    Int64Index, UInt64Index,  Float64Index\n    \"\"\"\n    # To hand over control to subclasses\n    _join_precedence = 1\n\n    # Cython methods; see github.com/cython/cython/issues/2647\n    #  for why we need to wrap these instead of making them class attributes\n    # Moreover, cython will choose the appropriate-dtyped sub-function\n    #  given the dtypes of the passed arguments\n    def _left_indexer_unique(self, left, right):\n        return libjoin.left_join_indexer_unique(left, right)\n\n    def _left_indexer(self, left, right):\n        return libjoin.left_join_indexer(left, right)\n\n    def _inner_indexer(self, left, right):\n        return libjoin.inner_join_indexer(left, right)\n\n    def _outer_indexer(self, left, right):\n        return libjoin.outer_join_indexer(left, right)\n\n    _typ = 'index'\n    _data = None\n    _id = None\n    name = None\n    asi8 = None\n    _comparables = ['name']\n    _attributes = ['name']\n    _is_numeric_dtype = False\n    _can_hold_na = True\n\n    # would we like our indexing holder to defer to us\n    _defer_to_indexing = False\n\n    # prioritize current class for _shallow_copy_with_infer,\n    # used to infer integers as datetime-likes\n    _infer_as_myclass = False\n\n    _engine_type = libindex.ObjectEngine\n\n    _accessors = {'str'}\n\n    str = CachedAccessor(\"str\", StringMethods)\n\n    # --------------------------------------------------------------------\n    # Constructors\n\n    def __new__(cls, data=None, dtype=None, copy=False, name=None,\n                fastpath=None, tupleize_cols=True, **kwargs):\n\n        if name is None and hasattr(data, 'name'):\n            name = data.name\n\n        if fastpath is not None:\n            warnings.warn(\"The 'fastpath' keyword is deprecated, and will be \"\n                          \"removed in a future version.\",\n                          FutureWarning, stacklevel=2)\n            if fastpath:\n                return cls._simple_new(data, name)\n\n        from .range import RangeIndex\n\n        # range\n        if isinstance(data, RangeIndex):\n            return RangeIndex(start=data, copy=copy, dtype=dtype, name=name)\n        elif isinstance(data, range):\n            return RangeIndex.from_range(data, copy=copy, dtype=dtype,\n                                         name=name)\n\n        # categorical\n        elif is_categorical_dtype(data) or is_categorical_dtype(dtype):\n            from .category import CategoricalIndex\n            return CategoricalIndex(data, dtype=dtype, copy=copy, name=name,\n                                    **kwargs)\n\n        # interval\n        elif ((is_interval_dtype(data) or is_interval_dtype(dtype)) and\n              not is_object_dtype(dtype)):\n            from .interval import IntervalIndex\n            closed = kwargs.get('closed', None)\n            return IntervalIndex(data, dtype=dtype, name=name, copy=copy,\n                                 closed=closed)\n\n        elif (is_datetime64_any_dtype(data) or\n              (dtype is not None and is_datetime64_any_dtype(dtype)) or\n                'tz' in kwargs):\n            from pandas import DatetimeIndex\n\n            if dtype is not None and is_dtype_equal(_o_dtype, dtype):\n                # GH#23524 passing `dtype=object` to DatetimeIndex is invalid,\n                #  will raise in the where `data` is already tz-aware.  So\n                #  we leave it out of this step and cast to object-dtype after\n                #  the DatetimeIndex construction.\n                # Note we can pass copy=False because the .astype below\n                #  will always make a copy\n                result = DatetimeIndex(data, copy=False, name=name, **kwargs)\n                return result.astype(object)\n            else:\n                result = DatetimeIndex(data, copy=copy, name=name,\n                                       dtype=dtype, **kwargs)\n                return result\n\n        elif (is_timedelta64_dtype(data) or\n              (dtype is not None and is_timedelta64_dtype(dtype))):\n            from pandas import TimedeltaIndex\n            result = TimedeltaIndex(data, copy=copy, name=name, **kwargs)\n            if dtype is not None and _o_dtype == dtype:\n                return Index(result.to_pytimedelta(), dtype=_o_dtype)\n            else:\n                return result\n\n        elif is_period_dtype(data) and not is_object_dtype(dtype):\n            from pandas import PeriodIndex\n            result = PeriodIndex(data, copy=copy, name=name, **kwargs)\n            return result\n\n        # extension dtype\n        elif is_extension_array_dtype(data) or is_extension_array_dtype(dtype):\n            data = np.asarray(data)\n            if not (dtype is None or is_object_dtype(dtype)):\n\n                # coerce to the provided dtype\n                data = dtype.construct_array_type()._from_sequence(\n                    data, dtype=dtype, copy=False)\n\n            # coerce to the object dtype\n            data = data.astype(object)\n            return Index(data, dtype=object, copy=copy, name=name,\n                         **kwargs)\n\n        # index-like\n        elif isinstance(data, (np.ndarray, Index, ABCSeries)):\n            if dtype is not None:\n                try:\n\n                    # we need to avoid having numpy coerce\n                    # things that look like ints/floats to ints unless\n                    # they are actually ints, e.g. '0' and 0.0\n                    # should not be coerced\n                    # GH 11836\n                    if is_integer_dtype(dtype):\n                        inferred = lib.infer_dtype(data)\n                        if inferred == 'integer':\n                            data = maybe_cast_to_integer_array(data, dtype,\n                                                               copy=copy)\n                        elif inferred in ['floating', 'mixed-integer-float']:\n                            if isna(data).any():\n                                raise ValueError('cannot convert float '\n                                                 'NaN to integer')\n\n                            if inferred == \"mixed-integer-float\":\n                                data = maybe_cast_to_integer_array(data, dtype)\n\n                            # If we are actually all equal to integers,\n                            # then coerce to integer.\n                            try:\n                                return cls._try_convert_to_int_index(\n                                    data, copy, name, dtype)\n                            except ValueError:\n                                pass\n\n                            # Return an actual float index.\n                            from .numeric import Float64Index\n                            return Float64Index(data, copy=copy, dtype=dtype,\n                                                name=name)\n\n                        elif inferred == 'string':\n                            pass\n                        else:\n                            data = data.astype(dtype)\n                    elif is_float_dtype(dtype):\n                        inferred = lib.infer_dtype(data)\n                        if inferred == 'string':\n                            pass\n                        else:\n                            data = data.astype(dtype)\n                    else:\n                        data = np.array(data, dtype=dtype, copy=copy)\n\n                except (TypeError, ValueError) as e:\n                    msg = str(e)\n                    if (\"cannot convert float\" in msg or\n                            \"Trying to coerce float values to integer\" in msg):\n                        raise\n\n            # maybe coerce to a sub-class\n            from pandas.core.indexes.period import (\n                PeriodIndex, IncompatibleFrequency)\n\n            if is_signed_integer_dtype(data.dtype):\n                from .numeric import Int64Index\n                return Int64Index(data, copy=copy, dtype=dtype, name=name)\n            elif is_unsigned_integer_dtype(data.dtype):\n                from .numeric import UInt64Index\n                return UInt64Index(data, copy=copy, dtype=dtype, name=name)\n            elif is_float_dtype(data.dtype):\n                from .numeric import Float64Index\n                return Float64Index(data, copy=copy, dtype=dtype, name=name)\n            elif issubclass(data.dtype.type, np.bool) or is_bool_dtype(data):\n                subarr = data.astype('object')\n            else:\n                subarr = com.asarray_tuplesafe(data, dtype=object)\n\n            # asarray_tuplesafe does not always copy underlying data,\n            # so need to make sure that this happens\n            if copy:\n                subarr = subarr.copy()\n\n            if dtype is None:\n                inferred = lib.infer_dtype(subarr)\n                if inferred == 'integer':\n                    try:\n                        return cls._try_convert_to_int_index(\n                            subarr, copy, name, dtype)\n                    except ValueError:\n                        pass\n\n                    return Index(subarr, copy=copy,\n                                 dtype=object, name=name)\n                elif inferred in ['floating', 'mixed-integer-float']:\n                    from .numeric import Float64Index\n                    return Float64Index(subarr, copy=copy, name=name)\n                elif inferred == 'interval':\n                    from .interval import IntervalIndex\n                    return IntervalIndex(subarr, name=name, copy=copy)\n                elif inferred == 'boolean':\n                    # don't support boolean explicitly ATM\n                    pass\n                elif inferred != 'string':\n                    if inferred.startswith('datetime'):\n                        if (lib.is_datetime_with_singletz_array(subarr) or\n                                'tz' in kwargs):\n                            # only when subarr has the same tz\n                            from pandas import DatetimeIndex\n                            try:\n                                return DatetimeIndex(subarr, copy=copy,\n                                                     name=name, **kwargs)\n                            except tslibs.OutOfBoundsDatetime:\n                                pass\n\n                    elif inferred.startswith('timedelta'):\n                        from pandas import TimedeltaIndex\n                        return TimedeltaIndex(subarr, copy=copy, name=name,\n                                              **kwargs)\n                    elif inferred == 'period':\n                        try:\n                            return PeriodIndex(subarr, name=name, **kwargs)\n                        except IncompatibleFrequency:\n                            pass\n            return cls._simple_new(subarr, name)\n\n        elif hasattr(data, '__array__'):\n            return Index(np.asarray(data), dtype=dtype, copy=copy, name=name,\n                         **kwargs)\n        elif data is None or is_scalar(data):\n            cls._scalar_data_error(data)\n        else:\n            if tupleize_cols and is_list_like(data):\n                # GH21470: convert iterable to list before determining if empty\n                if is_iterator(data):\n                    data = list(data)\n\n                if data and all(isinstance(e, tuple) for e in data):\n                    # we must be all tuples, otherwise don't construct\n                    # 10697\n                    from .multi import MultiIndex\n                    return MultiIndex.from_tuples(\n                        data, names=name or kwargs.get('names'))\n            # other iterable of some kind\n            subarr = com.asarray_tuplesafe(data, dtype=object)\n            return Index(subarr, dtype=dtype, copy=copy, name=name, **kwargs)\n\n    \"\"\"\n    NOTE for new Index creation:\n\n    - _simple_new: It returns new Index with the same type as the caller.\n      All metadata (such as name) must be provided by caller's responsibility.\n      Using _shallow_copy is recommended because it fills these metadata\n      otherwise specified.\n\n    - _shallow_copy: It returns new Index with the same type (using\n      _simple_new), but fills caller's metadata otherwise specified. Passed\n      kwargs will overwrite corresponding metadata.\n\n    - _shallow_copy_with_infer: It returns new Index inferring its type\n      from passed values. It fills caller's metadata otherwise specified as the\n      same as _shallow_copy.\n\n    See each method's docstring.\n    \"\"\"\n\n    @classmethod\n    def _simple_new(cls, values, name=None, dtype=None, **kwargs):\n        \"\"\"\n        We require that we have a dtype compat for the values. If we are passed\n        a non-dtype compat, then coerce using the constructor.\n\n        Must be careful not to recurse.\n        \"\"\"\n        if not hasattr(values, 'dtype'):\n            if (values is None or not len(values)) and dtype is not None:\n                values = np.empty(0, dtype=dtype)\n            else:\n                values = np.array(values, copy=False)\n                if is_object_dtype(values):\n                    values = cls(values, name=name, dtype=dtype,\n                                 **kwargs)._ndarray_values\n\n        if isinstance(values, (ABCSeries, ABCIndexClass)):\n            # Index._data must always be an ndarray.\n            # This is no-copy for when _values is an ndarray,\n            # which should be always at this point.\n            values = np.asarray(values._values)\n\n        result = object.__new__(cls)\n        result._data = values\n        result.name = name\n        for k, v in compat.iteritems(kwargs):\n            setattr(result, k, v)\n        return result._reset_identity()\n\n    @cache_readonly\n    def _constructor(self):\n        return type(self)\n\n    # --------------------------------------------------------------------\n    # Index Internals Methods\n\n    def _get_attributes_dict(self):\n        \"\"\"\n        Return an attributes dict for my class.\n        \"\"\"\n        return {k: getattr(self, k, None) for k in self._attributes}\n\n    _index_shared_docs['_shallow_copy'] = \"\"\"\n        Create a new Index with the same class as the caller, don't copy the\n        data, use the same object attributes with passed in attributes taking\n        precedence.\n\n        *this is an internal non-public method*\n\n        Parameters\n        ----------\n        values : the values to create the new Index, optional\n        kwargs : updates the default attributes for this Index\n        \"\"\"\n\n    @Appender(_index_shared_docs['_shallow_copy'])\n    def _shallow_copy(self, values=None, **kwargs):\n        if values is None:\n            values = self.values\n        attributes = self._get_attributes_dict()\n        attributes.update(kwargs)\n        if not len(values) and 'dtype' not in kwargs:\n            attributes['dtype'] = self.dtype\n\n        # _simple_new expects an ndarray\n        values = getattr(values, 'values', values)\n        if isinstance(values, ABCDatetimeIndex):\n            # `self.values` returns `self` for tz-aware, so we need to unwrap\n            #  more specifically\n            values = values.asi8\n\n        return self._simple_new(values, **attributes)\n\n    def _shallow_copy_with_infer(self, values, **kwargs):\n        \"\"\"\n        Create a new Index inferring the class with passed value, don't copy\n        the data, use the same object attributes with passed in attributes\n        taking precedence.\n\n        *this is an internal non-public method*\n\n        Parameters\n        ----------\n        values : the values to create the new Index, optional\n        kwargs : updates the default attributes for this Index\n        \"\"\"\n        attributes = self._get_attributes_dict()\n        attributes.update(kwargs)\n        attributes['copy'] = False\n        if not len(values) and 'dtype' not in kwargs:\n            attributes['dtype'] = self.dtype\n        if self._infer_as_myclass:\n            try:\n                return self._constructor(values, **attributes)\n            except (TypeError, ValueError):\n                pass\n        return Index(values, **attributes)\n\n    def _deepcopy_if_needed(self, orig, copy=False):\n        \"\"\"\n        Make a copy of self if data coincides (in memory) with orig.\n        Subclasses should override this if self._base is not an ndarray.\n\n        .. versionadded:: 0.19.0\n\n        Parameters\n        ----------\n        orig : ndarray\n            other ndarray to compare self._data against\n        copy : boolean, default False\n            when False, do not run any check, just return self\n\n        Returns\n        -------\n        A copy of self if needed, otherwise self : Index\n        \"\"\"\n        if copy:\n            # Retrieve the \"base objects\", i.e. the original memory allocations\n            if not isinstance(orig, np.ndarray):\n                # orig is a DatetimeIndex\n                orig = orig.values\n            orig = orig if orig.base is None else orig.base\n            new = self._data if self._data.base is None else self._data.base\n            if orig is new:\n                return self.copy(deep=True)\n\n        return self\n\n    def _update_inplace(self, result, **kwargs):\n        # guard when called from IndexOpsMixin\n        raise TypeError(\"Index can't be updated inplace\")\n\n    def is_(self, other):\n        \"\"\"\n        More flexible, faster check like ``is`` but that works through views.\n\n        Note: this is *not* the same as ``Index.identical()``, which checks\n        that metadata is also the same.\n\n        Parameters\n        ----------\n        other : object\n            other object to compare against.\n\n        Returns\n        -------\n        True if both have same underlying data, False otherwise : bool\n        \"\"\"\n        # use something other than None to be clearer\n        return self._id is getattr(\n            other, '_id', Ellipsis) and self._id is not None\n\n    def _reset_identity(self):\n        \"\"\"\n        Initializes or resets ``_id`` attribute with new object.\n        \"\"\"\n        self._id = _Identity()\n        return self\n\n    def _cleanup(self):\n        self._engine.clear_mapping()\n\n    @cache_readonly\n    def _engine(self):\n        # property, for now, slow to look up\n        return self._engine_type(lambda: self._ndarray_values, len(self))\n\n    # --------------------------------------------------------------------\n    # Array-Like Methods\n\n    # ndarray compat\n    def __len__(self):\n        \"\"\"\n        Return the length of the Index.\n        \"\"\"\n        return len(self._data)\n\n    def __array__(self, dtype=None):\n        \"\"\"\n        The array interface, return my values.\n        \"\"\"\n        return self._data.view(np.ndarray)\n\n    def __array_wrap__(self, result, context=None):\n        \"\"\"\n        Gets called after a ufunc.\n        \"\"\"\n        if is_bool_dtype(result):\n            return result\n\n        attrs = self._get_attributes_dict()\n        attrs = self._maybe_update_attributes(attrs)\n        return Index(result, **attrs)\n\n    @cache_readonly\n    def dtype(self):\n        \"\"\"\n        Return the dtype object of the underlying data.\n        \"\"\"\n        return self._data.dtype\n\n    @cache_readonly\n    def dtype_str(self):\n        \"\"\"\n        Return the dtype str of the underlying data.\n        \"\"\"\n        return str(self.dtype)\n\n    def ravel(self, order='C'):\n        \"\"\"\n        Return an ndarray of the flattened values of the underlying data.\n\n        See Also\n        --------\n        numpy.ndarray.ravel\n        \"\"\"\n        return self._ndarray_values.ravel(order=order)\n\n    def view(self, cls=None):\n\n        # we need to see if we are subclassing an\n        # index type here\n        if cls is not None and not hasattr(cls, '_typ'):\n            result = self._data.view(cls)\n        else:\n            result = self._shallow_copy()\n        if isinstance(result, Index):\n            result._id = self._id\n        return result\n\n    _index_shared_docs['astype'] = \"\"\"\n        Create an Index with values cast to dtypes. The class of a new Index\n        is determined by dtype. When conversion is impossible, a ValueError\n        exception is raised.\n\n        Parameters\n        ----------\n        dtype : numpy dtype or pandas type\n        copy : bool, default True\n            By default, astype always returns a newly allocated object.\n            If copy is set to False and internal requirements on dtype are\n            satisfied, the original data is used to create a new Index\n            or the original Index is returned.\n\n            .. versionadded:: 0.19.0\n        \"\"\"\n\n    @Appender(_index_shared_docs['astype'])\n    def astype(self, dtype, copy=True):\n        if is_dtype_equal(self.dtype, dtype):\n            return self.copy() if copy else self\n\n        elif is_categorical_dtype(dtype):\n            from .category import CategoricalIndex\n            return CategoricalIndex(self.values, name=self.name, dtype=dtype,\n                                    copy=copy)\n\n        elif is_extension_array_dtype(dtype):\n            return Index(np.asarray(self), dtype=dtype, copy=copy)\n\n        try:\n            if is_datetime64tz_dtype(dtype):\n                from pandas import DatetimeIndex\n                return DatetimeIndex(self.values, name=self.name, dtype=dtype,\n                                     copy=copy)\n            return Index(self.values.astype(dtype, copy=copy), name=self.name,\n                         dtype=dtype)\n        except (TypeError, ValueError):\n            msg = 'Cannot cast {name} to dtype {dtype}'\n            raise TypeError(msg.format(name=type(self).__name__, dtype=dtype))\n\n    _index_shared_docs['take'] = \"\"\"\n        Return a new %(klass)s of the values selected by the indices.\n\n        For internal compatibility with numpy arrays.\n\n        Parameters\n        ----------\n        indices : list\n            Indices to be taken\n        axis : int, optional\n            The axis over which to select values, always 0.\n        allow_fill : bool, default True\n        fill_value : bool, default None\n            If allow_fill=True and fill_value is not None, indices specified by\n            -1 is regarded as NA. If Index doesn't hold NA, raise ValueError\n\n        See Also\n        --------\n        numpy.ndarray.take\n        \"\"\"\n\n    @Appender(_index_shared_docs['take'] % _index_doc_kwargs)\n    def take(self, indices, axis=0, allow_fill=True,\n             fill_value=None, **kwargs):\n        if kwargs:\n            nv.validate_take(tuple(), kwargs)\n        indices = ensure_platform_int(indices)\n        if self._can_hold_na:\n            taken = self._assert_take_fillable(self.values, indices,\n                                               allow_fill=allow_fill,\n                                               fill_value=fill_value,\n                                               na_value=self._na_value)\n        else:\n            if allow_fill and fill_value is not None:\n                msg = 'Unable to fill values because {0} cannot contain NA'\n                raise ValueError(msg.format(self.__class__.__name__))\n            taken = self.values.take(indices)\n        return self._shallow_copy(taken)\n\n    def _assert_take_fillable(self, values, indices, allow_fill=True,\n                              fill_value=None, na_value=np.nan):\n        \"\"\"\n        Internal method to handle NA filling of take.\n        \"\"\"\n        indices = ensure_platform_int(indices)\n\n        # only fill if we are passing a non-None fill_value\n        if allow_fill and fill_value is not None:\n            if (indices < -1).any():\n                msg = ('When allow_fill=True and fill_value is not None, '\n                       'all indices must be >= -1')\n                raise ValueError(msg)\n            taken = algos.take(values,\n                               indices,\n                               allow_fill=allow_fill,\n                               fill_value=na_value)\n        else:\n            taken = values.take(indices)\n        return taken\n\n    def repeat(self, repeats, *args, **kwargs):\n        \"\"\"\n        Repeat elements of an Index.\n\n        Returns a new index where each element of the current index\n        is repeated consecutively a given number of times.\n\n        Parameters\n        ----------\n        repeats : int\n            The number of repetitions for each element.\n        **kwargs\n            Additional keywords have no effect but might be accepted for\n            compatibility with numpy.\n\n        Returns\n        -------\n        pandas.Index\n            Newly created Index with repeated elements.\n\n        See Also\n        --------\n        Series.repeat : Equivalent function for Series.\n        numpy.repeat : Underlying implementation.\n\n        Examples\n        --------\n        >>> idx = pd.Index([1, 2, 3])\n        >>> idx\n        Int64Index([1, 2, 3], dtype='int64')\n        >>> idx.repeat(2)\n        Int64Index([1, 1, 2, 2, 3, 3], dtype='int64')\n        >>> idx.repeat(3)\n        Int64Index([1, 1, 1, 2, 2, 2, 3, 3, 3], dtype='int64')\n        \"\"\"\n        nv.validate_repeat(args, kwargs)\n        return self._shallow_copy(self._values.repeat(repeats))\n\n    # --------------------------------------------------------------------\n    # Copying Methods\n\n    _index_shared_docs['copy'] = \"\"\"\n        Make a copy of this object.  Name and dtype sets those attributes on\n        the new object.\n\n        Parameters\n        ----------\n        name : string, optional\n        deep : boolean, default False\n        dtype : numpy dtype or pandas type\n\n        Returns\n        -------\n        copy : Index\n\n        Notes\n        -----\n        In most cases, there should be no functional difference from using\n        ``deep``, but if ``deep`` is passed it will attempt to deepcopy.\n        \"\"\"\n\n    @Appender(_index_shared_docs['copy'])\n    def copy(self, name=None, deep=False, dtype=None, **kwargs):\n        if deep:\n            new_index = self._shallow_copy(self._data.copy())\n        else:\n            new_index = self._shallow_copy()\n\n        names = kwargs.get('names')\n        names = self._validate_names(name=name, names=names, deep=deep)\n        new_index = new_index.set_names(names)\n\n        if dtype:\n            new_index = new_index.astype(dtype)\n        return new_index\n\n    def __copy__(self, **kwargs):\n        return self.copy(**kwargs)\n\n    def __deepcopy__(self, memo=None):\n        \"\"\"\n        Parameters\n        ----------\n        memo, default None\n            Standard signature. Unused\n        \"\"\"\n        if memo is None:\n            memo = {}\n        return self.copy(deep=True)\n\n    # --------------------------------------------------------------------\n    # Rendering Methods\n\n    def __unicode__(self):\n        \"\"\"\n        Return a string representation for this object.\n\n        Invoked by unicode(df) in py2 only. Yields a Unicode String in both\n        py2/py3.\n        \"\"\"\n        klass = self.__class__.__name__\n        data = self._format_data()\n        attrs = self._format_attrs()\n        space = self._format_space()\n\n        prepr = (u(\",%s\") %\n                 space).join(u(\"%s=%s\") % (k, v) for k, v in attrs)\n\n        # no data provided, just attributes\n        if data is None:\n            data = ''\n\n        res = u(\"%s(%s%s)\") % (klass, data, prepr)\n\n        return res\n\n    def _format_space(self):\n\n        # using space here controls if the attributes\n        # are line separated or not (the default)\n\n        # max_seq_items = get_option('display.max_seq_items')\n        # if len(self) > max_seq_items:\n        #    space = \"\\n%s\" % (' ' * (len(klass) + 1))\n        return \" \"\n\n    @property\n    def _formatter_func(self):\n        \"\"\"\n        Return the formatter function.\n        \"\"\"\n        return default_pprint\n\n    def _format_data(self, name=None):\n        \"\"\"\n        Return the formatted data as a unicode string.\n        \"\"\"\n\n        # do we want to justify (only do so for non-objects)\n        is_justify = not (self.inferred_type in ('string', 'unicode') or\n                          (self.inferred_type == 'categorical' and\n                           is_object_dtype(self.categories)))\n\n        return format_object_summary(self, self._formatter_func,\n                                     is_justify=is_justify, name=name)\n\n    def _format_attrs(self):\n        \"\"\"\n        Return a list of tuples of the (attr,formatted_value).\n        \"\"\"\n        return format_object_attrs(self)\n\n    def _mpl_repr(self):\n        # how to represent ourselves to matplotlib\n        return self.values\n\n    def format(self, name=False, formatter=None, **kwargs):\n        \"\"\"\n        Render a string representation of the Index.\n        \"\"\"\n        header = []\n        if name:\n            header.append(pprint_thing(self.name,\n                                       escape_chars=('\\t', '\\r', '\\n')) if\n                          self.name is not None else '')\n\n        if formatter is not None:\n            return header + list(self.map(formatter))\n\n        return self._format_with_header(header, **kwargs)\n\n    def _format_with_header(self, header, na_rep='NaN', **kwargs):\n        values = self.values\n\n        from pandas.io.formats.format import format_array\n\n        if is_categorical_dtype(values.dtype):\n            values = np.array(values)\n\n        elif is_object_dtype(values.dtype):\n            values = lib.maybe_convert_objects(values, safe=1)\n\n        if is_object_dtype(values.dtype):\n            result = [pprint_thing(x, escape_chars=('\\t', '\\r', '\\n'))\n                      for x in values]\n\n            # could have nans\n            mask = isna(values)\n            if mask.any():\n                result = np.array(result)\n                result[mask] = na_rep\n                result = result.tolist()\n\n        else:\n            result = _trim_front(format_array(values, None, justify='left'))\n        return header + result\n\n    def to_native_types(self, slicer=None, **kwargs):\n        \"\"\"\n        Format specified values of `self` and return them.\n\n        Parameters\n        ----------\n        slicer : int, array-like\n            An indexer into `self` that specifies which values\n            are used in the formatting process.\n        kwargs : dict\n            Options for specifying how the values should be formatted.\n            These options include the following:\n\n            1) na_rep : str\n                The value that serves as a placeholder for NULL values\n            2) quoting : bool or None\n                Whether or not there are quoted values in `self`\n            3) date_format : str\n                The format used to represent date-like values\n        \"\"\"\n\n        values = self\n        if slicer is not None:\n            values = values[slicer]\n        return values._format_native_types(**kwargs)\n\n    def _format_native_types(self, na_rep='', quoting=None, **kwargs):\n        \"\"\"\n        Actually format specific types of the index.\n        \"\"\"\n        mask = isna(self)\n        if not self.is_object() and not quoting:\n            values = np.asarray(self).astype(str)\n        else:\n            values = np.array(self, dtype=object, copy=True)\n\n        values[mask] = na_rep\n        return values\n\n    def _summary(self, name=None):\n        \"\"\"\n        Return a summarized representation.\n\n        Parameters\n        ----------\n        name : str\n            name to use in the summary representation\n\n        Returns\n        -------\n        String with a summarized representation of the index\n        \"\"\"\n        if len(self) > 0:\n            head = self[0]\n            if (hasattr(head, 'format') and\n                    not isinstance(head, compat.string_types)):\n                head = head.format()\n            tail = self[-1]\n            if (hasattr(tail, 'format') and\n                    not isinstance(tail, compat.string_types)):\n                tail = tail.format()\n            index_summary = ', %s to %s' % (pprint_thing(head),\n                                            pprint_thing(tail))\n        else:\n            index_summary = ''\n\n        if name is None:\n            name = type(self).__name__\n        return '%s: %s entries%s' % (name, len(self), index_summary)\n\n    def summary(self, name=None):\n        \"\"\"\n        Return a summarized representation.\n\n        .. deprecated:: 0.23.0\n        \"\"\"\n        warnings.warn(\"'summary' is deprecated and will be removed in a \"\n                      \"future version.\", FutureWarning, stacklevel=2)\n        return self._summary(name)\n\n    # --------------------------------------------------------------------\n    # Conversion Methods\n\n    def to_flat_index(self):\n        \"\"\"\n        Identity method.\n\n        .. versionadded:: 0.24.0\n\n        This is implemented for compatability with subclass implementations\n        when chaining.\n\n        Returns\n        -------\n        pd.Index\n            Caller.\n\n        See Also\n        --------\n        MultiIndex.to_flat_index : Subclass implementation.\n        \"\"\"\n        return self\n\n    def to_series(self, index=None, name=None):\n        \"\"\"\n        Create a Series with both index and values equal to the index keys\n        useful with map for returning an indexer based on an index.\n\n        Parameters\n        ----------\n        index : Index, optional\n            index of resulting Series. If None, defaults to original index\n        name : string, optional\n            name of resulting Series. If None, defaults to name of original\n            index\n\n        Returns\n        -------\n        Series : dtype will be based on the type of the Index values.\n        \"\"\"\n\n        from pandas import Series\n\n        if index is None:\n            index = self._shallow_copy()\n        if name is None:\n            name = self.name\n\n        return Series(self.values.copy(), index=index, name=name)\n\n    def to_frame(self, index=True, name=None):\n        \"\"\"\n        Create a DataFrame with a column containing the Index.\n\n        .. versionadded:: 0.24.0\n\n        Parameters\n        ----------\n        index : boolean, default True\n            Set the index of the returned DataFrame as the original Index.\n\n        name : object, default None\n            The passed name should substitute for the index name (if it has\n            one).\n\n        Returns\n        -------\n        DataFrame\n            DataFrame containing the original Index data.\n\n        See Also\n        --------\n        Index.to_series : Convert an Index to a Series.\n        Series.to_frame : Convert Series to DataFrame.\n\n        Examples\n        --------\n        >>> idx = pd.Index(['Ant', 'Bear', 'Cow'], name='animal')\n        >>> idx.to_frame()\n               animal\n        animal\n        Ant       Ant\n        Bear     Bear\n        Cow       Cow\n\n        By default, the original Index is reused. To enforce a new Index:\n\n        >>> idx.to_frame(index=False)\n            animal\n        0   Ant\n        1  Bear\n        2   Cow\n\n        To override the name of the resulting column, specify `name`:\n\n        >>> idx.to_frame(index=False, name='zoo')\n            zoo\n        0   Ant\n        1  Bear\n        2   Cow\n        \"\"\"\n\n        from pandas import DataFrame\n        if name is None:\n            name = self.name or 0\n        result = DataFrame({name: self.values.copy()})\n\n        if index:\n            result.index = self\n        return result\n\n    # --------------------------------------------------------------------\n    # Name-Centric Methods\n\n    def _validate_names(self, name=None, names=None, deep=False):\n        \"\"\"\n        Handles the quirks of having a singular 'name' parameter for general\n        Index and plural 'names' parameter for MultiIndex.\n        \"\"\"\n        from copy import deepcopy\n        if names is not None and name is not None:\n            raise TypeError(\"Can only provide one of `names` and `name`\")\n        elif names is None and name is None:\n            return deepcopy(self.names) if deep else self.names\n        elif names is not None:\n            if not is_list_like(names):\n                raise TypeError(\"Must pass list-like as `names`.\")\n            return names\n        else:\n            if not is_list_like(name):\n                return [name]\n            return name\n\n    def _get_names(self):\n        return FrozenList((self.name, ))\n\n    def _set_names(self, values, level=None):\n        \"\"\"\n        Set new names on index. Each name has to be a hashable type.\n\n        Parameters\n        ----------\n        values : str or sequence\n            name(s) to set\n        level : int, level name, or sequence of int/level names (default None)\n            If the index is a MultiIndex (hierarchical), level(s) to set (None\n            for all levels).  Otherwise level must be None\n\n        Raises\n        ------\n        TypeError if each name is not hashable.\n        \"\"\"\n        if not is_list_like(values):\n            raise ValueError('Names must be a list-like')\n        if len(values) != 1:\n            raise ValueError('Length of new names must be 1, got %d' %\n                             len(values))\n\n        # GH 20527\n        # All items in 'name' need to be hashable:\n        for name in values:\n            if not is_hashable(name):\n                raise TypeError('{}.name must be a hashable type'\n                                .format(self.__class__.__name__))\n        self.name = values[0]\n\n    names = property(fset=_set_names, fget=_get_names)\n\n    def set_names(self, names, level=None, inplace=False):\n        \"\"\"\n        Set Index or MultiIndex name.\n\n        Able to set new names partially and by level.\n\n        Parameters\n        ----------\n        names : label or list of label\n            Name(s) to set.\n        level : int, label or list of int or label, optional\n            If the index is a MultiIndex, level(s) to set (None for all\n            levels). Otherwise level must be None.\n        inplace : bool, default False\n            Modifies the object directly, instead of creating a new Index or\n            MultiIndex.\n\n        Returns\n        -------\n        Index\n            The same type as the caller or None if inplace is True.\n\n        See Also\n        --------\n        Index.rename : Able to set new names without level.\n\n        Examples\n        --------\n        >>> idx = pd.Index([1, 2, 3, 4])\n        >>> idx\n        Int64Index([1, 2, 3, 4], dtype='int64')\n        >>> idx.set_names('quarter')\n        Int64Index([1, 2, 3, 4], dtype='int64', name='quarter')\n\n        >>> idx = pd.MultiIndex.from_product([['python', 'cobra'],\n        ...                                   [2018, 2019]])\n        >>> idx\n        MultiIndex(levels=[['cobra', 'python'], [2018, 2019]],\n                   labels=[[1, 1, 0, 0], [0, 1, 0, 1]])\n        >>> idx.set_names(['kind', 'year'], inplace=True)\n        >>> idx\n        MultiIndex(levels=[['cobra', 'python'], [2018, 2019]],\n                   labels=[[1, 1, 0, 0], [0, 1, 0, 1]],\n                   names=['kind', 'year'])\n        >>> idx.set_names('species', level=0)\n        MultiIndex(levels=[['cobra', 'python'], [2018, 2019]],\n                   labels=[[1, 1, 0, 0], [0, 1, 0, 1]],\n                   names=['species', 'year'])\n        \"\"\"\n\n        from .multi import MultiIndex\n        if level is not None and not isinstance(self, MultiIndex):\n            raise ValueError('Level must be None for non-MultiIndex')\n\n        if level is not None and not is_list_like(level) and is_list_like(\n                names):\n            msg = \"Names must be a string when a single level is provided.\"\n            raise TypeError(msg)\n\n        if not is_list_like(names) and level is None and self.nlevels > 1:\n            raise TypeError(\"Must pass list-like as `names`.\")\n\n        if not is_list_like(names):\n            names = [names]\n        if level is not None and not is_list_like(level):\n            level = [level]\n\n        if inplace:\n            idx = self\n        else:\n            idx = self._shallow_copy()\n        idx._set_names(names, level=level)\n        if not inplace:\n            return idx\n\n    def rename(self, name, inplace=False):\n        \"\"\"\n        Alter Index or MultiIndex name.\n\n        Able to set new names without level. Defaults to returning new index.\n        Length of names must match number of levels in MultiIndex.\n\n        Parameters\n        ----------\n        name : label or list of labels\n            Name(s) to set.\n        inplace : boolean, default False\n            Modifies the object directly, instead of creating a new Index or\n            MultiIndex.\n\n        Returns\n        -------\n        Index\n            The same type as the caller or None if inplace is True.\n\n        See Also\n        --------\n        Index.set_names : Able to set new names partially and by level.\n\n        Examples\n        --------\n        >>> idx = pd.Index(['A', 'C', 'A', 'B'], name='score')\n        >>> idx.rename('grade')\n        Index(['A', 'C', 'A', 'B'], dtype='object', name='grade')\n\n        >>> idx = pd.MultiIndex.from_product([['python', 'cobra'],\n        ...                                   [2018, 2019]],\n        ...                                   names=['kind', 'year'])\n        >>> idx\n        MultiIndex(levels=[['cobra', 'python'], [2018, 2019]],\n                   labels=[[1, 1, 0, 0], [0, 1, 0, 1]],\n                   names=['kind', 'year'])\n        >>> idx.rename(['species', 'year'])\n        MultiIndex(levels=[['cobra', 'python'], [2018, 2019]],\n                   labels=[[1, 1, 0, 0], [0, 1, 0, 1]],\n                   names=['species', 'year'])\n        >>> idx.rename('species')\n        Traceback (most recent call last):\n        TypeError: Must pass list-like as `names`.\n        \"\"\"\n        return self.set_names([name], inplace=inplace)\n\n    # --------------------------------------------------------------------\n    # Level-Centric Methods\n\n    @property\n    def nlevels(self):\n        return 1\n\n    def _sort_levels_monotonic(self):\n        \"\"\"\n        Compat with MultiIndex.\n        \"\"\"\n        return self\n\n    def _validate_index_level(self, level):\n        \"\"\"\n        Validate index level.\n\n        For single-level Index getting level number is a no-op, but some\n        verification must be done like in MultiIndex.\n\n        \"\"\"\n        if isinstance(level, int):\n            if level < 0 and level != -1:\n                raise IndexError(\"Too many levels: Index has only 1 level,\"\n                                 \" %d is not a valid level number\" % (level, ))\n            elif level > 0:\n                raise IndexError(\"Too many levels:\"\n                                 \" Index has only 1 level, not %d\" %\n                                 (level + 1))\n        elif level != self.name:\n            raise KeyError('Level %s must be same as name (%s)' %\n                           (level, self.name))\n\n    def _get_level_number(self, level):\n        self._validate_index_level(level)\n        return 0\n\n    def sortlevel(self, level=None, ascending=True, sort_remaining=None):\n        \"\"\"\n        For internal compatibility with with the Index API.\n\n        Sort the Index. This is for compat with MultiIndex\n\n        Parameters\n        ----------\n        ascending : boolean, default True\n            False to sort in descending order\n\n        level, sort_remaining are compat parameters\n\n        Returns\n        -------\n        sorted_index : Index\n        \"\"\"\n        return self.sort_values(return_indexer=True, ascending=ascending)\n\n    def _get_level_values(self, level):\n        \"\"\"\n        Return an Index of values for requested level.\n\n        This is primarily useful to get an individual level of values from a\n        MultiIndex, but is provided on Index as well for compatability.\n\n        Parameters\n        ----------\n        level : int or str\n            It is either the integer position or the name of the level.\n\n        Returns\n        -------\n        values : Index\n            Calling object, as there is only one level in the Index.\n\n        See Also\n        --------\n        MultiIndex.get_level_values : Get values for a level of a MultiIndex.\n\n        Notes\n        -----\n        For Index, level should be 0, since there are no multiple levels.\n\n        Examples\n        --------\n\n        >>> idx = pd.Index(list('abc'))\n        >>> idx\n        Index(['a', 'b', 'c'], dtype='object')\n\n        Get level values by supplying `level` as integer:\n\n        >>> idx.get_level_values(0)\n        Index(['a', 'b', 'c'], dtype='object')\n        \"\"\"\n        self._validate_index_level(level)\n        return self\n\n    get_level_values = _get_level_values\n\n    def droplevel(self, level=0):\n        \"\"\"\n        Return index with requested level(s) removed.\n\n        If resulting index has only 1 level left, the result will be\n        of Index type, not MultiIndex.\n\n        .. versionadded:: 0.23.1 (support for non-MultiIndex)\n\n        Parameters\n        ----------\n        level : int, str, or list-like, default 0\n            If a string is given, must be the name of a level\n            If list-like, elements must be names or indexes of levels.\n\n        Returns\n        -------\n        index : Index or MultiIndex\n        \"\"\"\n        if not isinstance(level, (tuple, list)):\n            level = [level]\n\n        levnums = sorted(self._get_level_number(lev) for lev in level)[::-1]\n\n        if len(level) == 0:\n            return self\n        if len(level) >= self.nlevels:\n            raise ValueError(\"Cannot remove {} levels from an index with {} \"\n                             \"levels: at least one level must be \"\n                             \"left.\".format(len(level), self.nlevels))\n        # The two checks above guarantee that here self is a MultiIndex\n\n        new_levels = list(self.levels)\n        new_labels = list(self.labels)\n        new_names = list(self.names)\n\n        for i in levnums:\n            new_levels.pop(i)\n            new_labels.pop(i)\n            new_names.pop(i)\n\n        if len(new_levels) == 1:\n\n            # set nan if needed\n            mask = new_labels[0] == -1\n            result = new_levels[0].take(new_labels[0])\n            if mask.any():\n                result = result.putmask(mask, np.nan)\n\n            result.name = new_names[0]\n            return result\n        else:\n            from .multi import MultiIndex\n            return MultiIndex(levels=new_levels, labels=new_labels,\n                              names=new_names, verify_integrity=False)\n\n    _index_shared_docs['_get_grouper_for_level'] = \"\"\"\n        Get index grouper corresponding to an index level\n\n        Parameters\n        ----------\n        mapper: Group mapping function or None\n            Function mapping index values to groups\n        level : int or None\n            Index level\n\n        Returns\n        -------\n        grouper : Index\n            Index of values to group on\n        labels : ndarray of int or None\n            Array of locations in level_index\n        uniques : Index or None\n            Index of unique values for level\n        \"\"\"\n\n    @Appender(_index_shared_docs['_get_grouper_for_level'])\n    def _get_grouper_for_level(self, mapper, level=None):\n        assert level is None or level == 0\n        if mapper is None:\n            grouper = self\n        else:\n            grouper = self.map(mapper)\n\n        return grouper, None, None\n\n    # --------------------------------------------------------------------\n    # Introspection Methods\n\n    @property\n    def is_monotonic(self):\n        \"\"\"\n        Alias for is_monotonic_increasing.\n        \"\"\"\n        return self.is_monotonic_increasing\n\n    @property\n    def is_monotonic_increasing(self):\n        \"\"\"\n        Return if the index is monotonic increasing (only equal or\n        increasing) values.\n\n        Examples\n        --------\n        >>> Index([1, 2, 3]).is_monotonic_increasing\n        True\n        >>> Index([1, 2, 2]).is_monotonic_increasing\n        True\n        >>> Index([1, 3, 2]).is_monotonic_increasing\n        False\n        \"\"\"\n        return self._engine.is_monotonic_increasing\n\n    @property\n    def is_monotonic_decreasing(self):\n        \"\"\"\n        Return if the index is monotonic decreasing (only equal or\n        decreasing) values.\n\n        Examples\n        --------\n        >>> Index([3, 2, 1]).is_monotonic_decreasing\n        True\n        >>> Index([3, 2, 2]).is_monotonic_decreasing\n        True\n        >>> Index([3, 1, 2]).is_monotonic_decreasing\n        False\n        \"\"\"\n        return self._engine.is_monotonic_decreasing\n\n    @property\n    def _is_strictly_monotonic_increasing(self):\n        \"\"\"\n        Return if the index is strictly monotonic increasing\n        (only increasing) values.\n\n        Examples\n        --------\n        >>> Index([1, 2, 3])._is_strictly_monotonic_increasing\n        True\n        >>> Index([1, 2, 2])._is_strictly_monotonic_increasing\n        False\n        >>> Index([1, 3, 2])._is_strictly_monotonic_increasing\n        False\n        \"\"\"\n        return self.is_unique and self.is_monotonic_increasing\n\n    @property\n    def _is_strictly_monotonic_decreasing(self):\n        \"\"\"\n        Return if the index is strictly monotonic decreasing\n        (only decreasing) values.\n\n        Examples\n        --------\n        >>> Index([3, 2, 1])._is_strictly_monotonic_decreasing\n        True\n        >>> Index([3, 2, 2])._is_strictly_monotonic_decreasing\n        False\n        >>> Index([3, 1, 2])._is_strictly_monotonic_decreasing\n        False\n        \"\"\"\n        return self.is_unique and self.is_monotonic_decreasing\n\n    def is_lexsorted_for_tuple(self, tup):\n        return True\n\n    @cache_readonly\n    def is_unique(self):\n        \"\"\"\n        Return if the index has unique values.\n        \"\"\"\n        return self._engine.is_unique\n\n    @property\n    def has_duplicates(self):\n        return not self.is_unique\n\n    def is_boolean(self):\n        return self.inferred_type in ['boolean']\n\n    def is_integer(self):\n        return self.inferred_type in ['integer']\n\n    def is_floating(self):\n        return self.inferred_type in ['floating', 'mixed-integer-float']\n\n    def is_numeric(self):\n        return self.inferred_type in ['integer', 'floating']\n\n    def is_object(self):\n        return is_object_dtype(self.dtype)\n\n    def is_categorical(self):\n        \"\"\"\n        Check if the Index holds categorical data.\n\n        Returns\n        -------\n        boolean\n            True if the Index is categorical.\n\n        See Also\n        --------\n        CategoricalIndex : Index for categorical data.\n\n        Examples\n        --------\n        >>> idx = pd.Index([\"Watermelon\", \"Orange\", \"Apple\",\n        ...                 \"Watermelon\"]).astype(\"category\")\n        >>> idx.is_categorical()\n        True\n\n        >>> idx = pd.Index([1, 3, 5, 7])\n        >>> idx.is_categorical()\n        False\n\n        >>> s = pd.Series([\"Peter\", \"Victor\", \"Elisabeth\", \"Mar\"])\n        >>> s\n        0        Peter\n        1       Victor\n        2    Elisabeth\n        3          Mar\n        dtype: object\n        >>> s.index.is_categorical()\n        False\n        \"\"\"\n        return self.inferred_type in ['categorical']\n\n    def is_interval(self):\n        return self.inferred_type in ['interval']\n\n    def is_mixed(self):\n        return self.inferred_type in ['mixed']\n\n    def holds_integer(self):\n        return self.inferred_type in ['integer', 'mixed-integer']\n\n    @cache_readonly\n    def inferred_type(self):\n        \"\"\"\n        Return a string of the type inferred from the values.\n        \"\"\"\n        return lib.infer_dtype(self)\n\n    @cache_readonly\n    def is_all_dates(self):\n        if self._data is None:\n            return False\n        return is_datetime_array(ensure_object(self.values))\n\n    # --------------------------------------------------------------------\n    # Pickle Methods\n\n    def __reduce__(self):\n        d = dict(data=self._data)\n        d.update(self._get_attributes_dict())\n        return _new_Index, (self.__class__, d), None\n\n    def __setstate__(self, state):\n        \"\"\"\n        Necessary for making this object picklable.\n        \"\"\"\n\n        if isinstance(state, dict):\n            self._data = state.pop('data')\n            for k, v in compat.iteritems(state):\n                setattr(self, k, v)\n\n        elif isinstance(state, tuple):\n\n            if len(state) == 2:\n                nd_state, own_state = state\n                data = np.empty(nd_state[1], dtype=nd_state[2])\n                np.ndarray.__setstate__(data, nd_state)\n                self.name = own_state[0]\n\n            else:  # pragma: no cover\n                data = np.empty(state)\n                np.ndarray.__setstate__(data, state)\n\n            self._data = data\n            self._reset_identity()\n        else:\n            raise Exception(\"invalid pickle state\")\n\n    _unpickle_compat = __setstate__\n\n    # --------------------------------------------------------------------\n    # Null Handling Methods\n\n    _na_value = np.nan\n    \"\"\"The expected NA value to use with this index.\"\"\"\n\n    @cache_readonly\n    def _isnan(self):\n        \"\"\"\n        Return if each value is NaN.\n        \"\"\"\n        if self._can_hold_na:\n            return isna(self)\n        else:\n            # shouldn't reach to this condition by checking hasnans beforehand\n            values = np.empty(len(self), dtype=np.bool_)\n            values.fill(False)\n            return values\n\n    @cache_readonly\n    def _nan_idxs(self):\n        if self._can_hold_na:\n            w, = self._isnan.nonzero()\n            return w\n        else:\n            return np.array([], dtype=np.int64)\n\n    @cache_readonly\n    def hasnans(self):\n        \"\"\"\n        Return if I have any nans; enables various perf speedups.\n        \"\"\"\n        if self._can_hold_na:\n            return bool(self._isnan.any())\n        else:\n            return False\n\n    def isna(self):\n        \"\"\"\n        Detect missing values.\n\n        Return a boolean same-sized object indicating if the values are NA.\n        NA values, such as ``None``, :attr:`numpy.NaN` or :attr:`pd.NaT`, get\n        mapped to ``True`` values.\n        Everything else get mapped to ``False`` values. Characters such as\n        empty strings `''` or :attr:`numpy.inf` are not considered NA values\n        (unless you set ``pandas.options.mode.use_inf_as_na = True``).\n\n        .. versionadded:: 0.20.0\n\n        Returns\n        -------\n        numpy.ndarray\n            A boolean array of whether my values are NA\n\n        See Also\n        --------\n        pandas.Index.notna : Boolean inverse of isna.\n        pandas.Index.dropna : Omit entries with missing values.\n        pandas.isna : Top-level isna.\n        Series.isna : Detect missing values in Series object.\n\n        Examples\n        --------\n        Show which entries in a pandas.Index are NA. The result is an\n        array.\n\n        >>> idx = pd.Index([5.2, 6.0, np.NaN])\n        >>> idx\n        Float64Index([5.2, 6.0, nan], dtype='float64')\n        >>> idx.isna()\n        array([False, False,  True], dtype=bool)\n\n        Empty strings are not considered NA values. None is considered an NA\n        value.\n\n        >>> idx = pd.Index(['black', '', 'red', None])\n        >>> idx\n        Index(['black', '', 'red', None], dtype='object')\n        >>> idx.isna()\n        array([False, False, False,  True], dtype=bool)\n\n        For datetimes, `NaT` (Not a Time) is considered as an NA value.\n\n        >>> idx = pd.DatetimeIndex([pd.Timestamp('1940-04-25'),\n        ...                         pd.Timestamp(''), None, pd.NaT])\n        >>> idx\n        DatetimeIndex(['1940-04-25', 'NaT', 'NaT', 'NaT'],\n                      dtype='datetime64[ns]', freq=None)\n        >>> idx.isna()\n        array([False,  True,  True,  True], dtype=bool)\n        \"\"\"\n        return self._isnan\n    isnull = isna\n\n    def notna(self):\n        \"\"\"\n        Detect existing (non-missing) values.\n\n        Return a boolean same-sized object indicating if the values are not NA.\n        Non-missing values get mapped to ``True``. Characters such as empty\n        strings ``''`` or :attr:`numpy.inf` are not considered NA values\n        (unless you set ``pandas.options.mode.use_inf_as_na = True``).\n        NA values, such as None or :attr:`numpy.NaN`, get mapped to ``False``\n        values.\n\n        .. versionadded:: 0.20.0\n\n        Returns\n        -------\n        numpy.ndarray\n            Boolean array to indicate which entries are not NA.\n\n        See Also\n        --------\n        Index.notnull : Alias of notna.\n        Index.isna: Inverse of notna.\n        pandas.notna : Top-level notna.\n\n        Examples\n        --------\n        Show which entries in an Index are not NA. The result is an\n        array.\n\n        >>> idx = pd.Index([5.2, 6.0, np.NaN])\n        >>> idx\n        Float64Index([5.2, 6.0, nan], dtype='float64')\n        >>> idx.notna()\n        array([ True,  True, False])\n\n        Empty strings are not considered NA values. None is considered a NA\n        value.\n\n        >>> idx = pd.Index(['black', '', 'red', None])\n        >>> idx\n        Index(['black', '', 'red', None], dtype='object')\n        >>> idx.notna()\n        array([ True,  True,  True, False])\n        \"\"\"\n        return ~self.isna()\n    notnull = notna\n\n    _index_shared_docs['fillna'] = \"\"\"\n        Fill NA/NaN values with the specified value\n\n        Parameters\n        ----------\n        value : scalar\n            Scalar value to use to fill holes (e.g. 0).\n            This value cannot be a list-likes.\n        downcast : dict, default is None\n            a dict of item->dtype of what to downcast if possible,\n            or the string 'infer' which will try to downcast to an appropriate\n            equal type (e.g. float64 to int64 if possible)\n\n        Returns\n        -------\n        filled : %(klass)s\n        \"\"\"\n\n    @Appender(_index_shared_docs['fillna'])\n    def fillna(self, value=None, downcast=None):\n        self._assert_can_do_op(value)\n        if self.hasnans:\n            result = self.putmask(self._isnan, value)\n            if downcast is None:\n                # no need to care metadata other than name\n                # because it can't have freq if\n                return Index(result, name=self.name)\n        return self._shallow_copy()\n\n    _index_shared_docs['dropna'] = \"\"\"\n        Return Index without NA/NaN values\n\n        Parameters\n        ----------\n        how :  {'any', 'all'}, default 'any'\n            If the Index is a MultiIndex, drop the value when any or all levels\n            are NaN.\n\n        Returns\n        -------\n        valid : Index\n        \"\"\"\n\n    @Appender(_index_shared_docs['dropna'])\n    def dropna(self, how='any'):\n        if how not in ('any', 'all'):\n            raise ValueError(\"invalid how option: {0}\".format(how))\n\n        if self.hasnans:\n            return self._shallow_copy(self.values[~self._isnan])\n        return self._shallow_copy()\n\n    # --------------------------------------------------------------------\n    # Uniqueness Methods\n\n    _index_shared_docs['index_unique'] = (\n        \"\"\"\n        Return unique values in the index. Uniques are returned in order\n        of appearance, this does NOT sort.\n\n        Parameters\n        ----------\n        level : int or str, optional, default None\n            Only return values from specified level (for MultiIndex)\n\n            .. versionadded:: 0.23.0\n\n        Returns\n        -------\n        Index without duplicates\n\n        See Also\n        --------\n        unique\n        Series.unique\n        \"\"\")\n\n    @Appender(_index_shared_docs['index_unique'] % _index_doc_kwargs)\n    def unique(self, level=None):\n        if level is not None:\n            self._validate_index_level(level)\n        result = super(Index, self).unique()\n        return self._shallow_copy(result)\n\n    def drop_duplicates(self, keep='first'):\n        \"\"\"\n        Return Index with duplicate values removed.\n\n        Parameters\n        ----------\n        keep : {'first', 'last', ``False``}, default 'first'\n            - 'first' : Drop duplicates except for the first occurrence.\n            - 'last' : Drop duplicates except for the last occurrence.\n            - ``False`` : Drop all duplicates.\n\n        Returns\n        -------\n        deduplicated : Index\n\n        See Also\n        --------\n        Series.drop_duplicates : Equivalent method on Series.\n        DataFrame.drop_duplicates : Equivalent method on DataFrame.\n        Index.duplicated : Related method on Index, indicating duplicate\n            Index values.\n\n        Examples\n        --------\n        Generate an pandas.Index with duplicate values.\n\n        >>> idx = pd.Index(['lama', 'cow', 'lama', 'beetle', 'lama', 'hippo'])\n\n        The `keep` parameter controls  which duplicate values are removed.\n        The value 'first' keeps the first occurrence for each\n        set of duplicated entries. The default value of keep is 'first'.\n\n        >>> idx.drop_duplicates(keep='first')\n        Index(['lama', 'cow', 'beetle', 'hippo'], dtype='object')\n\n        The value 'last' keeps the last occurrence for each set of duplicated\n        entries.\n\n        >>> idx.drop_duplicates(keep='last')\n        Index(['cow', 'beetle', 'lama', 'hippo'], dtype='object')\n\n        The value ``False`` discards all sets of duplicated entries.\n\n        >>> idx.drop_duplicates(keep=False)\n        Index(['cow', 'beetle', 'hippo'], dtype='object')\n        \"\"\"\n        return super(Index, self).drop_duplicates(keep=keep)\n\n    def duplicated(self, keep='first'):\n        \"\"\"\n        Indicate duplicate index values.\n\n        Duplicated values are indicated as ``True`` values in the resulting\n        array. Either all duplicates, all except the first, or all except the\n        last occurrence of duplicates can be indicated.\n\n        Parameters\n        ----------\n        keep : {'first', 'last', False}, default 'first'\n            The value or values in a set of duplicates to mark as missing.\n\n            - 'first' : Mark duplicates as ``True`` except for the first\n              occurrence.\n            - 'last' : Mark duplicates as ``True`` except for the last\n              occurrence.\n            - ``False`` : Mark all duplicates as ``True``.\n\n        Examples\n        --------\n        By default, for each set of duplicated values, the first occurrence is\n        set to False and all others to True:\n\n        >>> idx = pd.Index(['lama', 'cow', 'lama', 'beetle', 'lama'])\n        >>> idx.duplicated()\n        array([False, False,  True, False,  True])\n\n        which is equivalent to\n\n        >>> idx.duplicated(keep='first')\n        array([False, False,  True, False,  True])\n\n        By using 'last', the last occurrence of each set of duplicated values\n        is set on False and all others on True:\n\n        >>> idx.duplicated(keep='last')\n        array([ True, False,  True, False, False])\n\n        By setting keep on ``False``, all duplicates are True:\n\n        >>> idx.duplicated(keep=False)\n        array([ True, False,  True, False,  True])\n\n        Returns\n        -------\n        numpy.ndarray\n\n        See Also\n        --------\n        pandas.Series.duplicated : Equivalent method on pandas.Series.\n        pandas.DataFrame.duplicated : Equivalent method on pandas.DataFrame.\n        pandas.Index.drop_duplicates : Remove duplicate values from Index.\n        \"\"\"\n        return super(Index, self).duplicated(keep=keep)\n\n    def get_duplicates(self):\n        \"\"\"\n        Extract duplicated index elements.\n\n        Returns a sorted list of index elements which appear more than once in\n        the index.\n\n        .. deprecated:: 0.23.0\n            Use idx[idx.duplicated()].unique() instead\n\n        Returns\n        -------\n        array-like\n            List of duplicated indexes.\n\n        See Also\n        --------\n        Index.duplicated : Return boolean array denoting duplicates.\n        Index.drop_duplicates : Return Index with duplicates removed.\n\n        Examples\n        --------\n\n        Works on different Index of types.\n\n        >>> pd.Index([1, 2, 2, 3, 3, 3, 4]).get_duplicates()  # doctest: +SKIP\n        [2, 3]\n\n        Note that for a DatetimeIndex, it does not return a list but a new\n        DatetimeIndex:\n\n        >>> dates = pd.to_datetime(['2018-01-01', '2018-01-02', '2018-01-03',\n        ...                         '2018-01-03', '2018-01-04', '2018-01-04'],\n        ...                        format='%Y-%m-%d')\n        >>> pd.Index(dates).get_duplicates()  # doctest: +SKIP\n        DatetimeIndex(['2018-01-03', '2018-01-04'],\n                      dtype='datetime64[ns]', freq=None)\n\n        Sorts duplicated elements even when indexes are unordered.\n\n        >>> pd.Index([1, 2, 3, 2, 3, 4, 3]).get_duplicates()  # doctest: +SKIP\n        [2, 3]\n\n        Return empty array-like structure when all elements are unique.\n\n        >>> pd.Index([1, 2, 3, 4]).get_duplicates()  # doctest: +SKIP\n        []\n        >>> dates = pd.to_datetime(['2018-01-01', '2018-01-02', '2018-01-03'],\n        ...                        format='%Y-%m-%d')\n        >>> pd.Index(dates).get_duplicates()  # doctest: +SKIP\n        DatetimeIndex([], dtype='datetime64[ns]', freq=None)\n        \"\"\"\n        warnings.warn(\"'get_duplicates' is deprecated and will be removed in \"\n                      \"a future release. You can use \"\n                      \"idx[idx.duplicated()].unique() instead\",\n                      FutureWarning, stacklevel=2)\n\n        return self[self.duplicated()].unique()\n\n    def _get_unique_index(self, dropna=False):\n        \"\"\"\n        Returns an index containing unique values.\n\n        Parameters\n        ----------\n        dropna : bool\n            If True, NaN values are dropped.\n\n        Returns\n        -------\n        uniques : index\n        \"\"\"\n        if self.is_unique and not dropna:\n            return self\n\n        values = self.values\n\n        if not self.is_unique:\n            values = self.unique()\n\n        if dropna:\n            try:\n                if self.hasnans:\n                    values = values[~isna(values)]\n            except NotImplementedError:\n                pass\n\n        return self._shallow_copy(values)\n\n    # --------------------------------------------------------------------\n    # Arithmetic & Logical Methods\n\n    def __add__(self, other):\n        if isinstance(other, (ABCSeries, ABCDataFrame)):\n            return NotImplemented\n        return Index(np.array(self) + other)\n\n    def __radd__(self, other):\n        return Index(other + np.array(self))\n\n    def __iadd__(self, other):\n        # alias for __add__\n        return self + other\n\n    def __sub__(self, other):\n        return Index(np.array(self) - other)\n\n    def __rsub__(self, other):\n        return Index(other - np.array(self))\n\n    def __and__(self, other):\n        return self.intersection(other)\n\n    def __or__(self, other):\n        return self.union(other)\n\n    def __xor__(self, other):\n        return self.symmetric_difference(other)\n\n    def __nonzero__(self):\n        raise ValueError(\"The truth value of a {0} is ambiguous. \"\n                         \"Use a.empty, a.bool(), a.item(), a.any() or a.all().\"\n                         .format(self.__class__.__name__))\n\n    __bool__ = __nonzero__\n\n    # --------------------------------------------------------------------\n    # Set Operation Methods\n\n    def _get_reconciled_name_object(self, other):\n        \"\"\"\n        If the result of a set operation will be self,\n        return self, unless the name changes, in which\n        case make a shallow copy of self.\n        \"\"\"\n        name = get_op_result_name(self, other)\n        if self.name != name:\n            return self._shallow_copy(name=name)\n        return self\n\n    def union(self, other):\n        \"\"\"\n        Form the union of two Index objects and sorts if possible.\n\n        Parameters\n        ----------\n        other : Index or array-like\n\n        Returns\n        -------\n        union : Index\n\n        Examples\n        --------\n\n        >>> idx1 = pd.Index([1, 2, 3, 4])\n        >>> idx2 = pd.Index([3, 4, 5, 6])\n        >>> idx1.union(idx2)\n        Int64Index([1, 2, 3, 4, 5, 6], dtype='int64')\n        \"\"\"\n        self._assert_can_do_setop(other)\n        other = ensure_index(other)\n\n        if len(other) == 0 or self.equals(other):\n            return self._get_reconciled_name_object(other)\n\n        if len(self) == 0:\n            return other._get_reconciled_name_object(self)\n\n        # TODO: is_dtype_union_equal is a hack around\n        # 1. buggy set ops with duplicates (GH #13432)\n        # 2. CategoricalIndex lacking setops (GH #10186)\n        # Once those are fixed, this workaround can be removed\n        if not is_dtype_union_equal(self.dtype, other.dtype):\n            this = self.astype('O')\n            other = other.astype('O')\n            return this.union(other)\n\n        # TODO(EA): setops-refactor, clean all this up\n        if is_period_dtype(self) or is_datetime64tz_dtype(self):\n            lvals = self._ndarray_values\n        else:\n            lvals = self._values\n        if is_period_dtype(other) or is_datetime64tz_dtype(other):\n            rvals = other._ndarray_values\n        else:\n            rvals = other._values\n\n        if self.is_monotonic and other.is_monotonic:\n            try:\n                result = self._outer_indexer(lvals, rvals)[0]\n            except TypeError:\n                # incomparable objects\n                result = list(lvals)\n\n                # worth making this faster? a very unusual case\n                value_set = set(lvals)\n                result.extend([x for x in rvals if x not in value_set])\n        else:\n            indexer = self.get_indexer(other)\n            indexer, = (indexer == -1).nonzero()\n\n            if len(indexer) > 0:\n                other_diff = algos.take_nd(rvals, indexer,\n                                           allow_fill=False)\n                result = _concat._concat_compat((lvals, other_diff))\n\n                try:\n                    lvals[0] < other_diff[0]\n                except TypeError as e:\n                    warnings.warn(\"%s, sort order is undefined for \"\n                                  \"incomparable objects\" % e, RuntimeWarning,\n                                  stacklevel=3)\n                else:\n                    types = frozenset((self.inferred_type,\n                                       other.inferred_type))\n                    if not types & _unsortable_types:\n                        result.sort()\n\n            else:\n                result = lvals\n\n                try:\n                    result = np.sort(result)\n                except TypeError as e:\n                    warnings.warn(\"%s, sort order is undefined for \"\n                                  \"incomparable objects\" % e, RuntimeWarning,\n                                  stacklevel=3)\n\n        # for subclasses\n        return self._wrap_setop_result(other, result)\n\n    def _wrap_setop_result(self, other, result):\n        return self._constructor(result, name=get_op_result_name(self, other))\n\n    def intersection(self, other):\n        \"\"\"\n        Form the intersection of two Index objects.\n\n        This returns a new Index with elements common to the index and `other`,\n        preserving the order of the calling index.\n\n        Parameters\n        ----------\n        other : Index or array-like\n\n        Returns\n        -------\n        intersection : Index\n\n        Examples\n        --------\n\n        >>> idx1 = pd.Index([1, 2, 3, 4])\n        >>> idx2 = pd.Index([3, 4, 5, 6])\n        >>> idx1.intersection(idx2)\n        Int64Index([3, 4], dtype='int64')\n        \"\"\"\n        self._assert_can_do_setop(other)\n        other = ensure_index(other)\n\n        if self.equals(other):\n            return self._get_reconciled_name_object(other)\n\n        if not is_dtype_equal(self.dtype, other.dtype):\n            this = self.astype('O')\n            other = other.astype('O')\n            return this.intersection(other)\n\n        # TODO(EA): setops-refactor, clean all this up\n        if is_period_dtype(self):\n            lvals = self._ndarray_values\n        else:\n            lvals = self._values\n        if is_period_dtype(other):\n            rvals = other._ndarray_values\n        else:\n            rvals = other._values\n\n        if self.is_monotonic and other.is_monotonic:\n            try:\n                result = self._inner_indexer(lvals, rvals)[0]\n                return self._wrap_setop_result(other, result)\n            except TypeError:\n                pass\n\n        try:\n            indexer = Index(rvals).get_indexer(lvals)\n            indexer = indexer.take((indexer != -1).nonzero()[0])\n        except Exception:\n            # duplicates\n            indexer = algos.unique1d(\n                Index(rvals).get_indexer_non_unique(lvals)[0])\n            indexer = indexer[indexer != -1]\n\n        taken = other.take(indexer)\n        if self.name != other.name:\n            taken.name = None\n        return taken\n\n    def difference(self, other, sort=True):\n        \"\"\"\n        Return a new Index with elements from the index that are not in\n        `other`.\n\n        This is the set difference of two Index objects.\n\n        Parameters\n        ----------\n        other : Index or array-like\n        sort : bool, default True\n            Sort the resulting index if possible\n\n            .. versionadded:: 0.24.0\n\n        Returns\n        -------\n        difference : Index\n\n        Examples\n        --------\n\n        >>> idx1 = pd.Index([2, 1, 3, 4])\n        >>> idx2 = pd.Index([3, 4, 5, 6])\n        >>> idx1.difference(idx2)\n        Int64Index([1, 2], dtype='int64')\n        >>> idx1.difference(idx2, sort=False)\n        Int64Index([2, 1], dtype='int64')\n        \"\"\"\n        self._assert_can_do_setop(other)\n\n        if self.equals(other):\n            # pass an empty np.ndarray with the appropriate dtype\n            return self._shallow_copy(self._data[:0])\n\n        other, result_name = self._convert_can_do_setop(other)\n\n        this = self._get_unique_index()\n\n        indexer = this.get_indexer(other)\n        indexer = indexer.take((indexer != -1).nonzero()[0])\n\n        label_diff = np.setdiff1d(np.arange(this.size), indexer,\n                                  assume_unique=True)\n        the_diff = this.values.take(label_diff)\n        if sort:\n            try:\n                the_diff = sorting.safe_sort(the_diff)\n            except TypeError:\n                pass\n\n        return this._shallow_copy(the_diff, name=result_name, freq=None)\n\n    def symmetric_difference(self, other, result_name=None):\n        \"\"\"\n        Compute the symmetric difference of two Index objects.\n\n        It's sorted if sorting is possible.\n\n        Parameters\n        ----------\n        other : Index or array-like\n        result_name : str\n\n        Returns\n        -------\n        symmetric_difference : Index\n\n        Notes\n        -----\n        ``symmetric_difference`` contains elements that appear in either\n        ``idx1`` or ``idx2`` but not both. Equivalent to the Index created by\n        ``idx1.difference(idx2) | idx2.difference(idx1)`` with duplicates\n        dropped.\n\n        Examples\n        --------\n        >>> idx1 = pd.Index([1, 2, 3, 4])\n        >>> idx2 = pd.Index([2, 3, 4, 5])\n        >>> idx1.symmetric_difference(idx2)\n        Int64Index([1, 5], dtype='int64')\n\n        You can also use the ``^`` operator:\n\n        >>> idx1 ^ idx2\n        Int64Index([1, 5], dtype='int64')\n        \"\"\"\n        self._assert_can_do_setop(other)\n        other, result_name_update = self._convert_can_do_setop(other)\n        if result_name is None:\n            result_name = result_name_update\n\n        this = self._get_unique_index()\n        other = other._get_unique_index()\n        indexer = this.get_indexer(other)\n\n        # {this} minus {other}\n        common_indexer = indexer.take((indexer != -1).nonzero()[0])\n        left_indexer = np.setdiff1d(np.arange(this.size), common_indexer,\n                                    assume_unique=True)\n        left_diff = this.values.take(left_indexer)\n\n        # {other} minus {this}\n        right_indexer = (indexer == -1).nonzero()[0]\n        right_diff = other.values.take(right_indexer)\n\n        the_diff = _concat._concat_compat([left_diff, right_diff])\n        try:\n            the_diff = sorting.safe_sort(the_diff)\n        except TypeError:\n            pass\n\n        attribs = self._get_attributes_dict()\n        attribs['name'] = result_name\n        if 'freq' in attribs:\n            attribs['freq'] = None\n        return self._shallow_copy_with_infer(the_diff, **attribs)\n\n    def _assert_can_do_setop(self, other):\n        if not is_list_like(other):\n            raise TypeError('Input must be Index or array-like')\n        return True\n\n    def _convert_can_do_setop(self, other):\n        if not isinstance(other, Index):\n            other = Index(other, name=self.name)\n            result_name = self.name\n        else:\n            result_name = get_op_result_name(self, other)\n        return other, result_name\n\n    # --------------------------------------------------------------------\n    # Indexing Methods\n\n    _index_shared_docs['get_loc'] = \"\"\"\n        Get integer location, slice or boolean mask for requested label.\n\n        Parameters\n        ----------\n        key : label\n        method : {None, 'pad'/'ffill', 'backfill'/'bfill', 'nearest'}, optional\n            * default: exact matches only.\n            * pad / ffill: find the PREVIOUS index value if no exact match.\n            * backfill / bfill: use NEXT index value if no exact match\n            * nearest: use the NEAREST index value if no exact match. Tied\n              distances are broken by preferring the larger index value.\n        tolerance : optional\n            Maximum distance from index value for inexact matches. The value of\n            the index at the matching location most satisfy the equation\n            ``abs(index[loc] - key) <= tolerance``.\n\n            Tolerance may be a scalar\n            value, which applies the same tolerance to all values, or\n            list-like, which applies variable tolerance per element. List-like\n            includes list, tuple, array, Series, and must be the same size as\n            the index and its dtype must exactly match the index's type.\n\n            .. versionadded:: 0.21.0 (list-like tolerance)\n\n        Returns\n        -------\n        loc : int if unique index, slice if monotonic index, else mask\n\n        Examples\n        ---------\n        >>> unique_index = pd.Index(list('abc'))\n        >>> unique_index.get_loc('b')\n        1\n\n        >>> monotonic_index = pd.Index(list('abbc'))\n        >>> monotonic_index.get_loc('b')\n        slice(1, 3, None)\n\n        >>> non_monotonic_index = pd.Index(list('abcb'))\n        >>> non_monotonic_index.get_loc('b')\n        array([False,  True, False,  True], dtype=bool)\n        \"\"\"\n\n    @Appender(_index_shared_docs['get_loc'])\n    def get_loc(self, key, method=None, tolerance=None):\n        if method is None:\n            if tolerance is not None:\n                raise ValueError('tolerance argument only valid if using pad, '\n                                 'backfill or nearest lookups')\n            try:\n                return self._engine.get_loc(key)\n            except KeyError:\n                return self._engine.get_loc(self._maybe_cast_indexer(key))\n        indexer = self.get_indexer([key], method=method, tolerance=tolerance)\n        if indexer.ndim > 1 or indexer.size > 1:\n            raise TypeError('get_loc requires scalar valued input')\n        loc = indexer.item()\n        if loc == -1:\n            raise KeyError(key)\n        return loc\n\n    _index_shared_docs['get_indexer'] = \"\"\"\n        Compute indexer and mask for new index given the current index. The\n        indexer should be then used as an input to ndarray.take to align the\n        current data to the new index.\n\n        Parameters\n        ----------\n        target : %(target_klass)s\n        method : {None, 'pad'/'ffill', 'backfill'/'bfill', 'nearest'}, optional\n            * default: exact matches only.\n            * pad / ffill: find the PREVIOUS index value if no exact match.\n            * backfill / bfill: use NEXT index value if no exact match\n            * nearest: use the NEAREST index value if no exact match. Tied\n              distances are broken by preferring the larger index value.\n        limit : int, optional\n            Maximum number of consecutive labels in ``target`` to match for\n            inexact matches.\n        tolerance : optional\n            Maximum distance between original and new labels for inexact\n            matches. The values of the index at the matching locations most\n            satisfy the equation ``abs(index[indexer] - target) <= tolerance``.\n\n            Tolerance may be a scalar value, which applies the same tolerance\n            to all values, or list-like, which applies variable tolerance per\n            element. List-like includes list, tuple, array, Series, and must be\n            the same size as the index and its dtype must exactly match the\n            index's type.\n\n            .. versionadded:: 0.21.0 (list-like tolerance)\n\n        Returns\n        -------\n        indexer : ndarray of int\n            Integers from 0 to n - 1 indicating that the index at these\n            positions matches the corresponding target values. Missing values\n            in the target are marked by -1.\n\n        Examples\n        --------\n        >>> index = pd.Index(['c', 'a', 'b'])\n        >>> index.get_indexer(['a', 'b', 'x'])\n        array([ 1,  2, -1])\n\n        Notice that the return value is an array of locations in ``index``\n        and ``x`` is marked by -1, as it is not in ``index``.\n        \"\"\"\n\n    @Appender(_index_shared_docs['get_indexer'] % _index_doc_kwargs)\n    def get_indexer(self, target, method=None, limit=None, tolerance=None):\n        method = missing.clean_reindex_fill_method(method)\n        target = ensure_index(target)\n        if tolerance is not None:\n            tolerance = self._convert_tolerance(tolerance, target)\n\n        # Treat boolean labels passed to a numeric index as not found. Without\n        # this fix False and True would be treated as 0 and 1 respectively.\n        # (GH #16877)\n        if target.is_boolean() and self.is_numeric():\n            return ensure_platform_int(np.repeat(-1, target.size))\n\n        pself, ptarget = self._maybe_promote(target)\n        if pself is not self or ptarget is not target:\n            return pself.get_indexer(ptarget, method=method, limit=limit,\n                                     tolerance=tolerance)\n\n        if not is_dtype_equal(self.dtype, target.dtype):\n            this = self.astype(object)\n            target = target.astype(object)\n            return this.get_indexer(target, method=method, limit=limit,\n                                    tolerance=tolerance)\n\n        if not self.is_unique:\n            raise InvalidIndexError('Reindexing only valid with uniquely'\n                                    ' valued Index objects')\n\n        if method == 'pad' or method == 'backfill':\n            indexer = self._get_fill_indexer(target, method, limit, tolerance)\n        elif method == 'nearest':\n            indexer = self._get_nearest_indexer(target, limit, tolerance)\n        else:\n            if tolerance is not None:\n                raise ValueError('tolerance argument only valid if doing pad, '\n                                 'backfill or nearest reindexing')\n            if limit is not None:\n                raise ValueError('limit argument only valid if doing pad, '\n                                 'backfill or nearest reindexing')\n\n            indexer = self._engine.get_indexer(target._ndarray_values)\n\n        return ensure_platform_int(indexer)\n\n    def _convert_tolerance(self, tolerance, target):\n        # override this method on subclasses\n        tolerance = np.asarray(tolerance)\n        if target.size != tolerance.size and tolerance.size > 1:\n            raise ValueError('list-like tolerance size must match '\n                             'target index size')\n        return tolerance\n\n    def _get_fill_indexer(self, target, method, limit=None, tolerance=None):\n        if self.is_monotonic_increasing and target.is_monotonic_increasing:\n            method = (self._engine.get_pad_indexer if method == 'pad' else\n                      self._engine.get_backfill_indexer)\n            indexer = method(target._ndarray_values, limit)\n        else:\n            indexer = self._get_fill_indexer_searchsorted(target, method,\n                                                          limit)\n        if tolerance is not None:\n            indexer = self._filter_indexer_tolerance(target._ndarray_values,\n                                                     indexer,\n                                                     tolerance)\n        return indexer\n\n    def _get_fill_indexer_searchsorted(self, target, method, limit=None):\n        \"\"\"\n        Fallback pad/backfill get_indexer that works for monotonic decreasing\n        indexes and non-monotonic targets.\n        \"\"\"\n        if limit is not None:\n            raise ValueError('limit argument for %r method only well-defined '\n                             'if index and target are monotonic' % method)\n\n        side = 'left' if method == 'pad' else 'right'\n\n        # find exact matches first (this simplifies the algorithm)\n        indexer = self.get_indexer(target)\n        nonexact = (indexer == -1)\n        indexer[nonexact] = self._searchsorted_monotonic(target[nonexact],\n                                                         side)\n        if side == 'left':\n            # searchsorted returns \"indices into a sorted array such that,\n            # if the corresponding elements in v were inserted before the\n            # indices, the order of a would be preserved\".\n            # Thus, we need to subtract 1 to find values to the left.\n            indexer[nonexact] -= 1\n            # This also mapped not found values (values of 0 from\n            # np.searchsorted) to -1, which conveniently is also our\n            # sentinel for missing values\n        else:\n            # Mark indices to the right of the largest value as not found\n            indexer[indexer == len(self)] = -1\n        return indexer\n\n    def _get_nearest_indexer(self, target, limit, tolerance):\n        \"\"\"\n        Get the indexer for the nearest index labels; requires an index with\n        values that can be subtracted from each other (e.g., not strings or\n        tuples).\n        \"\"\"\n        left_indexer = self.get_indexer(target, 'pad', limit=limit)\n        right_indexer = self.get_indexer(target, 'backfill', limit=limit)\n\n        target = np.asarray(target)\n        left_distances = abs(self.values[left_indexer] - target)\n        right_distances = abs(self.values[right_indexer] - target)\n\n        op = operator.lt if self.is_monotonic_increasing else operator.le\n        indexer = np.where(op(left_distances, right_distances) |\n                           (right_indexer == -1), left_indexer, right_indexer)\n        if tolerance is not None:\n            indexer = self._filter_indexer_tolerance(target, indexer,\n                                                     tolerance)\n        return indexer\n\n    def _filter_indexer_tolerance(self, target, indexer, tolerance):\n        distance = abs(self.values[indexer] - target)\n        indexer = np.where(distance <= tolerance, indexer, -1)\n        return indexer\n\n    # --------------------------------------------------------------------\n    # Indexer Conversion Methods\n\n    _index_shared_docs['_convert_scalar_indexer'] = \"\"\"\n        Convert a scalar indexer.\n\n        Parameters\n        ----------\n        key : label of the slice bound\n        kind : {'ix', 'loc', 'getitem', 'iloc'} or None\n    \"\"\"\n\n    @Appender(_index_shared_docs['_convert_scalar_indexer'])\n    def _convert_scalar_indexer(self, key, kind=None):\n        assert kind in ['ix', 'loc', 'getitem', 'iloc', None]\n\n        if kind == 'iloc':\n            return self._validate_indexer('positional', key, kind)\n\n        if len(self) and not isinstance(self, ABCMultiIndex,):\n\n            # we can raise here if we are definitive that this\n            # is positional indexing (eg. .ix on with a float)\n            # or label indexing if we are using a type able\n            # to be represented in the index\n\n            if kind in ['getitem', 'ix'] and is_float(key):\n                if not self.is_floating():\n                    return self._invalid_indexer('label', key)\n\n            elif kind in ['loc'] and is_float(key):\n\n                # we want to raise KeyError on string/mixed here\n                # technically we *could* raise a TypeError\n                # on anything but mixed though\n                if self.inferred_type not in ['floating',\n                                              'mixed-integer-float',\n                                              'string',\n                                              'unicode',\n                                              'mixed']:\n                    return self._invalid_indexer('label', key)\n\n            elif kind in ['loc'] and is_integer(key):\n                if not self.holds_integer():\n                    return self._invalid_indexer('label', key)\n\n        return key\n\n    _index_shared_docs['_convert_slice_indexer'] = \"\"\"\n        Convert a slice indexer.\n\n        By definition, these are labels unless 'iloc' is passed in.\n        Floats are not allowed as the start, step, or stop of the slice.\n\n        Parameters\n        ----------\n        key : label of the slice bound\n        kind : {'ix', 'loc', 'getitem', 'iloc'} or None\n    \"\"\"\n\n    @Appender(_index_shared_docs['_convert_slice_indexer'])\n    def _convert_slice_indexer(self, key, kind=None):\n        assert kind in ['ix', 'loc', 'getitem', 'iloc', None]\n\n        # if we are not a slice, then we are done\n        if not isinstance(key, slice):\n            return key\n\n        # validate iloc\n        if kind == 'iloc':\n            return slice(self._validate_indexer('slice', key.start, kind),\n                         self._validate_indexer('slice', key.stop, kind),\n                         self._validate_indexer('slice', key.step, kind))\n\n        # potentially cast the bounds to integers\n        start, stop, step = key.start, key.stop, key.step\n\n        # figure out if this is a positional indexer\n        def is_int(v):\n            return v is None or is_integer(v)\n\n        is_null_slicer = start is None and stop is None\n        is_index_slice = is_int(start) and is_int(stop)\n        is_positional = is_index_slice and not self.is_integer()\n\n        if kind == 'getitem':\n            \"\"\"\n            called from the getitem slicers, validate that we are in fact\n            integers\n            \"\"\"\n            if self.is_integer() or is_index_slice:\n                return slice(self._validate_indexer('slice', key.start, kind),\n                             self._validate_indexer('slice', key.stop, kind),\n                             self._validate_indexer('slice', key.step, kind))\n\n        # convert the slice to an indexer here\n\n        # if we are mixed and have integers\n        try:\n            if is_positional and self.is_mixed():\n                # Validate start & stop\n                if start is not None:\n                    self.get_loc(start)\n                if stop is not None:\n                    self.get_loc(stop)\n                is_positional = False\n        except KeyError:\n            if self.inferred_type == 'mixed-integer-float':\n                raise\n\n        if is_null_slicer:\n            indexer = key\n        elif is_positional:\n            indexer = key\n        else:\n            try:\n                indexer = self.slice_indexer(start, stop, step, kind=kind)\n            except Exception:\n                if is_index_slice:\n                    if self.is_integer():\n                        raise\n                    else:\n                        indexer = key\n                else:\n                    raise\n\n        return indexer\n\n    def _convert_listlike_indexer(self, keyarr, kind=None):\n        \"\"\"\n        Parameters\n        ----------\n        keyarr : list-like\n            Indexer to convert.\n\n        Returns\n        -------\n        tuple (indexer, keyarr)\n            indexer is an ndarray or None if cannot convert\n            keyarr are tuple-safe keys\n        \"\"\"\n        if isinstance(keyarr, Index):\n            keyarr = self._convert_index_indexer(keyarr)\n        else:\n            keyarr = self._convert_arr_indexer(keyarr)\n\n        indexer = self._convert_list_indexer(keyarr, kind=kind)\n        return indexer, keyarr\n\n    _index_shared_docs['_convert_arr_indexer'] = \"\"\"\n        Convert an array-like indexer to the appropriate dtype.\n\n        Parameters\n        ----------\n        keyarr : array-like\n            Indexer to convert.\n\n        Returns\n        -------\n        converted_keyarr : array-like\n    \"\"\"\n\n    @Appender(_index_shared_docs['_convert_arr_indexer'])\n    def _convert_arr_indexer(self, keyarr):\n        keyarr = com.asarray_tuplesafe(keyarr)\n        return keyarr\n\n    _index_shared_docs['_convert_index_indexer'] = \"\"\"\n        Convert an Index indexer to the appropriate dtype.\n\n        Parameters\n        ----------\n        keyarr : Index (or sub-class)\n            Indexer to convert.\n\n        Returns\n        -------\n        converted_keyarr : Index (or sub-class)\n    \"\"\"\n\n    @Appender(_index_shared_docs['_convert_index_indexer'])\n    def _convert_index_indexer(self, keyarr):\n        return keyarr\n\n    _index_shared_docs['_convert_list_indexer'] = \"\"\"\n        Convert a list-like indexer to the appropriate dtype.\n\n        Parameters\n        ----------\n        keyarr : Index (or sub-class)\n            Indexer to convert.\n        kind : iloc, ix, loc, optional\n\n        Returns\n        -------\n        positional indexer or None\n    \"\"\"\n\n    @Appender(_index_shared_docs['_convert_list_indexer'])\n    def _convert_list_indexer(self, keyarr, kind=None):\n        if (kind in [None, 'iloc', 'ix'] and\n                is_integer_dtype(keyarr) and not self.is_floating() and\n                not isinstance(keyarr, ABCPeriodIndex)):\n\n            if self.inferred_type == 'mixed-integer':\n                indexer = self.get_indexer(keyarr)\n                if (indexer >= 0).all():\n                    return indexer\n                # missing values are flagged as -1 by get_indexer and negative\n                # indices are already converted to positive indices in the\n                # above if-statement, so the negative flags are changed to\n                # values outside the range of indices so as to trigger an\n                # IndexError in maybe_convert_indices\n                indexer[indexer < 0] = len(self)\n                from pandas.core.indexing import maybe_convert_indices\n                return maybe_convert_indices(indexer, len(self))\n\n            elif not self.inferred_type == 'integer':\n                keyarr = np.where(keyarr < 0, len(self) + keyarr, keyarr)\n                return keyarr\n\n        return None\n\n    def _invalid_indexer(self, form, key):\n        \"\"\"\n        Consistent invalid indexer message.\n        \"\"\"\n        raise TypeError(\"cannot do {form} indexing on {klass} with these \"\n                        \"indexers [{key}] of {kind}\".format(\n                            form=form, klass=type(self), key=key,\n                            kind=type(key)))\n\n    # --------------------------------------------------------------------\n    # Reindex Methods\n\n    def _can_reindex(self, indexer):\n        \"\"\"\n        Check if we are allowing reindexing with this particular indexer.\n\n        Parameters\n        ----------\n        indexer : an integer indexer\n\n        Raises\n        ------\n        ValueError if its a duplicate axis\n        \"\"\"\n\n        # trying to reindex on an axis with duplicates\n        if not self.is_unique and len(indexer):\n            raise ValueError(\"cannot reindex from a duplicate axis\")\n\n    def reindex(self, target, method=None, level=None, limit=None,\n                tolerance=None):\n        \"\"\"\n        Create index with target's values (move/add/delete values\n        as necessary).\n\n        Parameters\n        ----------\n        target : an iterable\n\n        Returns\n        -------\n        new_index : pd.Index\n            Resulting index\n        indexer : np.ndarray or None\n            Indices of output values in original index\n\n        \"\"\"\n        # GH6552: preserve names when reindexing to non-named target\n        # (i.e. neither Index nor Series).\n        preserve_names = not hasattr(target, 'name')\n\n        # GH7774: preserve dtype/tz if target is empty and not an Index.\n        target = _ensure_has_len(target)  # target may be an iterator\n\n        if not isinstance(target, Index) and len(target) == 0:\n            attrs = self._get_attributes_dict()\n            attrs.pop('freq', None)  # don't preserve freq\n            values = self._data[:0]  # appropriately-dtyped empty array\n            target = self._simple_new(values, dtype=self.dtype, **attrs)\n        else:\n            target = ensure_index(target)\n\n        if level is not None:\n            if method is not None:\n                raise TypeError('Fill method not supported if level passed')\n            _, indexer, _ = self._join_level(target, level, how='right',\n                                             return_indexers=True)\n        else:\n            if self.equals(target):\n                indexer = None\n            else:\n\n                if self.is_unique:\n                    indexer = self.get_indexer(target, method=method,\n                                               limit=limit,\n                                               tolerance=tolerance)\n                else:\n                    if method is not None or limit is not None:\n                        raise ValueError(\"cannot reindex a non-unique index \"\n                                         \"with a method or limit\")\n                    indexer, missing = self.get_indexer_non_unique(target)\n\n        if preserve_names and target.nlevels == 1 and target.name != self.name:\n            target = target.copy()\n            target.name = self.name\n\n        return target, indexer\n\n    def _reindex_non_unique(self, target):\n        \"\"\"\n        Create a new index with target's values (move/add/delete values as\n        necessary) use with non-unique Index and a possibly non-unique target.\n\n        Parameters\n        ----------\n        target : an iterable\n\n        Returns\n        -------\n        new_index : pd.Index\n            Resulting index\n        indexer : np.ndarray or None\n            Indices of output values in original index\n\n        \"\"\"\n\n        target = ensure_index(target)\n        indexer, missing = self.get_indexer_non_unique(target)\n        check = indexer != -1\n        new_labels = self.take(indexer[check])\n        new_indexer = None\n\n        if len(missing):\n            length = np.arange(len(indexer))\n\n            missing = ensure_platform_int(missing)\n            missing_labels = target.take(missing)\n            missing_indexer = ensure_int64(length[~check])\n            cur_labels = self.take(indexer[check]).values\n            cur_indexer = ensure_int64(length[check])\n\n            new_labels = np.empty(tuple([len(indexer)]), dtype=object)\n            new_labels[cur_indexer] = cur_labels\n            new_labels[missing_indexer] = missing_labels\n\n            # a unique indexer\n            if target.is_unique:\n\n                # see GH5553, make sure we use the right indexer\n                new_indexer = np.arange(len(indexer))\n                new_indexer[cur_indexer] = np.arange(len(cur_labels))\n                new_indexer[missing_indexer] = -1\n\n            # we have a non_unique selector, need to use the original\n            # indexer here\n            else:\n\n                # need to retake to have the same size as the indexer\n                indexer[~check] = -1\n\n                # reset the new indexer to account for the new size\n                new_indexer = np.arange(len(self.take(indexer)))\n                new_indexer[~check] = -1\n\n        new_index = self._shallow_copy_with_infer(new_labels, freq=None)\n        return new_index, indexer, new_indexer\n\n    # --------------------------------------------------------------------\n    # Join Methods\n\n    _index_shared_docs['join'] = \"\"\"\n        Compute join_index and indexers to conform data\n        structures to the new index.\n\n        Parameters\n        ----------\n        other : Index\n        how : {'left', 'right', 'inner', 'outer'}\n        level : int or level name, default None\n        return_indexers : boolean, default False\n        sort : boolean, default False\n            Sort the join keys lexicographically in the result Index. If False,\n            the order of the join keys depends on the join type (how keyword)\n\n            .. versionadded:: 0.20.0\n\n        Returns\n        -------\n        join_index, (left_indexer, right_indexer)\n        \"\"\"\n\n    @Appender(_index_shared_docs['join'])\n    def join(self, other, how='left', level=None, return_indexers=False,\n             sort=False):\n        from .multi import MultiIndex\n        self_is_mi = isinstance(self, MultiIndex)\n        other_is_mi = isinstance(other, MultiIndex)\n\n        # try to figure out the join level\n        # GH3662\n        if level is None and (self_is_mi or other_is_mi):\n\n            # have the same levels/names so a simple join\n            if self.names == other.names:\n                pass\n            else:\n                return self._join_multi(other, how=how,\n                                        return_indexers=return_indexers)\n\n        # join on the level\n        if level is not None and (self_is_mi or other_is_mi):\n            return self._join_level(other, level, how=how,\n                                    return_indexers=return_indexers)\n\n        other = ensure_index(other)\n\n        if len(other) == 0 and how in ('left', 'outer'):\n            join_index = self._shallow_copy()\n            if return_indexers:\n                rindexer = np.repeat(-1, len(join_index))\n                return join_index, None, rindexer\n            else:\n                return join_index\n\n        if len(self) == 0 and how in ('right', 'outer'):\n            join_index = other._shallow_copy()\n            if return_indexers:\n                lindexer = np.repeat(-1, len(join_index))\n                return join_index, lindexer, None\n            else:\n                return join_index\n\n        if self._join_precedence < other._join_precedence:\n            how = {'right': 'left', 'left': 'right'}.get(how, how)\n            result = other.join(self, how=how, level=level,\n                                return_indexers=return_indexers)\n            if return_indexers:\n                x, y, z = result\n                result = x, z, y\n            return result\n\n        if not is_dtype_equal(self.dtype, other.dtype):\n            this = self.astype('O')\n            other = other.astype('O')\n            return this.join(other, how=how, return_indexers=return_indexers)\n\n        _validate_join_method(how)\n\n        if not self.is_unique and not other.is_unique:\n            return self._join_non_unique(other, how=how,\n                                         return_indexers=return_indexers)\n        elif not self.is_unique or not other.is_unique:\n            if self.is_monotonic and other.is_monotonic:\n                return self._join_monotonic(other, how=how,\n                                            return_indexers=return_indexers)\n            else:\n                return self._join_non_unique(other, how=how,\n                                             return_indexers=return_indexers)\n        elif self.is_monotonic and other.is_monotonic:\n            try:\n                return self._join_monotonic(other, how=how,\n                                            return_indexers=return_indexers)\n            except TypeError:\n                pass\n\n        if how == 'left':\n            join_index = self\n        elif how == 'right':\n            join_index = other\n        elif how == 'inner':\n            join_index = self.intersection(other)\n        elif how == 'outer':\n            join_index = self.union(other)\n\n        if sort:\n            join_index = join_index.sort_values()\n\n        if return_indexers:\n            if join_index is self:\n                lindexer = None\n            else:\n                lindexer = self.get_indexer(join_index)\n            if join_index is other:\n                rindexer = None\n            else:\n                rindexer = other.get_indexer(join_index)\n            return join_index, lindexer, rindexer\n        else:\n            return join_index\n\n    def _join_multi(self, other, how, return_indexers=True):\n        from .multi import MultiIndex\n        from pandas.core.reshape.merge import _restore_dropped_levels_multijoin\n\n        # figure out join names\n        self_names = set(com._not_none(*self.names))\n        other_names = set(com._not_none(*other.names))\n        overlap = self_names & other_names\n\n        # need at least 1 in common\n        if not overlap:\n            raise ValueError(\"cannot join with no overlapping index names\")\n\n        self_is_mi = isinstance(self, MultiIndex)\n        other_is_mi = isinstance(other, MultiIndex)\n\n        if self_is_mi and other_is_mi:\n\n            # Drop the non-matching levels from left and right respectively\n            ldrop_names = list(self_names - overlap)\n            rdrop_names = list(other_names - overlap)\n\n            self_jnlevels = self.droplevel(ldrop_names)\n            other_jnlevels = other.droplevel(rdrop_names)\n\n            # Join left and right\n            # Join on same leveled multi-index frames is supported\n            join_idx, lidx, ridx = self_jnlevels.join(other_jnlevels, how,\n                                                      return_indexers=True)\n\n            # Restore the dropped levels\n            # Returned index level order is\n            # common levels, ldrop_names, rdrop_names\n            dropped_names = ldrop_names + rdrop_names\n\n            levels, labels, names = (\n                _restore_dropped_levels_multijoin(self, other,\n                                                  dropped_names,\n                                                  join_idx,\n                                                  lidx, ridx))\n\n            # Re-create the multi-index\n            multi_join_idx = MultiIndex(levels=levels, labels=labels,\n                                        names=names, verify_integrity=False)\n\n            multi_join_idx = multi_join_idx.remove_unused_levels()\n\n            return multi_join_idx, lidx, ridx\n\n        jl = list(overlap)[0]\n\n        # Case where only one index is multi\n        # make the indices into mi's that match\n        flip_order = False\n        if self_is_mi:\n            self, other = other, self\n            flip_order = True\n            # flip if join method is right or left\n            how = {'right': 'left', 'left': 'right'}.get(how, how)\n\n        level = other.names.index(jl)\n        result = self._join_level(other, level, how=how,\n                                  return_indexers=return_indexers)\n\n        if flip_order:\n            if isinstance(result, tuple):\n                return result[0], result[2], result[1]\n        return result\n\n    def _join_non_unique(self, other, how='left', return_indexers=False):\n        from pandas.core.reshape.merge import _get_join_indexers\n\n        left_idx, right_idx = _get_join_indexers([self._ndarray_values],\n                                                 [other._ndarray_values],\n                                                 how=how,\n                                                 sort=True)\n\n        left_idx = ensure_platform_int(left_idx)\n        right_idx = ensure_platform_int(right_idx)\n\n        join_index = np.asarray(self._ndarray_values.take(left_idx))\n        mask = left_idx == -1\n        np.putmask(join_index, mask, other._ndarray_values.take(right_idx))\n\n        join_index = self._wrap_joined_index(join_index, other)\n\n        if return_indexers:\n            return join_index, left_idx, right_idx\n        else:\n            return join_index\n\n    def _join_level(self, other, level, how='left', return_indexers=False,\n                    keep_order=True):\n        \"\"\"\n        The join method *only* affects the level of the resulting\n        MultiIndex. Otherwise it just exactly aligns the Index data to the\n        labels of the level in the MultiIndex.\n\n        If ```keep_order == True```, the order of the data indexed by the\n        MultiIndex will not be changed; otherwise, it will tie out\n        with `other`.\n        \"\"\"\n        from .multi import MultiIndex\n\n        def _get_leaf_sorter(labels):\n            \"\"\"\n            Returns sorter for the inner most level while preserving the\n            order of higher levels.\n            \"\"\"\n            if labels[0].size == 0:\n                return np.empty(0, dtype='int64')\n\n            if len(labels) == 1:\n                lab = ensure_int64(labels[0])\n                sorter, _ = libalgos.groupsort_indexer(lab, 1 + lab.max())\n                return sorter\n\n            # find indexers of beginning of each set of\n            # same-key labels w.r.t all but last level\n            tic = labels[0][:-1] != labels[0][1:]\n            for lab in labels[1:-1]:\n                tic |= lab[:-1] != lab[1:]\n\n            starts = np.hstack(([True], tic, [True])).nonzero()[0]\n            lab = ensure_int64(labels[-1])\n            return lib.get_level_sorter(lab, ensure_int64(starts))\n\n        if isinstance(self, MultiIndex) and isinstance(other, MultiIndex):\n            raise TypeError('Join on level between two MultiIndex objects '\n                            'is ambiguous')\n\n        left, right = self, other\n\n        flip_order = not isinstance(self, MultiIndex)\n        if flip_order:\n            left, right = right, left\n            how = {'right': 'left', 'left': 'right'}.get(how, how)\n\n        level = left._get_level_number(level)\n        old_level = left.levels[level]\n\n        if not right.is_unique:\n            raise NotImplementedError('Index._join_level on non-unique index '\n                                      'is not implemented')\n\n        new_level, left_lev_indexer, right_lev_indexer = \\\n            old_level.join(right, how=how, return_indexers=True)\n\n        if left_lev_indexer is None:\n            if keep_order or len(left) == 0:\n                left_indexer = None\n                join_index = left\n            else:  # sort the leaves\n                left_indexer = _get_leaf_sorter(left.labels[:level + 1])\n                join_index = left[left_indexer]\n\n        else:\n            left_lev_indexer = ensure_int64(left_lev_indexer)\n            rev_indexer = lib.get_reverse_indexer(left_lev_indexer,\n                                                  len(old_level))\n\n            new_lev_labels = algos.take_nd(rev_indexer, left.labels[level],\n                                           allow_fill=False)\n\n            new_labels = list(left.labels)\n            new_labels[level] = new_lev_labels\n\n            new_levels = list(left.levels)\n            new_levels[level] = new_level\n\n            if keep_order:  # just drop missing values. o.w. keep order\n                left_indexer = np.arange(len(left), dtype=np.intp)\n                mask = new_lev_labels != -1\n                if not mask.all():\n                    new_labels = [lab[mask] for lab in new_labels]\n                    left_indexer = left_indexer[mask]\n\n            else:  # tie out the order with other\n                if level == 0:  # outer most level, take the fast route\n                    ngroups = 1 + new_lev_labels.max()\n                    left_indexer, counts = libalgos.groupsort_indexer(\n                        new_lev_labels, ngroups)\n\n                    # missing values are placed first; drop them!\n                    left_indexer = left_indexer[counts[0]:]\n                    new_labels = [lab[left_indexer] for lab in new_labels]\n\n                else:  # sort the leaves\n                    mask = new_lev_labels != -1\n                    mask_all = mask.all()\n                    if not mask_all:\n                        new_labels = [lab[mask] for lab in new_labels]\n\n                    left_indexer = _get_leaf_sorter(new_labels[:level + 1])\n                    new_labels = [lab[left_indexer] for lab in new_labels]\n\n                    # left_indexers are w.r.t masked frame.\n                    # reverse to original frame!\n                    if not mask_all:\n                        left_indexer = mask.nonzero()[0][left_indexer]\n\n            join_index = MultiIndex(levels=new_levels, labels=new_labels,\n                                    names=left.names, verify_integrity=False)\n\n        if right_lev_indexer is not None:\n            right_indexer = algos.take_nd(right_lev_indexer,\n                                          join_index.labels[level],\n                                          allow_fill=False)\n        else:\n            right_indexer = join_index.labels[level]\n\n        if flip_order:\n            left_indexer, right_indexer = right_indexer, left_indexer\n\n        if return_indexers:\n            left_indexer = (None if left_indexer is None\n                            else ensure_platform_int(left_indexer))\n            right_indexer = (None if right_indexer is None\n                             else ensure_platform_int(right_indexer))\n            return join_index, left_indexer, right_indexer\n        else:\n            return join_index\n\n    def _join_monotonic(self, other, how='left', return_indexers=False):\n        if self.equals(other):\n            ret_index = other if how == 'right' else self\n            if return_indexers:\n                return ret_index, None, None\n            else:\n                return ret_index\n\n        sv = self._ndarray_values\n        ov = other._ndarray_values\n\n        if self.is_unique and other.is_unique:\n            # We can perform much better than the general case\n            if how == 'left':\n                join_index = self\n                lidx = None\n                ridx = self._left_indexer_unique(sv, ov)\n            elif how == 'right':\n                join_index = other\n                lidx = self._left_indexer_unique(ov, sv)\n                ridx = None\n            elif how == 'inner':\n                join_index, lidx, ridx = self._inner_indexer(sv, ov)\n                join_index = self._wrap_joined_index(join_index, other)\n            elif how == 'outer':\n                join_index, lidx, ridx = self._outer_indexer(sv, ov)\n                join_index = self._wrap_joined_index(join_index, other)\n        else:\n            if how == 'left':\n                join_index, lidx, ridx = self._left_indexer(sv, ov)\n            elif how == 'right':\n                join_index, ridx, lidx = self._left_indexer(ov, sv)\n            elif how == 'inner':\n                join_index, lidx, ridx = self._inner_indexer(sv, ov)\n            elif how == 'outer':\n                join_index, lidx, ridx = self._outer_indexer(sv, ov)\n            join_index = self._wrap_joined_index(join_index, other)\n\n        if return_indexers:\n            lidx = None if lidx is None else ensure_platform_int(lidx)\n            ridx = None if ridx is None else ensure_platform_int(ridx)\n            return join_index, lidx, ridx\n        else:\n            return join_index\n\n    def _wrap_joined_index(self, joined, other):\n        name = get_op_result_name(self, other)\n        return Index(joined, name=name)\n\n    # --------------------------------------------------------------------\n    # Uncategorized Methods\n\n    @property\n    def values(self):\n        \"\"\"\n        Return the underlying data as an ndarray.\n        \"\"\"\n        return self._data.view(np.ndarray)\n\n    @property\n    def _values(self):\n        # type: () -> Union[ExtensionArray, Index, np.ndarray]\n        # TODO(EA): remove index types as they become extension arrays\n        \"\"\"\n        The best array representation.\n\n        This is an ndarray, ExtensionArray, or Index subclass. This differs\n        from ``_ndarray_values``, which always returns an ndarray.\n\n        Both ``_values`` and ``_ndarray_values`` are consistent between\n        ``Series`` and ``Index``.\n\n        It may differ from the public '.values' method.\n\n        index             | values          | _values       | _ndarray_values |\n        ----------------- | --------------- | ------------- | --------------- |\n        Index             | ndarray         | ndarray       | ndarray         |\n        CategoricalIndex  | Categorical     | Categorical   | ndarray[int]    |\n        DatetimeIndex     | ndarray[M8ns]   | ndarray[M8ns] | ndarray[M8ns]   |\n        DatetimeIndex[tz] | ndarray[M8ns]   | DTI[tz]       | ndarray[M8ns]   |\n        PeriodIndex       | ndarray[object] | PeriodArray   | ndarray[int]    |\n        IntervalIndex     | IntervalArray   | IntervalArray | ndarray[object] |\n\n        See Also\n        --------\n        values\n        _ndarray_values\n        \"\"\"\n        return self.values\n\n    def get_values(self):\n        \"\"\"\n        Return `Index` data as an `numpy.ndarray`.\n\n        Returns\n        -------\n        numpy.ndarray\n            A one-dimensional numpy array of the `Index` values.\n\n        See Also\n        --------\n        Index.values : The attribute that get_values wraps.\n\n        Examples\n        --------\n        Getting the `Index` values of a `DataFrame`:\n\n        >>> df = pd.DataFrame([[1, 2, 3], [4, 5, 6], [7, 8, 9]],\n        ...                    index=['a', 'b', 'c'], columns=['A', 'B', 'C'])\n        >>> df\n           A  B  C\n        a  1  2  3\n        b  4  5  6\n        c  7  8  9\n        >>> df.index.get_values()\n        array(['a', 'b', 'c'], dtype=object)\n\n        Standalone `Index` values:\n\n        >>> idx = pd.Index(['1', '2', '3'])\n        >>> idx.get_values()\n        array(['1', '2', '3'], dtype=object)\n\n        `MultiIndex` arrays also have only one dimension:\n\n        >>> midx = pd.MultiIndex.from_arrays([[1, 2, 3], ['a', 'b', 'c']],\n        ...                                  names=('number', 'letter'))\n        >>> midx.get_values()\n        array([(1, 'a'), (2, 'b'), (3, 'c')], dtype=object)\n        >>> midx.get_values().ndim\n        1\n        \"\"\"\n        return self.values\n\n    @Appender(IndexOpsMixin.memory_usage.__doc__)\n    def memory_usage(self, deep=False):\n        result = super(Index, self).memory_usage(deep=deep)\n\n        # include our engine hashtable\n        result += self._engine.sizeof(deep=deep)\n        return result\n\n    _index_shared_docs['where'] = \"\"\"\n        Return an Index of same shape as self and whose corresponding\n        entries are from self where cond is True and otherwise are from\n        other.\n\n        .. versionadded:: 0.19.0\n\n        Parameters\n        ----------\n        cond : boolean array-like with the same length as self\n        other : scalar, or array-like\n        \"\"\"\n\n    @Appender(_index_shared_docs['where'])\n    def where(self, cond, other=None):\n        if other is None:\n            other = self._na_value\n\n        dtype = self.dtype\n        values = self.values\n\n        if is_bool(other) or is_bool_dtype(other):\n\n            # bools force casting\n            values = values.astype(object)\n            dtype = None\n\n        values = np.where(cond, values, other)\n\n        if self._is_numeric_dtype and np.any(isna(values)):\n            # We can't coerce to the numeric dtype of \"self\" (unless\n            # it's float) if there are NaN values in our output.\n            dtype = None\n\n        return self._shallow_copy_with_infer(values, dtype=dtype)\n\n    # construction helpers\n    @classmethod\n    def _try_convert_to_int_index(cls, data, copy, name, dtype):\n        \"\"\"\n        Attempt to convert an array of data into an integer index.\n\n        Parameters\n        ----------\n        data : The data to convert.\n        copy : Whether to copy the data or not.\n        name : The name of the index returned.\n\n        Returns\n        -------\n        int_index : data converted to either an Int64Index or a\n                    UInt64Index\n\n        Raises\n        ------\n        ValueError if the conversion was not successful.\n        \"\"\"\n\n        from .numeric import Int64Index, UInt64Index\n        if not is_unsigned_integer_dtype(dtype):\n            # skip int64 conversion attempt if uint-like dtype is passed, as\n            # this could return Int64Index when UInt64Index is what's desrired\n            try:\n                res = data.astype('i8', copy=False)\n                if (res == data).all():\n                    return Int64Index(res, copy=copy, name=name)\n            except (OverflowError, TypeError, ValueError):\n                pass\n\n        # Conversion to int64 failed (possibly due to overflow) or was skipped,\n        # so let's try now with uint64.\n        try:\n            res = data.astype('u8', copy=False)\n            if (res == data).all():\n                return UInt64Index(res, copy=copy, name=name)\n        except (OverflowError, TypeError, ValueError):\n            pass\n\n        raise ValueError\n\n    @classmethod\n    def _scalar_data_error(cls, data):\n        raise TypeError('{0}(...) must be called with a collection of some '\n                        'kind, {1} was passed'.format(cls.__name__,\n                                                      repr(data)))\n\n    @classmethod\n    def _string_data_error(cls, data):\n        raise TypeError('String dtype not supported, you may need '\n                        'to explicitly cast to a numeric type')\n\n    @classmethod\n    def _coerce_to_ndarray(cls, data):\n        \"\"\"\n        Coerces data to ndarray.\n\n        Converts other iterables to list first and then to array.\n        Does not touch ndarrays.\n\n        Raises\n        ------\n        TypeError\n            When the data passed in is a scalar.\n        \"\"\"\n\n        if not isinstance(data, (np.ndarray, Index)):\n            if data is None or is_scalar(data):\n                cls._scalar_data_error(data)\n\n            # other iterable of some kind\n            if not isinstance(data, (ABCSeries, list, tuple)):\n                data = list(data)\n            data = np.asarray(data)\n        return data\n\n    def _coerce_scalar_to_index(self, item):\n        \"\"\"\n        We need to coerce a scalar to a compat for our index type.\n\n        Parameters\n        ----------\n        item : scalar item to coerce\n        \"\"\"\n        dtype = self.dtype\n\n        if self._is_numeric_dtype and isna(item):\n            # We can't coerce to the numeric dtype of \"self\" (unless\n            # it's float) if there are NaN values in our output.\n            dtype = None\n\n        return Index([item], dtype=dtype, **self._get_attributes_dict())\n\n    def _to_safe_for_reshape(self):\n        \"\"\"\n        Convert to object if we are a categorical.\n        \"\"\"\n        return self\n\n    def _convert_for_op(self, value):\n        \"\"\"\n        Convert value to be insertable to ndarray.\n        \"\"\"\n        return value\n\n    def _assert_can_do_op(self, value):\n        \"\"\"\n        Check value is valid for scalar op.\n        \"\"\"\n        if not is_scalar(value):\n            msg = \"'value' must be a scalar, passed: {0}\"\n            raise TypeError(msg.format(type(value).__name__))\n\n    @property\n    def _has_complex_internals(self):\n        # to disable groupby tricks in MultiIndex\n        return False\n\n    def _is_memory_usage_qualified(self):\n        \"\"\"\n        Return a boolean if we need a qualified .info display.\n        \"\"\"\n        return self.is_object()\n\n    def is_type_compatible(self, kind):\n        return kind == self.inferred_type\n\n    _index_shared_docs['__contains__'] = \"\"\"\n        Return a boolean if this key is IN the index.\n\n        Parameters\n        ----------\n        key : object\n\n        Returns\n        -------\n        boolean\n        \"\"\"\n\n    @Appender(_index_shared_docs['__contains__'] % _index_doc_kwargs)\n    def __contains__(self, key):\n        hash(key)\n        try:\n            return key in self._engine\n        except (OverflowError, TypeError, ValueError):\n            return False\n\n    _index_shared_docs['contains'] = \"\"\"\n        Return a boolean if this key is IN the index.\n\n        Parameters\n        ----------\n        key : object\n\n        Returns\n        -------\n        boolean\n        \"\"\"\n\n    @Appender(_index_shared_docs['contains'] % _index_doc_kwargs)\n    def contains(self, key):\n        hash(key)\n        try:\n            return key in self._engine\n        except (TypeError, ValueError):\n            return False\n\n    def __hash__(self):\n        raise TypeError(\"unhashable type: %r\" % type(self).__name__)\n\n    def __setitem__(self, key, value):\n        raise TypeError(\"Index does not support mutable operations\")\n\n    def __getitem__(self, key):\n        \"\"\"\n        Override numpy.ndarray's __getitem__ method to work as desired.\n\n        This function adds lists and Series as valid boolean indexers\n        (ndarrays only supports ndarray with dtype=bool).\n\n        If resulting ndim != 1, plain ndarray is returned instead of\n        corresponding `Index` subclass.\n\n        \"\"\"\n        # There's no custom logic to be implemented in __getslice__, so it's\n        # not overloaded intentionally.\n        getitem = self._data.__getitem__\n        promote = self._shallow_copy\n\n        if is_scalar(key):\n            key = com.cast_scalar_indexer(key)\n            return getitem(key)\n\n        if isinstance(key, slice):\n            # This case is separated from the conditional above to avoid\n            # pessimization of basic indexing.\n            return promote(getitem(key))\n\n        if com.is_bool_indexer(key):\n            key = np.asarray(key, dtype=bool)\n\n        key = com.values_from_object(key)\n        result = getitem(key)\n        if not is_scalar(result):\n            return promote(result)\n        else:\n            return result\n\n    def _can_hold_identifiers_and_holds_name(self, name):\n        \"\"\"\n        Faster check for ``name in self`` when we know `name` is a Python\n        identifier (e.g. in NDFrame.__getattr__, which hits this to support\n        . key lookup). For indexes that can't hold identifiers (everything\n        but object & categorical) we just return False.\n\n        https://github.com/pandas-dev/pandas/issues/19764\n        \"\"\"\n        if self.is_object() or self.is_categorical():\n            return name in self\n        return False\n\n    def append(self, other):\n        \"\"\"\n        Append a collection of Index options together.\n\n        Parameters\n        ----------\n        other : Index or list/tuple of indices\n\n        Returns\n        -------\n        appended : Index\n        \"\"\"\n\n        to_concat = [self]\n\n        if isinstance(other, (list, tuple)):\n            to_concat = to_concat + list(other)\n        else:\n            to_concat.append(other)\n\n        for obj in to_concat:\n            if not isinstance(obj, Index):\n                raise TypeError('all inputs must be Index')\n\n        names = {obj.name for obj in to_concat}\n        name = None if len(names) > 1 else self.name\n\n        return self._concat(to_concat, name)\n\n    def _concat(self, to_concat, name):\n\n        typs = _concat.get_dtype_kinds(to_concat)\n\n        if len(typs) == 1:\n            return self._concat_same_dtype(to_concat, name=name)\n        return _concat._concat_index_asobject(to_concat, name=name)\n\n    def _concat_same_dtype(self, to_concat, name):\n        \"\"\"\n        Concatenate to_concat which has the same class.\n        \"\"\"\n        # must be overridden in specific classes\n        return _concat._concat_index_asobject(to_concat, name)\n\n    def putmask(self, mask, value):\n        \"\"\"\n        Return a new Index of the values set with the mask.\n\n        See Also\n        --------\n        numpy.ndarray.putmask\n        \"\"\"\n        values = self.values.copy()\n        try:\n            np.putmask(values, mask, self._convert_for_op(value))\n            return self._shallow_copy(values)\n        except (ValueError, TypeError) as err:\n            if is_object_dtype(self):\n                raise err\n\n            # coerces to object\n            return self.astype(object).putmask(mask, value)\n\n    def equals(self, other):\n        \"\"\"\n        Determines if two Index objects contain the same elements.\n        \"\"\"\n        if self.is_(other):\n            return True\n\n        if not isinstance(other, Index):\n            return False\n\n        if is_object_dtype(self) and not is_object_dtype(other):\n            # if other is not object, use other's logic for coercion\n            return other.equals(self)\n\n        try:\n            return array_equivalent(com.values_from_object(self),\n                                    com.values_from_object(other))\n        except Exception:\n            return False\n\n    def identical(self, other):\n        \"\"\"\n        Similar to equals, but check that other comparable attributes are\n        also equal.\n        \"\"\"\n        return (self.equals(other) and\n                all((getattr(self, c, None) == getattr(other, c, None)\n                     for c in self._comparables)) and\n                type(self) == type(other))\n\n    def asof(self, label):\n        \"\"\"\n        Return the label from the index, or, if not present, the previous one.\n\n        Assuming that the index is sorted, return the passed index label if it\n        is in the index, or return the previous index label if the passed one\n        is not in the index.\n\n        Parameters\n        ----------\n        label : object\n            The label up to which the method returns the latest index label.\n\n        Returns\n        -------\n        object\n            The passed label if it is in the index. The previous label if the\n            passed label is not in the sorted index or `NaN` if there is no\n            such label.\n\n        See Also\n        --------\n        Series.asof : Return the latest value in a Series up to the\n            passed index.\n        merge_asof : Perform an asof merge (similar to left join but it\n            matches on nearest key rather than equal key).\n        Index.get_loc : An `asof` is a thin wrapper around `get_loc`\n            with method='pad'.\n\n        Examples\n        --------\n        `Index.asof` returns the latest index label up to the passed label.\n\n        >>> idx = pd.Index(['2013-12-31', '2014-01-02', '2014-01-03'])\n        >>> idx.asof('2014-01-01')\n        '2013-12-31'\n\n        If the label is in the index, the method returns the passed label.\n\n        >>> idx.asof('2014-01-02')\n        '2014-01-02'\n\n        If all of the labels in the index are later than the passed label,\n        NaN is returned.\n\n        >>> idx.asof('1999-01-02')\n        nan\n\n        If the index is not sorted, an error is raised.\n\n        >>> idx_not_sorted = pd.Index(['2013-12-31', '2015-01-02',\n        ...                            '2014-01-03'])\n        >>> idx_not_sorted.asof('2013-12-31')\n        Traceback (most recent call last):\n        ValueError: index must be monotonic increasing or decreasing\n        \"\"\"\n        try:\n            loc = self.get_loc(label, method='pad')\n        except KeyError:\n            return self._na_value\n        else:\n            if isinstance(loc, slice):\n                loc = loc.indices(len(self))[-1]\n            return self[loc]\n\n    def asof_locs(self, where, mask):\n        \"\"\"\n        Finds the locations (indices) of the labels from the index for\n        every entry in the `where` argument.\n\n        As in the `asof` function, if the label (a particular entry in\n        `where`) is not in the index, the latest index label upto the\n        passed label is chosen and its index returned.\n\n        If all of the labels in the index are later than a label in `where`,\n        -1 is returned.\n\n        `mask` is used to ignore NA values in the index during calculation.\n\n        Parameters\n        ----------\n        where : Index\n            An Index consisting of an array of timestamps.\n        mask : array-like\n            Array of booleans denoting where values in the original\n            data are not NA.\n\n        Returns\n        -------\n        numpy.ndarray\n            An array of locations (indices) of the labels from the Index\n            which correspond to the return values of the `asof` function\n            for every element in `where`.\n        \"\"\"\n        locs = self.values[mask].searchsorted(where.values, side='right')\n        locs = np.where(locs > 0, locs - 1, 0)\n\n        result = np.arange(len(self))[mask].take(locs)\n\n        first = mask.argmax()\n        result[(locs == 0) & (where.values < self.values[first])] = -1\n\n        return result\n\n    def sort_values(self, return_indexer=False, ascending=True):\n        \"\"\"\n        Return a sorted copy of the index.\n\n        Return a sorted copy of the index, and optionally return the indices\n        that sorted the index itself.\n\n        Parameters\n        ----------\n        return_indexer : bool, default False\n            Should the indices that would sort the index be returned.\n        ascending : bool, default True\n            Should the index values be sorted in an ascending order.\n\n        Returns\n        -------\n        sorted_index : pandas.Index\n            Sorted copy of the index.\n        indexer : numpy.ndarray, optional\n            The indices that the index itself was sorted by.\n\n        See Also\n        --------\n        pandas.Series.sort_values : Sort values of a Series.\n        pandas.DataFrame.sort_values : Sort values in a DataFrame.\n\n        Examples\n        --------\n        >>> idx = pd.Index([10, 100, 1, 1000])\n        >>> idx\n        Int64Index([10, 100, 1, 1000], dtype='int64')\n\n        Sort values in ascending order (default behavior).\n\n        >>> idx.sort_values()\n        Int64Index([1, 10, 100, 1000], dtype='int64')\n\n        Sort values in descending order, and also get the indices `idx` was\n        sorted by.\n\n        >>> idx.sort_values(ascending=False, return_indexer=True)\n        (Int64Index([1000, 100, 10, 1], dtype='int64'), array([3, 1, 0, 2]))\n        \"\"\"\n        _as = self.argsort()\n        if not ascending:\n            _as = _as[::-1]\n\n        sorted_index = self.take(_as)\n\n        if return_indexer:\n            return sorted_index, _as\n        else:\n            return sorted_index\n\n    def sort(self, *args, **kwargs):\n        raise TypeError(\"cannot sort an Index object in-place, use \"\n                        \"sort_values instead\")\n\n    def shift(self, periods=1, freq=None):\n        \"\"\"\n        Shift index by desired number of time frequency increments.\n\n        This method is for shifting the values of datetime-like indexes\n        by a specified time increment a given number of times.\n\n        Parameters\n        ----------\n        periods : int, default 1\n            Number of periods (or increments) to shift by,\n            can be positive or negative.\n        freq : pandas.DateOffset, pandas.Timedelta or string, optional\n            Frequency increment to shift by.\n            If None, the index is shifted by its own `freq` attribute.\n            Offset aliases are valid strings, e.g., 'D', 'W', 'M' etc.\n\n        Returns\n        -------\n        pandas.Index\n            shifted index\n\n        See Also\n        --------\n        Series.shift : Shift values of Series.\n\n        Examples\n        --------\n        Put the first 5 month starts of 2011 into an index.\n\n        >>> month_starts = pd.date_range('1/1/2011', periods=5, freq='MS')\n        >>> month_starts\n        DatetimeIndex(['2011-01-01', '2011-02-01', '2011-03-01', '2011-04-01',\n                       '2011-05-01'],\n                      dtype='datetime64[ns]', freq='MS')\n\n        Shift the index by 10 days.\n\n        >>> month_starts.shift(10, freq='D')\n        DatetimeIndex(['2011-01-11', '2011-02-11', '2011-03-11', '2011-04-11',\n                       '2011-05-11'],\n                      dtype='datetime64[ns]', freq=None)\n\n        The default value of `freq` is the `freq` attribute of the index,\n        which is 'MS' (month start) in this example.\n\n        >>> month_starts.shift(10)\n        DatetimeIndex(['2011-11-01', '2011-12-01', '2012-01-01', '2012-02-01',\n                       '2012-03-01'],\n                      dtype='datetime64[ns]', freq='MS')\n\n        Notes\n        -----\n        This method is only implemented for datetime-like index classes,\n        i.e., DatetimeIndex, PeriodIndex and TimedeltaIndex.\n        \"\"\"\n        raise NotImplementedError(\"Not supported for type %s\" %\n                                  type(self).__name__)\n\n    def argsort(self, *args, **kwargs):\n        \"\"\"\n        Return the integer indices that would sort the index.\n\n        Parameters\n        ----------\n        *args\n            Passed to `numpy.ndarray.argsort`.\n        **kwargs\n            Passed to `numpy.ndarray.argsort`.\n\n        Returns\n        -------\n        numpy.ndarray\n            Integer indices that would sort the index if used as\n            an indexer.\n\n        See Also\n        --------\n        numpy.argsort : Similar method for NumPy arrays.\n        Index.sort_values : Return sorted copy of Index.\n\n        Examples\n        --------\n        >>> idx = pd.Index(['b', 'a', 'd', 'c'])\n        >>> idx\n        Index(['b', 'a', 'd', 'c'], dtype='object')\n\n        >>> order = idx.argsort()\n        >>> order\n        array([1, 0, 3, 2])\n\n        >>> idx[order]\n        Index(['a', 'b', 'c', 'd'], dtype='object')\n        \"\"\"\n        result = self.asi8\n        if result is None:\n            result = np.array(self)\n        return result.argsort(*args, **kwargs)\n\n    def get_value(self, series, key):\n        \"\"\"\n        Fast lookup of value from 1-dimensional ndarray. Only use this if you\n        know what you're doing.\n        \"\"\"\n\n        # if we have something that is Index-like, then\n        # use this, e.g. DatetimeIndex\n        s = getattr(series, '_values', None)\n        if isinstance(s, (ExtensionArray, Index)) and is_scalar(key):\n            # GH 20882, 21257\n            # Unify Index and ExtensionArray treatment\n            # First try to convert the key to a location\n            # If that fails, raise a KeyError if an integer\n            # index, otherwise, see if key is an integer, and\n            # try that\n            try:\n                iloc = self.get_loc(key)\n                return s[iloc]\n            except KeyError:\n                if (len(self) > 0 and\n                        (self.holds_integer() or self.is_boolean())):\n                    raise\n                elif is_integer(key):\n                    return s[key]\n\n        s = com.values_from_object(series)\n        k = com.values_from_object(key)\n\n        k = self._convert_scalar_indexer(k, kind='getitem')\n        try:\n            return self._engine.get_value(s, k,\n                                          tz=getattr(series.dtype, 'tz', None))\n        except KeyError as e1:\n            if len(self) > 0 and (self.holds_integer() or self.is_boolean()):\n                raise\n\n            try:\n                return libindex.get_value_box(s, key)\n            except IndexError:\n                raise\n            except TypeError:\n                # generator/iterator-like\n                if is_iterator(key):\n                    raise InvalidIndexError(key)\n                else:\n                    raise e1\n            except Exception:  # pragma: no cover\n                raise e1\n        except TypeError:\n            # python 3\n            if is_scalar(key):  # pragma: no cover\n                raise IndexError(key)\n            raise InvalidIndexError(key)\n\n    def set_value(self, arr, key, value):\n        \"\"\"\n        Fast lookup of value from 1-dimensional ndarray.\n\n        Notes\n        -----\n        Only use this if you know what you're doing.\n        \"\"\"\n        self._engine.set_value(com.values_from_object(arr),\n                               com.values_from_object(key), value)\n\n    _index_shared_docs['get_indexer_non_unique'] = \"\"\"\n        Compute indexer and mask for new index given the current index. The\n        indexer should be then used as an input to ndarray.take to align the\n        current data to the new index.\n\n        Parameters\n        ----------\n        target : %(target_klass)s\n\n        Returns\n        -------\n        indexer : ndarray of int\n            Integers from 0 to n - 1 indicating that the index at these\n            positions matches the corresponding target values. Missing values\n            in the target are marked by -1.\n        missing : ndarray of int\n            An indexer into the target of the values not found.\n            These correspond to the -1 in the indexer array\n        \"\"\"\n\n    @Appender(_index_shared_docs['get_indexer_non_unique'] % _index_doc_kwargs)\n    def get_indexer_non_unique(self, target):\n        target = ensure_index(target)\n        if is_categorical(target):\n            target = target.astype(target.dtype.categories.dtype)\n        pself, ptarget = self._maybe_promote(target)\n        if pself is not self or ptarget is not target:\n            return pself.get_indexer_non_unique(ptarget)\n\n        if self.is_all_dates:\n            self = Index(self.asi8)\n            tgt_values = target.asi8\n        else:\n            tgt_values = target._ndarray_values\n\n        indexer, missing = self._engine.get_indexer_non_unique(tgt_values)\n        return ensure_platform_int(indexer), missing\n\n    def get_indexer_for(self, target, **kwargs):\n        \"\"\"\n        Guaranteed return of an indexer even when non-unique.\n\n        This dispatches to get_indexer or get_indexer_nonunique\n        as appropriate.\n        \"\"\"\n        if self.is_unique:\n            return self.get_indexer(target, **kwargs)\n        indexer, _ = self.get_indexer_non_unique(target, **kwargs)\n        return indexer\n\n    def _maybe_promote(self, other):\n        # A hack, but it works\n        from pandas import DatetimeIndex\n        if self.inferred_type == 'date' and isinstance(other, DatetimeIndex):\n            return DatetimeIndex(self), other\n        elif self.inferred_type == 'boolean':\n            if not is_object_dtype(self.dtype):\n                return self.astype('object'), other.astype('object')\n        return self, other\n\n    def groupby(self, values):\n        \"\"\"\n        Group the index labels by a given array of values.\n\n        Parameters\n        ----------\n        values : array\n            Values used to determine the groups.\n\n        Returns\n        -------\n        groups : dict\n            {group name -> group labels}\n        \"\"\"\n\n        # TODO: if we are a MultiIndex, we can do better\n        # that converting to tuples\n        from .multi import MultiIndex\n        if isinstance(values, MultiIndex):\n            values = values.values\n        values = ensure_categorical(values)\n        result = values._reverse_indexer()\n\n        # map to the label\n        result = {k: self.take(v) for k, v in compat.iteritems(result)}\n\n        return result\n\n    def map(self, mapper, na_action=None):\n        \"\"\"\n        Map values using input correspondence (a dict, Series, or function).\n\n        Parameters\n        ----------\n        mapper : function, dict, or Series\n            Mapping correspondence.\n        na_action : {None, 'ignore'}\n            If 'ignore', propagate NA values, without passing them to the\n            mapping correspondence.\n\n        Returns\n        -------\n        applied : Union[Index, MultiIndex], inferred\n            The output of the mapping function applied to the index.\n            If the function returns a tuple with more than one element\n            a MultiIndex will be returned.\n        \"\"\"\n\n        from .multi import MultiIndex\n        new_values = super(Index, self)._map_values(\n            mapper, na_action=na_action)\n\n        attributes = self._get_attributes_dict()\n\n        # we can return a MultiIndex\n        if new_values.size and isinstance(new_values[0], tuple):\n            if isinstance(self, MultiIndex):\n                names = self.names\n            elif attributes.get('name'):\n                names = [attributes.get('name')] * len(new_values[0])\n            else:\n                names = None\n            return MultiIndex.from_tuples(new_values,\n                                          names=names)\n\n        attributes['copy'] = False\n        if not new_values.size:\n            # empty\n            attributes['dtype'] = self.dtype\n\n        return Index(new_values, **attributes)\n\n    def isin(self, values, level=None):\n        \"\"\"\n        Return a boolean array where the index values are in `values`.\n\n        Compute boolean array of whether each index value is found in the\n        passed set of values. The length of the returned boolean array matches\n        the length of the index.\n\n        Parameters\n        ----------\n        values : set or list-like\n            Sought values.\n\n            .. versionadded:: 0.18.1\n\n               Support for values as a set.\n\n        level : str or int, optional\n            Name or position of the index level to use (if the index is a\n            `MultiIndex`).\n\n        Returns\n        -------\n        is_contained : ndarray\n            NumPy array of boolean values.\n\n        See Also\n        --------\n        Series.isin : Same for Series.\n        DataFrame.isin : Same method for DataFrames.\n\n        Notes\n        -----\n        In the case of `MultiIndex` you must either specify `values` as a\n        list-like object containing tuples that are the same length as the\n        number of levels, or specify `level`. Otherwise it will raise a\n        ``ValueError``.\n\n        If `level` is specified:\n\n        - if it is the name of one *and only one* index level, use that level;\n        - otherwise it should be a number indicating level position.\n\n        Examples\n        --------\n        >>> idx = pd.Index([1,2,3])\n        >>> idx\n        Int64Index([1, 2, 3], dtype='int64')\n\n        Check whether each index value in a list of values.\n        >>> idx.isin([1, 4])\n        array([ True, False, False])\n\n        >>> midx = pd.MultiIndex.from_arrays([[1,2,3],\n        ...                                  ['red', 'blue', 'green']],\n        ...                                  names=('number', 'color'))\n        >>> midx\n        MultiIndex(levels=[[1, 2, 3], ['blue', 'green', 'red']],\n                   labels=[[0, 1, 2], [2, 0, 1]],\n                   names=['number', 'color'])\n\n        Check whether the strings in the 'color' level of the MultiIndex\n        are in a list of colors.\n\n        >>> midx.isin(['red', 'orange', 'yellow'], level='color')\n        array([ True, False, False])\n\n        To check across the levels of a MultiIndex, pass a list of tuples:\n\n        >>> midx.isin([(1, 'red'), (3, 'red')])\n        array([ True, False, False])\n\n        For a DatetimeIndex, string values in `values` are converted to\n        Timestamps.\n\n        >>> dates = ['2000-03-11', '2000-03-12', '2000-03-13']\n        >>> dti = pd.to_datetime(dates)\n        >>> dti\n        DatetimeIndex(['2000-03-11', '2000-03-12', '2000-03-13'],\n        dtype='datetime64[ns]', freq=None)\n\n        >>> dti.isin(['2000-03-11'])\n        array([ True, False, False])\n        \"\"\"\n        if level is not None:\n            self._validate_index_level(level)\n        return algos.isin(self, values)\n\n    def _get_string_slice(self, key, use_lhs=True, use_rhs=True):\n        # this is for partial string indexing,\n        # overridden in DatetimeIndex, TimedeltaIndex and PeriodIndex\n        raise NotImplementedError\n\n    def slice_indexer(self, start=None, end=None, step=None, kind=None):\n        \"\"\"\n        For an ordered or unique index, compute the slice indexer for input\n        labels and step.\n\n        Parameters\n        ----------\n        start : label, default None\n            If None, defaults to the beginning\n        end : label, default None\n            If None, defaults to the end\n        step : int, default None\n        kind : string, default None\n\n        Returns\n        -------\n        indexer : slice\n\n        Raises\n        ------\n        KeyError : If key does not exist, or key is not unique and index is\n            not ordered.\n\n        Notes\n        -----\n        This function assumes that the data is sorted, so use at your own peril\n\n        Examples\n        ---------\n        This is a method on all index types. For example you can do:\n\n        >>> idx = pd.Index(list('abcd'))\n        >>> idx.slice_indexer(start='b', end='c')\n        slice(1, 3)\n\n        >>> idx = pd.MultiIndex.from_arrays([list('abcd'), list('efgh')])\n        >>> idx.slice_indexer(start='b', end=('c', 'g'))\n        slice(1, 3)\n        \"\"\"\n        start_slice, end_slice = self.slice_locs(start, end, step=step,\n                                                 kind=kind)\n\n        # return a slice\n        if not is_scalar(start_slice):\n            raise AssertionError(\"Start slice bound is non-scalar\")\n        if not is_scalar(end_slice):\n            raise AssertionError(\"End slice bound is non-scalar\")\n\n        return slice(start_slice, end_slice, step)\n\n    def _maybe_cast_indexer(self, key):\n        \"\"\"\n        If we have a float key and are not a floating index, then try to cast\n        to an int if equivalent.\n        \"\"\"\n\n        if is_float(key) and not self.is_floating():\n            try:\n                ckey = int(key)\n                if ckey == key:\n                    key = ckey\n            except (OverflowError, ValueError, TypeError):\n                pass\n        return key\n\n    def _validate_indexer(self, form, key, kind):\n        \"\"\"\n        If we are positional indexer, validate that we have appropriate\n        typed bounds must be an integer.\n        \"\"\"\n        assert kind in ['ix', 'loc', 'getitem', 'iloc']\n\n        if key is None:\n            pass\n        elif is_integer(key):\n            pass\n        elif kind in ['iloc', 'getitem']:\n            self._invalid_indexer(form, key)\n        return key\n\n    _index_shared_docs['_maybe_cast_slice_bound'] = \"\"\"\n        This function should be overloaded in subclasses that allow non-trivial\n        casting on label-slice bounds, e.g. datetime-like indices allowing\n        strings containing formatted datetimes.\n\n        Parameters\n        ----------\n        label : object\n        side : {'left', 'right'}\n        kind : {'ix', 'loc', 'getitem'}\n\n        Returns\n        -------\n        label :  object\n\n        Notes\n        -----\n        Value of `side` parameter should be validated in caller.\n\n        \"\"\"\n\n    @Appender(_index_shared_docs['_maybe_cast_slice_bound'])\n    def _maybe_cast_slice_bound(self, label, side, kind):\n        assert kind in ['ix', 'loc', 'getitem', None]\n\n        # We are a plain index here (sub-class override this method if they\n        # wish to have special treatment for floats/ints, e.g. Float64Index and\n        # datetimelike Indexes\n        # reject them\n        if is_float(label):\n            if not (kind in ['ix'] and (self.holds_integer() or\n                                        self.is_floating())):\n                self._invalid_indexer('slice', label)\n\n        # we are trying to find integer bounds on a non-integer based index\n        # this is rejected (generally .loc gets you here)\n        elif is_integer(label):\n            self._invalid_indexer('slice', label)\n\n        return label\n\n    def _searchsorted_monotonic(self, label, side='left'):\n        if self.is_monotonic_increasing:\n            return self.searchsorted(label, side=side)\n        elif self.is_monotonic_decreasing:\n            # np.searchsorted expects ascending sort order, have to reverse\n            # everything for it to work (element ordering, search side and\n            # resulting value).\n            pos = self[::-1].searchsorted(label, side='right' if side == 'left'\n                                          else 'left')\n            return len(self) - pos\n\n        raise ValueError('index must be monotonic increasing or decreasing')\n\n    def _get_loc_only_exact_matches(self, key):\n        \"\"\"\n        This is overridden on subclasses (namely, IntervalIndex) to control\n        get_slice_bound.\n        \"\"\"\n        return self.get_loc(key)\n\n    def get_slice_bound(self, label, side, kind):\n        \"\"\"\n        Calculate slice bound that corresponds to given label.\n\n        Returns leftmost (one-past-the-rightmost if ``side=='right'``) position\n        of given label.\n\n        Parameters\n        ----------\n        label : object\n        side : {'left', 'right'}\n        kind : {'ix', 'loc', 'getitem'}\n        \"\"\"\n        assert kind in ['ix', 'loc', 'getitem', None]\n\n        if side not in ('left', 'right'):\n            raise ValueError(\"Invalid value for side kwarg,\"\n                             \" must be either 'left' or 'right': %s\" %\n                             (side, ))\n\n        original_label = label\n\n        # For datetime indices label may be a string that has to be converted\n        # to datetime boundary according to its resolution.\n        label = self._maybe_cast_slice_bound(label, side, kind)\n\n        # we need to look up the label\n        try:\n            slc = self._get_loc_only_exact_matches(label)\n        except KeyError as err:\n            try:\n                return self._searchsorted_monotonic(label, side)\n            except ValueError:\n                # raise the original KeyError\n                raise err\n\n        if isinstance(slc, np.ndarray):\n            # get_loc may return a boolean array or an array of indices, which\n            # is OK as long as they are representable by a slice.\n            if is_bool_dtype(slc):\n                slc = lib.maybe_booleans_to_slice(slc.view('u1'))\n            else:\n                slc = lib.maybe_indices_to_slice(slc.astype('i8'), len(self))\n            if isinstance(slc, np.ndarray):\n                raise KeyError(\"Cannot get %s slice bound for non-unique \"\n                               \"label: %r\" % (side, original_label))\n\n        if isinstance(slc, slice):\n            if side == 'left':\n                return slc.start\n            else:\n                return slc.stop\n        else:\n            if side == 'right':\n                return slc + 1\n            else:\n                return slc\n\n    def slice_locs(self, start=None, end=None, step=None, kind=None):\n        \"\"\"\n        Compute slice locations for input labels.\n\n        Parameters\n        ----------\n        start : label, default None\n            If None, defaults to the beginning\n        end : label, default None\n            If None, defaults to the end\n        step : int, defaults None\n            If None, defaults to 1\n        kind : {'ix', 'loc', 'getitem'} or None\n\n        Returns\n        -------\n        start, end : int\n\n        Notes\n        -----\n        This method only works if the index is monotonic or unique.\n\n        Examples\n        ---------\n        >>> idx = pd.Index(list('abcd'))\n        >>> idx.slice_locs(start='b', end='c')\n        (1, 3)\n\n        See Also\n        --------\n        Index.get_loc : Get location for a single label.\n        \"\"\"\n        inc = (step is None or step >= 0)\n\n        if not inc:\n            # If it's a reverse slice, temporarily swap bounds.\n            start, end = end, start\n\n        start_slice = None\n        if start is not None:\n            start_slice = self.get_slice_bound(start, 'left', kind)\n        if start_slice is None:\n            start_slice = 0\n\n        end_slice = None\n        if end is not None:\n            end_slice = self.get_slice_bound(end, 'right', kind)\n        if end_slice is None:\n            end_slice = len(self)\n\n        if not inc:\n            # Bounds at this moment are swapped, swap them back and shift by 1.\n            #\n            # slice_locs('B', 'A', step=-1): s='B', e='A'\n            #\n            #              s='A'                 e='B'\n            # AFTER SWAP:    |                     |\n            #                v ------------------> V\n            #           -----------------------------------\n            #           | | |A|A|A|A| | | | | |B|B| | | | |\n            #           -----------------------------------\n            #              ^ <------------------ ^\n            # SHOULD BE:   |                     |\n            #           end=s-1              start=e-1\n            #\n            end_slice, start_slice = start_slice - 1, end_slice - 1\n\n            # i == -1 triggers ``len(self) + i`` selection that points to the\n            # last element, not before-the-first one, subtracting len(self)\n            # compensates that.\n            if end_slice == -1:\n                end_slice -= len(self)\n            if start_slice == -1:\n                start_slice -= len(self)\n\n        return start_slice, end_slice\n\n    def delete(self, loc):\n        \"\"\"\n        Make new Index with passed location(-s) deleted.\n\n        Returns\n        -------\n        new_index : Index\n        \"\"\"\n        return self._shallow_copy(np.delete(self._data, loc))\n\n    def insert(self, loc, item):\n        \"\"\"\n        Make new Index inserting new item at location.\n\n        Follows Python list.append semantics for negative values.\n\n        Parameters\n        ----------\n        loc : int\n        item : object\n\n        Returns\n        -------\n        new_index : Index\n        \"\"\"\n        _self = np.asarray(self)\n        item = self._coerce_scalar_to_index(item)._ndarray_values\n        idx = np.concatenate((_self[:loc], item, _self[loc:]))\n        return self._shallow_copy_with_infer(idx)\n\n    def drop(self, labels, errors='raise'):\n        \"\"\"\n        Make new Index with passed list of labels deleted.\n\n        Parameters\n        ----------\n        labels : array-like\n        errors : {'ignore', 'raise'}, default 'raise'\n            If 'ignore', suppress error and existing labels are dropped.\n\n        Returns\n        -------\n        dropped : Index\n\n        Raises\n        ------\n        KeyError\n            If not all of the labels are found in the selected axis\n        \"\"\"\n        arr_dtype = 'object' if self.dtype == 'object' else None\n        labels = com.index_labels_to_array(labels, dtype=arr_dtype)\n        indexer = self.get_indexer(labels)\n        mask = indexer == -1\n        if mask.any():\n            if errors != 'ignore':\n                raise KeyError(\n                    '{} not found in axis'.format(labels[mask]))\n            indexer = indexer[~mask]\n        return self.delete(indexer)\n\n    # --------------------------------------------------------------------\n    # Generated Arithmetic, Comparison, and Unary Methods\n\n    def _evaluate_with_timedelta_like(self, other, op):\n        # Timedelta knows how to operate with np.array, so dispatch to that\n        # operation and then wrap the results\n        if self._is_numeric_dtype and op.__name__ in ['add', 'sub',\n                                                      'radd', 'rsub']:\n            raise TypeError(\"Operation {opname} between {cls} and {other} \"\n                            \"is invalid\".format(opname=op.__name__,\n                                                cls=self.dtype,\n                                                other=type(other).__name__))\n\n        other = Timedelta(other)\n        values = self.values\n\n        with np.errstate(all='ignore'):\n            result = op(values, other)\n\n        attrs = self._get_attributes_dict()\n        attrs = self._maybe_update_attributes(attrs)\n        if op == divmod:\n            return Index(result[0], **attrs), Index(result[1], **attrs)\n        return Index(result, **attrs)\n\n    def _evaluate_with_datetime_like(self, other, op):\n        raise TypeError(\"can only perform ops with datetime like values\")\n\n    @classmethod\n    def _add_comparison_methods(cls):\n        \"\"\"\n        Add in comparison methods.\n        \"\"\"\n        cls.__eq__ = _make_comparison_op(operator.eq, cls)\n        cls.__ne__ = _make_comparison_op(operator.ne, cls)\n        cls.__lt__ = _make_comparison_op(operator.lt, cls)\n        cls.__gt__ = _make_comparison_op(operator.gt, cls)\n        cls.__le__ = _make_comparison_op(operator.le, cls)\n        cls.__ge__ = _make_comparison_op(operator.ge, cls)\n\n    @classmethod\n    def _add_numeric_methods_add_sub_disabled(cls):\n        \"\"\"\n        Add in the numeric add/sub methods to disable.\n        \"\"\"\n        cls.__add__ = make_invalid_op('__add__')\n        cls.__radd__ = make_invalid_op('__radd__')\n        cls.__iadd__ = make_invalid_op('__iadd__')\n        cls.__sub__ = make_invalid_op('__sub__')\n        cls.__rsub__ = make_invalid_op('__rsub__')\n        cls.__isub__ = make_invalid_op('__isub__')\n\n    @classmethod\n    def _add_numeric_methods_disabled(cls):\n        \"\"\"\n        Add in numeric methods to disable other than add/sub.\n        \"\"\"\n        cls.__pow__ = make_invalid_op('__pow__')\n        cls.__rpow__ = make_invalid_op('__rpow__')\n        cls.__mul__ = make_invalid_op('__mul__')\n        cls.__rmul__ = make_invalid_op('__rmul__')\n        cls.__floordiv__ = make_invalid_op('__floordiv__')\n        cls.__rfloordiv__ = make_invalid_op('__rfloordiv__')\n        cls.__truediv__ = make_invalid_op('__truediv__')\n        cls.__rtruediv__ = make_invalid_op('__rtruediv__')\n        if not compat.PY3:\n            cls.__div__ = make_invalid_op('__div__')\n            cls.__rdiv__ = make_invalid_op('__rdiv__')\n        cls.__mod__ = make_invalid_op('__mod__')\n        cls.__divmod__ = make_invalid_op('__divmod__')\n        cls.__neg__ = make_invalid_op('__neg__')\n        cls.__pos__ = make_invalid_op('__pos__')\n        cls.__abs__ = make_invalid_op('__abs__')\n        cls.__inv__ = make_invalid_op('__inv__')\n\n    def _maybe_update_attributes(self, attrs):\n        \"\"\"\n        Update Index attributes (e.g. freq) depending on op.\n        \"\"\"\n        return attrs\n\n    def _validate_for_numeric_unaryop(self, op, opstr):\n        \"\"\"\n        Validate if we can perform a numeric unary operation.\n        \"\"\"\n        if not self._is_numeric_dtype:\n            raise TypeError(\"cannot evaluate a numeric op \"\n                            \"{opstr} for type: {typ}\"\n                            .format(opstr=opstr, typ=type(self).__name__))\n\n    def _validate_for_numeric_binop(self, other, op):\n        \"\"\"\n        Return valid other; evaluate or raise TypeError if we are not of\n        the appropriate type.\n\n        Notes\n        -----\n        This is an internal method called by ops.\n        \"\"\"\n        opstr = '__{opname}__'.format(opname=op.__name__)\n        # if we are an inheritor of numeric,\n        # but not actually numeric (e.g. DatetimeIndex/PeriodIndex)\n        if not self._is_numeric_dtype:\n            raise TypeError(\"cannot evaluate a numeric op {opstr} \"\n                            \"for type: {typ}\"\n                            .format(opstr=opstr, typ=type(self).__name__))\n\n        if isinstance(other, Index):\n            if not other._is_numeric_dtype:\n                raise TypeError(\"cannot evaluate a numeric op \"\n                                \"{opstr} with type: {typ}\"\n                                .format(opstr=opstr, typ=type(other)))\n        elif isinstance(other, np.ndarray) and not other.ndim:\n            other = other.item()\n\n        if isinstance(other, (Index, ABCSeries, np.ndarray)):\n            if len(self) != len(other):\n                raise ValueError(\"cannot evaluate a numeric op with \"\n                                 \"unequal lengths\")\n            other = com.values_from_object(other)\n            if other.dtype.kind not in ['f', 'i', 'u']:\n                raise TypeError(\"cannot evaluate a numeric op \"\n                                \"with a non-numeric dtype\")\n        elif isinstance(other, (ABCDateOffset, np.timedelta64, timedelta)):\n            # higher up to handle\n            pass\n        elif isinstance(other, (datetime, np.datetime64)):\n            # higher up to handle\n            pass\n        else:\n            if not (is_float(other) or is_integer(other)):\n                raise TypeError(\"can only perform ops with scalar values\")\n\n        return other\n\n    @classmethod\n    def _add_numeric_methods_binary(cls):\n        \"\"\"\n        Add in numeric methods.\n        \"\"\"\n        cls.__add__ = _make_arithmetic_op(operator.add, cls)\n        cls.__radd__ = _make_arithmetic_op(ops.radd, cls)\n        cls.__sub__ = _make_arithmetic_op(operator.sub, cls)\n        cls.__rsub__ = _make_arithmetic_op(ops.rsub, cls)\n        cls.__mul__ = _make_arithmetic_op(operator.mul, cls)\n        cls.__rmul__ = _make_arithmetic_op(ops.rmul, cls)\n        cls.__rpow__ = _make_arithmetic_op(ops.rpow, cls)\n        cls.__pow__ = _make_arithmetic_op(operator.pow, cls)\n        cls.__mod__ = _make_arithmetic_op(operator.mod, cls)\n        cls.__floordiv__ = _make_arithmetic_op(operator.floordiv, cls)\n        cls.__rfloordiv__ = _make_arithmetic_op(ops.rfloordiv, cls)\n        cls.__truediv__ = _make_arithmetic_op(operator.truediv, cls)\n        cls.__rtruediv__ = _make_arithmetic_op(ops.rtruediv, cls)\n        if not compat.PY3:\n            cls.__div__ = _make_arithmetic_op(operator.div, cls)\n            cls.__rdiv__ = _make_arithmetic_op(ops.rdiv, cls)\n\n        cls.__divmod__ = _make_arithmetic_op(divmod, cls)\n\n    @classmethod\n    def _add_numeric_methods_unary(cls):\n        \"\"\"\n        Add in numeric unary methods.\n        \"\"\"\n        def _make_evaluate_unary(op, opstr):\n\n            def _evaluate_numeric_unary(self):\n\n                self._validate_for_numeric_unaryop(op, opstr)\n                attrs = self._get_attributes_dict()\n                attrs = self._maybe_update_attributes(attrs)\n                return Index(op(self.values), **attrs)\n\n            return _evaluate_numeric_unary\n\n        cls.__neg__ = _make_evaluate_unary(operator.neg, '__neg__')\n        cls.__pos__ = _make_evaluate_unary(operator.pos, '__pos__')\n        cls.__abs__ = _make_evaluate_unary(np.abs, '__abs__')\n        cls.__inv__ = _make_evaluate_unary(lambda x: -x, '__inv__')\n\n    @classmethod\n    def _add_numeric_methods(cls):\n        cls._add_numeric_methods_unary()\n        cls._add_numeric_methods_binary()\n\n    @classmethod\n    def _add_logical_methods(cls):\n        \"\"\"\n        Add in logical methods.\n        \"\"\"\n        _doc = \"\"\"\n        %(desc)s\n\n        Parameters\n        ----------\n        *args\n            These parameters will be passed to numpy.%(outname)s.\n        **kwargs\n            These parameters will be passed to numpy.%(outname)s.\n\n        Returns\n        -------\n        %(outname)s : bool or array_like (if axis is specified)\n            A single element array_like may be converted to bool.\"\"\"\n\n        _index_shared_docs['index_all'] = dedent(\"\"\"\n\n        See Also\n        --------\n        pandas.Index.any : Return whether any element in an Index is True.\n        pandas.Series.any : Return whether any element in a Series is True.\n        pandas.Series.all : Return whether all elements in a Series are True.\n\n        Notes\n        -----\n        Not a Number (NaN), positive infinity and negative infinity\n        evaluate to True because these are not equal to zero.\n\n        Examples\n        --------\n        **all**\n\n        True, because nonzero integers are considered True.\n\n        >>> pd.Index([1, 2, 3]).all()\n        True\n\n        False, because ``0`` is considered False.\n\n        >>> pd.Index([0, 1, 2]).all()\n        False\n\n        **any**\n\n        True, because ``1`` is considered True.\n\n        >>> pd.Index([0, 0, 1]).any()\n        True\n\n        False, because ``0`` is considered False.\n\n        >>> pd.Index([0, 0, 0]).any()\n        False\n        \"\"\")\n\n        _index_shared_docs['index_any'] = dedent(\"\"\"\n\n        See Also\n        --------\n        pandas.Index.all : Return whether all elements are True.\n        pandas.Series.all : Return whether all elements are True.\n\n        Notes\n        -----\n        Not a Number (NaN), positive infinity and negative infinity\n        evaluate to True because these are not equal to zero.\n\n        Examples\n        --------\n        >>> index = pd.Index([0, 1, 2])\n        >>> index.any()\n        True\n\n        >>> index = pd.Index([0, 0, 0])\n        >>> index.any()\n        False\n        \"\"\")\n\n        def _make_logical_function(name, desc, f):\n            @Substitution(outname=name, desc=desc)\n            @Appender(_index_shared_docs['index_' + name])\n            @Appender(_doc)\n            def logical_func(self, *args, **kwargs):\n                result = f(self.values)\n                if (isinstance(result, (np.ndarray, ABCSeries, Index)) and\n                        result.ndim == 0):\n                    # return NumPy type\n                    return result.dtype.type(result.item())\n                else:  # pragma: no cover\n                    return result\n\n            logical_func.__name__ = name\n            return logical_func\n\n        cls.all = _make_logical_function('all', 'Return whether all elements '\n                                                'are True.',\n                                         np.all)\n        cls.any = _make_logical_function('any',\n                                         'Return whether any element is True.',\n                                         np.any)\n\n    @classmethod\n    def _add_logical_methods_disabled(cls):\n        \"\"\"\n        Add in logical methods to disable.\n        \"\"\"\n        cls.all = make_invalid_op('all')\n        cls.any = make_invalid_op('any')\n\n\nIndex._add_numeric_methods_disabled()\nIndex._add_logical_methods()\nIndex._add_comparison_methods()\n\n\ndef ensure_index_from_sequences(sequences, names=None):\n    \"\"\"\n    Construct an index from sequences of data.\n\n    A single sequence returns an Index. Many sequences returns a\n    MultiIndex.\n\n    Parameters\n    ----------\n    sequences : sequence of sequences\n    names : sequence of str\n\n    Returns\n    -------\n    index : Index or MultiIndex\n\n    Examples\n    --------\n    >>> ensure_index_from_sequences([[1, 2, 3]], names=['name'])\n    Int64Index([1, 2, 3], dtype='int64', name='name')\n\n    >>> ensure_index_from_sequences([['a', 'a'], ['a', 'b']],\n                                    names=['L1', 'L2'])\n    MultiIndex(levels=[['a'], ['a', 'b']],\n               labels=[[0, 0], [0, 1]],\n               names=['L1', 'L2'])\n\n    See Also\n    --------\n    ensure_index\n    \"\"\"\n    from .multi import MultiIndex\n\n    if len(sequences) == 1:\n        if names is not None:\n            names = names[0]\n        return Index(sequences[0], name=names)\n    else:\n        return MultiIndex.from_arrays(sequences, names=names)\n\n\ndef ensure_index(index_like, copy=False):\n    \"\"\"\n    Ensure that we have an index from some index-like object.\n\n    Parameters\n    ----------\n    index : sequence\n        An Index or other sequence\n    copy : bool\n\n    Returns\n    -------\n    index : Index or MultiIndex\n\n    Examples\n    --------\n    >>> ensure_index(['a', 'b'])\n    Index(['a', 'b'], dtype='object')\n\n    >>> ensure_index([('a', 'a'),  ('b', 'c')])\n    Index([('a', 'a'), ('b', 'c')], dtype='object')\n\n    >>> ensure_index([['a', 'a'], ['b', 'c']])\n    MultiIndex(levels=[['a'], ['b', 'c']],\n               labels=[[0, 0], [0, 1]])\n\n    See Also\n    --------\n    ensure_index_from_sequences\n    \"\"\"\n    if isinstance(index_like, Index):\n        if copy:\n            index_like = index_like.copy()\n        return index_like\n    if hasattr(index_like, 'name'):\n        return Index(index_like, name=index_like.name, copy=copy)\n\n    if is_iterator(index_like):\n        index_like = list(index_like)\n\n    # must check for exactly list here because of strict type\n    # check in clean_index_list\n    if isinstance(index_like, list):\n        if type(index_like) != list:\n            index_like = list(index_like)\n\n        converted, all_arrays = lib.clean_index_list(index_like)\n\n        if len(converted) > 0 and all_arrays:\n            from .multi import MultiIndex\n            return MultiIndex.from_arrays(converted)\n        else:\n            index_like = converted\n    else:\n        # clean_index_list does the equivalent of copying\n        # so only need to do this if not list instance\n        if copy:\n            from copy import copy\n            index_like = copy(index_like)\n\n    return Index(index_like)\n\n\ndef _ensure_has_len(seq):\n    \"\"\"\n    If seq is an iterator, put its values into a list.\n    \"\"\"\n    try:\n        len(seq)\n    except TypeError:\n        return list(seq)\n    else:\n        return seq\n\n\ndef _trim_front(strings):\n    \"\"\"\n    Trims zeros and decimal points.\n    \"\"\"\n    trimmed = strings\n    while len(strings) > 0 and all(x[0] == ' ' for x in trimmed):\n        trimmed = [x[1:] for x in trimmed]\n    return trimmed\n\n\ndef _validate_join_method(method):\n    if method not in ['left', 'right', 'inner', 'outer']:\n        raise ValueError('do not recognize join method %s' % method)\n\n\ndef default_index(n):\n    from pandas.core.index import RangeIndex\n    return RangeIndex(0, n, name=None)\n",
          "file_patch": "@@ -244,6 +244,9 @@ class Index(IndexOpsMixin, PandasObject):\n \n     str = CachedAccessor(\"str\", StringMethods)\n \n+    # --------------------------------------------------------------------\n+    # Constructors\n+\n     def __new__(cls, data=None, dtype=None, copy=False, name=None,\n                 fastpath=None, tupleize_cols=True, **kwargs):\n \n@@ -518,6 +521,19 @@ class Index(IndexOpsMixin, PandasObject):\n             setattr(result, k, v)\n         return result._reset_identity()\n \n+    @cache_readonly\n+    def _constructor(self):\n+        return type(self)\n+\n+    # --------------------------------------------------------------------\n+    # Index Internals Methods\n+\n+    def _get_attributes_dict(self):\n+        \"\"\"\n+        Return an attributes dict for my class.\n+        \"\"\"\n+        return {k: getattr(self, k, None) for k in self._attributes}\n+\n     _index_shared_docs['_shallow_copy'] = \"\"\"\n         Create a new Index with the same class as the caller, don't copy the\n         data, use the same object attributes with passed in attributes taking\n@@ -608,42 +624,6 @@ class Index(IndexOpsMixin, PandasObject):\n         # guard when called from IndexOpsMixin\n         raise TypeError(\"Index can't be updated inplace\")\n \n-    def _sort_levels_monotonic(self):\n-        \"\"\"\n-        Compat with MultiIndex.\n-        \"\"\"\n-        return self\n-\n-    _index_shared_docs['_get_grouper_for_level'] = \"\"\"\n-        Get index grouper corresponding to an index level\n-\n-        Parameters\n-        ----------\n-        mapper: Group mapping function or None\n-            Function mapping index values to groups\n-        level : int or None\n-            Index level\n-\n-        Returns\n-        -------\n-        grouper : Index\n-            Index of values to group on\n-        labels : ndarray of int or None\n-            Array of locations in level_index\n-        uniques : Index or None\n-            Index of unique values for level\n-        \"\"\"\n-\n-    @Appender(_index_shared_docs['_get_grouper_for_level'])\n-    def _get_grouper_for_level(self, mapper, level=None):\n-        assert level is None or level == 0\n-        if mapper is None:\n-            grouper = self\n-        else:\n-            grouper = self.map(mapper)\n-\n-        return grouper, None, None\n-\n     def is_(self, other):\n         \"\"\"\n         More flexible, faster check like ``is`` but that works through views.\n@@ -671,6 +651,17 @@ class Index(IndexOpsMixin, PandasObject):\n         self._id = _Identity()\n         return self\n \n+    def _cleanup(self):\n+        self._engine.clear_mapping()\n+\n+    @cache_readonly\n+    def _engine(self):\n+        # property, for now, slow to look up\n+        return self._engine_type(lambda: self._ndarray_values, len(self))\n+\n+    # --------------------------------------------------------------------\n+    # Array-Like Methods\n+\n     # ndarray compat\n     def __len__(self):\n         \"\"\"\n@@ -709,97 +700,129 @@ class Index(IndexOpsMixin, PandasObject):\n         \"\"\"\n         return str(self.dtype)\n \n-    @property\n-    def values(self):\n-        \"\"\"\n-        Return the underlying data as an ndarray.\n+    def ravel(self, order='C'):\n         \"\"\"\n-        return self._data.view(np.ndarray)\n+        Return an ndarray of the flattened values of the underlying data.\n \n-    @property\n-    def _values(self):\n-        # type: () -> Union[ExtensionArray, Index, np.ndarray]\n-        # TODO(EA): remove index types as they become extension arrays\n+        See Also\n+        --------\n+        numpy.ndarray.ravel\n         \"\"\"\n-        The best array representation.\n-\n-        This is an ndarray, ExtensionArray, or Index subclass. This differs\n-        from ``_ndarray_values``, which always returns an ndarray.\n+        return self._ndarray_values.ravel(order=order)\n \n-        Both ``_values`` and ``_ndarray_values`` are consistent between\n-        ``Series`` and ``Index``.\n+    def view(self, cls=None):\n \n-        It may differ from the public '.values' method.\n+        # we need to see if we are subclassing an\n+        # index type here\n+        if cls is not None and not hasattr(cls, '_typ'):\n+            result = self._data.view(cls)\n+        else:\n+            result = self._shallow_copy()\n+        if isinstance(result, Index):\n+            result._id = self._id\n+        return result\n \n-        index             | values          | _values       | _ndarray_values |\n-        ----------------- | --------------- | ------------- | --------------- |\n-        Index             | ndarray         | ndarray       | ndarray         |\n-        CategoricalIndex  | Categorical     | Categorical   | ndarray[int]    |\n-        DatetimeIndex     | ndarray[M8ns]   | ndarray[M8ns] | ndarray[M8ns]   |\n-        DatetimeIndex[tz] | ndarray[M8ns]   | DTI[tz]       | ndarray[M8ns]   |\n-        PeriodIndex       | ndarray[object] | PeriodArray   | ndarray[int]    |\n-        IntervalIndex     | IntervalArray   | IntervalArray | ndarray[object] |\n+    _index_shared_docs['astype'] = \"\"\"\n+        Create an Index with values cast to dtypes. The class of a new Index\n+        is determined by dtype. When conversion is impossible, a ValueError\n+        exception is raised.\n \n-        See Also\n-        --------\n-        values\n-        _ndarray_values\n-        \"\"\"\n-        return self.values\n+        Parameters\n+        ----------\n+        dtype : numpy dtype or pandas type\n+        copy : bool, default True\n+            By default, astype always returns a newly allocated object.\n+            If copy is set to False and internal requirements on dtype are\n+            satisfied, the original data is used to create a new Index\n+            or the original Index is returned.\n \n-    def get_values(self):\n+            .. versionadded:: 0.19.0\n         \"\"\"\n-        Return `Index` data as an `numpy.ndarray`.\n \n-        Returns\n-        -------\n-        numpy.ndarray\n-            A one-dimensional numpy array of the `Index` values.\n+    @Appender(_index_shared_docs['astype'])\n+    def astype(self, dtype, copy=True):\n+        if is_dtype_equal(self.dtype, dtype):\n+            return self.copy() if copy else self\n \n-        See Also\n-        --------\n-        Index.values : The attribute that get_values wraps.\n+        elif is_categorical_dtype(dtype):\n+            from .category import CategoricalIndex\n+            return CategoricalIndex(self.values, name=self.name, dtype=dtype,\n+                                    copy=copy)\n \n-        Examples\n-        --------\n-        Getting the `Index` values of a `DataFrame`:\n+        elif is_extension_array_dtype(dtype):\n+            return Index(np.asarray(self), dtype=dtype, copy=copy)\n \n-        >>> df = pd.DataFrame([[1, 2, 3], [4, 5, 6], [7, 8, 9]],\n-        ...                    index=['a', 'b', 'c'], columns=['A', 'B', 'C'])\n-        >>> df\n-           A  B  C\n-        a  1  2  3\n-        b  4  5  6\n-        c  7  8  9\n-        >>> df.index.get_values()\n-        array(['a', 'b', 'c'], dtype=object)\n+        try:\n+            if is_datetime64tz_dtype(dtype):\n+                from pandas import DatetimeIndex\n+                return DatetimeIndex(self.values, name=self.name, dtype=dtype,\n+                                     copy=copy)\n+            return Index(self.values.astype(dtype, copy=copy), name=self.name,\n+                         dtype=dtype)\n+        except (TypeError, ValueError):\n+            msg = 'Cannot cast {name} to dtype {dtype}'\n+            raise TypeError(msg.format(name=type(self).__name__, dtype=dtype))\n \n-        Standalone `Index` values:\n+    _index_shared_docs['take'] = \"\"\"\n+        Return a new %(klass)s of the values selected by the indices.\n \n-        >>> idx = pd.Index(['1', '2', '3'])\n-        >>> idx.get_values()\n-        array(['1', '2', '3'], dtype=object)\n+        For internal compatibility with numpy arrays.\n \n-        `MultiIndex` arrays also have only one dimension:\n+        Parameters\n+        ----------\n+        indices : list\n+            Indices to be taken\n+        axis : int, optional\n+            The axis over which to select values, always 0.\n+        allow_fill : bool, default True\n+        fill_value : bool, default None\n+            If allow_fill=True and fill_value is not None, indices specified by\n+            -1 is regarded as NA. If Index doesn't hold NA, raise ValueError\n \n-        >>> midx = pd.MultiIndex.from_arrays([[1, 2, 3], ['a', 'b', 'c']],\n-        ...                                  names=('number', 'letter'))\n-        >>> midx.get_values()\n-        array([(1, 'a'), (2, 'b'), (3, 'c')], dtype=object)\n-        >>> midx.get_values().ndim\n-        1\n+        See Also\n+        --------\n+        numpy.ndarray.take\n         \"\"\"\n-        return self.values\n \n-    @Appender(IndexOpsMixin.memory_usage.__doc__)\n-    def memory_usage(self, deep=False):\n-        result = super(Index, self).memory_usage(deep=deep)\n+    @Appender(_index_shared_docs['take'] % _index_doc_kwargs)\n+    def take(self, indices, axis=0, allow_fill=True,\n+             fill_value=None, **kwargs):\n+        if kwargs:\n+            nv.validate_take(tuple(), kwargs)\n+        indices = ensure_platform_int(indices)\n+        if self._can_hold_na:\n+            taken = self._assert_take_fillable(self.values, indices,\n+                                               allow_fill=allow_fill,\n+                                               fill_value=fill_value,\n+                                               na_value=self._na_value)\n+        else:\n+            if allow_fill and fill_value is not None:\n+                msg = 'Unable to fill values because {0} cannot contain NA'\n+                raise ValueError(msg.format(self.__class__.__name__))\n+            taken = self.values.take(indices)\n+        return self._shallow_copy(taken)\n \n-        # include our engine hashtable\n-        result += self._engine.sizeof(deep=deep)\n-        return result\n+    def _assert_take_fillable(self, values, indices, allow_fill=True,\n+                              fill_value=None, na_value=np.nan):\n+        \"\"\"\n+        Internal method to handle NA filling of take.\n+        \"\"\"\n+        indices = ensure_platform_int(indices)\n+\n+        # only fill if we are passing a non-None fill_value\n+        if allow_fill and fill_value is not None:\n+            if (indices < -1).any():\n+                msg = ('When allow_fill=True and fill_value is not None, '\n+                       'all indices must be >= -1')\n+                raise ValueError(msg)\n+            taken = algos.take(values,\n+                               indices,\n+                               allow_fill=allow_fill,\n+                               fill_value=na_value)\n+        else:\n+            taken = values.take(indices)\n+        return taken\n \n-    # ops compat\n     def repeat(self, repeats, *args, **kwargs):\n         \"\"\"\n         Repeat elements of an Index.\n@@ -838,185 +861,28 @@ class Index(IndexOpsMixin, PandasObject):\n         nv.validate_repeat(args, kwargs)\n         return self._shallow_copy(self._values.repeat(repeats))\n \n-    _index_shared_docs['where'] = \"\"\"\n-        Return an Index of same shape as self and whose corresponding\n-        entries are from self where cond is True and otherwise are from\n-        other.\n+    # --------------------------------------------------------------------\n+    # Copying Methods\n \n-        .. versionadded:: 0.19.0\n+    _index_shared_docs['copy'] = \"\"\"\n+        Make a copy of this object.  Name and dtype sets those attributes on\n+        the new object.\n \n         Parameters\n         ----------\n-        cond : boolean array-like with the same length as self\n-        other : scalar, or array-like\n-        \"\"\"\n+        name : string, optional\n+        deep : boolean, default False\n+        dtype : numpy dtype or pandas type\n \n-    @Appender(_index_shared_docs['where'])\n-    def where(self, cond, other=None):\n-        if other is None:\n-            other = self._na_value\n+        Returns\n+        -------\n+        copy : Index\n \n-        dtype = self.dtype\n-        values = self.values\n-\n-        if is_bool(other) or is_bool_dtype(other):\n-\n-            # bools force casting\n-            values = values.astype(object)\n-            dtype = None\n-\n-        values = np.where(cond, values, other)\n-\n-        if self._is_numeric_dtype and np.any(isna(values)):\n-            # We can't coerce to the numeric dtype of \"self\" (unless\n-            # it's float) if there are NaN values in our output.\n-            dtype = None\n-\n-        return self._shallow_copy_with_infer(values, dtype=dtype)\n-\n-    def ravel(self, order='C'):\n-        \"\"\"\n-        Return an ndarray of the flattened values of the underlying data.\n-\n-        See Also\n-        --------\n-        numpy.ndarray.ravel\n-        \"\"\"\n-        return self._ndarray_values.ravel(order=order)\n-\n-    # construction helpers\n-    @classmethod\n-    def _try_convert_to_int_index(cls, data, copy, name, dtype):\n-        \"\"\"\n-        Attempt to convert an array of data into an integer index.\n-\n-        Parameters\n-        ----------\n-        data : The data to convert.\n-        copy : Whether to copy the data or not.\n-        name : The name of the index returned.\n-\n-        Returns\n-        -------\n-        int_index : data converted to either an Int64Index or a\n-                    UInt64Index\n-\n-        Raises\n-        ------\n-        ValueError if the conversion was not successful.\n-        \"\"\"\n-\n-        from .numeric import Int64Index, UInt64Index\n-        if not is_unsigned_integer_dtype(dtype):\n-            # skip int64 conversion attempt if uint-like dtype is passed, as\n-            # this could return Int64Index when UInt64Index is what's desrired\n-            try:\n-                res = data.astype('i8', copy=False)\n-                if (res == data).all():\n-                    return Int64Index(res, copy=copy, name=name)\n-            except (OverflowError, TypeError, ValueError):\n-                pass\n-\n-        # Conversion to int64 failed (possibly due to overflow) or was skipped,\n-        # so let's try now with uint64.\n-        try:\n-            res = data.astype('u8', copy=False)\n-            if (res == data).all():\n-                return UInt64Index(res, copy=copy, name=name)\n-        except (OverflowError, TypeError, ValueError):\n-            pass\n-\n-        raise ValueError\n-\n-    @classmethod\n-    def _scalar_data_error(cls, data):\n-        raise TypeError('{0}(...) must be called with a collection of some '\n-                        'kind, {1} was passed'.format(cls.__name__,\n-                                                      repr(data)))\n-\n-    @classmethod\n-    def _string_data_error(cls, data):\n-        raise TypeError('String dtype not supported, you may need '\n-                        'to explicitly cast to a numeric type')\n-\n-    @classmethod\n-    def _coerce_to_ndarray(cls, data):\n-        \"\"\"\n-        Coerces data to ndarray.\n-\n-        Converts other iterables to list first and then to array.\n-        Does not touch ndarrays.\n-\n-        Raises\n-        ------\n-        TypeError\n-            When the data passed in is a scalar.\n-        \"\"\"\n-\n-        if not isinstance(data, (np.ndarray, Index)):\n-            if data is None or is_scalar(data):\n-                cls._scalar_data_error(data)\n-\n-            # other iterable of some kind\n-            if not isinstance(data, (ABCSeries, list, tuple)):\n-                data = list(data)\n-            data = np.asarray(data)\n-        return data\n-\n-    def _get_attributes_dict(self):\n-        \"\"\"\n-        Return an attributes dict for my class.\n-        \"\"\"\n-        return {k: getattr(self, k, None) for k in self._attributes}\n-\n-    def view(self, cls=None):\n-\n-        # we need to see if we are subclassing an\n-        # index type here\n-        if cls is not None and not hasattr(cls, '_typ'):\n-            result = self._data.view(cls)\n-        else:\n-            result = self._shallow_copy()\n-        if isinstance(result, Index):\n-            result._id = self._id\n-        return result\n-\n-    def _coerce_scalar_to_index(self, item):\n-        \"\"\"\n-        We need to coerce a scalar to a compat for our index type.\n-\n-        Parameters\n-        ----------\n-        item : scalar item to coerce\n-        \"\"\"\n-        dtype = self.dtype\n-\n-        if self._is_numeric_dtype and isna(item):\n-            # We can't coerce to the numeric dtype of \"self\" (unless\n-            # it's float) if there are NaN values in our output.\n-            dtype = None\n-\n-        return Index([item], dtype=dtype, **self._get_attributes_dict())\n-\n-    _index_shared_docs['copy'] = \"\"\"\n-        Make a copy of this object.  Name and dtype sets those attributes on\n-        the new object.\n-\n-        Parameters\n-        ----------\n-        name : string, optional\n-        deep : boolean, default False\n-        dtype : numpy dtype or pandas type\n-\n-        Returns\n-        -------\n-        copy : Index\n-\n-        Notes\n-        -----\n-        In most cases, there should be no functional difference from using\n-        ``deep``, but if ``deep`` is passed it will attempt to deepcopy.\n-        \"\"\"\n+        Notes\n+        -----\n+        In most cases, there should be no functional difference from using\n+        ``deep``, but if ``deep`` is passed it will attempt to deepcopy.\n+        \"\"\"\n \n     @Appender(_index_shared_docs['copy'])\n     def copy(self, name=None, deep=False, dtype=None, **kwargs):\n@@ -1047,24 +913,8 @@ class Index(IndexOpsMixin, PandasObject):\n             memo = {}\n         return self.copy(deep=True)\n \n-    def _validate_names(self, name=None, names=None, deep=False):\n-        \"\"\"\n-        Handles the quirks of having a singular 'name' parameter for general\n-        Index and plural 'names' parameter for MultiIndex.\n-        \"\"\"\n-        from copy import deepcopy\n-        if names is not None and name is not None:\n-            raise TypeError(\"Can only provide one of `names` and `name`\")\n-        elif names is None and name is None:\n-            return deepcopy(self.names) if deep else self.names\n-        elif names is not None:\n-            if not is_list_like(names):\n-                raise TypeError(\"Must pass list-like as `names`.\")\n-            return names\n-        else:\n-            if not is_list_like(name):\n-                return [name]\n-            return name\n+    # --------------------------------------------------------------------\n+    # Rendering Methods\n \n     def __unicode__(self):\n         \"\"\"\n@@ -1125,64 +975,192 @@ class Index(IndexOpsMixin, PandasObject):\n         \"\"\"\n         return format_object_attrs(self)\n \n-    def to_flat_index(self):\n-        \"\"\"\n-        Identity method.\n+    def _mpl_repr(self):\n+        # how to represent ourselves to matplotlib\n+        return self.values\n \n-        .. versionadded:: 0.24.0\n+    def format(self, name=False, formatter=None, **kwargs):\n+        \"\"\"\n+        Render a string representation of the Index.\n+        \"\"\"\n+        header = []\n+        if name:\n+            header.append(pprint_thing(self.name,\n+                                       escape_chars=('\\t', '\\r', '\\n')) if\n+                          self.name is not None else '')\n \n-        This is implemented for compatability with subclass implementations\n-        when chaining.\n+        if formatter is not None:\n+            return header + list(self.map(formatter))\n \n-        Returns\n-        -------\n-        pd.Index\n-            Caller.\n+        return self._format_with_header(header, **kwargs)\n \n-        See Also\n-        --------\n-        MultiIndex.to_flat_index : Subclass implementation.\n-        \"\"\"\n-        return self\n+    def _format_with_header(self, header, na_rep='NaN', **kwargs):\n+        values = self.values\n \n-    def to_series(self, index=None, name=None):\n-        \"\"\"\n-        Create a Series with both index and values equal to the index keys\n-        useful with map for returning an indexer based on an index.\n+        from pandas.io.formats.format import format_array\n \n-        Parameters\n-        ----------\n-        index : Index, optional\n-            index of resulting Series. If None, defaults to original index\n-        name : string, optional\n-            name of resulting Series. If None, defaults to name of original\n-            index\n+        if is_categorical_dtype(values.dtype):\n+            values = np.array(values)\n \n-        Returns\n-        -------\n-        Series : dtype will be based on the type of the Index values.\n-        \"\"\"\n+        elif is_object_dtype(values.dtype):\n+            values = lib.maybe_convert_objects(values, safe=1)\n \n-        from pandas import Series\n+        if is_object_dtype(values.dtype):\n+            result = [pprint_thing(x, escape_chars=('\\t', '\\r', '\\n'))\n+                      for x in values]\n \n-        if index is None:\n-            index = self._shallow_copy()\n-        if name is None:\n-            name = self.name\n+            # could have nans\n+            mask = isna(values)\n+            if mask.any():\n+                result = np.array(result)\n+                result[mask] = na_rep\n+                result = result.tolist()\n \n-        return Series(self.values.copy(), index=index, name=name)\n+        else:\n+            result = _trim_front(format_array(values, None, justify='left'))\n+        return header + result\n \n-    def to_frame(self, index=True, name=None):\n+    def to_native_types(self, slicer=None, **kwargs):\n         \"\"\"\n-        Create a DataFrame with a column containing the Index.\n-\n-        .. versionadded:: 0.24.0\n+        Format specified values of `self` and return them.\n \n         Parameters\n         ----------\n-        index : boolean, default True\n-            Set the index of the returned DataFrame as the original Index.\n-\n+        slicer : int, array-like\n+            An indexer into `self` that specifies which values\n+            are used in the formatting process.\n+        kwargs : dict\n+            Options for specifying how the values should be formatted.\n+            These options include the following:\n+\n+            1) na_rep : str\n+                The value that serves as a placeholder for NULL values\n+            2) quoting : bool or None\n+                Whether or not there are quoted values in `self`\n+            3) date_format : str\n+                The format used to represent date-like values\n+        \"\"\"\n+\n+        values = self\n+        if slicer is not None:\n+            values = values[slicer]\n+        return values._format_native_types(**kwargs)\n+\n+    def _format_native_types(self, na_rep='', quoting=None, **kwargs):\n+        \"\"\"\n+        Actually format specific types of the index.\n+        \"\"\"\n+        mask = isna(self)\n+        if not self.is_object() and not quoting:\n+            values = np.asarray(self).astype(str)\n+        else:\n+            values = np.array(self, dtype=object, copy=True)\n+\n+        values[mask] = na_rep\n+        return values\n+\n+    def _summary(self, name=None):\n+        \"\"\"\n+        Return a summarized representation.\n+\n+        Parameters\n+        ----------\n+        name : str\n+            name to use in the summary representation\n+\n+        Returns\n+        -------\n+        String with a summarized representation of the index\n+        \"\"\"\n+        if len(self) > 0:\n+            head = self[0]\n+            if (hasattr(head, 'format') and\n+                    not isinstance(head, compat.string_types)):\n+                head = head.format()\n+            tail = self[-1]\n+            if (hasattr(tail, 'format') and\n+                    not isinstance(tail, compat.string_types)):\n+                tail = tail.format()\n+            index_summary = ', %s to %s' % (pprint_thing(head),\n+                                            pprint_thing(tail))\n+        else:\n+            index_summary = ''\n+\n+        if name is None:\n+            name = type(self).__name__\n+        return '%s: %s entries%s' % (name, len(self), index_summary)\n+\n+    def summary(self, name=None):\n+        \"\"\"\n+        Return a summarized representation.\n+\n+        .. deprecated:: 0.23.0\n+        \"\"\"\n+        warnings.warn(\"'summary' is deprecated and will be removed in a \"\n+                      \"future version.\", FutureWarning, stacklevel=2)\n+        return self._summary(name)\n+\n+    # --------------------------------------------------------------------\n+    # Conversion Methods\n+\n+    def to_flat_index(self):\n+        \"\"\"\n+        Identity method.\n+\n+        .. versionadded:: 0.24.0\n+\n+        This is implemented for compatability with subclass implementations\n+        when chaining.\n+\n+        Returns\n+        -------\n+        pd.Index\n+            Caller.\n+\n+        See Also\n+        --------\n+        MultiIndex.to_flat_index : Subclass implementation.\n+        \"\"\"\n+        return self\n+\n+    def to_series(self, index=None, name=None):\n+        \"\"\"\n+        Create a Series with both index and values equal to the index keys\n+        useful with map for returning an indexer based on an index.\n+\n+        Parameters\n+        ----------\n+        index : Index, optional\n+            index of resulting Series. If None, defaults to original index\n+        name : string, optional\n+            name of resulting Series. If None, defaults to name of original\n+            index\n+\n+        Returns\n+        -------\n+        Series : dtype will be based on the type of the Index values.\n+        \"\"\"\n+\n+        from pandas import Series\n+\n+        if index is None:\n+            index = self._shallow_copy()\n+        if name is None:\n+            name = self.name\n+\n+        return Series(self.values.copy(), index=index, name=name)\n+\n+    def to_frame(self, index=True, name=None):\n+        \"\"\"\n+        Create a DataFrame with a column containing the Index.\n+\n+        .. versionadded:: 0.24.0\n+\n+        Parameters\n+        ----------\n+        index : boolean, default True\n+            Set the index of the returned DataFrame as the original Index.\n+\n         name : object, default None\n             The passed name should substitute for the index name (if it has\n             one).\n@@ -1233,83 +1211,27 @@ class Index(IndexOpsMixin, PandasObject):\n             result.index = self\n         return result\n \n-    _index_shared_docs['astype'] = \"\"\"\n-        Create an Index with values cast to dtypes. The class of a new Index\n-        is determined by dtype. When conversion is impossible, a ValueError\n-        exception is raised.\n-\n-        Parameters\n-        ----------\n-        dtype : numpy dtype or pandas type\n-        copy : bool, default True\n-            By default, astype always returns a newly allocated object.\n-            If copy is set to False and internal requirements on dtype are\n-            satisfied, the original data is used to create a new Index\n-            or the original Index is returned.\n-\n-            .. versionadded:: 0.19.0\n-        \"\"\"\n-\n-    @Appender(_index_shared_docs['astype'])\n-    def astype(self, dtype, copy=True):\n-        if is_dtype_equal(self.dtype, dtype):\n-            return self.copy() if copy else self\n-\n-        elif is_categorical_dtype(dtype):\n-            from .category import CategoricalIndex\n-            return CategoricalIndex(self.values, name=self.name, dtype=dtype,\n-                                    copy=copy)\n-\n-        elif is_extension_array_dtype(dtype):\n-            return Index(np.asarray(self), dtype=dtype, copy=copy)\n-\n-        try:\n-            if is_datetime64tz_dtype(dtype):\n-                from pandas import DatetimeIndex\n-                return DatetimeIndex(self.values, name=self.name, dtype=dtype,\n-                                     copy=copy)\n-            return Index(self.values.astype(dtype, copy=copy), name=self.name,\n-                         dtype=dtype)\n-        except (TypeError, ValueError):\n-            msg = 'Cannot cast {name} to dtype {dtype}'\n-            raise TypeError(msg.format(name=type(self).__name__, dtype=dtype))\n+    # --------------------------------------------------------------------\n+    # Name-Centric Methods\n \n-    def _to_safe_for_reshape(self):\n+    def _validate_names(self, name=None, names=None, deep=False):\n         \"\"\"\n-        Convert to object if we are a categorical.\n+        Handles the quirks of having a singular 'name' parameter for general\n+        Index and plural 'names' parameter for MultiIndex.\n         \"\"\"\n-        return self\n-\n-    def _assert_can_do_setop(self, other):\n-        if not is_list_like(other):\n-            raise TypeError('Input must be Index or array-like')\n-        return True\n-\n-    def _convert_can_do_setop(self, other):\n-        if not isinstance(other, Index):\n-            other = Index(other, name=self.name)\n-            result_name = self.name\n+        from copy import deepcopy\n+        if names is not None and name is not None:\n+            raise TypeError(\"Can only provide one of `names` and `name`\")\n+        elif names is None and name is None:\n+            return deepcopy(self.names) if deep else self.names\n+        elif names is not None:\n+            if not is_list_like(names):\n+                raise TypeError(\"Must pass list-like as `names`.\")\n+            return names\n         else:\n-            result_name = get_op_result_name(self, other)\n-        return other, result_name\n-\n-    def _convert_for_op(self, value):\n-        \"\"\"\n-        Convert value to be insertable to ndarray.\n-        \"\"\"\n-        return value\n-\n-    def _assert_can_do_op(self, value):\n-        \"\"\"\n-        Check value is valid for scalar op.\n-        \"\"\"\n-        if not is_scalar(value):\n-            msg = \"'value' must be a scalar, passed: {0}\"\n-            raise TypeError(msg.format(type(value).__name__))\n-\n-    @property\n-    def nlevels(self):\n-        return 1\n+            if not is_list_like(name):\n+                return [name]\n+            return name\n \n     def _get_names(self):\n         return FrozenList((self.name, ))\n@@ -1468,60 +1390,193 @@ class Index(IndexOpsMixin, PandasObject):\n         \"\"\"\n         return self.set_names([name], inplace=inplace)\n \n+    # --------------------------------------------------------------------\n+    # Level-Centric Methods\n+\n     @property\n-    def _has_complex_internals(self):\n-        # to disable groupby tricks in MultiIndex\n-        return False\n+    def nlevels(self):\n+        return 1\n \n-    def _summary(self, name=None):\n+    def _sort_levels_monotonic(self):\n         \"\"\"\n-        Return a summarized representation.\n-\n-        Parameters\n-        ----------\n-        name : str\n-            name to use in the summary representation\n+        Compat with MultiIndex.\n+        \"\"\"\n+        return self\n \n-        Returns\n-        -------\n-        String with a summarized representation of the index\n+    def _validate_index_level(self, level):\n         \"\"\"\n-        if len(self) > 0:\n-            head = self[0]\n-            if (hasattr(head, 'format') and\n-                    not isinstance(head, compat.string_types)):\n-                head = head.format()\n-            tail = self[-1]\n-            if (hasattr(tail, 'format') and\n-                    not isinstance(tail, compat.string_types)):\n-                tail = tail.format()\n-            index_summary = ', %s to %s' % (pprint_thing(head),\n-                                            pprint_thing(tail))\n-        else:\n-            index_summary = ''\n+        Validate index level.\n \n-        if name is None:\n-            name = type(self).__name__\n-        return '%s: %s entries%s' % (name, len(self), index_summary)\n+        For single-level Index getting level number is a no-op, but some\n+        verification must be done like in MultiIndex.\n \n-    def summary(self, name=None):\n         \"\"\"\n-        Return a summarized representation.\n+        if isinstance(level, int):\n+            if level < 0 and level != -1:\n+                raise IndexError(\"Too many levels: Index has only 1 level,\"\n+                                 \" %d is not a valid level number\" % (level, ))\n+            elif level > 0:\n+                raise IndexError(\"Too many levels:\"\n+                                 \" Index has only 1 level, not %d\" %\n+                                 (level + 1))\n+        elif level != self.name:\n+            raise KeyError('Level %s must be same as name (%s)' %\n+                           (level, self.name))\n \n-        .. deprecated:: 0.23.0\n+    def _get_level_number(self, level):\n+        self._validate_index_level(level)\n+        return 0\n+\n+    def sortlevel(self, level=None, ascending=True, sort_remaining=None):\n         \"\"\"\n-        warnings.warn(\"'summary' is deprecated and will be removed in a \"\n-                      \"future version.\", FutureWarning, stacklevel=2)\n-        return self._summary(name)\n+        For internal compatibility with with the Index API.\n \n-    def _mpl_repr(self):\n-        # how to represent ourselves to matplotlib\n-        return self.values\n+        Sort the Index. This is for compat with MultiIndex\n \n-    _na_value = np.nan\n-    \"\"\"The expected NA value to use with this index.\"\"\"\n+        Parameters\n+        ----------\n+        ascending : boolean, default True\n+            False to sort in descending order\n+\n+        level, sort_remaining are compat parameters\n+\n+        Returns\n+        -------\n+        sorted_index : Index\n+        \"\"\"\n+        return self.sort_values(return_indexer=True, ascending=ascending)\n+\n+    def _get_level_values(self, level):\n+        \"\"\"\n+        Return an Index of values for requested level.\n+\n+        This is primarily useful to get an individual level of values from a\n+        MultiIndex, but is provided on Index as well for compatability.\n+\n+        Parameters\n+        ----------\n+        level : int or str\n+            It is either the integer position or the name of the level.\n+\n+        Returns\n+        -------\n+        values : Index\n+            Calling object, as there is only one level in the Index.\n+\n+        See Also\n+        --------\n+        MultiIndex.get_level_values : Get values for a level of a MultiIndex.\n+\n+        Notes\n+        -----\n+        For Index, level should be 0, since there are no multiple levels.\n+\n+        Examples\n+        --------\n+\n+        >>> idx = pd.Index(list('abc'))\n+        >>> idx\n+        Index(['a', 'b', 'c'], dtype='object')\n+\n+        Get level values by supplying `level` as integer:\n+\n+        >>> idx.get_level_values(0)\n+        Index(['a', 'b', 'c'], dtype='object')\n+        \"\"\"\n+        self._validate_index_level(level)\n+        return self\n+\n+    get_level_values = _get_level_values\n+\n+    def droplevel(self, level=0):\n+        \"\"\"\n+        Return index with requested level(s) removed.\n+\n+        If resulting index has only 1 level left, the result will be\n+        of Index type, not MultiIndex.\n+\n+        .. versionadded:: 0.23.1 (support for non-MultiIndex)\n+\n+        Parameters\n+        ----------\n+        level : int, str, or list-like, default 0\n+            If a string is given, must be the name of a level\n+            If list-like, elements must be names or indexes of levels.\n+\n+        Returns\n+        -------\n+        index : Index or MultiIndex\n+        \"\"\"\n+        if not isinstance(level, (tuple, list)):\n+            level = [level]\n+\n+        levnums = sorted(self._get_level_number(lev) for lev in level)[::-1]\n+\n+        if len(level) == 0:\n+            return self\n+        if len(level) >= self.nlevels:\n+            raise ValueError(\"Cannot remove {} levels from an index with {} \"\n+                             \"levels: at least one level must be \"\n+                             \"left.\".format(len(level), self.nlevels))\n+        # The two checks above guarantee that here self is a MultiIndex\n+\n+        new_levels = list(self.levels)\n+        new_labels = list(self.labels)\n+        new_names = list(self.names)\n+\n+        for i in levnums:\n+            new_levels.pop(i)\n+            new_labels.pop(i)\n+            new_names.pop(i)\n+\n+        if len(new_levels) == 1:\n+\n+            # set nan if needed\n+            mask = new_labels[0] == -1\n+            result = new_levels[0].take(new_labels[0])\n+            if mask.any():\n+                result = result.putmask(mask, np.nan)\n+\n+            result.name = new_names[0]\n+            return result\n+        else:\n+            from .multi import MultiIndex\n+            return MultiIndex(levels=new_levels, labels=new_labels,\n+                              names=new_names, verify_integrity=False)\n+\n+    _index_shared_docs['_get_grouper_for_level'] = \"\"\"\n+        Get index grouper corresponding to an index level\n+\n+        Parameters\n+        ----------\n+        mapper: Group mapping function or None\n+            Function mapping index values to groups\n+        level : int or None\n+            Index level\n+\n+        Returns\n+        -------\n+        grouper : Index\n+            Index of values to group on\n+        labels : ndarray of int or None\n+            Array of locations in level_index\n+        uniques : Index or None\n+            Index of unique values for level\n+        \"\"\"\n+\n+    @Appender(_index_shared_docs['_get_grouper_for_level'])\n+    def _get_grouper_for_level(self, mapper, level=None):\n+        assert level is None or level == 0\n+        if mapper is None:\n+            grouper = self\n+        else:\n+            grouper = self.map(mapper)\n+\n+        return grouper, None, None\n+\n+    # --------------------------------------------------------------------\n+    # Introspection Methods\n \n-    # introspection\n     @property\n     def is_monotonic(self):\n         \"\"\"\n@@ -1671,234 +1726,385 @@ class Index(IndexOpsMixin, PandasObject):\n     def holds_integer(self):\n         return self.inferred_type in ['integer', 'mixed-integer']\n \n-    _index_shared_docs['_convert_scalar_indexer'] = \"\"\"\n-        Convert a scalar indexer.\n+    @cache_readonly\n+    def inferred_type(self):\n+        \"\"\"\n+        Return a string of the type inferred from the values.\n+        \"\"\"\n+        return lib.infer_dtype(self)\n \n-        Parameters\n-        ----------\n-        key : label of the slice bound\n-        kind : {'ix', 'loc', 'getitem', 'iloc'} or None\n-    \"\"\"\n+    @cache_readonly\n+    def is_all_dates(self):\n+        if self._data is None:\n+            return False\n+        return is_datetime_array(ensure_object(self.values))\n \n-    @Appender(_index_shared_docs['_convert_scalar_indexer'])\n-    def _convert_scalar_indexer(self, key, kind=None):\n-        assert kind in ['ix', 'loc', 'getitem', 'iloc', None]\n+    # --------------------------------------------------------------------\n+    # Pickle Methods\n \n-        if kind == 'iloc':\n-            return self._validate_indexer('positional', key, kind)\n+    def __reduce__(self):\n+        d = dict(data=self._data)\n+        d.update(self._get_attributes_dict())\n+        return _new_Index, (self.__class__, d), None\n \n-        if len(self) and not isinstance(self, ABCMultiIndex,):\n+    def __setstate__(self, state):\n+        \"\"\"\n+        Necessary for making this object picklable.\n+        \"\"\"\n \n-            # we can raise here if we are definitive that this\n-            # is positional indexing (eg. .ix on with a float)\n-            # or label indexing if we are using a type able\n-            # to be represented in the index\n+        if isinstance(state, dict):\n+            self._data = state.pop('data')\n+            for k, v in compat.iteritems(state):\n+                setattr(self, k, v)\n \n-            if kind in ['getitem', 'ix'] and is_float(key):\n-                if not self.is_floating():\n-                    return self._invalid_indexer('label', key)\n+        elif isinstance(state, tuple):\n \n-            elif kind in ['loc'] and is_float(key):\n+            if len(state) == 2:\n+                nd_state, own_state = state\n+                data = np.empty(nd_state[1], dtype=nd_state[2])\n+                np.ndarray.__setstate__(data, nd_state)\n+                self.name = own_state[0]\n \n-                # we want to raise KeyError on string/mixed here\n-                # technically we *could* raise a TypeError\n-                # on anything but mixed though\n-                if self.inferred_type not in ['floating',\n-                                              'mixed-integer-float',\n-                                              'string',\n-                                              'unicode',\n-                                              'mixed']:\n-                    return self._invalid_indexer('label', key)\n+            else:  # pragma: no cover\n+                data = np.empty(state)\n+                np.ndarray.__setstate__(data, state)\n \n-            elif kind in ['loc'] and is_integer(key):\n-                if not self.holds_integer():\n-                    return self._invalid_indexer('label', key)\n+            self._data = data\n+            self._reset_identity()\n+        else:\n+            raise Exception(\"invalid pickle state\")\n \n-        return key\n+    _unpickle_compat = __setstate__\n \n-    _index_shared_docs['_convert_slice_indexer'] = \"\"\"\n-        Convert a slice indexer.\n+    # --------------------------------------------------------------------\n+    # Null Handling Methods\n \n-        By definition, these are labels unless 'iloc' is passed in.\n-        Floats are not allowed as the start, step, or stop of the slice.\n+    _na_value = np.nan\n+    \"\"\"The expected NA value to use with this index.\"\"\"\n \n-        Parameters\n-        ----------\n-        key : label of the slice bound\n-        kind : {'ix', 'loc', 'getitem', 'iloc'} or None\n-    \"\"\"\n+    @cache_readonly\n+    def _isnan(self):\n+        \"\"\"\n+        Return if each value is NaN.\n+        \"\"\"\n+        if self._can_hold_na:\n+            return isna(self)\n+        else:\n+            # shouldn't reach to this condition by checking hasnans beforehand\n+            values = np.empty(len(self), dtype=np.bool_)\n+            values.fill(False)\n+            return values\n \n-    @Appender(_index_shared_docs['_convert_slice_indexer'])\n-    def _convert_slice_indexer(self, key, kind=None):\n-        assert kind in ['ix', 'loc', 'getitem', 'iloc', None]\n+    @cache_readonly\n+    def _nan_idxs(self):\n+        if self._can_hold_na:\n+            w, = self._isnan.nonzero()\n+            return w\n+        else:\n+            return np.array([], dtype=np.int64)\n \n-        # if we are not a slice, then we are done\n-        if not isinstance(key, slice):\n-            return key\n+    @cache_readonly\n+    def hasnans(self):\n+        \"\"\"\n+        Return if I have any nans; enables various perf speedups.\n+        \"\"\"\n+        if self._can_hold_na:\n+            return bool(self._isnan.any())\n+        else:\n+            return False\n \n-        # validate iloc\n-        if kind == 'iloc':\n-            return slice(self._validate_indexer('slice', key.start, kind),\n-                         self._validate_indexer('slice', key.stop, kind),\n-                         self._validate_indexer('slice', key.step, kind))\n+    def isna(self):\n+        \"\"\"\n+        Detect missing values.\n \n-        # potentially cast the bounds to integers\n-        start, stop, step = key.start, key.stop, key.step\n+        Return a boolean same-sized object indicating if the values are NA.\n+        NA values, such as ``None``, :attr:`numpy.NaN` or :attr:`pd.NaT`, get\n+        mapped to ``True`` values.\n+        Everything else get mapped to ``False`` values. Characters such as\n+        empty strings `''` or :attr:`numpy.inf` are not considered NA values\n+        (unless you set ``pandas.options.mode.use_inf_as_na = True``).\n \n-        # figure out if this is a positional indexer\n-        def is_int(v):\n-            return v is None or is_integer(v)\n+        .. versionadded:: 0.20.0\n \n-        is_null_slicer = start is None and stop is None\n-        is_index_slice = is_int(start) and is_int(stop)\n-        is_positional = is_index_slice and not self.is_integer()\n+        Returns\n+        -------\n+        numpy.ndarray\n+            A boolean array of whether my values are NA\n \n-        if kind == 'getitem':\n-            \"\"\"\n-            called from the getitem slicers, validate that we are in fact\n-            integers\n-            \"\"\"\n-            if self.is_integer() or is_index_slice:\n-                return slice(self._validate_indexer('slice', key.start, kind),\n-                             self._validate_indexer('slice', key.stop, kind),\n-                             self._validate_indexer('slice', key.step, kind))\n+        See Also\n+        --------\n+        pandas.Index.notna : Boolean inverse of isna.\n+        pandas.Index.dropna : Omit entries with missing values.\n+        pandas.isna : Top-level isna.\n+        Series.isna : Detect missing values in Series object.\n \n-        # convert the slice to an indexer here\n+        Examples\n+        --------\n+        Show which entries in a pandas.Index are NA. The result is an\n+        array.\n \n-        # if we are mixed and have integers\n-        try:\n-            if is_positional and self.is_mixed():\n-                # Validate start & stop\n-                if start is not None:\n-                    self.get_loc(start)\n-                if stop is not None:\n-                    self.get_loc(stop)\n-                is_positional = False\n-        except KeyError:\n-            if self.inferred_type == 'mixed-integer-float':\n-                raise\n+        >>> idx = pd.Index([5.2, 6.0, np.NaN])\n+        >>> idx\n+        Float64Index([5.2, 6.0, nan], dtype='float64')\n+        >>> idx.isna()\n+        array([False, False,  True], dtype=bool)\n \n-        if is_null_slicer:\n-            indexer = key\n-        elif is_positional:\n-            indexer = key\n-        else:\n-            try:\n-                indexer = self.slice_indexer(start, stop, step, kind=kind)\n-            except Exception:\n-                if is_index_slice:\n-                    if self.is_integer():\n-                        raise\n-                    else:\n-                        indexer = key\n-                else:\n-                    raise\n+        Empty strings are not considered NA values. None is considered an NA\n+        value.\n \n-        return indexer\n+        >>> idx = pd.Index(['black', '', 'red', None])\n+        >>> idx\n+        Index(['black', '', 'red', None], dtype='object')\n+        >>> idx.isna()\n+        array([False, False, False,  True], dtype=bool)\n \n-    def _convert_listlike_indexer(self, keyarr, kind=None):\n-        \"\"\"\n-        Parameters\n-        ----------\n-        keyarr : list-like\n-            Indexer to convert.\n+        For datetimes, `NaT` (Not a Time) is considered as an NA value.\n \n-        Returns\n-        -------\n-        tuple (indexer, keyarr)\n-            indexer is an ndarray or None if cannot convert\n-            keyarr are tuple-safe keys\n+        >>> idx = pd.DatetimeIndex([pd.Timestamp('1940-04-25'),\n+        ...                         pd.Timestamp(''), None, pd.NaT])\n+        >>> idx\n+        DatetimeIndex(['1940-04-25', 'NaT', 'NaT', 'NaT'],\n+                      dtype='datetime64[ns]', freq=None)\n+        >>> idx.isna()\n+        array([False,  True,  True,  True], dtype=bool)\n         \"\"\"\n-        if isinstance(keyarr, Index):\n-            keyarr = self._convert_index_indexer(keyarr)\n-        else:\n-            keyarr = self._convert_arr_indexer(keyarr)\n+        return self._isnan\n+    isnull = isna\n \n-        indexer = self._convert_list_indexer(keyarr, kind=kind)\n-        return indexer, keyarr\n+    def notna(self):\n+        \"\"\"\n+        Detect existing (non-missing) values.\n \n-    _index_shared_docs['_convert_arr_indexer'] = \"\"\"\n-        Convert an array-like indexer to the appropriate dtype.\n+        Return a boolean same-sized object indicating if the values are not NA.\n+        Non-missing values get mapped to ``True``. Characters such as empty\n+        strings ``''`` or :attr:`numpy.inf` are not considered NA values\n+        (unless you set ``pandas.options.mode.use_inf_as_na = True``).\n+        NA values, such as None or :attr:`numpy.NaN`, get mapped to ``False``\n+        values.\n \n-        Parameters\n-        ----------\n-        keyarr : array-like\n-            Indexer to convert.\n+        .. versionadded:: 0.20.0\n \n         Returns\n         -------\n-        converted_keyarr : array-like\n-    \"\"\"\n+        numpy.ndarray\n+            Boolean array to indicate which entries are not NA.\n \n-    @Appender(_index_shared_docs['_convert_arr_indexer'])\n-    def _convert_arr_indexer(self, keyarr):\n-        keyarr = com.asarray_tuplesafe(keyarr)\n-        return keyarr\n+        See Also\n+        --------\n+        Index.notnull : Alias of notna.\n+        Index.isna: Inverse of notna.\n+        pandas.notna : Top-level notna.\n \n-    _index_shared_docs['_convert_index_indexer'] = \"\"\"\n-        Convert an Index indexer to the appropriate dtype.\n+        Examples\n+        --------\n+        Show which entries in an Index are not NA. The result is an\n+        array.\n+\n+        >>> idx = pd.Index([5.2, 6.0, np.NaN])\n+        >>> idx\n+        Float64Index([5.2, 6.0, nan], dtype='float64')\n+        >>> idx.notna()\n+        array([ True,  True, False])\n+\n+        Empty strings are not considered NA values. None is considered a NA\n+        value.\n+\n+        >>> idx = pd.Index(['black', '', 'red', None])\n+        >>> idx\n+        Index(['black', '', 'red', None], dtype='object')\n+        >>> idx.notna()\n+        array([ True,  True,  True, False])\n+        \"\"\"\n+        return ~self.isna()\n+    notnull = notna\n+\n+    _index_shared_docs['fillna'] = \"\"\"\n+        Fill NA/NaN values with the specified value\n \n         Parameters\n         ----------\n-        keyarr : Index (or sub-class)\n-            Indexer to convert.\n+        value : scalar\n+            Scalar value to use to fill holes (e.g. 0).\n+            This value cannot be a list-likes.\n+        downcast : dict, default is None\n+            a dict of item->dtype of what to downcast if possible,\n+            or the string 'infer' which will try to downcast to an appropriate\n+            equal type (e.g. float64 to int64 if possible)\n \n         Returns\n         -------\n-        converted_keyarr : Index (or sub-class)\n-    \"\"\"\n+        filled : %(klass)s\n+        \"\"\"\n \n-    @Appender(_index_shared_docs['_convert_index_indexer'])\n-    def _convert_index_indexer(self, keyarr):\n-        return keyarr\n+    @Appender(_index_shared_docs['fillna'])\n+    def fillna(self, value=None, downcast=None):\n+        self._assert_can_do_op(value)\n+        if self.hasnans:\n+            result = self.putmask(self._isnan, value)\n+            if downcast is None:\n+                # no need to care metadata other than name\n+                # because it can't have freq if\n+                return Index(result, name=self.name)\n+        return self._shallow_copy()\n \n-    _index_shared_docs['_convert_list_indexer'] = \"\"\"\n-        Convert a list-like indexer to the appropriate dtype.\n+    _index_shared_docs['dropna'] = \"\"\"\n+        Return Index without NA/NaN values\n \n         Parameters\n         ----------\n-        keyarr : Index (or sub-class)\n-            Indexer to convert.\n-        kind : iloc, ix, loc, optional\n+        how :  {'any', 'all'}, default 'any'\n+            If the Index is a MultiIndex, drop the value when any or all levels\n+            are NaN.\n \n         Returns\n         -------\n-        positional indexer or None\n-    \"\"\"\n+        valid : Index\n+        \"\"\"\n \n-    @Appender(_index_shared_docs['_convert_list_indexer'])\n-    def _convert_list_indexer(self, keyarr, kind=None):\n-        if (kind in [None, 'iloc', 'ix'] and\n-                is_integer_dtype(keyarr) and not self.is_floating() and\n-                not isinstance(keyarr, ABCPeriodIndex)):\n+    @Appender(_index_shared_docs['dropna'])\n+    def dropna(self, how='any'):\n+        if how not in ('any', 'all'):\n+            raise ValueError(\"invalid how option: {0}\".format(how))\n \n-            if self.inferred_type == 'mixed-integer':\n-                indexer = self.get_indexer(keyarr)\n-                if (indexer >= 0).all():\n-                    return indexer\n-                # missing values are flagged as -1 by get_indexer and negative\n-                # indices are already converted to positive indices in the\n-                # above if-statement, so the negative flags are changed to\n-                # values outside the range of indices so as to trigger an\n-                # IndexError in maybe_convert_indices\n-                indexer[indexer < 0] = len(self)\n-                from pandas.core.indexing import maybe_convert_indices\n-                return maybe_convert_indices(indexer, len(self))\n+        if self.hasnans:\n+            return self._shallow_copy(self.values[~self._isnan])\n+        return self._shallow_copy()\n \n-            elif not self.inferred_type == 'integer':\n-                keyarr = np.where(keyarr < 0, len(self) + keyarr, keyarr)\n-                return keyarr\n+    # --------------------------------------------------------------------\n+    # Uniqueness Methods\n \n-        return None\n+    _index_shared_docs['index_unique'] = (\n+        \"\"\"\n+        Return unique values in the index. Uniques are returned in order\n+        of appearance, this does NOT sort.\n \n-    def _invalid_indexer(self, form, key):\n+        Parameters\n+        ----------\n+        level : int or str, optional, default None\n+            Only return values from specified level (for MultiIndex)\n+\n+            .. versionadded:: 0.23.0\n+\n+        Returns\n+        -------\n+        Index without duplicates\n+\n+        See Also\n+        --------\n+        unique\n+        Series.unique\n+        \"\"\")\n+\n+    @Appender(_index_shared_docs['index_unique'] % _index_doc_kwargs)\n+    def unique(self, level=None):\n+        if level is not None:\n+            self._validate_index_level(level)\n+        result = super(Index, self).unique()\n+        return self._shallow_copy(result)\n+\n+    def drop_duplicates(self, keep='first'):\n         \"\"\"\n-        Consistent invalid indexer message.\n+        Return Index with duplicate values removed.\n+\n+        Parameters\n+        ----------\n+        keep : {'first', 'last', ``False``}, default 'first'\n+            - 'first' : Drop duplicates except for the first occurrence.\n+            - 'last' : Drop duplicates except for the last occurrence.\n+            - ``False`` : Drop all duplicates.\n+\n+        Returns\n+        -------\n+        deduplicated : Index\n+\n+        See Also\n+        --------\n+        Series.drop_duplicates : Equivalent method on Series.\n+        DataFrame.drop_duplicates : Equivalent method on DataFrame.\n+        Index.duplicated : Related method on Index, indicating duplicate\n+            Index values.\n+\n+        Examples\n+        --------\n+        Generate an pandas.Index with duplicate values.\n+\n+        >>> idx = pd.Index(['lama', 'cow', 'lama', 'beetle', 'lama', 'hippo'])\n+\n+        The `keep` parameter controls  which duplicate values are removed.\n+        The value 'first' keeps the first occurrence for each\n+        set of duplicated entries. The default value of keep is 'first'.\n+\n+        >>> idx.drop_duplicates(keep='first')\n+        Index(['lama', 'cow', 'beetle', 'hippo'], dtype='object')\n+\n+        The value 'last' keeps the last occurrence for each set of duplicated\n+        entries.\n+\n+        >>> idx.drop_duplicates(keep='last')\n+        Index(['cow', 'beetle', 'lama', 'hippo'], dtype='object')\n+\n+        The value ``False`` discards all sets of duplicated entries.\n+\n+        >>> idx.drop_duplicates(keep=False)\n+        Index(['cow', 'beetle', 'hippo'], dtype='object')\n         \"\"\"\n-        raise TypeError(\"cannot do {form} indexing on {klass} with these \"\n-                        \"indexers [{key}] of {kind}\".format(\n-                            form=form, klass=type(self), key=key,\n-                            kind=type(key)))\n+        return super(Index, self).drop_duplicates(keep=keep)\n+\n+    def duplicated(self, keep='first'):\n+        \"\"\"\n+        Indicate duplicate index values.\n+\n+        Duplicated values are indicated as ``True`` values in the resulting\n+        array. Either all duplicates, all except the first, or all except the\n+        last occurrence of duplicates can be indicated.\n+\n+        Parameters\n+        ----------\n+        keep : {'first', 'last', False}, default 'first'\n+            The value or values in a set of duplicates to mark as missing.\n+\n+            - 'first' : Mark duplicates as ``True`` except for the first\n+              occurrence.\n+            - 'last' : Mark duplicates as ``True`` except for the last\n+              occurrence.\n+            - ``False`` : Mark all duplicates as ``True``.\n+\n+        Examples\n+        --------\n+        By default, for each set of duplicated values, the first occurrence is\n+        set to False and all others to True:\n+\n+        >>> idx = pd.Index(['lama', 'cow', 'lama', 'beetle', 'lama'])\n+        >>> idx.duplicated()\n+        array([False, False,  True, False,  True])\n+\n+        which is equivalent to\n+\n+        >>> idx.duplicated(keep='first')\n+        array([False, False,  True, False,  True])\n+\n+        By using 'last', the last occurrence of each set of duplicated values\n+        is set on False and all others on True:\n+\n+        >>> idx.duplicated(keep='last')\n+        array([ True, False,  True, False, False])\n+\n+        By setting keep on ``False``, all duplicates are True:\n+\n+        >>> idx.duplicated(keep=False)\n+        array([ True, False,  True, False,  True])\n+\n+        Returns\n+        -------\n+        numpy.ndarray\n+\n+        See Also\n+        --------\n+        pandas.Series.duplicated : Equivalent method on pandas.Series.\n+        pandas.DataFrame.duplicated : Equivalent method on pandas.DataFrame.\n+        pandas.Index.drop_duplicates : Remove duplicate values from Index.\n+        \"\"\"\n+        return super(Index, self).duplicated(keep=keep)\n \n     def get_duplicates(self):\n         \"\"\"\n@@ -1959,97 +2165,65 @@ class Index(IndexOpsMixin, PandasObject):\n \n         return self[self.duplicated()].unique()\n \n-    def _cleanup(self):\n-        self._engine.clear_mapping()\n-\n-    @cache_readonly\n-    def _constructor(self):\n-        return type(self)\n+    def _get_unique_index(self, dropna=False):\n+        \"\"\"\n+        Returns an index containing unique values.\n \n-    @cache_readonly\n-    def _engine(self):\n-        # property, for now, slow to look up\n-        return self._engine_type(lambda: self._ndarray_values, len(self))\n+        Parameters\n+        ----------\n+        dropna : bool\n+            If True, NaN values are dropped.\n \n-    def _validate_index_level(self, level):\n+        Returns\n+        -------\n+        uniques : index\n         \"\"\"\n-        Validate index level.\n+        if self.is_unique and not dropna:\n+            return self\n \n-        For single-level Index getting level number is a no-op, but some\n-        verification must be done like in MultiIndex.\n+        values = self.values\n \n-        \"\"\"\n-        if isinstance(level, int):\n-            if level < 0 and level != -1:\n-                raise IndexError(\"Too many levels: Index has only 1 level,\"\n-                                 \" %d is not a valid level number\" % (level, ))\n-            elif level > 0:\n-                raise IndexError(\"Too many levels:\"\n-                                 \" Index has only 1 level, not %d\" %\n-                                 (level + 1))\n-        elif level != self.name:\n-            raise KeyError('Level %s must be same as name (%s)' %\n-                           (level, self.name))\n+        if not self.is_unique:\n+            values = self.unique()\n \n-    def _get_level_number(self, level):\n-        self._validate_index_level(level)\n-        return 0\n+        if dropna:\n+            try:\n+                if self.hasnans:\n+                    values = values[~isna(values)]\n+            except NotImplementedError:\n+                pass\n \n-    @cache_readonly\n-    def inferred_type(self):\n-        \"\"\"\n-        Return a string of the type inferred from the values.\n-        \"\"\"\n-        return lib.infer_dtype(self)\n+        return self._shallow_copy(values)\n \n-    def _is_memory_usage_qualified(self):\n-        \"\"\"\n-        Return a boolean if we need a qualified .info display.\n-        \"\"\"\n-        return self.is_object()\n+    # --------------------------------------------------------------------\n+    # Arithmetic & Logical Methods\n \n-    def is_type_compatible(self, kind):\n-        return kind == self.inferred_type\n+    def __add__(self, other):\n+        if isinstance(other, (ABCSeries, ABCDataFrame)):\n+            return NotImplemented\n+        return Index(np.array(self) + other)\n \n-    @cache_readonly\n-    def is_all_dates(self):\n-        if self._data is None:\n-            return False\n-        return is_datetime_array(ensure_object(self.values))\n+    def __radd__(self, other):\n+        return Index(other + np.array(self))\n \n-    def __reduce__(self):\n-        d = dict(data=self._data)\n-        d.update(self._get_attributes_dict())\n-        return _new_Index, (self.__class__, d), None\n+    def __iadd__(self, other):\n+        # alias for __add__\n+        return self + other\n \n-    def __setstate__(self, state):\n-        \"\"\"\n-        Necessary for making this object picklable.\n-        \"\"\"\n+    def __sub__(self, other):\n+        return Index(np.array(self) - other)\n \n-        if isinstance(state, dict):\n-            self._data = state.pop('data')\n-            for k, v in compat.iteritems(state):\n-                setattr(self, k, v)\n+    def __rsub__(self, other):\n+        return Index(other - np.array(self))\n \n-        elif isinstance(state, tuple):\n+    def __and__(self, other):\n+        return self.intersection(other)\n \n-            if len(state) == 2:\n-                nd_state, own_state = state\n-                data = np.empty(nd_state[1], dtype=nd_state[2])\n-                np.ndarray.__setstate__(data, nd_state)\n-                self.name = own_state[0]\n+    def __or__(self, other):\n+        return self.union(other)\n \n-            else:  # pragma: no cover\n-                data = np.empty(state)\n-                np.ndarray.__setstate__(data, state)\n-\n-            self._data = data\n-            self._reset_identity()\n-        else:\n-            raise Exception(\"invalid pickle state\")\n-\n-    _unpickle_compat = __setstate__\n+    def __xor__(self, other):\n+        return self.symmetric_difference(other)\n \n     def __nonzero__(self):\n         raise ValueError(\"The truth value of a {0} is ambiguous. \"\n@@ -2058,2236 +2232,2302 @@ class Index(IndexOpsMixin, PandasObject):\n \n     __bool__ = __nonzero__\n \n-    _index_shared_docs['__contains__'] = \"\"\"\n-        Return a boolean if this key is IN the index.\n-\n-        Parameters\n-        ----------\n-        key : object\n+    # --------------------------------------------------------------------\n+    # Set Operation Methods\n \n-        Returns\n-        -------\n-        boolean\n+    def _get_reconciled_name_object(self, other):\n         \"\"\"\n+        If the result of a set operation will be self,\n+        return self, unless the name changes, in which\n+        case make a shallow copy of self.\n+        \"\"\"\n+        name = get_op_result_name(self, other)\n+        if self.name != name:\n+            return self._shallow_copy(name=name)\n+        return self\n \n-    @Appender(_index_shared_docs['__contains__'] % _index_doc_kwargs)\n-    def __contains__(self, key):\n-        hash(key)\n-        try:\n-            return key in self._engine\n-        except (OverflowError, TypeError, ValueError):\n-            return False\n-\n-    _index_shared_docs['contains'] = \"\"\"\n-        Return a boolean if this key is IN the index.\n+    def union(self, other):\n+        \"\"\"\n+        Form the union of two Index objects and sorts if possible.\n \n         Parameters\n         ----------\n-        key : object\n+        other : Index or array-like\n \n         Returns\n         -------\n-        boolean\n+        union : Index\n+\n+        Examples\n+        --------\n+\n+        >>> idx1 = pd.Index([1, 2, 3, 4])\n+        >>> idx2 = pd.Index([3, 4, 5, 6])\n+        >>> idx1.union(idx2)\n+        Int64Index([1, 2, 3, 4, 5, 6], dtype='int64')\n         \"\"\"\n+        self._assert_can_do_setop(other)\n+        other = ensure_index(other)\n \n-    @Appender(_index_shared_docs['contains'] % _index_doc_kwargs)\n-    def contains(self, key):\n-        hash(key)\n-        try:\n-            return key in self._engine\n-        except (TypeError, ValueError):\n-            return False\n+        if len(other) == 0 or self.equals(other):\n+            return self._get_reconciled_name_object(other)\n \n-    def __hash__(self):\n-        raise TypeError(\"unhashable type: %r\" % type(self).__name__)\n+        if len(self) == 0:\n+            return other._get_reconciled_name_object(self)\n \n-    def __setitem__(self, key, value):\n-        raise TypeError(\"Index does not support mutable operations\")\n+        # TODO: is_dtype_union_equal is a hack around\n+        # 1. buggy set ops with duplicates (GH #13432)\n+        # 2. CategoricalIndex lacking setops (GH #10186)\n+        # Once those are fixed, this workaround can be removed\n+        if not is_dtype_union_equal(self.dtype, other.dtype):\n+            this = self.astype('O')\n+            other = other.astype('O')\n+            return this.union(other)\n \n-    def __getitem__(self, key):\n-        \"\"\"\n-        Override numpy.ndarray's __getitem__ method to work as desired.\n+        # TODO(EA): setops-refactor, clean all this up\n+        if is_period_dtype(self) or is_datetime64tz_dtype(self):\n+            lvals = self._ndarray_values\n+        else:\n+            lvals = self._values\n+        if is_period_dtype(other) or is_datetime64tz_dtype(other):\n+            rvals = other._ndarray_values\n+        else:\n+            rvals = other._values\n \n-        This function adds lists and Series as valid boolean indexers\n-        (ndarrays only supports ndarray with dtype=bool).\n+        if self.is_monotonic and other.is_monotonic:\n+            try:\n+                result = self._outer_indexer(lvals, rvals)[0]\n+            except TypeError:\n+                # incomparable objects\n+                result = list(lvals)\n \n-        If resulting ndim != 1, plain ndarray is returned instead of\n-        corresponding `Index` subclass.\n+                # worth making this faster? a very unusual case\n+                value_set = set(lvals)\n+                result.extend([x for x in rvals if x not in value_set])\n+        else:\n+            indexer = self.get_indexer(other)\n+            indexer, = (indexer == -1).nonzero()\n \n-        \"\"\"\n-        # There's no custom logic to be implemented in __getslice__, so it's\n-        # not overloaded intentionally.\n-        getitem = self._data.__getitem__\n-        promote = self._shallow_copy\n+            if len(indexer) > 0:\n+                other_diff = algos.take_nd(rvals, indexer,\n+                                           allow_fill=False)\n+                result = _concat._concat_compat((lvals, other_diff))\n \n-        if is_scalar(key):\n-            key = com.cast_scalar_indexer(key)\n-            return getitem(key)\n+                try:\n+                    lvals[0] < other_diff[0]\n+                except TypeError as e:\n+                    warnings.warn(\"%s, sort order is undefined for \"\n+                                  \"incomparable objects\" % e, RuntimeWarning,\n+                                  stacklevel=3)\n+                else:\n+                    types = frozenset((self.inferred_type,\n+                                       other.inferred_type))\n+                    if not types & _unsortable_types:\n+                        result.sort()\n \n-        if isinstance(key, slice):\n-            # This case is separated from the conditional above to avoid\n-            # pessimization of basic indexing.\n-            return promote(getitem(key))\n+            else:\n+                result = lvals\n \n-        if com.is_bool_indexer(key):\n-            key = np.asarray(key, dtype=bool)\n+                try:\n+                    result = np.sort(result)\n+                except TypeError as e:\n+                    warnings.warn(\"%s, sort order is undefined for \"\n+                                  \"incomparable objects\" % e, RuntimeWarning,\n+                                  stacklevel=3)\n \n-        key = com.values_from_object(key)\n-        result = getitem(key)\n-        if not is_scalar(result):\n-            return promote(result)\n-        else:\n-            return result\n+        # for subclasses\n+        return self._wrap_setop_result(other, result)\n \n-    def _can_hold_identifiers_and_holds_name(self, name):\n-        \"\"\"\n-        Faster check for ``name in self`` when we know `name` is a Python\n-        identifier (e.g. in NDFrame.__getattr__, which hits this to support\n-        . key lookup). For indexes that can't hold identifiers (everything\n-        but object & categorical) we just return False.\n+    def _wrap_setop_result(self, other, result):\n+        return self._constructor(result, name=get_op_result_name(self, other))\n \n-        https://github.com/pandas-dev/pandas/issues/19764\n+    def intersection(self, other):\n         \"\"\"\n-        if self.is_object() or self.is_categorical():\n-            return name in self\n-        return False\n+        Form the intersection of two Index objects.\n \n-    def append(self, other):\n-        \"\"\"\n-        Append a collection of Index options together.\n+        This returns a new Index with elements common to the index and `other`,\n+        preserving the order of the calling index.\n \n         Parameters\n         ----------\n-        other : Index or list/tuple of indices\n+        other : Index or array-like\n \n         Returns\n         -------\n-        appended : Index\n-        \"\"\"\n+        intersection : Index\n \n-        to_concat = [self]\n+        Examples\n+        --------\n \n-        if isinstance(other, (list, tuple)):\n-            to_concat = to_concat + list(other)\n-        else:\n-            to_concat.append(other)\n+        >>> idx1 = pd.Index([1, 2, 3, 4])\n+        >>> idx2 = pd.Index([3, 4, 5, 6])\n+        >>> idx1.intersection(idx2)\n+        Int64Index([3, 4], dtype='int64')\n+        \"\"\"\n+        self._assert_can_do_setop(other)\n+        other = ensure_index(other)\n \n-        for obj in to_concat:\n-            if not isinstance(obj, Index):\n-                raise TypeError('all inputs must be Index')\n+        if self.equals(other):\n+            return self._get_reconciled_name_object(other)\n \n-        names = {obj.name for obj in to_concat}\n-        name = None if len(names) > 1 else self.name\n+        if not is_dtype_equal(self.dtype, other.dtype):\n+            this = self.astype('O')\n+            other = other.astype('O')\n+            return this.intersection(other)\n \n-        return self._concat(to_concat, name)\n+        # TODO(EA): setops-refactor, clean all this up\n+        if is_period_dtype(self):\n+            lvals = self._ndarray_values\n+        else:\n+            lvals = self._values\n+        if is_period_dtype(other):\n+            rvals = other._ndarray_values\n+        else:\n+            rvals = other._values\n \n-    def _concat(self, to_concat, name):\n+        if self.is_monotonic and other.is_monotonic:\n+            try:\n+                result = self._inner_indexer(lvals, rvals)[0]\n+                return self._wrap_setop_result(other, result)\n+            except TypeError:\n+                pass\n \n-        typs = _concat.get_dtype_kinds(to_concat)\n+        try:\n+            indexer = Index(rvals).get_indexer(lvals)\n+            indexer = indexer.take((indexer != -1).nonzero()[0])\n+        except Exception:\n+            # duplicates\n+            indexer = algos.unique1d(\n+                Index(rvals).get_indexer_non_unique(lvals)[0])\n+            indexer = indexer[indexer != -1]\n \n-        if len(typs) == 1:\n-            return self._concat_same_dtype(to_concat, name=name)\n-        return _concat._concat_index_asobject(to_concat, name=name)\n+        taken = other.take(indexer)\n+        if self.name != other.name:\n+            taken.name = None\n+        return taken\n \n-    def _concat_same_dtype(self, to_concat, name):\n-        \"\"\"\n-        Concatenate to_concat which has the same class.\n+    def difference(self, other, sort=True):\n         \"\"\"\n-        # must be overridden in specific classes\n-        return _concat._concat_index_asobject(to_concat, name)\n-\n-    _index_shared_docs['take'] = \"\"\"\n-        Return a new %(klass)s of the values selected by the indices.\n+        Return a new Index with elements from the index that are not in\n+        `other`.\n \n-        For internal compatibility with numpy arrays.\n+        This is the set difference of two Index objects.\n \n         Parameters\n         ----------\n-        indices : list\n-            Indices to be taken\n-        axis : int, optional\n-            The axis over which to select values, always 0.\n-        allow_fill : bool, default True\n-        fill_value : bool, default None\n-            If allow_fill=True and fill_value is not None, indices specified by\n-            -1 is regarded as NA. If Index doesn't hold NA, raise ValueError\n+        other : Index or array-like\n+        sort : bool, default True\n+            Sort the resulting index if possible\n \n-        See Also\n+            .. versionadded:: 0.24.0\n+\n+        Returns\n+        -------\n+        difference : Index\n+\n+        Examples\n         --------\n-        numpy.ndarray.take\n+\n+        >>> idx1 = pd.Index([2, 1, 3, 4])\n+        >>> idx2 = pd.Index([3, 4, 5, 6])\n+        >>> idx1.difference(idx2)\n+        Int64Index([1, 2], dtype='int64')\n+        >>> idx1.difference(idx2, sort=False)\n+        Int64Index([2, 1], dtype='int64')\n         \"\"\"\n+        self._assert_can_do_setop(other)\n \n-    @Appender(_index_shared_docs['take'] % _index_doc_kwargs)\n-    def take(self, indices, axis=0, allow_fill=True,\n-             fill_value=None, **kwargs):\n-        if kwargs:\n-            nv.validate_take(tuple(), kwargs)\n-        indices = ensure_platform_int(indices)\n-        if self._can_hold_na:\n-            taken = self._assert_take_fillable(self.values, indices,\n-                                               allow_fill=allow_fill,\n-                                               fill_value=fill_value,\n-                                               na_value=self._na_value)\n-        else:\n-            if allow_fill and fill_value is not None:\n-                msg = 'Unable to fill values because {0} cannot contain NA'\n-                raise ValueError(msg.format(self.__class__.__name__))\n-            taken = self.values.take(indices)\n-        return self._shallow_copy(taken)\n+        if self.equals(other):\n+            # pass an empty np.ndarray with the appropriate dtype\n+            return self._shallow_copy(self._data[:0])\n \n-    def _assert_take_fillable(self, values, indices, allow_fill=True,\n-                              fill_value=None, na_value=np.nan):\n-        \"\"\"\n-        Internal method to handle NA filling of take.\n-        \"\"\"\n-        indices = ensure_platform_int(indices)\n+        other, result_name = self._convert_can_do_setop(other)\n \n-        # only fill if we are passing a non-None fill_value\n-        if allow_fill and fill_value is not None:\n-            if (indices < -1).any():\n-                msg = ('When allow_fill=True and fill_value is not None, '\n-                       'all indices must be >= -1')\n-                raise ValueError(msg)\n-            taken = algos.take(values,\n-                               indices,\n-                               allow_fill=allow_fill,\n-                               fill_value=na_value)\n-        else:\n-            taken = values.take(indices)\n-        return taken\n+        this = self._get_unique_index()\n \n-    @cache_readonly\n-    def _isnan(self):\n-        \"\"\"\n-        Return if each value is NaN.\n-        \"\"\"\n-        if self._can_hold_na:\n-            return isna(self)\n-        else:\n-            # shouldn't reach to this condition by checking hasnans beforehand\n-            values = np.empty(len(self), dtype=np.bool_)\n-            values.fill(False)\n-            return values\n+        indexer = this.get_indexer(other)\n+        indexer = indexer.take((indexer != -1).nonzero()[0])\n \n-    @cache_readonly\n-    def _nan_idxs(self):\n-        if self._can_hold_na:\n-            w, = self._isnan.nonzero()\n-            return w\n-        else:\n-            return np.array([], dtype=np.int64)\n+        label_diff = np.setdiff1d(np.arange(this.size), indexer,\n+                                  assume_unique=True)\n+        the_diff = this.values.take(label_diff)\n+        if sort:\n+            try:\n+                the_diff = sorting.safe_sort(the_diff)\n+            except TypeError:\n+                pass\n \n-    @cache_readonly\n-    def hasnans(self):\n-        \"\"\"\n-        Return if I have any nans; enables various perf speedups.\n-        \"\"\"\n-        if self._can_hold_na:\n-            return bool(self._isnan.any())\n-        else:\n-            return False\n+        return this._shallow_copy(the_diff, name=result_name, freq=None)\n \n-    def isna(self):\n+    def symmetric_difference(self, other, result_name=None):\n         \"\"\"\n-        Detect missing values.\n+        Compute the symmetric difference of two Index objects.\n \n-        Return a boolean same-sized object indicating if the values are NA.\n-        NA values, such as ``None``, :attr:`numpy.NaN` or :attr:`pd.NaT`, get\n-        mapped to ``True`` values.\n-        Everything else get mapped to ``False`` values. Characters such as\n-        empty strings `''` or :attr:`numpy.inf` are not considered NA values\n-        (unless you set ``pandas.options.mode.use_inf_as_na = True``).\n+        It's sorted if sorting is possible.\n \n-        .. versionadded:: 0.20.0\n+        Parameters\n+        ----------\n+        other : Index or array-like\n+        result_name : str\n \n         Returns\n         -------\n-        numpy.ndarray\n-            A boolean array of whether my values are NA\n+        symmetric_difference : Index\n \n-        See Also\n-        --------\n-        pandas.Index.notna : Boolean inverse of isna.\n-        pandas.Index.dropna : Omit entries with missing values.\n-        pandas.isna : Top-level isna.\n-        Series.isna : Detect missing values in Series object.\n+        Notes\n+        -----\n+        ``symmetric_difference`` contains elements that appear in either\n+        ``idx1`` or ``idx2`` but not both. Equivalent to the Index created by\n+        ``idx1.difference(idx2) | idx2.difference(idx1)`` with duplicates\n+        dropped.\n \n         Examples\n         --------\n-        Show which entries in a pandas.Index are NA. The result is an\n-        array.\n+        >>> idx1 = pd.Index([1, 2, 3, 4])\n+        >>> idx2 = pd.Index([2, 3, 4, 5])\n+        >>> idx1.symmetric_difference(idx2)\n+        Int64Index([1, 5], dtype='int64')\n \n-        >>> idx = pd.Index([5.2, 6.0, np.NaN])\n-        >>> idx\n-        Float64Index([5.2, 6.0, nan], dtype='float64')\n-        >>> idx.isna()\n-        array([False, False,  True], dtype=bool)\n+        You can also use the ``^`` operator:\n \n-        Empty strings are not considered NA values. None is considered an NA\n-        value.\n+        >>> idx1 ^ idx2\n+        Int64Index([1, 5], dtype='int64')\n+        \"\"\"\n+        self._assert_can_do_setop(other)\n+        other, result_name_update = self._convert_can_do_setop(other)\n+        if result_name is None:\n+            result_name = result_name_update\n \n-        >>> idx = pd.Index(['black', '', 'red', None])\n-        >>> idx\n-        Index(['black', '', 'red', None], dtype='object')\n-        >>> idx.isna()\n-        array([False, False, False,  True], dtype=bool)\n+        this = self._get_unique_index()\n+        other = other._get_unique_index()\n+        indexer = this.get_indexer(other)\n \n-        For datetimes, `NaT` (Not a Time) is considered as an NA value.\n+        # {this} minus {other}\n+        common_indexer = indexer.take((indexer != -1).nonzero()[0])\n+        left_indexer = np.setdiff1d(np.arange(this.size), common_indexer,\n+                                    assume_unique=True)\n+        left_diff = this.values.take(left_indexer)\n \n-        >>> idx = pd.DatetimeIndex([pd.Timestamp('1940-04-25'),\n-        ...                         pd.Timestamp(''), None, pd.NaT])\n-        >>> idx\n-        DatetimeIndex(['1940-04-25', 'NaT', 'NaT', 'NaT'],\n-                      dtype='datetime64[ns]', freq=None)\n-        >>> idx.isna()\n-        array([False,  True,  True,  True], dtype=bool)\n-        \"\"\"\n-        return self._isnan\n-    isnull = isna\n+        # {other} minus {this}\n+        right_indexer = (indexer == -1).nonzero()[0]\n+        right_diff = other.values.take(right_indexer)\n \n-    def notna(self):\n-        \"\"\"\n-        Detect existing (non-missing) values.\n+        the_diff = _concat._concat_compat([left_diff, right_diff])\n+        try:\n+            the_diff = sorting.safe_sort(the_diff)\n+        except TypeError:\n+            pass\n \n-        Return a boolean same-sized object indicating if the values are not NA.\n-        Non-missing values get mapped to ``True``. Characters such as empty\n-        strings ``''`` or :attr:`numpy.inf` are not considered NA values\n-        (unless you set ``pandas.options.mode.use_inf_as_na = True``).\n-        NA values, such as None or :attr:`numpy.NaN`, get mapped to ``False``\n-        values.\n+        attribs = self._get_attributes_dict()\n+        attribs['name'] = result_name\n+        if 'freq' in attribs:\n+            attribs['freq'] = None\n+        return self._shallow_copy_with_infer(the_diff, **attribs)\n \n-        .. versionadded:: 0.20.0\n+    def _assert_can_do_setop(self, other):\n+        if not is_list_like(other):\n+            raise TypeError('Input must be Index or array-like')\n+        return True\n \n-        Returns\n-        -------\n-        numpy.ndarray\n-            Boolean array to indicate which entries are not NA.\n+    def _convert_can_do_setop(self, other):\n+        if not isinstance(other, Index):\n+            other = Index(other, name=self.name)\n+            result_name = self.name\n+        else:\n+            result_name = get_op_result_name(self, other)\n+        return other, result_name\n \n-        See Also\n-        --------\n-        Index.notnull : Alias of notna.\n-        Index.isna: Inverse of notna.\n-        pandas.notna : Top-level notna.\n+    # --------------------------------------------------------------------\n+    # Indexing Methods\n \n-        Examples\n-        --------\n-        Show which entries in an Index are not NA. The result is an\n-        array.\n+    _index_shared_docs['get_loc'] = \"\"\"\n+        Get integer location, slice or boolean mask for requested label.\n \n-        >>> idx = pd.Index([5.2, 6.0, np.NaN])\n-        >>> idx\n-        Float64Index([5.2, 6.0, nan], dtype='float64')\n-        >>> idx.notna()\n-        array([ True,  True, False])\n+        Parameters\n+        ----------\n+        key : label\n+        method : {None, 'pad'/'ffill', 'backfill'/'bfill', 'nearest'}, optional\n+            * default: exact matches only.\n+            * pad / ffill: find the PREVIOUS index value if no exact match.\n+            * backfill / bfill: use NEXT index value if no exact match\n+            * nearest: use the NEAREST index value if no exact match. Tied\n+              distances are broken by preferring the larger index value.\n+        tolerance : optional\n+            Maximum distance from index value for inexact matches. The value of\n+            the index at the matching location most satisfy the equation\n+            ``abs(index[loc] - key) <= tolerance``.\n \n-        Empty strings are not considered NA values. None is considered a NA\n-        value.\n+            Tolerance may be a scalar\n+            value, which applies the same tolerance to all values, or\n+            list-like, which applies variable tolerance per element. List-like\n+            includes list, tuple, array, Series, and must be the same size as\n+            the index and its dtype must exactly match the index's type.\n \n-        >>> idx = pd.Index(['black', '', 'red', None])\n-        >>> idx\n-        Index(['black', '', 'red', None], dtype='object')\n-        >>> idx.notna()\n-        array([ True,  True,  True, False])\n-        \"\"\"\n-        return ~self.isna()\n-    notnull = notna\n+            .. versionadded:: 0.21.0 (list-like tolerance)\n \n-    def putmask(self, mask, value):\n-        \"\"\"\n-        Return a new Index of the values set with the mask.\n+        Returns\n+        -------\n+        loc : int if unique index, slice if monotonic index, else mask\n \n-        See Also\n-        --------\n-        numpy.ndarray.putmask\n-        \"\"\"\n-        values = self.values.copy()\n-        try:\n-            np.putmask(values, mask, self._convert_for_op(value))\n-            return self._shallow_copy(values)\n-        except (ValueError, TypeError) as err:\n-            if is_object_dtype(self):\n-                raise err\n+        Examples\n+        ---------\n+        >>> unique_index = pd.Index(list('abc'))\n+        >>> unique_index.get_loc('b')\n+        1\n \n-            # coerces to object\n-            return self.astype(object).putmask(mask, value)\n+        >>> monotonic_index = pd.Index(list('abbc'))\n+        >>> monotonic_index.get_loc('b')\n+        slice(1, 3, None)\n \n-    def format(self, name=False, formatter=None, **kwargs):\n-        \"\"\"\n-        Render a string representation of the Index.\n+        >>> non_monotonic_index = pd.Index(list('abcb'))\n+        >>> non_monotonic_index.get_loc('b')\n+        array([False,  True, False,  True], dtype=bool)\n         \"\"\"\n-        header = []\n-        if name:\n-            header.append(pprint_thing(self.name,\n-                                       escape_chars=('\\t', '\\r', '\\n')) if\n-                          self.name is not None else '')\n \n-        if formatter is not None:\n-            return header + list(self.map(formatter))\n+    @Appender(_index_shared_docs['get_loc'])\n+    def get_loc(self, key, method=None, tolerance=None):\n+        if method is None:\n+            if tolerance is not None:\n+                raise ValueError('tolerance argument only valid if using pad, '\n+                                 'backfill or nearest lookups')\n+            try:\n+                return self._engine.get_loc(key)\n+            except KeyError:\n+                return self._engine.get_loc(self._maybe_cast_indexer(key))\n+        indexer = self.get_indexer([key], method=method, tolerance=tolerance)\n+        if indexer.ndim > 1 or indexer.size > 1:\n+            raise TypeError('get_loc requires scalar valued input')\n+        loc = indexer.item()\n+        if loc == -1:\n+            raise KeyError(key)\n+        return loc\n \n-        return self._format_with_header(header, **kwargs)\n+    _index_shared_docs['get_indexer'] = \"\"\"\n+        Compute indexer and mask for new index given the current index. The\n+        indexer should be then used as an input to ndarray.take to align the\n+        current data to the new index.\n \n-    def _format_with_header(self, header, na_rep='NaN', **kwargs):\n-        values = self.values\n+        Parameters\n+        ----------\n+        target : %(target_klass)s\n+        method : {None, 'pad'/'ffill', 'backfill'/'bfill', 'nearest'}, optional\n+            * default: exact matches only.\n+            * pad / ffill: find the PREVIOUS index value if no exact match.\n+            * backfill / bfill: use NEXT index value if no exact match\n+            * nearest: use the NEAREST index value if no exact match. Tied\n+              distances are broken by preferring the larger index value.\n+        limit : int, optional\n+            Maximum number of consecutive labels in ``target`` to match for\n+            inexact matches.\n+        tolerance : optional\n+            Maximum distance between original and new labels for inexact\n+            matches. The values of the index at the matching locations most\n+            satisfy the equation ``abs(index[indexer] - target) <= tolerance``.\n \n-        from pandas.io.formats.format import format_array\n+            Tolerance may be a scalar value, which applies the same tolerance\n+            to all values, or list-like, which applies variable tolerance per\n+            element. List-like includes list, tuple, array, Series, and must be\n+            the same size as the index and its dtype must exactly match the\n+            index's type.\n \n-        if is_categorical_dtype(values.dtype):\n-            values = np.array(values)\n+            .. versionadded:: 0.21.0 (list-like tolerance)\n \n-        elif is_object_dtype(values.dtype):\n-            values = lib.maybe_convert_objects(values, safe=1)\n+        Returns\n+        -------\n+        indexer : ndarray of int\n+            Integers from 0 to n - 1 indicating that the index at these\n+            positions matches the corresponding target values. Missing values\n+            in the target are marked by -1.\n \n-        if is_object_dtype(values.dtype):\n-            result = [pprint_thing(x, escape_chars=('\\t', '\\r', '\\n'))\n-                      for x in values]\n+        Examples\n+        --------\n+        >>> index = pd.Index(['c', 'a', 'b'])\n+        >>> index.get_indexer(['a', 'b', 'x'])\n+        array([ 1,  2, -1])\n \n-            # could have nans\n-            mask = isna(values)\n-            if mask.any():\n-                result = np.array(result)\n-                result[mask] = na_rep\n-                result = result.tolist()\n+        Notice that the return value is an array of locations in ``index``\n+        and ``x`` is marked by -1, as it is not in ``index``.\n+        \"\"\"\n \n-        else:\n-            result = _trim_front(format_array(values, None, justify='left'))\n-        return header + result\n+    @Appender(_index_shared_docs['get_indexer'] % _index_doc_kwargs)\n+    def get_indexer(self, target, method=None, limit=None, tolerance=None):\n+        method = missing.clean_reindex_fill_method(method)\n+        target = ensure_index(target)\n+        if tolerance is not None:\n+            tolerance = self._convert_tolerance(tolerance, target)\n \n-    def to_native_types(self, slicer=None, **kwargs):\n-        \"\"\"\n-        Format specified values of `self` and return them.\n+        # Treat boolean labels passed to a numeric index as not found. Without\n+        # this fix False and True would be treated as 0 and 1 respectively.\n+        # (GH #16877)\n+        if target.is_boolean() and self.is_numeric():\n+            return ensure_platform_int(np.repeat(-1, target.size))\n \n-        Parameters\n-        ----------\n-        slicer : int, array-like\n-            An indexer into `self` that specifies which values\n-            are used in the formatting process.\n-        kwargs : dict\n-            Options for specifying how the values should be formatted.\n-            These options include the following:\n+        pself, ptarget = self._maybe_promote(target)\n+        if pself is not self or ptarget is not target:\n+            return pself.get_indexer(ptarget, method=method, limit=limit,\n+                                     tolerance=tolerance)\n \n-            1) na_rep : str\n-                The value that serves as a placeholder for NULL values\n-            2) quoting : bool or None\n-                Whether or not there are quoted values in `self`\n-            3) date_format : str\n-                The format used to represent date-like values\n-        \"\"\"\n+        if not is_dtype_equal(self.dtype, target.dtype):\n+            this = self.astype(object)\n+            target = target.astype(object)\n+            return this.get_indexer(target, method=method, limit=limit,\n+                                    tolerance=tolerance)\n \n-        values = self\n-        if slicer is not None:\n-            values = values[slicer]\n-        return values._format_native_types(**kwargs)\n+        if not self.is_unique:\n+            raise InvalidIndexError('Reindexing only valid with uniquely'\n+                                    ' valued Index objects')\n \n-    def _format_native_types(self, na_rep='', quoting=None, **kwargs):\n-        \"\"\"\n-        Actually format specific types of the index.\n-        \"\"\"\n-        mask = isna(self)\n-        if not self.is_object() and not quoting:\n-            values = np.asarray(self).astype(str)\n+        if method == 'pad' or method == 'backfill':\n+            indexer = self._get_fill_indexer(target, method, limit, tolerance)\n+        elif method == 'nearest':\n+            indexer = self._get_nearest_indexer(target, limit, tolerance)\n         else:\n-            values = np.array(self, dtype=object, copy=True)\n-\n-        values[mask] = na_rep\n-        return values\n+            if tolerance is not None:\n+                raise ValueError('tolerance argument only valid if doing pad, '\n+                                 'backfill or nearest reindexing')\n+            if limit is not None:\n+                raise ValueError('limit argument only valid if doing pad, '\n+                                 'backfill or nearest reindexing')\n \n-    def equals(self, other):\n-        \"\"\"\n-        Determines if two Index objects contain the same elements.\n-        \"\"\"\n-        if self.is_(other):\n-            return True\n+            indexer = self._engine.get_indexer(target._ndarray_values)\n \n-        if not isinstance(other, Index):\n-            return False\n+        return ensure_platform_int(indexer)\n \n-        if is_object_dtype(self) and not is_object_dtype(other):\n-            # if other is not object, use other's logic for coercion\n-            return other.equals(self)\n+    def _convert_tolerance(self, tolerance, target):\n+        # override this method on subclasses\n+        tolerance = np.asarray(tolerance)\n+        if target.size != tolerance.size and tolerance.size > 1:\n+            raise ValueError('list-like tolerance size must match '\n+                             'target index size')\n+        return tolerance\n \n-        try:\n-            return array_equivalent(com.values_from_object(self),\n-                                    com.values_from_object(other))\n-        except Exception:\n-            return False\n+    def _get_fill_indexer(self, target, method, limit=None, tolerance=None):\n+        if self.is_monotonic_increasing and target.is_monotonic_increasing:\n+            method = (self._engine.get_pad_indexer if method == 'pad' else\n+                      self._engine.get_backfill_indexer)\n+            indexer = method(target._ndarray_values, limit)\n+        else:\n+            indexer = self._get_fill_indexer_searchsorted(target, method,\n+                                                          limit)\n+        if tolerance is not None:\n+            indexer = self._filter_indexer_tolerance(target._ndarray_values,\n+                                                     indexer,\n+                                                     tolerance)\n+        return indexer\n \n-    def identical(self, other):\n-        \"\"\"\n-        Similar to equals, but check that other comparable attributes are\n-        also equal.\n+    def _get_fill_indexer_searchsorted(self, target, method, limit=None):\n         \"\"\"\n-        return (self.equals(other) and\n-                all((getattr(self, c, None) == getattr(other, c, None)\n-                     for c in self._comparables)) and\n-                type(self) == type(other))\n-\n-    def asof(self, label):\n+        Fallback pad/backfill get_indexer that works for monotonic decreasing\n+        indexes and non-monotonic targets.\n         \"\"\"\n-        Return the label from the index, or, if not present, the previous one.\n-\n-        Assuming that the index is sorted, return the passed index label if it\n-        is in the index, or return the previous index label if the passed one\n-        is not in the index.\n+        if limit is not None:\n+            raise ValueError('limit argument for %r method only well-defined '\n+                             'if index and target are monotonic' % method)\n \n-        Parameters\n-        ----------\n-        label : object\n-            The label up to which the method returns the latest index label.\n+        side = 'left' if method == 'pad' else 'right'\n \n-        Returns\n-        -------\n-        object\n-            The passed label if it is in the index. The previous label if the\n-            passed label is not in the sorted index or `NaN` if there is no\n-            such label.\n-\n-        See Also\n-        --------\n-        Series.asof : Return the latest value in a Series up to the\n-            passed index.\n-        merge_asof : Perform an asof merge (similar to left join but it\n-            matches on nearest key rather than equal key).\n-        Index.get_loc : An `asof` is a thin wrapper around `get_loc`\n-            with method='pad'.\n-\n-        Examples\n-        --------\n-        `Index.asof` returns the latest index label up to the passed label.\n+        # find exact matches first (this simplifies the algorithm)\n+        indexer = self.get_indexer(target)\n+        nonexact = (indexer == -1)\n+        indexer[nonexact] = self._searchsorted_monotonic(target[nonexact],\n+                                                         side)\n+        if side == 'left':\n+            # searchsorted returns \"indices into a sorted array such that,\n+            # if the corresponding elements in v were inserted before the\n+            # indices, the order of a would be preserved\".\n+            # Thus, we need to subtract 1 to find values to the left.\n+            indexer[nonexact] -= 1\n+            # This also mapped not found values (values of 0 from\n+            # np.searchsorted) to -1, which conveniently is also our\n+            # sentinel for missing values\n+        else:\n+            # Mark indices to the right of the largest value as not found\n+            indexer[indexer == len(self)] = -1\n+        return indexer\n \n-        >>> idx = pd.Index(['2013-12-31', '2014-01-02', '2014-01-03'])\n-        >>> idx.asof('2014-01-01')\n-        '2013-12-31'\n+    def _get_nearest_indexer(self, target, limit, tolerance):\n+        \"\"\"\n+        Get the indexer for the nearest index labels; requires an index with\n+        values that can be subtracted from each other (e.g., not strings or\n+        tuples).\n+        \"\"\"\n+        left_indexer = self.get_indexer(target, 'pad', limit=limit)\n+        right_indexer = self.get_indexer(target, 'backfill', limit=limit)\n \n-        If the label is in the index, the method returns the passed label.\n+        target = np.asarray(target)\n+        left_distances = abs(self.values[left_indexer] - target)\n+        right_distances = abs(self.values[right_indexer] - target)\n \n-        >>> idx.asof('2014-01-02')\n-        '2014-01-02'\n+        op = operator.lt if self.is_monotonic_increasing else operator.le\n+        indexer = np.where(op(left_distances, right_distances) |\n+                           (right_indexer == -1), left_indexer, right_indexer)\n+        if tolerance is not None:\n+            indexer = self._filter_indexer_tolerance(target, indexer,\n+                                                     tolerance)\n+        return indexer\n \n-        If all of the labels in the index are later than the passed label,\n-        NaN is returned.\n+    def _filter_indexer_tolerance(self, target, indexer, tolerance):\n+        distance = abs(self.values[indexer] - target)\n+        indexer = np.where(distance <= tolerance, indexer, -1)\n+        return indexer\n \n-        >>> idx.asof('1999-01-02')\n-        nan\n+    # --------------------------------------------------------------------\n+    # Indexer Conversion Methods\n \n-        If the index is not sorted, an error is raised.\n+    _index_shared_docs['_convert_scalar_indexer'] = \"\"\"\n+        Convert a scalar indexer.\n \n-        >>> idx_not_sorted = pd.Index(['2013-12-31', '2015-01-02',\n-        ...                            '2014-01-03'])\n-        >>> idx_not_sorted.asof('2013-12-31')\n-        Traceback (most recent call last):\n-        ValueError: index must be monotonic increasing or decreasing\n-        \"\"\"\n-        try:\n-            loc = self.get_loc(label, method='pad')\n-        except KeyError:\n-            return self._na_value\n-        else:\n-            if isinstance(loc, slice):\n-                loc = loc.indices(len(self))[-1]\n-            return self[loc]\n+        Parameters\n+        ----------\n+        key : label of the slice bound\n+        kind : {'ix', 'loc', 'getitem', 'iloc'} or None\n+    \"\"\"\n \n-    def asof_locs(self, where, mask):\n-        \"\"\"\n-        Finds the locations (indices) of the labels from the index for\n-        every entry in the `where` argument.\n+    @Appender(_index_shared_docs['_convert_scalar_indexer'])\n+    def _convert_scalar_indexer(self, key, kind=None):\n+        assert kind in ['ix', 'loc', 'getitem', 'iloc', None]\n \n-        As in the `asof` function, if the label (a particular entry in\n-        `where`) is not in the index, the latest index label upto the\n-        passed label is chosen and its index returned.\n+        if kind == 'iloc':\n+            return self._validate_indexer('positional', key, kind)\n \n-        If all of the labels in the index are later than a label in `where`,\n-        -1 is returned.\n+        if len(self) and not isinstance(self, ABCMultiIndex,):\n \n-        `mask` is used to ignore NA values in the index during calculation.\n+            # we can raise here if we are definitive that this\n+            # is positional indexing (eg. .ix on with a float)\n+            # or label indexing if we are using a type able\n+            # to be represented in the index\n \n-        Parameters\n-        ----------\n-        where : Index\n-            An Index consisting of an array of timestamps.\n-        mask : array-like\n-            Array of booleans denoting where values in the original\n-            data are not NA.\n+            if kind in ['getitem', 'ix'] and is_float(key):\n+                if not self.is_floating():\n+                    return self._invalid_indexer('label', key)\n \n-        Returns\n-        -------\n-        numpy.ndarray\n-            An array of locations (indices) of the labels from the Index\n-            which correspond to the return values of the `asof` function\n-            for every element in `where`.\n-        \"\"\"\n-        locs = self.values[mask].searchsorted(where.values, side='right')\n-        locs = np.where(locs > 0, locs - 1, 0)\n+            elif kind in ['loc'] and is_float(key):\n \n-        result = np.arange(len(self))[mask].take(locs)\n+                # we want to raise KeyError on string/mixed here\n+                # technically we *could* raise a TypeError\n+                # on anything but mixed though\n+                if self.inferred_type not in ['floating',\n+                                              'mixed-integer-float',\n+                                              'string',\n+                                              'unicode',\n+                                              'mixed']:\n+                    return self._invalid_indexer('label', key)\n \n-        first = mask.argmax()\n-        result[(locs == 0) & (where.values < self.values[first])] = -1\n+            elif kind in ['loc'] and is_integer(key):\n+                if not self.holds_integer():\n+                    return self._invalid_indexer('label', key)\n \n-        return result\n+        return key\n \n-    def sort_values(self, return_indexer=False, ascending=True):\n-        \"\"\"\n-        Return a sorted copy of the index.\n+    _index_shared_docs['_convert_slice_indexer'] = \"\"\"\n+        Convert a slice indexer.\n \n-        Return a sorted copy of the index, and optionally return the indices\n-        that sorted the index itself.\n+        By definition, these are labels unless 'iloc' is passed in.\n+        Floats are not allowed as the start, step, or stop of the slice.\n \n         Parameters\n         ----------\n-        return_indexer : bool, default False\n-            Should the indices that would sort the index be returned.\n-        ascending : bool, default True\n-            Should the index values be sorted in an ascending order.\n+        key : label of the slice bound\n+        kind : {'ix', 'loc', 'getitem', 'iloc'} or None\n+    \"\"\"\n \n-        Returns\n-        -------\n-        sorted_index : pandas.Index\n-            Sorted copy of the index.\n-        indexer : numpy.ndarray, optional\n-            The indices that the index itself was sorted by.\n+    @Appender(_index_shared_docs['_convert_slice_indexer'])\n+    def _convert_slice_indexer(self, key, kind=None):\n+        assert kind in ['ix', 'loc', 'getitem', 'iloc', None]\n \n-        See Also\n-        --------\n-        pandas.Series.sort_values : Sort values of a Series.\n-        pandas.DataFrame.sort_values : Sort values in a DataFrame.\n+        # if we are not a slice, then we are done\n+        if not isinstance(key, slice):\n+            return key\n \n-        Examples\n-        --------\n-        >>> idx = pd.Index([10, 100, 1, 1000])\n-        >>> idx\n-        Int64Index([10, 100, 1, 1000], dtype='int64')\n+        # validate iloc\n+        if kind == 'iloc':\n+            return slice(self._validate_indexer('slice', key.start, kind),\n+                         self._validate_indexer('slice', key.stop, kind),\n+                         self._validate_indexer('slice', key.step, kind))\n \n-        Sort values in ascending order (default behavior).\n+        # potentially cast the bounds to integers\n+        start, stop, step = key.start, key.stop, key.step\n \n-        >>> idx.sort_values()\n-        Int64Index([1, 10, 100, 1000], dtype='int64')\n+        # figure out if this is a positional indexer\n+        def is_int(v):\n+            return v is None or is_integer(v)\n \n-        Sort values in descending order, and also get the indices `idx` was\n-        sorted by.\n+        is_null_slicer = start is None and stop is None\n+        is_index_slice = is_int(start) and is_int(stop)\n+        is_positional = is_index_slice and not self.is_integer()\n \n-        >>> idx.sort_values(ascending=False, return_indexer=True)\n-        (Int64Index([1000, 100, 10, 1], dtype='int64'), array([3, 1, 0, 2]))\n-        \"\"\"\n-        _as = self.argsort()\n-        if not ascending:\n-            _as = _as[::-1]\n+        if kind == 'getitem':\n+            \"\"\"\n+            called from the getitem slicers, validate that we are in fact\n+            integers\n+            \"\"\"\n+            if self.is_integer() or is_index_slice:\n+                return slice(self._validate_indexer('slice', key.start, kind),\n+                             self._validate_indexer('slice', key.stop, kind),\n+                             self._validate_indexer('slice', key.step, kind))\n \n-        sorted_index = self.take(_as)\n+        # convert the slice to an indexer here\n \n-        if return_indexer:\n-            return sorted_index, _as\n+        # if we are mixed and have integers\n+        try:\n+            if is_positional and self.is_mixed():\n+                # Validate start & stop\n+                if start is not None:\n+                    self.get_loc(start)\n+                if stop is not None:\n+                    self.get_loc(stop)\n+                is_positional = False\n+        except KeyError:\n+            if self.inferred_type == 'mixed-integer-float':\n+                raise\n+\n+        if is_null_slicer:\n+            indexer = key\n+        elif is_positional:\n+            indexer = key\n         else:\n-            return sorted_index\n+            try:\n+                indexer = self.slice_indexer(start, stop, step, kind=kind)\n+            except Exception:\n+                if is_index_slice:\n+                    if self.is_integer():\n+                        raise\n+                    else:\n+                        indexer = key\n+                else:\n+                    raise\n \n-    def sort(self, *args, **kwargs):\n-        raise TypeError(\"cannot sort an Index object in-place, use \"\n-                        \"sort_values instead\")\n+        return indexer\n \n-    def sortlevel(self, level=None, ascending=True, sort_remaining=None):\n+    def _convert_listlike_indexer(self, keyarr, kind=None):\n         \"\"\"\n-        For internal compatibility with with the Index API.\n-\n-        Sort the Index. This is for compat with MultiIndex\n-\n         Parameters\n         ----------\n-        ascending : boolean, default True\n-            False to sort in descending order\n-\n-        level, sort_remaining are compat parameters\n+        keyarr : list-like\n+            Indexer to convert.\n \n         Returns\n         -------\n-        sorted_index : Index\n+        tuple (indexer, keyarr)\n+            indexer is an ndarray or None if cannot convert\n+            keyarr are tuple-safe keys\n         \"\"\"\n-        return self.sort_values(return_indexer=True, ascending=ascending)\n+        if isinstance(keyarr, Index):\n+            keyarr = self._convert_index_indexer(keyarr)\n+        else:\n+            keyarr = self._convert_arr_indexer(keyarr)\n \n-    def shift(self, periods=1, freq=None):\n-        \"\"\"\n-        Shift index by desired number of time frequency increments.\n+        indexer = self._convert_list_indexer(keyarr, kind=kind)\n+        return indexer, keyarr\n \n-        This method is for shifting the values of datetime-like indexes\n-        by a specified time increment a given number of times.\n+    _index_shared_docs['_convert_arr_indexer'] = \"\"\"\n+        Convert an array-like indexer to the appropriate dtype.\n \n         Parameters\n         ----------\n-        periods : int, default 1\n-            Number of periods (or increments) to shift by,\n-            can be positive or negative.\n-        freq : pandas.DateOffset, pandas.Timedelta or string, optional\n-            Frequency increment to shift by.\n-            If None, the index is shifted by its own `freq` attribute.\n-            Offset aliases are valid strings, e.g., 'D', 'W', 'M' etc.\n+        keyarr : array-like\n+            Indexer to convert.\n \n         Returns\n         -------\n-        pandas.Index\n-            shifted index\n-\n-        See Also\n-        --------\n-        Series.shift : Shift values of Series.\n-\n-        Examples\n-        --------\n-        Put the first 5 month starts of 2011 into an index.\n-\n-        >>> month_starts = pd.date_range('1/1/2011', periods=5, freq='MS')\n-        >>> month_starts\n-        DatetimeIndex(['2011-01-01', '2011-02-01', '2011-03-01', '2011-04-01',\n-                       '2011-05-01'],\n-                      dtype='datetime64[ns]', freq='MS')\n-\n-        Shift the index by 10 days.\n-\n-        >>> month_starts.shift(10, freq='D')\n-        DatetimeIndex(['2011-01-11', '2011-02-11', '2011-03-11', '2011-04-11',\n-                       '2011-05-11'],\n-                      dtype='datetime64[ns]', freq=None)\n-\n-        The default value of `freq` is the `freq` attribute of the index,\n-        which is 'MS' (month start) in this example.\n-\n-        >>> month_starts.shift(10)\n-        DatetimeIndex(['2011-11-01', '2011-12-01', '2012-01-01', '2012-02-01',\n-                       '2012-03-01'],\n-                      dtype='datetime64[ns]', freq='MS')\n+        converted_keyarr : array-like\n+    \"\"\"\n \n-        Notes\n-        -----\n-        This method is only implemented for datetime-like index classes,\n-        i.e., DatetimeIndex, PeriodIndex and TimedeltaIndex.\n-        \"\"\"\n-        raise NotImplementedError(\"Not supported for type %s\" %\n-                                  type(self).__name__)\n+    @Appender(_index_shared_docs['_convert_arr_indexer'])\n+    def _convert_arr_indexer(self, keyarr):\n+        keyarr = com.asarray_tuplesafe(keyarr)\n+        return keyarr\n \n-    def argsort(self, *args, **kwargs):\n-        \"\"\"\n-        Return the integer indices that would sort the index.\n+    _index_shared_docs['_convert_index_indexer'] = \"\"\"\n+        Convert an Index indexer to the appropriate dtype.\n \n         Parameters\n         ----------\n-        *args\n-            Passed to `numpy.ndarray.argsort`.\n-        **kwargs\n-            Passed to `numpy.ndarray.argsort`.\n+        keyarr : Index (or sub-class)\n+            Indexer to convert.\n \n         Returns\n         -------\n-        numpy.ndarray\n-            Integer indices that would sort the index if used as\n-            an indexer.\n+        converted_keyarr : Index (or sub-class)\n+    \"\"\"\n \n-        See Also\n-        --------\n-        numpy.argsort : Similar method for NumPy arrays.\n-        Index.sort_values : Return sorted copy of Index.\n+    @Appender(_index_shared_docs['_convert_index_indexer'])\n+    def _convert_index_indexer(self, keyarr):\n+        return keyarr\n \n-        Examples\n-        --------\n-        >>> idx = pd.Index(['b', 'a', 'd', 'c'])\n-        >>> idx\n-        Index(['b', 'a', 'd', 'c'], dtype='object')\n+    _index_shared_docs['_convert_list_indexer'] = \"\"\"\n+        Convert a list-like indexer to the appropriate dtype.\n \n-        >>> order = idx.argsort()\n-        >>> order\n-        array([1, 0, 3, 2])\n+        Parameters\n+        ----------\n+        keyarr : Index (or sub-class)\n+            Indexer to convert.\n+        kind : iloc, ix, loc, optional\n \n-        >>> idx[order]\n-        Index(['a', 'b', 'c', 'd'], dtype='object')\n-        \"\"\"\n-        result = self.asi8\n-        if result is None:\n-            result = np.array(self)\n-        return result.argsort(*args, **kwargs)\n+        Returns\n+        -------\n+        positional indexer or None\n+    \"\"\"\n \n-    def __add__(self, other):\n-        if isinstance(other, (ABCSeries, ABCDataFrame)):\n-            return NotImplemented\n-        return Index(np.array(self) + other)\n+    @Appender(_index_shared_docs['_convert_list_indexer'])\n+    def _convert_list_indexer(self, keyarr, kind=None):\n+        if (kind in [None, 'iloc', 'ix'] and\n+                is_integer_dtype(keyarr) and not self.is_floating() and\n+                not isinstance(keyarr, ABCPeriodIndex)):\n \n-    def __radd__(self, other):\n-        return Index(other + np.array(self))\n+            if self.inferred_type == 'mixed-integer':\n+                indexer = self.get_indexer(keyarr)\n+                if (indexer >= 0).all():\n+                    return indexer\n+                # missing values are flagged as -1 by get_indexer and negative\n+                # indices are already converted to positive indices in the\n+                # above if-statement, so the negative flags are changed to\n+                # values outside the range of indices so as to trigger an\n+                # IndexError in maybe_convert_indices\n+                indexer[indexer < 0] = len(self)\n+                from pandas.core.indexing import maybe_convert_indices\n+                return maybe_convert_indices(indexer, len(self))\n \n-    def __iadd__(self, other):\n-        # alias for __add__\n-        return self + other\n+            elif not self.inferred_type == 'integer':\n+                keyarr = np.where(keyarr < 0, len(self) + keyarr, keyarr)\n+                return keyarr\n \n-    def __sub__(self, other):\n-        return Index(np.array(self) - other)\n+        return None\n \n-    def __rsub__(self, other):\n-        return Index(other - np.array(self))\n+    def _invalid_indexer(self, form, key):\n+        \"\"\"\n+        Consistent invalid indexer message.\n+        \"\"\"\n+        raise TypeError(\"cannot do {form} indexing on {klass} with these \"\n+                        \"indexers [{key}] of {kind}\".format(\n+                            form=form, klass=type(self), key=key,\n+                            kind=type(key)))\n \n-    def __and__(self, other):\n-        return self.intersection(other)\n+    # --------------------------------------------------------------------\n+    # Reindex Methods\n \n-    def __or__(self, other):\n-        return self.union(other)\n+    def _can_reindex(self, indexer):\n+        \"\"\"\n+        Check if we are allowing reindexing with this particular indexer.\n \n-    def __xor__(self, other):\n-        return self.symmetric_difference(other)\n+        Parameters\n+        ----------\n+        indexer : an integer indexer\n \n-    def _get_reconciled_name_object(self, other):\n-        \"\"\"\n-        If the result of a set operation will be self,\n-        return self, unless the name changes, in which\n-        case make a shallow copy of self.\n+        Raises\n+        ------\n+        ValueError if its a duplicate axis\n         \"\"\"\n-        name = get_op_result_name(self, other)\n-        if self.name != name:\n-            return self._shallow_copy(name=name)\n-        return self\n \n-    def union(self, other):\n+        # trying to reindex on an axis with duplicates\n+        if not self.is_unique and len(indexer):\n+            raise ValueError(\"cannot reindex from a duplicate axis\")\n+\n+    def reindex(self, target, method=None, level=None, limit=None,\n+                tolerance=None):\n         \"\"\"\n-        Form the union of two Index objects and sorts if possible.\n+        Create index with target's values (move/add/delete values\n+        as necessary).\n \n         Parameters\n         ----------\n-        other : Index or array-like\n+        target : an iterable\n \n         Returns\n         -------\n-        union : Index\n-\n-        Examples\n-        --------\n+        new_index : pd.Index\n+            Resulting index\n+        indexer : np.ndarray or None\n+            Indices of output values in original index\n \n-        >>> idx1 = pd.Index([1, 2, 3, 4])\n-        >>> idx2 = pd.Index([3, 4, 5, 6])\n-        >>> idx1.union(idx2)\n-        Int64Index([1, 2, 3, 4, 5, 6], dtype='int64')\n         \"\"\"\n-        self._assert_can_do_setop(other)\n-        other = ensure_index(other)\n-\n-        if len(other) == 0 or self.equals(other):\n-            return self._get_reconciled_name_object(other)\n-\n-        if len(self) == 0:\n-            return other._get_reconciled_name_object(self)\n+        # GH6552: preserve names when reindexing to non-named target\n+        # (i.e. neither Index nor Series).\n+        preserve_names = not hasattr(target, 'name')\n \n-        # TODO: is_dtype_union_equal is a hack around\n-        # 1. buggy set ops with duplicates (GH #13432)\n-        # 2. CategoricalIndex lacking setops (GH #10186)\n-        # Once those are fixed, this workaround can be removed\n-        if not is_dtype_union_equal(self.dtype, other.dtype):\n-            this = self.astype('O')\n-            other = other.astype('O')\n-            return this.union(other)\n+        # GH7774: preserve dtype/tz if target is empty and not an Index.\n+        target = _ensure_has_len(target)  # target may be an iterator\n \n-        # TODO(EA): setops-refactor, clean all this up\n-        if is_period_dtype(self) or is_datetime64tz_dtype(self):\n-            lvals = self._ndarray_values\n-        else:\n-            lvals = self._values\n-        if is_period_dtype(other) or is_datetime64tz_dtype(other):\n-            rvals = other._ndarray_values\n+        if not isinstance(target, Index) and len(target) == 0:\n+            attrs = self._get_attributes_dict()\n+            attrs.pop('freq', None)  # don't preserve freq\n+            values = self._data[:0]  # appropriately-dtyped empty array\n+            target = self._simple_new(values, dtype=self.dtype, **attrs)\n         else:\n-            rvals = other._values\n-\n-        if self.is_monotonic and other.is_monotonic:\n-            try:\n-                result = self._outer_indexer(lvals, rvals)[0]\n-            except TypeError:\n-                # incomparable objects\n-                result = list(lvals)\n+            target = ensure_index(target)\n \n-                # worth making this faster? a very unusual case\n-                value_set = set(lvals)\n-                result.extend([x for x in rvals if x not in value_set])\n+        if level is not None:\n+            if method is not None:\n+                raise TypeError('Fill method not supported if level passed')\n+            _, indexer, _ = self._join_level(target, level, how='right',\n+                                             return_indexers=True)\n         else:\n-            indexer = self.get_indexer(other)\n-            indexer, = (indexer == -1).nonzero()\n-\n-            if len(indexer) > 0:\n-                other_diff = algos.take_nd(rvals, indexer,\n-                                           allow_fill=False)\n-                result = _concat._concat_compat((lvals, other_diff))\n-\n-                try:\n-                    lvals[0] < other_diff[0]\n-                except TypeError as e:\n-                    warnings.warn(\"%s, sort order is undefined for \"\n-                                  \"incomparable objects\" % e, RuntimeWarning,\n-                                  stacklevel=3)\n-                else:\n-                    types = frozenset((self.inferred_type,\n-                                       other.inferred_type))\n-                    if not types & _unsortable_types:\n-                        result.sort()\n-\n+            if self.equals(target):\n+                indexer = None\n             else:\n-                result = lvals\n \n-                try:\n-                    result = np.sort(result)\n-                except TypeError as e:\n-                    warnings.warn(\"%s, sort order is undefined for \"\n-                                  \"incomparable objects\" % e, RuntimeWarning,\n-                                  stacklevel=3)\n+                if self.is_unique:\n+                    indexer = self.get_indexer(target, method=method,\n+                                               limit=limit,\n+                                               tolerance=tolerance)\n+                else:\n+                    if method is not None or limit is not None:\n+                        raise ValueError(\"cannot reindex a non-unique index \"\n+                                         \"with a method or limit\")\n+                    indexer, missing = self.get_indexer_non_unique(target)\n \n-        # for subclasses\n-        return self._wrap_setop_result(other, result)\n+        if preserve_names and target.nlevels == 1 and target.name != self.name:\n+            target = target.copy()\n+            target.name = self.name\n \n-    def _wrap_setop_result(self, other, result):\n-        return self._constructor(result, name=get_op_result_name(self, other))\n+        return target, indexer\n \n-    def intersection(self, other):\n+    def _reindex_non_unique(self, target):\n         \"\"\"\n-        Form the intersection of two Index objects.\n-\n-        This returns a new Index with elements common to the index and `other`,\n-        preserving the order of the calling index.\n+        Create a new index with target's values (move/add/delete values as\n+        necessary) use with non-unique Index and a possibly non-unique target.\n \n         Parameters\n         ----------\n-        other : Index or array-like\n+        target : an iterable\n \n         Returns\n         -------\n-        intersection : Index\n-\n-        Examples\n-        --------\n+        new_index : pd.Index\n+            Resulting index\n+        indexer : np.ndarray or None\n+            Indices of output values in original index\n \n-        >>> idx1 = pd.Index([1, 2, 3, 4])\n-        >>> idx2 = pd.Index([3, 4, 5, 6])\n-        >>> idx1.intersection(idx2)\n-        Int64Index([3, 4], dtype='int64')\n         \"\"\"\n-        self._assert_can_do_setop(other)\n-        other = ensure_index(other)\n \n-        if self.equals(other):\n-            return self._get_reconciled_name_object(other)\n+        target = ensure_index(target)\n+        indexer, missing = self.get_indexer_non_unique(target)\n+        check = indexer != -1\n+        new_labels = self.take(indexer[check])\n+        new_indexer = None\n \n-        if not is_dtype_equal(self.dtype, other.dtype):\n-            this = self.astype('O')\n-            other = other.astype('O')\n-            return this.intersection(other)\n+        if len(missing):\n+            length = np.arange(len(indexer))\n \n-        # TODO(EA): setops-refactor, clean all this up\n-        if is_period_dtype(self):\n-            lvals = self._ndarray_values\n-        else:\n-            lvals = self._values\n-        if is_period_dtype(other):\n-            rvals = other._ndarray_values\n-        else:\n-            rvals = other._values\n+            missing = ensure_platform_int(missing)\n+            missing_labels = target.take(missing)\n+            missing_indexer = ensure_int64(length[~check])\n+            cur_labels = self.take(indexer[check]).values\n+            cur_indexer = ensure_int64(length[check])\n \n-        if self.is_monotonic and other.is_monotonic:\n-            try:\n-                result = self._inner_indexer(lvals, rvals)[0]\n-                return self._wrap_setop_result(other, result)\n-            except TypeError:\n-                pass\n+            new_labels = np.empty(tuple([len(indexer)]), dtype=object)\n+            new_labels[cur_indexer] = cur_labels\n+            new_labels[missing_indexer] = missing_labels\n \n-        try:\n-            indexer = Index(rvals).get_indexer(lvals)\n-            indexer = indexer.take((indexer != -1).nonzero()[0])\n-        except Exception:\n-            # duplicates\n-            indexer = algos.unique1d(\n-                Index(rvals).get_indexer_non_unique(lvals)[0])\n-            indexer = indexer[indexer != -1]\n+            # a unique indexer\n+            if target.is_unique:\n \n-        taken = other.take(indexer)\n-        if self.name != other.name:\n-            taken.name = None\n-        return taken\n+                # see GH5553, make sure we use the right indexer\n+                new_indexer = np.arange(len(indexer))\n+                new_indexer[cur_indexer] = np.arange(len(cur_labels))\n+                new_indexer[missing_indexer] = -1\n \n-    def difference(self, other, sort=True):\n-        \"\"\"\n-        Return a new Index with elements from the index that are not in\n-        `other`.\n+            # we have a non_unique selector, need to use the original\n+            # indexer here\n+            else:\n \n-        This is the set difference of two Index objects.\n+                # need to retake to have the same size as the indexer\n+                indexer[~check] = -1\n+\n+                # reset the new indexer to account for the new size\n+                new_indexer = np.arange(len(self.take(indexer)))\n+                new_indexer[~check] = -1\n+\n+        new_index = self._shallow_copy_with_infer(new_labels, freq=None)\n+        return new_index, indexer, new_indexer\n+\n+    # --------------------------------------------------------------------\n+    # Join Methods\n+\n+    _index_shared_docs['join'] = \"\"\"\n+        Compute join_index and indexers to conform data\n+        structures to the new index.\n \n         Parameters\n         ----------\n-        other : Index or array-like\n-        sort : bool, default True\n-            Sort the resulting index if possible\n+        other : Index\n+        how : {'left', 'right', 'inner', 'outer'}\n+        level : int or level name, default None\n+        return_indexers : boolean, default False\n+        sort : boolean, default False\n+            Sort the join keys lexicographically in the result Index. If False,\n+            the order of the join keys depends on the join type (how keyword)\n \n-            .. versionadded:: 0.24.0\n+            .. versionadded:: 0.20.0\n \n         Returns\n         -------\n-        difference : Index\n-\n-        Examples\n-        --------\n-\n-        >>> idx1 = pd.Index([2, 1, 3, 4])\n-        >>> idx2 = pd.Index([3, 4, 5, 6])\n-        >>> idx1.difference(idx2)\n-        Int64Index([1, 2], dtype='int64')\n-        >>> idx1.difference(idx2, sort=False)\n-        Int64Index([2, 1], dtype='int64')\n+        join_index, (left_indexer, right_indexer)\n         \"\"\"\n-        self._assert_can_do_setop(other)\n \n-        if self.equals(other):\n-            # pass an empty np.ndarray with the appropriate dtype\n-            return self._shallow_copy(self._data[:0])\n+    @Appender(_index_shared_docs['join'])\n+    def join(self, other, how='left', level=None, return_indexers=False,\n+             sort=False):\n+        from .multi import MultiIndex\n+        self_is_mi = isinstance(self, MultiIndex)\n+        other_is_mi = isinstance(other, MultiIndex)\n \n-        other, result_name = self._convert_can_do_setop(other)\n+        # try to figure out the join level\n+        # GH3662\n+        if level is None and (self_is_mi or other_is_mi):\n \n-        this = self._get_unique_index()\n+            # have the same levels/names so a simple join\n+            if self.names == other.names:\n+                pass\n+            else:\n+                return self._join_multi(other, how=how,\n+                                        return_indexers=return_indexers)\n \n-        indexer = this.get_indexer(other)\n-        indexer = indexer.take((indexer != -1).nonzero()[0])\n+        # join on the level\n+        if level is not None and (self_is_mi or other_is_mi):\n+            return self._join_level(other, level, how=how,\n+                                    return_indexers=return_indexers)\n \n-        label_diff = np.setdiff1d(np.arange(this.size), indexer,\n-                                  assume_unique=True)\n-        the_diff = this.values.take(label_diff)\n-        if sort:\n-            try:\n-                the_diff = sorting.safe_sort(the_diff)\n-            except TypeError:\n-                pass\n+        other = ensure_index(other)\n \n-        return this._shallow_copy(the_diff, name=result_name, freq=None)\n+        if len(other) == 0 and how in ('left', 'outer'):\n+            join_index = self._shallow_copy()\n+            if return_indexers:\n+                rindexer = np.repeat(-1, len(join_index))\n+                return join_index, None, rindexer\n+            else:\n+                return join_index\n \n-    def symmetric_difference(self, other, result_name=None):\n-        \"\"\"\n-        Compute the symmetric difference of two Index objects.\n+        if len(self) == 0 and how in ('right', 'outer'):\n+            join_index = other._shallow_copy()\n+            if return_indexers:\n+                lindexer = np.repeat(-1, len(join_index))\n+                return join_index, lindexer, None\n+            else:\n+                return join_index\n \n-        It's sorted if sorting is possible.\n+        if self._join_precedence < other._join_precedence:\n+            how = {'right': 'left', 'left': 'right'}.get(how, how)\n+            result = other.join(self, how=how, level=level,\n+                                return_indexers=return_indexers)\n+            if return_indexers:\n+                x, y, z = result\n+                result = x, z, y\n+            return result\n \n-        Parameters\n-        ----------\n-        other : Index or array-like\n-        result_name : str\n+        if not is_dtype_equal(self.dtype, other.dtype):\n+            this = self.astype('O')\n+            other = other.astype('O')\n+            return this.join(other, how=how, return_indexers=return_indexers)\n \n-        Returns\n-        -------\n-        symmetric_difference : Index\n+        _validate_join_method(how)\n \n-        Notes\n-        -----\n-        ``symmetric_difference`` contains elements that appear in either\n-        ``idx1`` or ``idx2`` but not both. Equivalent to the Index created by\n-        ``idx1.difference(idx2) | idx2.difference(idx1)`` with duplicates\n-        dropped.\n+        if not self.is_unique and not other.is_unique:\n+            return self._join_non_unique(other, how=how,\n+                                         return_indexers=return_indexers)\n+        elif not self.is_unique or not other.is_unique:\n+            if self.is_monotonic and other.is_monotonic:\n+                return self._join_monotonic(other, how=how,\n+                                            return_indexers=return_indexers)\n+            else:\n+                return self._join_non_unique(other, how=how,\n+                                             return_indexers=return_indexers)\n+        elif self.is_monotonic and other.is_monotonic:\n+            try:\n+                return self._join_monotonic(other, how=how,\n+                                            return_indexers=return_indexers)\n+            except TypeError:\n+                pass\n \n-        Examples\n-        --------\n-        >>> idx1 = pd.Index([1, 2, 3, 4])\n-        >>> idx2 = pd.Index([2, 3, 4, 5])\n-        >>> idx1.symmetric_difference(idx2)\n-        Int64Index([1, 5], dtype='int64')\n+        if how == 'left':\n+            join_index = self\n+        elif how == 'right':\n+            join_index = other\n+        elif how == 'inner':\n+            join_index = self.intersection(other)\n+        elif how == 'outer':\n+            join_index = self.union(other)\n \n-        You can also use the ``^`` operator:\n+        if sort:\n+            join_index = join_index.sort_values()\n \n-        >>> idx1 ^ idx2\n-        Int64Index([1, 5], dtype='int64')\n-        \"\"\"\n-        self._assert_can_do_setop(other)\n-        other, result_name_update = self._convert_can_do_setop(other)\n-        if result_name is None:\n-            result_name = result_name_update\n+        if return_indexers:\n+            if join_index is self:\n+                lindexer = None\n+            else:\n+                lindexer = self.get_indexer(join_index)\n+            if join_index is other:\n+                rindexer = None\n+            else:\n+                rindexer = other.get_indexer(join_index)\n+            return join_index, lindexer, rindexer\n+        else:\n+            return join_index\n \n-        this = self._get_unique_index()\n-        other = other._get_unique_index()\n-        indexer = this.get_indexer(other)\n+    def _join_multi(self, other, how, return_indexers=True):\n+        from .multi import MultiIndex\n+        from pandas.core.reshape.merge import _restore_dropped_levels_multijoin\n \n-        # {this} minus {other}\n-        common_indexer = indexer.take((indexer != -1).nonzero()[0])\n-        left_indexer = np.setdiff1d(np.arange(this.size), common_indexer,\n-                                    assume_unique=True)\n-        left_diff = this.values.take(left_indexer)\n+        # figure out join names\n+        self_names = set(com._not_none(*self.names))\n+        other_names = set(com._not_none(*other.names))\n+        overlap = self_names & other_names\n \n-        # {other} minus {this}\n-        right_indexer = (indexer == -1).nonzero()[0]\n-        right_diff = other.values.take(right_indexer)\n+        # need at least 1 in common\n+        if not overlap:\n+            raise ValueError(\"cannot join with no overlapping index names\")\n \n-        the_diff = _concat._concat_compat([left_diff, right_diff])\n-        try:\n-            the_diff = sorting.safe_sort(the_diff)\n-        except TypeError:\n-            pass\n+        self_is_mi = isinstance(self, MultiIndex)\n+        other_is_mi = isinstance(other, MultiIndex)\n \n-        attribs = self._get_attributes_dict()\n-        attribs['name'] = result_name\n-        if 'freq' in attribs:\n-            attribs['freq'] = None\n-        return self._shallow_copy_with_infer(the_diff, **attribs)\n+        if self_is_mi and other_is_mi:\n \n-    def _get_unique_index(self, dropna=False):\n-        \"\"\"\n-        Returns an index containing unique values.\n+            # Drop the non-matching levels from left and right respectively\n+            ldrop_names = list(self_names - overlap)\n+            rdrop_names = list(other_names - overlap)\n \n-        Parameters\n-        ----------\n-        dropna : bool\n-            If True, NaN values are dropped.\n+            self_jnlevels = self.droplevel(ldrop_names)\n+            other_jnlevels = other.droplevel(rdrop_names)\n \n-        Returns\n-        -------\n-        uniques : index\n-        \"\"\"\n-        if self.is_unique and not dropna:\n-            return self\n+            # Join left and right\n+            # Join on same leveled multi-index frames is supported\n+            join_idx, lidx, ridx = self_jnlevels.join(other_jnlevels, how,\n+                                                      return_indexers=True)\n \n-        values = self.values\n+            # Restore the dropped levels\n+            # Returned index level order is\n+            # common levels, ldrop_names, rdrop_names\n+            dropped_names = ldrop_names + rdrop_names\n \n-        if not self.is_unique:\n-            values = self.unique()\n+            levels, labels, names = (\n+                _restore_dropped_levels_multijoin(self, other,\n+                                                  dropped_names,\n+                                                  join_idx,\n+                                                  lidx, ridx))\n \n-        if dropna:\n-            try:\n-                if self.hasnans:\n-                    values = values[~isna(values)]\n-            except NotImplementedError:\n-                pass\n+            # Re-create the multi-index\n+            multi_join_idx = MultiIndex(levels=levels, labels=labels,\n+                                        names=names, verify_integrity=False)\n \n-        return self._shallow_copy(values)\n+            multi_join_idx = multi_join_idx.remove_unused_levels()\n \n-    _index_shared_docs['get_loc'] = \"\"\"\n-        Get integer location, slice or boolean mask for requested label.\n+            return multi_join_idx, lidx, ridx\n \n-        Parameters\n-        ----------\n-        key : label\n-        method : {None, 'pad'/'ffill', 'backfill'/'bfill', 'nearest'}, optional\n-            * default: exact matches only.\n-            * pad / ffill: find the PREVIOUS index value if no exact match.\n-            * backfill / bfill: use NEXT index value if no exact match\n-            * nearest: use the NEAREST index value if no exact match. Tied\n-              distances are broken by preferring the larger index value.\n-        tolerance : optional\n-            Maximum distance from index value for inexact matches. The value of\n-            the index at the matching location most satisfy the equation\n-            ``abs(index[loc] - key) <= tolerance``.\n+        jl = list(overlap)[0]\n \n-            Tolerance may be a scalar\n-            value, which applies the same tolerance to all values, or\n-            list-like, which applies variable tolerance per element. List-like\n-            includes list, tuple, array, Series, and must be the same size as\n-            the index and its dtype must exactly match the index's type.\n+        # Case where only one index is multi\n+        # make the indices into mi's that match\n+        flip_order = False\n+        if self_is_mi:\n+            self, other = other, self\n+            flip_order = True\n+            # flip if join method is right or left\n+            how = {'right': 'left', 'left': 'right'}.get(how, how)\n \n-            .. versionadded:: 0.21.0 (list-like tolerance)\n+        level = other.names.index(jl)\n+        result = self._join_level(other, level, how=how,\n+                                  return_indexers=return_indexers)\n \n-        Returns\n-        -------\n-        loc : int if unique index, slice if monotonic index, else mask\n+        if flip_order:\n+            if isinstance(result, tuple):\n+                return result[0], result[2], result[1]\n+        return result\n \n-        Examples\n-        ---------\n-        >>> unique_index = pd.Index(list('abc'))\n-        >>> unique_index.get_loc('b')\n-        1\n+    def _join_non_unique(self, other, how='left', return_indexers=False):\n+        from pandas.core.reshape.merge import _get_join_indexers\n \n-        >>> monotonic_index = pd.Index(list('abbc'))\n-        >>> monotonic_index.get_loc('b')\n-        slice(1, 3, None)\n+        left_idx, right_idx = _get_join_indexers([self._ndarray_values],\n+                                                 [other._ndarray_values],\n+                                                 how=how,\n+                                                 sort=True)\n \n-        >>> non_monotonic_index = pd.Index(list('abcb'))\n-        >>> non_monotonic_index.get_loc('b')\n-        array([False,  True, False,  True], dtype=bool)\n-        \"\"\"\n+        left_idx = ensure_platform_int(left_idx)\n+        right_idx = ensure_platform_int(right_idx)\n \n-    @Appender(_index_shared_docs['get_loc'])\n-    def get_loc(self, key, method=None, tolerance=None):\n-        if method is None:\n-            if tolerance is not None:\n-                raise ValueError('tolerance argument only valid if using pad, '\n-                                 'backfill or nearest lookups')\n-            try:\n-                return self._engine.get_loc(key)\n-            except KeyError:\n-                return self._engine.get_loc(self._maybe_cast_indexer(key))\n-        indexer = self.get_indexer([key], method=method, tolerance=tolerance)\n-        if indexer.ndim > 1 or indexer.size > 1:\n-            raise TypeError('get_loc requires scalar valued input')\n-        loc = indexer.item()\n-        if loc == -1:\n-            raise KeyError(key)\n-        return loc\n+        join_index = np.asarray(self._ndarray_values.take(left_idx))\n+        mask = left_idx == -1\n+        np.putmask(join_index, mask, other._ndarray_values.take(right_idx))\n \n-    def get_value(self, series, key):\n-        \"\"\"\n-        Fast lookup of value from 1-dimensional ndarray. Only use this if you\n-        know what you're doing.\n-        \"\"\"\n+        join_index = self._wrap_joined_index(join_index, other)\n \n-        # if we have something that is Index-like, then\n-        # use this, e.g. DatetimeIndex\n-        s = getattr(series, '_values', None)\n-        if isinstance(s, (ExtensionArray, Index)) and is_scalar(key):\n-            # GH 20882, 21257\n-            # Unify Index and ExtensionArray treatment\n-            # First try to convert the key to a location\n-            # If that fails, raise a KeyError if an integer\n-            # index, otherwise, see if key is an integer, and\n-            # try that\n-            try:\n-                iloc = self.get_loc(key)\n-                return s[iloc]\n-            except KeyError:\n-                if (len(self) > 0 and\n-                        (self.holds_integer() or self.is_boolean())):\n-                    raise\n-                elif is_integer(key):\n-                    return s[key]\n+        if return_indexers:\n+            return join_index, left_idx, right_idx\n+        else:\n+            return join_index\n \n-        s = com.values_from_object(series)\n-        k = com.values_from_object(key)\n+    def _join_level(self, other, level, how='left', return_indexers=False,\n+                    keep_order=True):\n+        \"\"\"\n+        The join method *only* affects the level of the resulting\n+        MultiIndex. Otherwise it just exactly aligns the Index data to the\n+        labels of the level in the MultiIndex.\n \n-        k = self._convert_scalar_indexer(k, kind='getitem')\n-        try:\n-            return self._engine.get_value(s, k,\n-                                          tz=getattr(series.dtype, 'tz', None))\n-        except KeyError as e1:\n-            if len(self) > 0 and (self.holds_integer() or self.is_boolean()):\n-                raise\n-\n-            try:\n-                return libindex.get_value_box(s, key)\n-            except IndexError:\n-                raise\n-            except TypeError:\n-                # generator/iterator-like\n-                if is_iterator(key):\n-                    raise InvalidIndexError(key)\n-                else:\n-                    raise e1\n-            except Exception:  # pragma: no cover\n-                raise e1\n-        except TypeError:\n-            # python 3\n-            if is_scalar(key):  # pragma: no cover\n-                raise IndexError(key)\n-            raise InvalidIndexError(key)\n-\n-    def set_value(self, arr, key, value):\n-        \"\"\"\n-        Fast lookup of value from 1-dimensional ndarray.\n-\n-        Notes\n-        -----\n-        Only use this if you know what you're doing.\n+        If ```keep_order == True```, the order of the data indexed by the\n+        MultiIndex will not be changed; otherwise, it will tie out\n+        with `other`.\n         \"\"\"\n-        self._engine.set_value(com.values_from_object(arr),\n-                               com.values_from_object(key), value)\n+        from .multi import MultiIndex\n \n-    def _get_level_values(self, level):\n-        \"\"\"\n-        Return an Index of values for requested level.\n+        def _get_leaf_sorter(labels):\n+            \"\"\"\n+            Returns sorter for the inner most level while preserving the\n+            order of higher levels.\n+            \"\"\"\n+            if labels[0].size == 0:\n+                return np.empty(0, dtype='int64')\n \n-        This is primarily useful to get an individual level of values from a\n-        MultiIndex, but is provided on Index as well for compatability.\n+            if len(labels) == 1:\n+                lab = ensure_int64(labels[0])\n+                sorter, _ = libalgos.groupsort_indexer(lab, 1 + lab.max())\n+                return sorter\n \n-        Parameters\n-        ----------\n-        level : int or str\n-            It is either the integer position or the name of the level.\n+            # find indexers of beginning of each set of\n+            # same-key labels w.r.t all but last level\n+            tic = labels[0][:-1] != labels[0][1:]\n+            for lab in labels[1:-1]:\n+                tic |= lab[:-1] != lab[1:]\n \n-        Returns\n-        -------\n-        values : Index\n-            Calling object, as there is only one level in the Index.\n+            starts = np.hstack(([True], tic, [True])).nonzero()[0]\n+            lab = ensure_int64(labels[-1])\n+            return lib.get_level_sorter(lab, ensure_int64(starts))\n \n-        See Also\n-        --------\n-        MultiIndex.get_level_values : Get values for a level of a MultiIndex.\n+        if isinstance(self, MultiIndex) and isinstance(other, MultiIndex):\n+            raise TypeError('Join on level between two MultiIndex objects '\n+                            'is ambiguous')\n \n-        Notes\n-        -----\n-        For Index, level should be 0, since there are no multiple levels.\n+        left, right = self, other\n \n-        Examples\n-        --------\n+        flip_order = not isinstance(self, MultiIndex)\n+        if flip_order:\n+            left, right = right, left\n+            how = {'right': 'left', 'left': 'right'}.get(how, how)\n \n-        >>> idx = pd.Index(list('abc'))\n-        >>> idx\n-        Index(['a', 'b', 'c'], dtype='object')\n+        level = left._get_level_number(level)\n+        old_level = left.levels[level]\n \n-        Get level values by supplying `level` as integer:\n+        if not right.is_unique:\n+            raise NotImplementedError('Index._join_level on non-unique index '\n+                                      'is not implemented')\n \n-        >>> idx.get_level_values(0)\n-        Index(['a', 'b', 'c'], dtype='object')\n-        \"\"\"\n-        self._validate_index_level(level)\n-        return self\n+        new_level, left_lev_indexer, right_lev_indexer = \\\n+            old_level.join(right, how=how, return_indexers=True)\n \n-    get_level_values = _get_level_values\n+        if left_lev_indexer is None:\n+            if keep_order or len(left) == 0:\n+                left_indexer = None\n+                join_index = left\n+            else:  # sort the leaves\n+                left_indexer = _get_leaf_sorter(left.labels[:level + 1])\n+                join_index = left[left_indexer]\n \n-    def droplevel(self, level=0):\n-        \"\"\"\n-        Return index with requested level(s) removed.\n+        else:\n+            left_lev_indexer = ensure_int64(left_lev_indexer)\n+            rev_indexer = lib.get_reverse_indexer(left_lev_indexer,\n+                                                  len(old_level))\n \n-        If resulting index has only 1 level left, the result will be\n-        of Index type, not MultiIndex.\n+            new_lev_labels = algos.take_nd(rev_indexer, left.labels[level],\n+                                           allow_fill=False)\n \n-        .. versionadded:: 0.23.1 (support for non-MultiIndex)\n+            new_labels = list(left.labels)\n+            new_labels[level] = new_lev_labels\n \n-        Parameters\n-        ----------\n-        level : int, str, or list-like, default 0\n-            If a string is given, must be the name of a level\n-            If list-like, elements must be names or indexes of levels.\n+            new_levels = list(left.levels)\n+            new_levels[level] = new_level\n \n-        Returns\n-        -------\n-        index : Index or MultiIndex\n-        \"\"\"\n-        if not isinstance(level, (tuple, list)):\n-            level = [level]\n+            if keep_order:  # just drop missing values. o.w. keep order\n+                left_indexer = np.arange(len(left), dtype=np.intp)\n+                mask = new_lev_labels != -1\n+                if not mask.all():\n+                    new_labels = [lab[mask] for lab in new_labels]\n+                    left_indexer = left_indexer[mask]\n \n-        levnums = sorted(self._get_level_number(lev) for lev in level)[::-1]\n+            else:  # tie out the order with other\n+                if level == 0:  # outer most level, take the fast route\n+                    ngroups = 1 + new_lev_labels.max()\n+                    left_indexer, counts = libalgos.groupsort_indexer(\n+                        new_lev_labels, ngroups)\n \n-        if len(level) == 0:\n-            return self\n-        if len(level) >= self.nlevels:\n-            raise ValueError(\"Cannot remove {} levels from an index with {} \"\n-                             \"levels: at least one level must be \"\n-                             \"left.\".format(len(level), self.nlevels))\n-        # The two checks above guarantee that here self is a MultiIndex\n+                    # missing values are placed first; drop them!\n+                    left_indexer = left_indexer[counts[0]:]\n+                    new_labels = [lab[left_indexer] for lab in new_labels]\n \n-        new_levels = list(self.levels)\n-        new_labels = list(self.labels)\n-        new_names = list(self.names)\n+                else:  # sort the leaves\n+                    mask = new_lev_labels != -1\n+                    mask_all = mask.all()\n+                    if not mask_all:\n+                        new_labels = [lab[mask] for lab in new_labels]\n \n-        for i in levnums:\n-            new_levels.pop(i)\n-            new_labels.pop(i)\n-            new_names.pop(i)\n+                    left_indexer = _get_leaf_sorter(new_labels[:level + 1])\n+                    new_labels = [lab[left_indexer] for lab in new_labels]\n \n-        if len(new_levels) == 1:\n+                    # left_indexers are w.r.t masked frame.\n+                    # reverse to original frame!\n+                    if not mask_all:\n+                        left_indexer = mask.nonzero()[0][left_indexer]\n \n-            # set nan if needed\n-            mask = new_labels[0] == -1\n-            result = new_levels[0].take(new_labels[0])\n-            if mask.any():\n-                result = result.putmask(mask, np.nan)\n+            join_index = MultiIndex(levels=new_levels, labels=new_labels,\n+                                    names=left.names, verify_integrity=False)\n \n-            result.name = new_names[0]\n-            return result\n+        if right_lev_indexer is not None:\n+            right_indexer = algos.take_nd(right_lev_indexer,\n+                                          join_index.labels[level],\n+                                          allow_fill=False)\n         else:\n-            from .multi import MultiIndex\n-            return MultiIndex(levels=new_levels, labels=new_labels,\n-                              names=new_names, verify_integrity=False)\n-\n-    _index_shared_docs['get_indexer'] = \"\"\"\n-        Compute indexer and mask for new index given the current index. The\n-        indexer should be then used as an input to ndarray.take to align the\n-        current data to the new index.\n-\n-        Parameters\n-        ----------\n-        target : %(target_klass)s\n-        method : {None, 'pad'/'ffill', 'backfill'/'bfill', 'nearest'}, optional\n-            * default: exact matches only.\n-            * pad / ffill: find the PREVIOUS index value if no exact match.\n-            * backfill / bfill: use NEXT index value if no exact match\n-            * nearest: use the NEAREST index value if no exact match. Tied\n-              distances are broken by preferring the larger index value.\n-        limit : int, optional\n-            Maximum number of consecutive labels in ``target`` to match for\n-            inexact matches.\n-        tolerance : optional\n-            Maximum distance between original and new labels for inexact\n-            matches. The values of the index at the matching locations most\n-            satisfy the equation ``abs(index[indexer] - target) <= tolerance``.\n-\n-            Tolerance may be a scalar value, which applies the same tolerance\n-            to all values, or list-like, which applies variable tolerance per\n-            element. List-like includes list, tuple, array, Series, and must be\n-            the same size as the index and its dtype must exactly match the\n-            index's type.\n+            right_indexer = join_index.labels[level]\n \n-            .. versionadded:: 0.21.0 (list-like tolerance)\n+        if flip_order:\n+            left_indexer, right_indexer = right_indexer, left_indexer\n \n-        Returns\n-        -------\n-        indexer : ndarray of int\n-            Integers from 0 to n - 1 indicating that the index at these\n-            positions matches the corresponding target values. Missing values\n-            in the target are marked by -1.\n+        if return_indexers:\n+            left_indexer = (None if left_indexer is None\n+                            else ensure_platform_int(left_indexer))\n+            right_indexer = (None if right_indexer is None\n+                             else ensure_platform_int(right_indexer))\n+            return join_index, left_indexer, right_indexer\n+        else:\n+            return join_index\n \n-        Examples\n-        --------\n-        >>> index = pd.Index(['c', 'a', 'b'])\n-        >>> index.get_indexer(['a', 'b', 'x'])\n-        array([ 1,  2, -1])\n+    def _join_monotonic(self, other, how='left', return_indexers=False):\n+        if self.equals(other):\n+            ret_index = other if how == 'right' else self\n+            if return_indexers:\n+                return ret_index, None, None\n+            else:\n+                return ret_index\n \n-        Notice that the return value is an array of locations in ``index``\n-        and ``x`` is marked by -1, as it is not in ``index``.\n-        \"\"\"\n+        sv = self._ndarray_values\n+        ov = other._ndarray_values\n \n-    @Appender(_index_shared_docs['get_indexer'] % _index_doc_kwargs)\n-    def get_indexer(self, target, method=None, limit=None, tolerance=None):\n-        method = missing.clean_reindex_fill_method(method)\n-        target = ensure_index(target)\n-        if tolerance is not None:\n-            tolerance = self._convert_tolerance(tolerance, target)\n+        if self.is_unique and other.is_unique:\n+            # We can perform much better than the general case\n+            if how == 'left':\n+                join_index = self\n+                lidx = None\n+                ridx = self._left_indexer_unique(sv, ov)\n+            elif how == 'right':\n+                join_index = other\n+                lidx = self._left_indexer_unique(ov, sv)\n+                ridx = None\n+            elif how == 'inner':\n+                join_index, lidx, ridx = self._inner_indexer(sv, ov)\n+                join_index = self._wrap_joined_index(join_index, other)\n+            elif how == 'outer':\n+                join_index, lidx, ridx = self._outer_indexer(sv, ov)\n+                join_index = self._wrap_joined_index(join_index, other)\n+        else:\n+            if how == 'left':\n+                join_index, lidx, ridx = self._left_indexer(sv, ov)\n+            elif how == 'right':\n+                join_index, ridx, lidx = self._left_indexer(ov, sv)\n+            elif how == 'inner':\n+                join_index, lidx, ridx = self._inner_indexer(sv, ov)\n+            elif how == 'outer':\n+                join_index, lidx, ridx = self._outer_indexer(sv, ov)\n+            join_index = self._wrap_joined_index(join_index, other)\n \n-        # Treat boolean labels passed to a numeric index as not found. Without\n-        # this fix False and True would be treated as 0 and 1 respectively.\n-        # (GH #16877)\n-        if target.is_boolean() and self.is_numeric():\n-            return ensure_platform_int(np.repeat(-1, target.size))\n+        if return_indexers:\n+            lidx = None if lidx is None else ensure_platform_int(lidx)\n+            ridx = None if ridx is None else ensure_platform_int(ridx)\n+            return join_index, lidx, ridx\n+        else:\n+            return join_index\n \n-        pself, ptarget = self._maybe_promote(target)\n-        if pself is not self or ptarget is not target:\n-            return pself.get_indexer(ptarget, method=method, limit=limit,\n-                                     tolerance=tolerance)\n+    def _wrap_joined_index(self, joined, other):\n+        name = get_op_result_name(self, other)\n+        return Index(joined, name=name)\n \n-        if not is_dtype_equal(self.dtype, target.dtype):\n-            this = self.astype(object)\n-            target = target.astype(object)\n-            return this.get_indexer(target, method=method, limit=limit,\n-                                    tolerance=tolerance)\n+    # --------------------------------------------------------------------\n+    # Uncategorized Methods\n \n-        if not self.is_unique:\n-            raise InvalidIndexError('Reindexing only valid with uniquely'\n-                                    ' valued Index objects')\n+    @property\n+    def values(self):\n+        \"\"\"\n+        Return the underlying data as an ndarray.\n+        \"\"\"\n+        return self._data.view(np.ndarray)\n \n-        if method == 'pad' or method == 'backfill':\n-            indexer = self._get_fill_indexer(target, method, limit, tolerance)\n-        elif method == 'nearest':\n-            indexer = self._get_nearest_indexer(target, limit, tolerance)\n-        else:\n-            if tolerance is not None:\n-                raise ValueError('tolerance argument only valid if doing pad, '\n-                                 'backfill or nearest reindexing')\n-            if limit is not None:\n-                raise ValueError('limit argument only valid if doing pad, '\n-                                 'backfill or nearest reindexing')\n+    @property\n+    def _values(self):\n+        # type: () -> Union[ExtensionArray, Index, np.ndarray]\n+        # TODO(EA): remove index types as they become extension arrays\n+        \"\"\"\n+        The best array representation.\n \n-            indexer = self._engine.get_indexer(target._ndarray_values)\n+        This is an ndarray, ExtensionArray, or Index subclass. This differs\n+        from ``_ndarray_values``, which always returns an ndarray.\n \n-        return ensure_platform_int(indexer)\n+        Both ``_values`` and ``_ndarray_values`` are consistent between\n+        ``Series`` and ``Index``.\n \n-    def _convert_tolerance(self, tolerance, target):\n-        # override this method on subclasses\n-        tolerance = np.asarray(tolerance)\n-        if target.size != tolerance.size and tolerance.size > 1:\n-            raise ValueError('list-like tolerance size must match '\n-                             'target index size')\n-        return tolerance\n+        It may differ from the public '.values' method.\n \n-    def _get_fill_indexer(self, target, method, limit=None, tolerance=None):\n-        if self.is_monotonic_increasing and target.is_monotonic_increasing:\n-            method = (self._engine.get_pad_indexer if method == 'pad' else\n-                      self._engine.get_backfill_indexer)\n-            indexer = method(target._ndarray_values, limit)\n-        else:\n-            indexer = self._get_fill_indexer_searchsorted(target, method,\n-                                                          limit)\n-        if tolerance is not None:\n-            indexer = self._filter_indexer_tolerance(target._ndarray_values,\n-                                                     indexer,\n-                                                     tolerance)\n-        return indexer\n+        index             | values          | _values       | _ndarray_values |\n+        ----------------- | --------------- | ------------- | --------------- |\n+        Index             | ndarray         | ndarray       | ndarray         |\n+        CategoricalIndex  | Categorical     | Categorical   | ndarray[int]    |\n+        DatetimeIndex     | ndarray[M8ns]   | ndarray[M8ns] | ndarray[M8ns]   |\n+        DatetimeIndex[tz] | ndarray[M8ns]   | DTI[tz]       | ndarray[M8ns]   |\n+        PeriodIndex       | ndarray[object] | PeriodArray   | ndarray[int]    |\n+        IntervalIndex     | IntervalArray   | IntervalArray | ndarray[object] |\n \n-    def _get_fill_indexer_searchsorted(self, target, method, limit=None):\n-        \"\"\"\n-        Fallback pad/backfill get_indexer that works for monotonic decreasing\n-        indexes and non-monotonic targets.\n+        See Also\n+        --------\n+        values\n+        _ndarray_values\n         \"\"\"\n-        if limit is not None:\n-            raise ValueError('limit argument for %r method only well-defined '\n-                             'if index and target are monotonic' % method)\n+        return self.values\n \n-        side = 'left' if method == 'pad' else 'right'\n+    def get_values(self):\n+        \"\"\"\n+        Return `Index` data as an `numpy.ndarray`.\n \n-        # find exact matches first (this simplifies the algorithm)\n-        indexer = self.get_indexer(target)\n-        nonexact = (indexer == -1)\n-        indexer[nonexact] = self._searchsorted_monotonic(target[nonexact],\n-                                                         side)\n-        if side == 'left':\n-            # searchsorted returns \"indices into a sorted array such that,\n-            # if the corresponding elements in v were inserted before the\n-            # indices, the order of a would be preserved\".\n-            # Thus, we need to subtract 1 to find values to the left.\n-            indexer[nonexact] -= 1\n-            # This also mapped not found values (values of 0 from\n-            # np.searchsorted) to -1, which conveniently is also our\n-            # sentinel for missing values\n-        else:\n-            # Mark indices to the right of the largest value as not found\n-            indexer[indexer == len(self)] = -1\n-        return indexer\n+        Returns\n+        -------\n+        numpy.ndarray\n+            A one-dimensional numpy array of the `Index` values.\n \n-    def _get_nearest_indexer(self, target, limit, tolerance):\n-        \"\"\"\n-        Get the indexer for the nearest index labels; requires an index with\n-        values that can be subtracted from each other (e.g., not strings or\n-        tuples).\n-        \"\"\"\n-        left_indexer = self.get_indexer(target, 'pad', limit=limit)\n-        right_indexer = self.get_indexer(target, 'backfill', limit=limit)\n+        See Also\n+        --------\n+        Index.values : The attribute that get_values wraps.\n \n-        target = np.asarray(target)\n-        left_distances = abs(self.values[left_indexer] - target)\n-        right_distances = abs(self.values[right_indexer] - target)\n+        Examples\n+        --------\n+        Getting the `Index` values of a `DataFrame`:\n \n-        op = operator.lt if self.is_monotonic_increasing else operator.le\n-        indexer = np.where(op(left_distances, right_distances) |\n-                           (right_indexer == -1), left_indexer, right_indexer)\n-        if tolerance is not None:\n-            indexer = self._filter_indexer_tolerance(target, indexer,\n-                                                     tolerance)\n-        return indexer\n+        >>> df = pd.DataFrame([[1, 2, 3], [4, 5, 6], [7, 8, 9]],\n+        ...                    index=['a', 'b', 'c'], columns=['A', 'B', 'C'])\n+        >>> df\n+           A  B  C\n+        a  1  2  3\n+        b  4  5  6\n+        c  7  8  9\n+        >>> df.index.get_values()\n+        array(['a', 'b', 'c'], dtype=object)\n \n-    def _filter_indexer_tolerance(self, target, indexer, tolerance):\n-        distance = abs(self.values[indexer] - target)\n-        indexer = np.where(distance <= tolerance, indexer, -1)\n-        return indexer\n+        Standalone `Index` values:\n \n-    _index_shared_docs['get_indexer_non_unique'] = \"\"\"\n-        Compute indexer and mask for new index given the current index. The\n-        indexer should be then used as an input to ndarray.take to align the\n-        current data to the new index.\n+        >>> idx = pd.Index(['1', '2', '3'])\n+        >>> idx.get_values()\n+        array(['1', '2', '3'], dtype=object)\n \n-        Parameters\n-        ----------\n-        target : %(target_klass)s\n+        `MultiIndex` arrays also have only one dimension:\n \n-        Returns\n-        -------\n-        indexer : ndarray of int\n-            Integers from 0 to n - 1 indicating that the index at these\n-            positions matches the corresponding target values. Missing values\n-            in the target are marked by -1.\n-        missing : ndarray of int\n-            An indexer into the target of the values not found.\n-            These correspond to the -1 in the indexer array\n+        >>> midx = pd.MultiIndex.from_arrays([[1, 2, 3], ['a', 'b', 'c']],\n+        ...                                  names=('number', 'letter'))\n+        >>> midx.get_values()\n+        array([(1, 'a'), (2, 'b'), (3, 'c')], dtype=object)\n+        >>> midx.get_values().ndim\n+        1\n         \"\"\"\n+        return self.values\n \n-    @Appender(_index_shared_docs['get_indexer_non_unique'] % _index_doc_kwargs)\n-    def get_indexer_non_unique(self, target):\n-        target = ensure_index(target)\n-        if is_categorical(target):\n-            target = target.astype(target.dtype.categories.dtype)\n-        pself, ptarget = self._maybe_promote(target)\n-        if pself is not self or ptarget is not target:\n-            return pself.get_indexer_non_unique(ptarget)\n+    @Appender(IndexOpsMixin.memory_usage.__doc__)\n+    def memory_usage(self, deep=False):\n+        result = super(Index, self).memory_usage(deep=deep)\n \n-        if self.is_all_dates:\n-            self = Index(self.asi8)\n-            tgt_values = target.asi8\n-        else:\n-            tgt_values = target._ndarray_values\n+        # include our engine hashtable\n+        result += self._engine.sizeof(deep=deep)\n+        return result\n \n-        indexer, missing = self._engine.get_indexer_non_unique(tgt_values)\n-        return ensure_platform_int(indexer), missing\n+    _index_shared_docs['where'] = \"\"\"\n+        Return an Index of same shape as self and whose corresponding\n+        entries are from self where cond is True and otherwise are from\n+        other.\n \n-    def get_indexer_for(self, target, **kwargs):\n-        \"\"\"\n-        Guaranteed return of an indexer even when non-unique.\n+        .. versionadded:: 0.19.0\n \n-        This dispatches to get_indexer or get_indexer_nonunique\n-        as appropriate.\n+        Parameters\n+        ----------\n+        cond : boolean array-like with the same length as self\n+        other : scalar, or array-like\n         \"\"\"\n-        if self.is_unique:\n-            return self.get_indexer(target, **kwargs)\n-        indexer, _ = self.get_indexer_non_unique(target, **kwargs)\n-        return indexer\n \n-    def _maybe_promote(self, other):\n-        # A hack, but it works\n-        from pandas import DatetimeIndex\n-        if self.inferred_type == 'date' and isinstance(other, DatetimeIndex):\n-            return DatetimeIndex(self), other\n-        elif self.inferred_type == 'boolean':\n-            if not is_object_dtype(self.dtype):\n-                return self.astype('object'), other.astype('object')\n-        return self, other\n+    @Appender(_index_shared_docs['where'])\n+    def where(self, cond, other=None):\n+        if other is None:\n+            other = self._na_value\n \n-    def groupby(self, values):\n-        \"\"\"\n-        Group the index labels by a given array of values.\n+        dtype = self.dtype\n+        values = self.values\n \n-        Parameters\n-        ----------\n-        values : array\n-            Values used to determine the groups.\n+        if is_bool(other) or is_bool_dtype(other):\n \n-        Returns\n-        -------\n-        groups : dict\n-            {group name -> group labels}\n-        \"\"\"\n+            # bools force casting\n+            values = values.astype(object)\n+            dtype = None\n \n-        # TODO: if we are a MultiIndex, we can do better\n-        # that converting to tuples\n-        from .multi import MultiIndex\n-        if isinstance(values, MultiIndex):\n-            values = values.values\n-        values = ensure_categorical(values)\n-        result = values._reverse_indexer()\n+        values = np.where(cond, values, other)\n \n-        # map to the label\n-        result = {k: self.take(v) for k, v in compat.iteritems(result)}\n+        if self._is_numeric_dtype and np.any(isna(values)):\n+            # We can't coerce to the numeric dtype of \"self\" (unless\n+            # it's float) if there are NaN values in our output.\n+            dtype = None\n \n-        return result\n+        return self._shallow_copy_with_infer(values, dtype=dtype)\n \n-    def map(self, mapper, na_action=None):\n+    # construction helpers\n+    @classmethod\n+    def _try_convert_to_int_index(cls, data, copy, name, dtype):\n         \"\"\"\n-        Map values using input correspondence (a dict, Series, or function).\n+        Attempt to convert an array of data into an integer index.\n \n         Parameters\n         ----------\n-        mapper : function, dict, or Series\n-            Mapping correspondence.\n-        na_action : {None, 'ignore'}\n-            If 'ignore', propagate NA values, without passing them to the\n-            mapping correspondence.\n+        data : The data to convert.\n+        copy : Whether to copy the data or not.\n+        name : The name of the index returned.\n \n         Returns\n         -------\n-        applied : Union[Index, MultiIndex], inferred\n-            The output of the mapping function applied to the index.\n-            If the function returns a tuple with more than one element\n-            a MultiIndex will be returned.\n+        int_index : data converted to either an Int64Index or a\n+                    UInt64Index\n+\n+        Raises\n+        ------\n+        ValueError if the conversion was not successful.\n         \"\"\"\n \n-        from .multi import MultiIndex\n-        new_values = super(Index, self)._map_values(\n-            mapper, na_action=na_action)\n+        from .numeric import Int64Index, UInt64Index\n+        if not is_unsigned_integer_dtype(dtype):\n+            # skip int64 conversion attempt if uint-like dtype is passed, as\n+            # this could return Int64Index when UInt64Index is what's desrired\n+            try:\n+                res = data.astype('i8', copy=False)\n+                if (res == data).all():\n+                    return Int64Index(res, copy=copy, name=name)\n+            except (OverflowError, TypeError, ValueError):\n+                pass\n \n-        attributes = self._get_attributes_dict()\n+        # Conversion to int64 failed (possibly due to overflow) or was skipped,\n+        # so let's try now with uint64.\n+        try:\n+            res = data.astype('u8', copy=False)\n+            if (res == data).all():\n+                return UInt64Index(res, copy=copy, name=name)\n+        except (OverflowError, TypeError, ValueError):\n+            pass\n \n-        # we can return a MultiIndex\n-        if new_values.size and isinstance(new_values[0], tuple):\n-            if isinstance(self, MultiIndex):\n-                names = self.names\n-            elif attributes.get('name'):\n-                names = [attributes.get('name')] * len(new_values[0])\n-            else:\n-                names = None\n-            return MultiIndex.from_tuples(new_values,\n-                                          names=names)\n+        raise ValueError\n \n-        attributes['copy'] = False\n-        if not new_values.size:\n-            # empty\n-            attributes['dtype'] = self.dtype\n+    @classmethod\n+    def _scalar_data_error(cls, data):\n+        raise TypeError('{0}(...) must be called with a collection of some '\n+                        'kind, {1} was passed'.format(cls.__name__,\n+                                                      repr(data)))\n \n-        return Index(new_values, **attributes)\n+    @classmethod\n+    def _string_data_error(cls, data):\n+        raise TypeError('String dtype not supported, you may need '\n+                        'to explicitly cast to a numeric type')\n \n-    def isin(self, values, level=None):\n+    @classmethod\n+    def _coerce_to_ndarray(cls, data):\n         \"\"\"\n-        Return a boolean array where the index values are in `values`.\n+        Coerces data to ndarray.\n \n-        Compute boolean array of whether each index value is found in the\n-        passed set of values. The length of the returned boolean array matches\n-        the length of the index.\n+        Converts other iterables to list first and then to array.\n+        Does not touch ndarrays.\n \n-        Parameters\n-        ----------\n-        values : set or list-like\n-            Sought values.\n+        Raises\n+        ------\n+        TypeError\n+            When the data passed in is a scalar.\n+        \"\"\"\n \n-            .. versionadded:: 0.18.1\n+        if not isinstance(data, (np.ndarray, Index)):\n+            if data is None or is_scalar(data):\n+                cls._scalar_data_error(data)\n \n-               Support for values as a set.\n+            # other iterable of some kind\n+            if not isinstance(data, (ABCSeries, list, tuple)):\n+                data = list(data)\n+            data = np.asarray(data)\n+        return data\n \n-        level : str or int, optional\n-            Name or position of the index level to use (if the index is a\n-            `MultiIndex`).\n+    def _coerce_scalar_to_index(self, item):\n+        \"\"\"\n+        We need to coerce a scalar to a compat for our index type.\n \n-        Returns\n-        -------\n-        is_contained : ndarray\n-            NumPy array of boolean values.\n+        Parameters\n+        ----------\n+        item : scalar item to coerce\n+        \"\"\"\n+        dtype = self.dtype\n \n-        See Also\n-        --------\n-        Series.isin : Same for Series.\n-        DataFrame.isin : Same method for DataFrames.\n+        if self._is_numeric_dtype and isna(item):\n+            # We can't coerce to the numeric dtype of \"self\" (unless\n+            # it's float) if there are NaN values in our output.\n+            dtype = None\n \n-        Notes\n-        -----\n-        In the case of `MultiIndex` you must either specify `values` as a\n-        list-like object containing tuples that are the same length as the\n-        number of levels, or specify `level`. Otherwise it will raise a\n-        ``ValueError``.\n+        return Index([item], dtype=dtype, **self._get_attributes_dict())\n \n-        If `level` is specified:\n+    def _to_safe_for_reshape(self):\n+        \"\"\"\n+        Convert to object if we are a categorical.\n+        \"\"\"\n+        return self\n \n-        - if it is the name of one *and only one* index level, use that level;\n-        - otherwise it should be a number indicating level position.\n+    def _convert_for_op(self, value):\n+        \"\"\"\n+        Convert value to be insertable to ndarray.\n+        \"\"\"\n+        return value\n \n-        Examples\n-        --------\n-        >>> idx = pd.Index([1,2,3])\n-        >>> idx\n-        Int64Index([1, 2, 3], dtype='int64')\n+    def _assert_can_do_op(self, value):\n+        \"\"\"\n+        Check value is valid for scalar op.\n+        \"\"\"\n+        if not is_scalar(value):\n+            msg = \"'value' must be a scalar, passed: {0}\"\n+            raise TypeError(msg.format(type(value).__name__))\n \n-        Check whether each index value in a list of values.\n-        >>> idx.isin([1, 4])\n-        array([ True, False, False])\n+    @property\n+    def _has_complex_internals(self):\n+        # to disable groupby tricks in MultiIndex\n+        return False\n \n-        >>> midx = pd.MultiIndex.from_arrays([[1,2,3],\n-        ...                                  ['red', 'blue', 'green']],\n-        ...                                  names=('number', 'color'))\n-        >>> midx\n-        MultiIndex(levels=[[1, 2, 3], ['blue', 'green', 'red']],\n-                   labels=[[0, 1, 2], [2, 0, 1]],\n-                   names=['number', 'color'])\n+    def _is_memory_usage_qualified(self):\n+        \"\"\"\n+        Return a boolean if we need a qualified .info display.\n+        \"\"\"\n+        return self.is_object()\n \n-        Check whether the strings in the 'color' level of the MultiIndex\n-        are in a list of colors.\n+    def is_type_compatible(self, kind):\n+        return kind == self.inferred_type\n \n-        >>> midx.isin(['red', 'orange', 'yellow'], level='color')\n-        array([ True, False, False])\n+    _index_shared_docs['__contains__'] = \"\"\"\n+        Return a boolean if this key is IN the index.\n \n-        To check across the levels of a MultiIndex, pass a list of tuples:\n+        Parameters\n+        ----------\n+        key : object\n \n-        >>> midx.isin([(1, 'red'), (3, 'red')])\n-        array([ True, False, False])\n+        Returns\n+        -------\n+        boolean\n+        \"\"\"\n \n-        For a DatetimeIndex, string values in `values` are converted to\n-        Timestamps.\n+    @Appender(_index_shared_docs['__contains__'] % _index_doc_kwargs)\n+    def __contains__(self, key):\n+        hash(key)\n+        try:\n+            return key in self._engine\n+        except (OverflowError, TypeError, ValueError):\n+            return False\n \n-        >>> dates = ['2000-03-11', '2000-03-12', '2000-03-13']\n-        >>> dti = pd.to_datetime(dates)\n-        >>> dti\n-        DatetimeIndex(['2000-03-11', '2000-03-12', '2000-03-13'],\n-        dtype='datetime64[ns]', freq=None)\n-\n-        >>> dti.isin(['2000-03-11'])\n-        array([ True, False, False])\n-        \"\"\"\n-        if level is not None:\n-            self._validate_index_level(level)\n-        return algos.isin(self, values)\n-\n-    def _can_reindex(self, indexer):\n-        \"\"\"\n-        Check if we are allowing reindexing with this particular indexer.\n+    _index_shared_docs['contains'] = \"\"\"\n+        Return a boolean if this key is IN the index.\n \n         Parameters\n         ----------\n-        indexer : an integer indexer\n+        key : object\n \n-        Raises\n-        ------\n-        ValueError if its a duplicate axis\n+        Returns\n+        -------\n+        boolean\n         \"\"\"\n \n-        # trying to reindex on an axis with duplicates\n-        if not self.is_unique and len(indexer):\n-            raise ValueError(\"cannot reindex from a duplicate axis\")\n+    @Appender(_index_shared_docs['contains'] % _index_doc_kwargs)\n+    def contains(self, key):\n+        hash(key)\n+        try:\n+            return key in self._engine\n+        except (TypeError, ValueError):\n+            return False\n \n-    def reindex(self, target, method=None, level=None, limit=None,\n-                tolerance=None):\n+    def __hash__(self):\n+        raise TypeError(\"unhashable type: %r\" % type(self).__name__)\n+\n+    def __setitem__(self, key, value):\n+        raise TypeError(\"Index does not support mutable operations\")\n+\n+    def __getitem__(self, key):\n         \"\"\"\n-        Create index with target's values (move/add/delete values\n-        as necessary).\n+        Override numpy.ndarray's __getitem__ method to work as desired.\n \n-        Parameters\n-        ----------\n-        target : an iterable\n+        This function adds lists and Series as valid boolean indexers\n+        (ndarrays only supports ndarray with dtype=bool).\n \n-        Returns\n-        -------\n-        new_index : pd.Index\n-            Resulting index\n-        indexer : np.ndarray or None\n-            Indices of output values in original index\n+        If resulting ndim != 1, plain ndarray is returned instead of\n+        corresponding `Index` subclass.\n \n         \"\"\"\n-        # GH6552: preserve names when reindexing to non-named target\n-        # (i.e. neither Index nor Series).\n-        preserve_names = not hasattr(target, 'name')\n+        # There's no custom logic to be implemented in __getslice__, so it's\n+        # not overloaded intentionally.\n+        getitem = self._data.__getitem__\n+        promote = self._shallow_copy\n \n-        # GH7774: preserve dtype/tz if target is empty and not an Index.\n-        target = _ensure_has_len(target)  # target may be an iterator\n+        if is_scalar(key):\n+            key = com.cast_scalar_indexer(key)\n+            return getitem(key)\n \n-        if not isinstance(target, Index) and len(target) == 0:\n-            attrs = self._get_attributes_dict()\n-            attrs.pop('freq', None)  # don't preserve freq\n-            values = self._data[:0]  # appropriately-dtyped empty array\n-            target = self._simple_new(values, dtype=self.dtype, **attrs)\n-        else:\n-            target = ensure_index(target)\n+        if isinstance(key, slice):\n+            # This case is separated from the conditional above to avoid\n+            # pessimization of basic indexing.\n+            return promote(getitem(key))\n \n-        if level is not None:\n-            if method is not None:\n-                raise TypeError('Fill method not supported if level passed')\n-            _, indexer, _ = self._join_level(target, level, how='right',\n-                                             return_indexers=True)\n-        else:\n-            if self.equals(target):\n-                indexer = None\n-            else:\n+        if com.is_bool_indexer(key):\n+            key = np.asarray(key, dtype=bool)\n \n-                if self.is_unique:\n-                    indexer = self.get_indexer(target, method=method,\n-                                               limit=limit,\n-                                               tolerance=tolerance)\n-                else:\n-                    if method is not None or limit is not None:\n-                        raise ValueError(\"cannot reindex a non-unique index \"\n-                                         \"with a method or limit\")\n-                    indexer, missing = self.get_indexer_non_unique(target)\n+        key = com.values_from_object(key)\n+        result = getitem(key)\n+        if not is_scalar(result):\n+            return promote(result)\n+        else:\n+            return result\n \n-        if preserve_names and target.nlevels == 1 and target.name != self.name:\n-            target = target.copy()\n-            target.name = self.name\n+    def _can_hold_identifiers_and_holds_name(self, name):\n+        \"\"\"\n+        Faster check for ``name in self`` when we know `name` is a Python\n+        identifier (e.g. in NDFrame.__getattr__, which hits this to support\n+        . key lookup). For indexes that can't hold identifiers (everything\n+        but object & categorical) we just return False.\n \n-        return target, indexer\n+        https://github.com/pandas-dev/pandas/issues/19764\n+        \"\"\"\n+        if self.is_object() or self.is_categorical():\n+            return name in self\n+        return False\n \n-    def _reindex_non_unique(self, target):\n+    def append(self, other):\n         \"\"\"\n-        Create a new index with target's values (move/add/delete values as\n-        necessary) use with non-unique Index and a possibly non-unique target.\n+        Append a collection of Index options together.\n \n         Parameters\n         ----------\n-        target : an iterable\n+        other : Index or list/tuple of indices\n \n         Returns\n         -------\n-        new_index : pd.Index\n-            Resulting index\n-        indexer : np.ndarray or None\n-            Indices of output values in original index\n-\n+        appended : Index\n         \"\"\"\n \n-        target = ensure_index(target)\n-        indexer, missing = self.get_indexer_non_unique(target)\n-        check = indexer != -1\n-        new_labels = self.take(indexer[check])\n-        new_indexer = None\n-\n-        if len(missing):\n-            length = np.arange(len(indexer))\n-\n-            missing = ensure_platform_int(missing)\n-            missing_labels = target.take(missing)\n-            missing_indexer = ensure_int64(length[~check])\n-            cur_labels = self.take(indexer[check]).values\n-            cur_indexer = ensure_int64(length[check])\n-\n-            new_labels = np.empty(tuple([len(indexer)]), dtype=object)\n-            new_labels[cur_indexer] = cur_labels\n-            new_labels[missing_indexer] = missing_labels\n+        to_concat = [self]\n \n-            # a unique indexer\n-            if target.is_unique:\n+        if isinstance(other, (list, tuple)):\n+            to_concat = to_concat + list(other)\n+        else:\n+            to_concat.append(other)\n \n-                # see GH5553, make sure we use the right indexer\n-                new_indexer = np.arange(len(indexer))\n-                new_indexer[cur_indexer] = np.arange(len(cur_labels))\n-                new_indexer[missing_indexer] = -1\n+        for obj in to_concat:\n+            if not isinstance(obj, Index):\n+                raise TypeError('all inputs must be Index')\n \n-            # we have a non_unique selector, need to use the original\n-            # indexer here\n-            else:\n+        names = {obj.name for obj in to_concat}\n+        name = None if len(names) > 1 else self.name\n \n-                # need to retake to have the same size as the indexer\n-                indexer[~check] = -1\n+        return self._concat(to_concat, name)\n \n-                # reset the new indexer to account for the new size\n-                new_indexer = np.arange(len(self.take(indexer)))\n-                new_indexer[~check] = -1\n+    def _concat(self, to_concat, name):\n \n-        new_index = self._shallow_copy_with_infer(new_labels, freq=None)\n-        return new_index, indexer, new_indexer\n+        typs = _concat.get_dtype_kinds(to_concat)\n \n-    _index_shared_docs['join'] = \"\"\"\n-        Compute join_index and indexers to conform data\n-        structures to the new index.\n+        if len(typs) == 1:\n+            return self._concat_same_dtype(to_concat, name=name)\n+        return _concat._concat_index_asobject(to_concat, name=name)\n \n-        Parameters\n-        ----------\n-        other : Index\n-        how : {'left', 'right', 'inner', 'outer'}\n-        level : int or level name, default None\n-        return_indexers : boolean, default False\n-        sort : boolean, default False\n-            Sort the join keys lexicographically in the result Index. If False,\n-            the order of the join keys depends on the join type (how keyword)\n+    def _concat_same_dtype(self, to_concat, name):\n+        \"\"\"\n+        Concatenate to_concat which has the same class.\n+        \"\"\"\n+        # must be overridden in specific classes\n+        return _concat._concat_index_asobject(to_concat, name)\n \n-            .. versionadded:: 0.20.0\n+    def putmask(self, mask, value):\n+        \"\"\"\n+        Return a new Index of the values set with the mask.\n \n-        Returns\n-        -------\n-        join_index, (left_indexer, right_indexer)\n+        See Also\n+        --------\n+        numpy.ndarray.putmask\n         \"\"\"\n+        values = self.values.copy()\n+        try:\n+            np.putmask(values, mask, self._convert_for_op(value))\n+            return self._shallow_copy(values)\n+        except (ValueError, TypeError) as err:\n+            if is_object_dtype(self):\n+                raise err\n \n-    @Appender(_index_shared_docs['join'])\n-    def join(self, other, how='left', level=None, return_indexers=False,\n-             sort=False):\n-        from .multi import MultiIndex\n-        self_is_mi = isinstance(self, MultiIndex)\n-        other_is_mi = isinstance(other, MultiIndex)\n+            # coerces to object\n+            return self.astype(object).putmask(mask, value)\n \n-        # try to figure out the join level\n-        # GH3662\n-        if level is None and (self_is_mi or other_is_mi):\n+    def equals(self, other):\n+        \"\"\"\n+        Determines if two Index objects contain the same elements.\n+        \"\"\"\n+        if self.is_(other):\n+            return True\n \n-            # have the same levels/names so a simple join\n-            if self.names == other.names:\n-                pass\n-            else:\n-                return self._join_multi(other, how=how,\n-                                        return_indexers=return_indexers)\n+        if not isinstance(other, Index):\n+            return False\n \n-        # join on the level\n-        if level is not None and (self_is_mi or other_is_mi):\n-            return self._join_level(other, level, how=how,\n-                                    return_indexers=return_indexers)\n+        if is_object_dtype(self) and not is_object_dtype(other):\n+            # if other is not object, use other's logic for coercion\n+            return other.equals(self)\n \n-        other = ensure_index(other)\n+        try:\n+            return array_equivalent(com.values_from_object(self),\n+                                    com.values_from_object(other))\n+        except Exception:\n+            return False\n \n-        if len(other) == 0 and how in ('left', 'outer'):\n-            join_index = self._shallow_copy()\n-            if return_indexers:\n-                rindexer = np.repeat(-1, len(join_index))\n-                return join_index, None, rindexer\n-            else:\n-                return join_index\n+    def identical(self, other):\n+        \"\"\"\n+        Similar to equals, but check that other comparable attributes are\n+        also equal.\n+        \"\"\"\n+        return (self.equals(other) and\n+                all((getattr(self, c, None) == getattr(other, c, None)\n+                     for c in self._comparables)) and\n+                type(self) == type(other))\n \n-        if len(self) == 0 and how in ('right', 'outer'):\n-            join_index = other._shallow_copy()\n-            if return_indexers:\n-                lindexer = np.repeat(-1, len(join_index))\n-                return join_index, lindexer, None\n-            else:\n-                return join_index\n+    def asof(self, label):\n+        \"\"\"\n+        Return the label from the index, or, if not present, the previous one.\n \n-        if self._join_precedence < other._join_precedence:\n-            how = {'right': 'left', 'left': 'right'}.get(how, how)\n-            result = other.join(self, how=how, level=level,\n-                                return_indexers=return_indexers)\n-            if return_indexers:\n-                x, y, z = result\n-                result = x, z, y\n-            return result\n+        Assuming that the index is sorted, return the passed index label if it\n+        is in the index, or return the previous index label if the passed one\n+        is not in the index.\n \n-        if not is_dtype_equal(self.dtype, other.dtype):\n-            this = self.astype('O')\n-            other = other.astype('O')\n-            return this.join(other, how=how, return_indexers=return_indexers)\n+        Parameters\n+        ----------\n+        label : object\n+            The label up to which the method returns the latest index label.\n \n-        _validate_join_method(how)\n+        Returns\n+        -------\n+        object\n+            The passed label if it is in the index. The previous label if the\n+            passed label is not in the sorted index or `NaN` if there is no\n+            such label.\n \n-        if not self.is_unique and not other.is_unique:\n-            return self._join_non_unique(other, how=how,\n-                                         return_indexers=return_indexers)\n-        elif not self.is_unique or not other.is_unique:\n-            if self.is_monotonic and other.is_monotonic:\n-                return self._join_monotonic(other, how=how,\n-                                            return_indexers=return_indexers)\n-            else:\n-                return self._join_non_unique(other, how=how,\n-                                             return_indexers=return_indexers)\n-        elif self.is_monotonic and other.is_monotonic:\n-            try:\n-                return self._join_monotonic(other, how=how,\n-                                            return_indexers=return_indexers)\n-            except TypeError:\n-                pass\n+        See Also\n+        --------\n+        Series.asof : Return the latest value in a Series up to the\n+            passed index.\n+        merge_asof : Perform an asof merge (similar to left join but it\n+            matches on nearest key rather than equal key).\n+        Index.get_loc : An `asof` is a thin wrapper around `get_loc`\n+            with method='pad'.\n \n-        if how == 'left':\n-            join_index = self\n-        elif how == 'right':\n-            join_index = other\n-        elif how == 'inner':\n-            join_index = self.intersection(other)\n-        elif how == 'outer':\n-            join_index = self.union(other)\n+        Examples\n+        --------\n+        `Index.asof` returns the latest index label up to the passed label.\n \n-        if sort:\n-            join_index = join_index.sort_values()\n+        >>> idx = pd.Index(['2013-12-31', '2014-01-02', '2014-01-03'])\n+        >>> idx.asof('2014-01-01')\n+        '2013-12-31'\n \n-        if return_indexers:\n-            if join_index is self:\n-                lindexer = None\n-            else:\n-                lindexer = self.get_indexer(join_index)\n-            if join_index is other:\n-                rindexer = None\n-            else:\n-                rindexer = other.get_indexer(join_index)\n-            return join_index, lindexer, rindexer\n+        If the label is in the index, the method returns the passed label.\n+\n+        >>> idx.asof('2014-01-02')\n+        '2014-01-02'\n+\n+        If all of the labels in the index are later than the passed label,\n+        NaN is returned.\n+\n+        >>> idx.asof('1999-01-02')\n+        nan\n+\n+        If the index is not sorted, an error is raised.\n+\n+        >>> idx_not_sorted = pd.Index(['2013-12-31', '2015-01-02',\n+        ...                            '2014-01-03'])\n+        >>> idx_not_sorted.asof('2013-12-31')\n+        Traceback (most recent call last):\n+        ValueError: index must be monotonic increasing or decreasing\n+        \"\"\"\n+        try:\n+            loc = self.get_loc(label, method='pad')\n+        except KeyError:\n+            return self._na_value\n         else:\n-            return join_index\n+            if isinstance(loc, slice):\n+                loc = loc.indices(len(self))[-1]\n+            return self[loc]\n \n-    def _join_multi(self, other, how, return_indexers=True):\n-        from .multi import MultiIndex\n-        from pandas.core.reshape.merge import _restore_dropped_levels_multijoin\n+    def asof_locs(self, where, mask):\n+        \"\"\"\n+        Finds the locations (indices) of the labels from the index for\n+        every entry in the `where` argument.\n \n-        # figure out join names\n-        self_names = set(com._not_none(*self.names))\n-        other_names = set(com._not_none(*other.names))\n-        overlap = self_names & other_names\n+        As in the `asof` function, if the label (a particular entry in\n+        `where`) is not in the index, the latest index label upto the\n+        passed label is chosen and its index returned.\n \n-        # need at least 1 in common\n-        if not overlap:\n-            raise ValueError(\"cannot join with no overlapping index names\")\n+        If all of the labels in the index are later than a label in `where`,\n+        -1 is returned.\n \n-        self_is_mi = isinstance(self, MultiIndex)\n-        other_is_mi = isinstance(other, MultiIndex)\n+        `mask` is used to ignore NA values in the index during calculation.\n \n-        if self_is_mi and other_is_mi:\n+        Parameters\n+        ----------\n+        where : Index\n+            An Index consisting of an array of timestamps.\n+        mask : array-like\n+            Array of booleans denoting where values in the original\n+            data are not NA.\n \n-            # Drop the non-matching levels from left and right respectively\n-            ldrop_names = list(self_names - overlap)\n-            rdrop_names = list(other_names - overlap)\n+        Returns\n+        -------\n+        numpy.ndarray\n+            An array of locations (indices) of the labels from the Index\n+            which correspond to the return values of the `asof` function\n+            for every element in `where`.\n+        \"\"\"\n+        locs = self.values[mask].searchsorted(where.values, side='right')\n+        locs = np.where(locs > 0, locs - 1, 0)\n \n-            self_jnlevels = self.droplevel(ldrop_names)\n-            other_jnlevels = other.droplevel(rdrop_names)\n+        result = np.arange(len(self))[mask].take(locs)\n \n-            # Join left and right\n-            # Join on same leveled multi-index frames is supported\n-            join_idx, lidx, ridx = self_jnlevels.join(other_jnlevels, how,\n-                                                      return_indexers=True)\n+        first = mask.argmax()\n+        result[(locs == 0) & (where.values < self.values[first])] = -1\n \n-            # Restore the dropped levels\n-            # Returned index level order is\n-            # common levels, ldrop_names, rdrop_names\n-            dropped_names = ldrop_names + rdrop_names\n+        return result\n \n-            levels, labels, names = (\n-                _restore_dropped_levels_multijoin(self, other,\n-                                                  dropped_names,\n-                                                  join_idx,\n-                                                  lidx, ridx))\n+    def sort_values(self, return_indexer=False, ascending=True):\n+        \"\"\"\n+        Return a sorted copy of the index.\n \n-            # Re-create the multi-index\n-            multi_join_idx = MultiIndex(levels=levels, labels=labels,\n-                                        names=names, verify_integrity=False)\n+        Return a sorted copy of the index, and optionally return the indices\n+        that sorted the index itself.\n \n-            multi_join_idx = multi_join_idx.remove_unused_levels()\n+        Parameters\n+        ----------\n+        return_indexer : bool, default False\n+            Should the indices that would sort the index be returned.\n+        ascending : bool, default True\n+            Should the index values be sorted in an ascending order.\n \n-            return multi_join_idx, lidx, ridx\n+        Returns\n+        -------\n+        sorted_index : pandas.Index\n+            Sorted copy of the index.\n+        indexer : numpy.ndarray, optional\n+            The indices that the index itself was sorted by.\n \n-        jl = list(overlap)[0]\n+        See Also\n+        --------\n+        pandas.Series.sort_values : Sort values of a Series.\n+        pandas.DataFrame.sort_values : Sort values in a DataFrame.\n \n-        # Case where only one index is multi\n-        # make the indices into mi's that match\n-        flip_order = False\n-        if self_is_mi:\n-            self, other = other, self\n-            flip_order = True\n-            # flip if join method is right or left\n-            how = {'right': 'left', 'left': 'right'}.get(how, how)\n+        Examples\n+        --------\n+        >>> idx = pd.Index([10, 100, 1, 1000])\n+        >>> idx\n+        Int64Index([10, 100, 1, 1000], dtype='int64')\n+\n+        Sort values in ascending order (default behavior).\n+\n+        >>> idx.sort_values()\n+        Int64Index([1, 10, 100, 1000], dtype='int64')\n+\n+        Sort values in descending order, and also get the indices `idx` was\n+        sorted by.\n+\n+        >>> idx.sort_values(ascending=False, return_indexer=True)\n+        (Int64Index([1000, 100, 10, 1], dtype='int64'), array([3, 1, 0, 2]))\n+        \"\"\"\n+        _as = self.argsort()\n+        if not ascending:\n+            _as = _as[::-1]\n+\n+        sorted_index = self.take(_as)\n+\n+        if return_indexer:\n+            return sorted_index, _as\n+        else:\n+            return sorted_index\n+\n+    def sort(self, *args, **kwargs):\n+        raise TypeError(\"cannot sort an Index object in-place, use \"\n+                        \"sort_values instead\")\n+\n+    def shift(self, periods=1, freq=None):\n+        \"\"\"\n+        Shift index by desired number of time frequency increments.\n+\n+        This method is for shifting the values of datetime-like indexes\n+        by a specified time increment a given number of times.\n+\n+        Parameters\n+        ----------\n+        periods : int, default 1\n+            Number of periods (or increments) to shift by,\n+            can be positive or negative.\n+        freq : pandas.DateOffset, pandas.Timedelta or string, optional\n+            Frequency increment to shift by.\n+            If None, the index is shifted by its own `freq` attribute.\n+            Offset aliases are valid strings, e.g., 'D', 'W', 'M' etc.\n+\n+        Returns\n+        -------\n+        pandas.Index\n+            shifted index\n+\n+        See Also\n+        --------\n+        Series.shift : Shift values of Series.\n+\n+        Examples\n+        --------\n+        Put the first 5 month starts of 2011 into an index.\n+\n+        >>> month_starts = pd.date_range('1/1/2011', periods=5, freq='MS')\n+        >>> month_starts\n+        DatetimeIndex(['2011-01-01', '2011-02-01', '2011-03-01', '2011-04-01',\n+                       '2011-05-01'],\n+                      dtype='datetime64[ns]', freq='MS')\n+\n+        Shift the index by 10 days.\n+\n+        >>> month_starts.shift(10, freq='D')\n+        DatetimeIndex(['2011-01-11', '2011-02-11', '2011-03-11', '2011-04-11',\n+                       '2011-05-11'],\n+                      dtype='datetime64[ns]', freq=None)\n+\n+        The default value of `freq` is the `freq` attribute of the index,\n+        which is 'MS' (month start) in this example.\n+\n+        >>> month_starts.shift(10)\n+        DatetimeIndex(['2011-11-01', '2011-12-01', '2012-01-01', '2012-02-01',\n+                       '2012-03-01'],\n+                      dtype='datetime64[ns]', freq='MS')\n+\n+        Notes\n+        -----\n+        This method is only implemented for datetime-like index classes,\n+        i.e., DatetimeIndex, PeriodIndex and TimedeltaIndex.\n+        \"\"\"\n+        raise NotImplementedError(\"Not supported for type %s\" %\n+                                  type(self).__name__)\n+\n+    def argsort(self, *args, **kwargs):\n+        \"\"\"\n+        Return the integer indices that would sort the index.\n+\n+        Parameters\n+        ----------\n+        *args\n+            Passed to `numpy.ndarray.argsort`.\n+        **kwargs\n+            Passed to `numpy.ndarray.argsort`.\n+\n+        Returns\n+        -------\n+        numpy.ndarray\n+            Integer indices that would sort the index if used as\n+            an indexer.\n+\n+        See Also\n+        --------\n+        numpy.argsort : Similar method for NumPy arrays.\n+        Index.sort_values : Return sorted copy of Index.\n+\n+        Examples\n+        --------\n+        >>> idx = pd.Index(['b', 'a', 'd', 'c'])\n+        >>> idx\n+        Index(['b', 'a', 'd', 'c'], dtype='object')\n+\n+        >>> order = idx.argsort()\n+        >>> order\n+        array([1, 0, 3, 2])\n+\n+        >>> idx[order]\n+        Index(['a', 'b', 'c', 'd'], dtype='object')\n+        \"\"\"\n+        result = self.asi8\n+        if result is None:\n+            result = np.array(self)\n+        return result.argsort(*args, **kwargs)\n+\n+    def get_value(self, series, key):\n+        \"\"\"\n+        Fast lookup of value from 1-dimensional ndarray. Only use this if you\n+        know what you're doing.\n+        \"\"\"\n+\n+        # if we have something that is Index-like, then\n+        # use this, e.g. DatetimeIndex\n+        s = getattr(series, '_values', None)\n+        if isinstance(s, (ExtensionArray, Index)) and is_scalar(key):\n+            # GH 20882, 21257\n+            # Unify Index and ExtensionArray treatment\n+            # First try to convert the key to a location\n+            # If that fails, raise a KeyError if an integer\n+            # index, otherwise, see if key is an integer, and\n+            # try that\n+            try:\n+                iloc = self.get_loc(key)\n+                return s[iloc]\n+            except KeyError:\n+                if (len(self) > 0 and\n+                        (self.holds_integer() or self.is_boolean())):\n+                    raise\n+                elif is_integer(key):\n+                    return s[key]\n+\n+        s = com.values_from_object(series)\n+        k = com.values_from_object(key)\n+\n+        k = self._convert_scalar_indexer(k, kind='getitem')\n+        try:\n+            return self._engine.get_value(s, k,\n+                                          tz=getattr(series.dtype, 'tz', None))\n+        except KeyError as e1:\n+            if len(self) > 0 and (self.holds_integer() or self.is_boolean()):\n+                raise\n+\n+            try:\n+                return libindex.get_value_box(s, key)\n+            except IndexError:\n+                raise\n+            except TypeError:\n+                # generator/iterator-like\n+                if is_iterator(key):\n+                    raise InvalidIndexError(key)\n+                else:\n+                    raise e1\n+            except Exception:  # pragma: no cover\n+                raise e1\n+        except TypeError:\n+            # python 3\n+            if is_scalar(key):  # pragma: no cover\n+                raise IndexError(key)\n+            raise InvalidIndexError(key)\n+\n+    def set_value(self, arr, key, value):\n+        \"\"\"\n+        Fast lookup of value from 1-dimensional ndarray.\n+\n+        Notes\n+        -----\n+        Only use this if you know what you're doing.\n+        \"\"\"\n+        self._engine.set_value(com.values_from_object(arr),\n+                               com.values_from_object(key), value)\n+\n+    _index_shared_docs['get_indexer_non_unique'] = \"\"\"\n+        Compute indexer and mask for new index given the current index. The\n+        indexer should be then used as an input to ndarray.take to align the\n+        current data to the new index.\n+\n+        Parameters\n+        ----------\n+        target : %(target_klass)s\n+\n+        Returns\n+        -------\n+        indexer : ndarray of int\n+            Integers from 0 to n - 1 indicating that the index at these\n+            positions matches the corresponding target values. Missing values\n+            in the target are marked by -1.\n+        missing : ndarray of int\n+            An indexer into the target of the values not found.\n+            These correspond to the -1 in the indexer array\n+        \"\"\"\n \n-        level = other.names.index(jl)\n-        result = self._join_level(other, level, how=how,\n-                                  return_indexers=return_indexers)\n+    @Appender(_index_shared_docs['get_indexer_non_unique'] % _index_doc_kwargs)\n+    def get_indexer_non_unique(self, target):\n+        target = ensure_index(target)\n+        if is_categorical(target):\n+            target = target.astype(target.dtype.categories.dtype)\n+        pself, ptarget = self._maybe_promote(target)\n+        if pself is not self or ptarget is not target:\n+            return pself.get_indexer_non_unique(ptarget)\n \n-        if flip_order:\n-            if isinstance(result, tuple):\n-                return result[0], result[2], result[1]\n-        return result\n+        if self.is_all_dates:\n+            self = Index(self.asi8)\n+            tgt_values = target.asi8\n+        else:\n+            tgt_values = target._ndarray_values\n \n-    def _join_non_unique(self, other, how='left', return_indexers=False):\n-        from pandas.core.reshape.merge import _get_join_indexers\n+        indexer, missing = self._engine.get_indexer_non_unique(tgt_values)\n+        return ensure_platform_int(indexer), missing\n \n-        left_idx, right_idx = _get_join_indexers([self._ndarray_values],\n-                                                 [other._ndarray_values],\n-                                                 how=how,\n-                                                 sort=True)\n+    def get_indexer_for(self, target, **kwargs):\n+        \"\"\"\n+        Guaranteed return of an indexer even when non-unique.\n \n-        left_idx = ensure_platform_int(left_idx)\n-        right_idx = ensure_platform_int(right_idx)\n+        This dispatches to get_indexer or get_indexer_nonunique\n+        as appropriate.\n+        \"\"\"\n+        if self.is_unique:\n+            return self.get_indexer(target, **kwargs)\n+        indexer, _ = self.get_indexer_non_unique(target, **kwargs)\n+        return indexer\n \n-        join_index = np.asarray(self._ndarray_values.take(left_idx))\n-        mask = left_idx == -1\n-        np.putmask(join_index, mask, other._ndarray_values.take(right_idx))\n+    def _maybe_promote(self, other):\n+        # A hack, but it works\n+        from pandas import DatetimeIndex\n+        if self.inferred_type == 'date' and isinstance(other, DatetimeIndex):\n+            return DatetimeIndex(self), other\n+        elif self.inferred_type == 'boolean':\n+            if not is_object_dtype(self.dtype):\n+                return self.astype('object'), other.astype('object')\n+        return self, other\n \n-        join_index = self._wrap_joined_index(join_index, other)\n+    def groupby(self, values):\n+        \"\"\"\n+        Group the index labels by a given array of values.\n \n-        if return_indexers:\n-            return join_index, left_idx, right_idx\n-        else:\n-            return join_index\n+        Parameters\n+        ----------\n+        values : array\n+            Values used to determine the groups.\n \n-    def _join_level(self, other, level, how='left', return_indexers=False,\n-                    keep_order=True):\n+        Returns\n+        -------\n+        groups : dict\n+            {group name -> group labels}\n         \"\"\"\n-        The join method *only* affects the level of the resulting\n-        MultiIndex. Otherwise it just exactly aligns the Index data to the\n-        labels of the level in the MultiIndex.\n \n-        If ```keep_order == True```, the order of the data indexed by the\n-        MultiIndex will not be changed; otherwise, it will tie out\n-        with `other`.\n-        \"\"\"\n+        # TODO: if we are a MultiIndex, we can do better\n+        # that converting to tuples\n         from .multi import MultiIndex\n+        if isinstance(values, MultiIndex):\n+            values = values.values\n+        values = ensure_categorical(values)\n+        result = values._reverse_indexer()\n \n-        def _get_leaf_sorter(labels):\n-            \"\"\"\n-            Returns sorter for the inner most level while preserving the\n-            order of higher levels.\n-            \"\"\"\n-            if labels[0].size == 0:\n-                return np.empty(0, dtype='int64')\n+        # map to the label\n+        result = {k: self.take(v) for k, v in compat.iteritems(result)}\n \n-            if len(labels) == 1:\n-                lab = ensure_int64(labels[0])\n-                sorter, _ = libalgos.groupsort_indexer(lab, 1 + lab.max())\n-                return sorter\n+        return result\n \n-            # find indexers of beginning of each set of\n-            # same-key labels w.r.t all but last level\n-            tic = labels[0][:-1] != labels[0][1:]\n-            for lab in labels[1:-1]:\n-                tic |= lab[:-1] != lab[1:]\n+    def map(self, mapper, na_action=None):\n+        \"\"\"\n+        Map values using input correspondence (a dict, Series, or function).\n \n-            starts = np.hstack(([True], tic, [True])).nonzero()[0]\n-            lab = ensure_int64(labels[-1])\n-            return lib.get_level_sorter(lab, ensure_int64(starts))\n+        Parameters\n+        ----------\n+        mapper : function, dict, or Series\n+            Mapping correspondence.\n+        na_action : {None, 'ignore'}\n+            If 'ignore', propagate NA values, without passing them to the\n+            mapping correspondence.\n \n-        if isinstance(self, MultiIndex) and isinstance(other, MultiIndex):\n-            raise TypeError('Join on level between two MultiIndex objects '\n-                            'is ambiguous')\n+        Returns\n+        -------\n+        applied : Union[Index, MultiIndex], inferred\n+            The output of the mapping function applied to the index.\n+            If the function returns a tuple with more than one element\n+            a MultiIndex will be returned.\n+        \"\"\"\n \n-        left, right = self, other\n+        from .multi import MultiIndex\n+        new_values = super(Index, self)._map_values(\n+            mapper, na_action=na_action)\n \n-        flip_order = not isinstance(self, MultiIndex)\n-        if flip_order:\n-            left, right = right, left\n-            how = {'right': 'left', 'left': 'right'}.get(how, how)\n+        attributes = self._get_attributes_dict()\n \n-        level = left._get_level_number(level)\n-        old_level = left.levels[level]\n+        # we can return a MultiIndex\n+        if new_values.size and isinstance(new_values[0], tuple):\n+            if isinstance(self, MultiIndex):\n+                names = self.names\n+            elif attributes.get('name'):\n+                names = [attributes.get('name')] * len(new_values[0])\n+            else:\n+                names = None\n+            return MultiIndex.from_tuples(new_values,\n+                                          names=names)\n \n-        if not right.is_unique:\n-            raise NotImplementedError('Index._join_level on non-unique index '\n-                                      'is not implemented')\n+        attributes['copy'] = False\n+        if not new_values.size:\n+            # empty\n+            attributes['dtype'] = self.dtype\n \n-        new_level, left_lev_indexer, right_lev_indexer = \\\n-            old_level.join(right, how=how, return_indexers=True)\n+        return Index(new_values, **attributes)\n \n-        if left_lev_indexer is None:\n-            if keep_order or len(left) == 0:\n-                left_indexer = None\n-                join_index = left\n-            else:  # sort the leaves\n-                left_indexer = _get_leaf_sorter(left.labels[:level + 1])\n-                join_index = left[left_indexer]\n+    def isin(self, values, level=None):\n+        \"\"\"\n+        Return a boolean array where the index values are in `values`.\n \n-        else:\n-            left_lev_indexer = ensure_int64(left_lev_indexer)\n-            rev_indexer = lib.get_reverse_indexer(left_lev_indexer,\n-                                                  len(old_level))\n+        Compute boolean array of whether each index value is found in the\n+        passed set of values. The length of the returned boolean array matches\n+        the length of the index.\n \n-            new_lev_labels = algos.take_nd(rev_indexer, left.labels[level],\n-                                           allow_fill=False)\n+        Parameters\n+        ----------\n+        values : set or list-like\n+            Sought values.\n \n-            new_labels = list(left.labels)\n-            new_labels[level] = new_lev_labels\n+            .. versionadded:: 0.18.1\n \n-            new_levels = list(left.levels)\n-            new_levels[level] = new_level\n+               Support for values as a set.\n \n-            if keep_order:  # just drop missing values. o.w. keep order\n-                left_indexer = np.arange(len(left), dtype=np.intp)\n-                mask = new_lev_labels != -1\n-                if not mask.all():\n-                    new_labels = [lab[mask] for lab in new_labels]\n-                    left_indexer = left_indexer[mask]\n+        level : str or int, optional\n+            Name or position of the index level to use (if the index is a\n+            `MultiIndex`).\n \n-            else:  # tie out the order with other\n-                if level == 0:  # outer most level, take the fast route\n-                    ngroups = 1 + new_lev_labels.max()\n-                    left_indexer, counts = libalgos.groupsort_indexer(\n-                        new_lev_labels, ngroups)\n+        Returns\n+        -------\n+        is_contained : ndarray\n+            NumPy array of boolean values.\n \n-                    # missing values are placed first; drop them!\n-                    left_indexer = left_indexer[counts[0]:]\n-                    new_labels = [lab[left_indexer] for lab in new_labels]\n+        See Also\n+        --------\n+        Series.isin : Same for Series.\n+        DataFrame.isin : Same method for DataFrames.\n \n-                else:  # sort the leaves\n-                    mask = new_lev_labels != -1\n-                    mask_all = mask.all()\n-                    if not mask_all:\n-                        new_labels = [lab[mask] for lab in new_labels]\n+        Notes\n+        -----\n+        In the case of `MultiIndex` you must either specify `values` as a\n+        list-like object containing tuples that are the same length as the\n+        number of levels, or specify `level`. Otherwise it will raise a\n+        ``ValueError``.\n \n-                    left_indexer = _get_leaf_sorter(new_labels[:level + 1])\n-                    new_labels = [lab[left_indexer] for lab in new_labels]\n+        If `level` is specified:\n \n-                    # left_indexers are w.r.t masked frame.\n-                    # reverse to original frame!\n-                    if not mask_all:\n-                        left_indexer = mask.nonzero()[0][left_indexer]\n+        - if it is the name of one *and only one* index level, use that level;\n+        - otherwise it should be a number indicating level position.\n \n-            join_index = MultiIndex(levels=new_levels, labels=new_labels,\n-                                    names=left.names, verify_integrity=False)\n+        Examples\n+        --------\n+        >>> idx = pd.Index([1,2,3])\n+        >>> idx\n+        Int64Index([1, 2, 3], dtype='int64')\n \n-        if right_lev_indexer is not None:\n-            right_indexer = algos.take_nd(right_lev_indexer,\n-                                          join_index.labels[level],\n-                                          allow_fill=False)\n-        else:\n-            right_indexer = join_index.labels[level]\n+        Check whether each index value in a list of values.\n+        >>> idx.isin([1, 4])\n+        array([ True, False, False])\n \n-        if flip_order:\n-            left_indexer, right_indexer = right_indexer, left_indexer\n+        >>> midx = pd.MultiIndex.from_arrays([[1,2,3],\n+        ...                                  ['red', 'blue', 'green']],\n+        ...                                  names=('number', 'color'))\n+        >>> midx\n+        MultiIndex(levels=[[1, 2, 3], ['blue', 'green', 'red']],\n+                   labels=[[0, 1, 2], [2, 0, 1]],\n+                   names=['number', 'color'])\n+\n+        Check whether the strings in the 'color' level of the MultiIndex\n+        are in a list of colors.\n \n-        if return_indexers:\n-            left_indexer = (None if left_indexer is None\n-                            else ensure_platform_int(left_indexer))\n-            right_indexer = (None if right_indexer is None\n-                             else ensure_platform_int(right_indexer))\n-            return join_index, left_indexer, right_indexer\n-        else:\n-            return join_index\n+        >>> midx.isin(['red', 'orange', 'yellow'], level='color')\n+        array([ True, False, False])\n \n-    def _join_monotonic(self, other, how='left', return_indexers=False):\n-        if self.equals(other):\n-            ret_index = other if how == 'right' else self\n-            if return_indexers:\n-                return ret_index, None, None\n-            else:\n-                return ret_index\n+        To check across the levels of a MultiIndex, pass a list of tuples:\n \n-        sv = self._ndarray_values\n-        ov = other._ndarray_values\n+        >>> midx.isin([(1, 'red'), (3, 'red')])\n+        array([ True, False, False])\n \n-        if self.is_unique and other.is_unique:\n-            # We can perform much better than the general case\n-            if how == 'left':\n-                join_index = self\n-                lidx = None\n-                ridx = self._left_indexer_unique(sv, ov)\n-            elif how == 'right':\n-                join_index = other\n-                lidx = self._left_indexer_unique(ov, sv)\n-                ridx = None\n-            elif how == 'inner':\n-                join_index, lidx, ridx = self._inner_indexer(sv, ov)\n-                join_index = self._wrap_joined_index(join_index, other)\n-            elif how == 'outer':\n-                join_index, lidx, ridx = self._outer_indexer(sv, ov)\n-                join_index = self._wrap_joined_index(join_index, other)\n-        else:\n-            if how == 'left':\n-                join_index, lidx, ridx = self._left_indexer(sv, ov)\n-            elif how == 'right':\n-                join_index, ridx, lidx = self._left_indexer(ov, sv)\n-            elif how == 'inner':\n-                join_index, lidx, ridx = self._inner_indexer(sv, ov)\n-            elif how == 'outer':\n-                join_index, lidx, ridx = self._outer_indexer(sv, ov)\n-            join_index = self._wrap_joined_index(join_index, other)\n+        For a DatetimeIndex, string values in `values` are converted to\n+        Timestamps.\n \n-        if return_indexers:\n-            lidx = None if lidx is None else ensure_platform_int(lidx)\n-            ridx = None if ridx is None else ensure_platform_int(ridx)\n-            return join_index, lidx, ridx\n-        else:\n-            return join_index\n+        >>> dates = ['2000-03-11', '2000-03-12', '2000-03-13']\n+        >>> dti = pd.to_datetime(dates)\n+        >>> dti\n+        DatetimeIndex(['2000-03-11', '2000-03-12', '2000-03-13'],\n+        dtype='datetime64[ns]', freq=None)\n \n-    def _wrap_joined_index(self, joined, other):\n-        name = get_op_result_name(self, other)\n-        return Index(joined, name=name)\n+        >>> dti.isin(['2000-03-11'])\n+        array([ True, False, False])\n+        \"\"\"\n+        if level is not None:\n+            self._validate_index_level(level)\n+        return algos.isin(self, values)\n \n     def _get_string_slice(self, key, use_lhs=True, use_rhs=True):\n         # this is for partial string indexing,\n@@ -4630,190 +4870,8 @@ class Index(IndexOpsMixin, PandasObject):\n             indexer = indexer[~mask]\n         return self.delete(indexer)\n \n-    _index_shared_docs['index_unique'] = (\n-        \"\"\"\n-        Return unique values in the index. Uniques are returned in order\n-        of appearance, this does NOT sort.\n-\n-        Parameters\n-        ----------\n-        level : int or str, optional, default None\n-            Only return values from specified level (for MultiIndex)\n-\n-            .. versionadded:: 0.23.0\n-\n-        Returns\n-        -------\n-        Index without duplicates\n-\n-        See Also\n-        --------\n-        unique\n-        Series.unique\n-        \"\"\")\n-\n-    @Appender(_index_shared_docs['index_unique'] % _index_doc_kwargs)\n-    def unique(self, level=None):\n-        if level is not None:\n-            self._validate_index_level(level)\n-        result = super(Index, self).unique()\n-        return self._shallow_copy(result)\n-\n-    def drop_duplicates(self, keep='first'):\n-        \"\"\"\n-        Return Index with duplicate values removed.\n-\n-        Parameters\n-        ----------\n-        keep : {'first', 'last', ``False``}, default 'first'\n-            - 'first' : Drop duplicates except for the first occurrence.\n-            - 'last' : Drop duplicates except for the last occurrence.\n-            - ``False`` : Drop all duplicates.\n-\n-        Returns\n-        -------\n-        deduplicated : Index\n-\n-        See Also\n-        --------\n-        Series.drop_duplicates : Equivalent method on Series.\n-        DataFrame.drop_duplicates : Equivalent method on DataFrame.\n-        Index.duplicated : Related method on Index, indicating duplicate\n-            Index values.\n-\n-        Examples\n-        --------\n-        Generate an pandas.Index with duplicate values.\n-\n-        >>> idx = pd.Index(['lama', 'cow', 'lama', 'beetle', 'lama', 'hippo'])\n-\n-        The `keep` parameter controls  which duplicate values are removed.\n-        The value 'first' keeps the first occurrence for each\n-        set of duplicated entries. The default value of keep is 'first'.\n-\n-        >>> idx.drop_duplicates(keep='first')\n-        Index(['lama', 'cow', 'beetle', 'hippo'], dtype='object')\n-\n-        The value 'last' keeps the last occurrence for each set of duplicated\n-        entries.\n-\n-        >>> idx.drop_duplicates(keep='last')\n-        Index(['cow', 'beetle', 'lama', 'hippo'], dtype='object')\n-\n-        The value ``False`` discards all sets of duplicated entries.\n-\n-        >>> idx.drop_duplicates(keep=False)\n-        Index(['cow', 'beetle', 'hippo'], dtype='object')\n-        \"\"\"\n-        return super(Index, self).drop_duplicates(keep=keep)\n-\n-    def duplicated(self, keep='first'):\n-        \"\"\"\n-        Indicate duplicate index values.\n-\n-        Duplicated values are indicated as ``True`` values in the resulting\n-        array. Either all duplicates, all except the first, or all except the\n-        last occurrence of duplicates can be indicated.\n-\n-        Parameters\n-        ----------\n-        keep : {'first', 'last', False}, default 'first'\n-            The value or values in a set of duplicates to mark as missing.\n-\n-            - 'first' : Mark duplicates as ``True`` except for the first\n-              occurrence.\n-            - 'last' : Mark duplicates as ``True`` except for the last\n-              occurrence.\n-            - ``False`` : Mark all duplicates as ``True``.\n-\n-        Examples\n-        --------\n-        By default, for each set of duplicated values, the first occurrence is\n-        set to False and all others to True:\n-\n-        >>> idx = pd.Index(['lama', 'cow', 'lama', 'beetle', 'lama'])\n-        >>> idx.duplicated()\n-        array([False, False,  True, False,  True])\n-\n-        which is equivalent to\n-\n-        >>> idx.duplicated(keep='first')\n-        array([False, False,  True, False,  True])\n-\n-        By using 'last', the last occurrence of each set of duplicated values\n-        is set on False and all others on True:\n-\n-        >>> idx.duplicated(keep='last')\n-        array([ True, False,  True, False, False])\n-\n-        By setting keep on ``False``, all duplicates are True:\n-\n-        >>> idx.duplicated(keep=False)\n-        array([ True, False,  True, False,  True])\n-\n-        Returns\n-        -------\n-        numpy.ndarray\n-\n-        See Also\n-        --------\n-        pandas.Series.duplicated : Equivalent method on pandas.Series.\n-        pandas.DataFrame.duplicated : Equivalent method on pandas.DataFrame.\n-        pandas.Index.drop_duplicates : Remove duplicate values from Index.\n-        \"\"\"\n-        return super(Index, self).duplicated(keep=keep)\n-\n-    _index_shared_docs['fillna'] = \"\"\"\n-        Fill NA/NaN values with the specified value\n-\n-        Parameters\n-        ----------\n-        value : scalar\n-            Scalar value to use to fill holes (e.g. 0).\n-            This value cannot be a list-likes.\n-        downcast : dict, default is None\n-            a dict of item->dtype of what to downcast if possible,\n-            or the string 'infer' which will try to downcast to an appropriate\n-            equal type (e.g. float64 to int64 if possible)\n-\n-        Returns\n-        -------\n-        filled : %(klass)s\n-        \"\"\"\n-\n-    @Appender(_index_shared_docs['fillna'])\n-    def fillna(self, value=None, downcast=None):\n-        self._assert_can_do_op(value)\n-        if self.hasnans:\n-            result = self.putmask(self._isnan, value)\n-            if downcast is None:\n-                # no need to care metadata other than name\n-                # because it can't have freq if\n-                return Index(result, name=self.name)\n-        return self._shallow_copy()\n-\n-    _index_shared_docs['dropna'] = \"\"\"\n-        Return Index without NA/NaN values\n-\n-        Parameters\n-        ----------\n-        how :  {'any', 'all'}, default 'any'\n-            If the Index is a MultiIndex, drop the value when any or all levels\n-            are NaN.\n-\n-        Returns\n-        -------\n-        valid : Index\n-        \"\"\"\n-\n-    @Appender(_index_shared_docs['dropna'])\n-    def dropna(self, how='any'):\n-        if how not in ('any', 'all'):\n-            raise ValueError(\"invalid how option: {0}\".format(how))\n-\n-        if self.hasnans:\n-            return self._shallow_copy(self.values[~self._isnan])\n-        return self._shallow_copy()\n+    # --------------------------------------------------------------------\n+    # Generated Arithmetic, Comparison, and Unary Methods\n \n     def _evaluate_with_timedelta_like(self, other, op):\n         # Timedelta knows how to operate with np.array, so dispatch to that\n",
          "files_name_in_blame_commit": [
            "category.py",
            "datetimelike.py",
            "period.py",
            "base.py",
            "datetimes.py",
            "timedeltas.py",
            "interval.py",
            "range.py",
            "multi.py"
          ]
        }
      }
    }
  }
}