{
  "1": {
    "description": "NameError: global name 'output_path' is not defined\nhttps://github.com/cool-RR/PySnooper/blob/6e3d797be3fa0a746fb5b1b7c7fea78eb926c208/pysnooper/pysnooper.py#L26\r\n\r\nshould probably read\r\n```\r\nwith open(output, 'a') as output_file:\r\n```\n",
    "desc_source": "github_issue"
  },
  "2": {
    "description": "\"Named Response Middleware\" executed in wrong order\n**Describe the bug**\r\nPR https://github.com/huge-success/sanic/pull/1690 Introduced \"named response middlware\" that is, middleware which is only executed in a given request context. For example a blueprint middleware is only executed on a route which is defined in _that_ blueprint.\r\nThere was a copy+paste error in the `register_named_middleware` function, here: https://github.com/huge-success/sanic/blob/e7001b00747b659f7042b0534802b936ee8a53e0/sanic/app.py#L656\r\nWhen registering a \"response\" middleware, they are supposed to be added to the left in reverse. So `appendleft()` should be used instead of  `append()`. The correct behavior is seen in the normal `register_middleware` function.\r\n\r\n**Code snippet**\r\nSee these two examples, the first using normal middleware, and the second using named middleware:\r\n```\r\nfrom sanic import Sanic\r\nfrom sanic.response import text\r\napp = Sanic(__name__)\r\n\r\n@app.middleware('request')\r\ndef request_middleware_1(request):\r\n    print('1')\r\n\r\n@app.middleware('request')\r\ndef request_middleware_2(request):\r\n    print('2')\r\n\r\n@app.middleware('request')\r\ndef request_middleware_3(request):\r\n    print('3')\r\n\r\n@app.middleware('response')\r\ndef resp_middleware_4(request, response):\r\n    print('4')\r\n\r\n@app.middleware('response')\r\ndef resp_middleware_5(request, response):\r\n    print('5')\r\n\r\n@app.middleware('response')\r\ndef resp_middleware_6(request, response):\r\n    print('6')\r\n\r\n@app.route('/')\r\ndef pop_handler(request):\r\n    return text('hello world')\r\n\r\napp.run(host=\"0.0.0.0\", port=8000, debug=True, auto_reload=False)\r\n```\r\nvs:\r\n```\r\nfrom sanic import Sanic, Blueprint\r\nfrom sanic.response import text\r\n\r\napp = Sanic(__name__)\r\nbp = Blueprint(\"bp_\"+__name__)\r\n\r\n@bp.middleware('request')\r\ndef request_middleware_1(request):\r\n    print('1')\r\n\r\n@bp.middleware('request')\r\ndef request_middleware_2(request):\r\n    print('2')\r\n\r\n@bp.middleware('request')\r\ndef request_middleware_3(request):\r\n    print('3')\r\n\r\n@bp.middleware('response')\r\ndef resp_middleware_4(request, response):\r\n    print('4')\r\n\r\n@bp.middleware('response')\r\ndef resp_middleware_5(request, response):\r\n    print('5')\r\n\r\n@bp.middleware('response')\r\ndef resp_middleware_6(request, response):\r\n    print('6')\r\n\r\n@bp.route('/')\r\ndef pop_handler(request):\r\n    return text('hello world')\r\n\r\napp.blueprint(bp, url_prefix='/bp')\r\n\r\napp.run(host=\"0.0.0.0\", port=8000, debug=True, auto_reload=False)\r\n```\r\n\r\n**Expected behavior**\r\nSee the first snippet prints \"1 2 3 6 5 4\" (correct) but the second snippet prints \"1 2 3 4 5 6\". This should match the first.\r\n\r\n**Additional Context**\r\nThis bug is _similar to_ but not the same as https://github.com/huge-success/sanic/issues/1845\r\nThis bug was uncovered while looking deeper into https://github.com/huge-success/sanic/issues/1845\n",
    "desc_source": "github_issue"
  },
  "3": {
    "description": "Redshift COPY fails in luigi 2.7.1 when columns are not provided\nRunning Redshift COPY jobs with `columns = None` to prohibit table creation fails in luigi 2.7.1 with \r\n```\r\nTypeError: object of type 'NoneType' has no len()\r\n```\r\n\r\nThe root cause seems to be https://github.com/spotify/luigi/pull/2245/files#diff-778ea3db4cccaf4de6564889c5eb670fR338\r\n\r\nA possible solution would be to change the line to\r\n```python\r\nif self.columns and len(self.columns) > 0:\r\n```\r\nunless I am missing some reason to explicitly ask only for `len(self.columns)`.\r\n\r\n\r\n<!---\r\nWe use GitHub issues mainly for tracking bugs and feature requests.\r\nQuestions for how to use luigi can be sent to the mailing list.\r\n\r\nCurrently, there are no strict procedures or guidelines for submitting issues.\r\nIn short, please just use common sense.\r\n\r\nCommon sense includes this at bare-minimum:\r\n\r\n * search for similar issues posted before creating a new issue.\r\n * Use markdown to format all code/logs. Issues which are hard to read\r\n   when rendered on GitHub might be closed with a friendly reminder of this.\r\n * If applicable, reading relevant parts of the documentation.\r\n\r\nAlso, add steps to reproduce the bug, if applicable. Sample code would be nice too :)\r\n\r\nFor more information on how to submit valuable contributions,\r\nsee https://opensource.guide/how-to-contribute/#how-to-submit-a-contribution\r\n-->\r\n\n",
    "desc_source": "github_issue"
  },
  "4": {
    "description": "Filters tasks in second branch of Worker.get_pending_tasks (#1849)\n\nWhen a worker has many DONE tasks, get_pending_tasks may switch to using\r\nstate.get_pending_tasks in order to speed up the process. This can include\r\npending tasks not owned by the worker, invalidating the result and causing\r\nfunctions like is_trivial_worker to return erroneous results.\r\n\r\nTo fix this, we simply filter the results of state.get_pending_tasks to\r\nremove any tasks that don't include this worker.",
    "desc_source": "commit_msg"
  },
  "5": {
    "description": "Fix bug in LocalFileSystem.move (#1643)",
    "desc_source": "commit_msg"
  },
  "6": {
    "description": "Overrides scheduler disable with manual disable by worker\n\nWhen the scheduler disables a task, we then ignore most other status changes\nrequested by the worker until re-enable. This had a weird result for me recently\nwhen a group of tasks were repeatedly failing and many of them were disabled.\nIn order to stop the repeated runs, I marked them all as disabled. A few hours\nlater when the disable period expired for the automatic scheduler disables,\nmany of the tasks became PENDING again and started running. It took a while\nbefore enough of them were scheduled with the right timing to become permanently\ndisabled.\n\nTo fix this, we now allow the permanent disable requested by the worker to\noverride the temporary disable triggered within the scheduler. Note that this\nwas the original behavior but it wasn't enforced in unit tests and got lost in a\nrefactor.",
    "desc_source": "commit_msg"
  },
  "7": {
    "description": "Fixes a bug with auto-reenable when disable_failures is 1\n\nWhen disable_failures is 1, there was a bug in which the re-enable would cause\nan immediate disable because re-enable is achieved by setting the task to\nFAILED. This fixes the bug by not counting a failure toward the number needed\nfor a disable if the task is disabled.",
    "desc_source": "commit_msg"
  },
  "8": {
    "description": "A Fix the TypeError whenever prune is called before update has been invoked\nin worker.",
    "desc_source": "commit_msg"
  },
  "9": {
    "description": "hive table_exists should be case insensitive?\nAny thoughts on this one?\n\nIn https://github.com/spotify/luigi/blob/master/luigi/contrib/hive.py#L141\n(possibly here too, but we're not on CDH: https://github.com/spotify/luigi/blob/master/luigi/contrib/hive.py#L192)\n\nSometimes we have tables that are defined as capitalized, rather than lower case underscored names.  These are easier to read in code if left capitalized, though hive is case insensitive, and will return them as lower case.\n\nE.g. when checking for an existing table `table = 'FooBar'`, stdout will return with `foobar` and the test will fail\n\nThis wasn't an issue in older versions, which just checked for string \"does not exist\" or \"Table not found\" in stdout.\n\nWould be easy to fix using `return stdout and table.lower() in stdout` or `return stdout and table.lower() in stdout.lower()`\n\nLet me know your thoughts on this.  I can supply a pull request if necessary.\n\nThanks,\nLin.\n\n",
    "desc_source": "github_issue"
  },
  "10": {
    "description": "Prevents non-runnable tasks from going to assistants\n\nI was seeing a lot of errors from assistants trying to run external tasks and\nbeing unable to even find and load the class. It turns out assistants were\nbeing treated as potential workers even by tasks with no workers. As a partial\nfix, assistants can now only work on tasks that some worker is capable of.\n\nThis fix is great for workflows where external tasks only exist to check that\nnon-luigi jobs are complete, but won't work for tasks that are external to the\nassistant but not to all workers.",
    "desc_source": "commit_msg"
  },
  "11": {
    "description": "Latest Luigi version breaks my hadoop jobs\nI used to be able to initialize a class like\n\n```\nclass MyStreamingJob(luigi.hadoop.JobTask):\n    param = luigi.Parameter()\n```\n\nwith code like\n\n```\nMyStreamingJob('param_value')\n```\n\nbut now this results in an error\n\n```\nluigi.parameter.MissingParameterException: MyStreamingJob[args=('param_value',), kwargs={}]: requires the 'param' parameter to be set\n```\n\nI believe it's now interpreting the first argument as pool, where it never did that before.\n\n",
    "desc_source": "github_issue"
  },
  "12": {
    "description": "Fix default values being loaded with wrong encoding on Windows (#1414)\n\nExplicitly set the encoding to utf-8 when reading the context file to\r\nensure values are correctly loaded.\r\n\r\nCo-authored-by: Andrey Shpak <insspb@users.noreply.github.com>",
    "desc_source": "commit_msg"
  },
  "13": {
    "description": "Prevent click API v7.0 from showing choices when already shown",
    "desc_source": "commit_msg"
  },
  "14": {
    "description": "Fix for Issue 4665 - conllu2json (#4953)\n\n* Fix for Issue 4665 - conllu2json\r\n\r\n- Allowing HEAD to be an underscore\r\n\r\n* Added contributor agreement",
    "desc_source": "commit_msg"
  },
  "15": {
    "description": "Fix bug in Language.evaluate for components without .pipe (#4662)",
    "desc_source": "commit_msg"
  },
  "16": {
    "description": "cda.pl trouble with extract the videos URLs\n## Please follow the guide below\r\n\r\n- You will be asked some questions and requested to provide some information, please read them **carefully** and answer honestly\r\n- Put an `x` into all the boxes [ ] relevant to your *issue* (like this: `[x]`)\r\n- Use the *Preview* tab to see what your issue will actually look like\r\n\r\n---\r\n\r\n### Make sure you are using the *latest* version: run `youtube-dl --version` and ensure your version is *2017.08.13*. If it's not, read [this FAQ entry](https://github.com/rg3/youtube-dl/blob/master/README.md#how-do-i-update-youtube-dl) and update. Issues with outdated version will be rejected.\r\n- [ x] I've **verified** and **I assure** that I'm running youtube-dl **2017.08.13**\r\n\r\n### Before submitting an *issue* make sure you have:\r\n- [x] At least skimmed through the [README](https://github.com/rg3/youtube-dl/blob/master/README.md), **most notably** the [FAQ](https://github.com/rg3/youtube-dl#faq) and [BUGS](https://github.com/rg3/youtube-dl#bugs) sections\r\n- [X] [Searched](https://github.com/rg3/youtube-dl/search?type=Issues) the bugtracker for similar issues including closed ones\r\n\r\n### What is the purpose of your *issue*?\r\n- [x] Bug report (encountered problems with youtube-dl)\r\n- [ ] Site support request (request for adding support for a new site)\r\n- [ ] Feature request (request for a new functionality)\r\n- [ ] Question\r\n- [ ] Other\r\n\r\n---\r\n\r\n### The following sections concretize particular purposed issues, you can erase any section (the contents between triple ---) not applicable to your *issue*\r\n\r\n---\r\n\r\n### If the purpose of this *issue* is a *bug report*, *site support request* or you are not completely sure provide the full verbose output as follows:\r\n\r\nAdd the `-v` flag to **your command line** you run youtube-dl with (`youtube-dl -v <your command line>`), copy the **whole** output and insert it here. It should look similar to one below (replace it with **your** log inserted between triple ```):\r\n\r\n```\r\n youtube-dl -gvv https://www.cda.pl/video/9443700b\r\n[debug] System config: [u'--prefer-free-formats']\r\n[debug] User config: []\r\n[debug] Custom config: []\r\n[debug] Command-line args: [u'-gvv', u'https://www.cda.pl/video/9443700b']\r\n[debug] Encodings: locale UTF-8, fs UTF-8, out UTF-8, pref UTF-8\r\n[debug] youtube-dl version 2017.08.13\r\n[debug] Python version 2.7.13 - Linux-4.11.12-200.fc25.x86_64-x86_64-with-fedora-25-Gurgle\r\n[debug] exe versions: ffmpeg 3.1.9, ffprobe 3.1.9\r\n[debug] Proxy map: {}\r\nWARNING: [CDA] default player_data: Failed to parse JSON Expecting property name enclosed in double quotes: line 1 column 2 (char 1)\r\nWARNING: [CDA] 480p player_data: Failed to parse JSON Expecting property name enclosed in double quotes: line 1 column 2 (char 1)\r\nWARNING: [CDA] 720p player_data: Failed to parse JSON Expecting property name enclosed in double quotes: line 1 column 2 (char 1)\r\nWARNING: [CDA] 1080p player_data: Failed to parse JSON Expecting property name enclosed in double quotes: line 1 column 2 (char 1)\r\nERROR: No video formats found; please report this issue on https://yt-dl.org/bug . Make sure you are using the latest version; type  youtube-dl -U  to update. Be sure to call youtube-dl with the --verbose flag and include its complete output.\r\nTraceback (most recent call last):\r\n  File \"/usr/bin/youtube-dl/youtube_dl/YoutubeDL.py\", line 776, in extract_info\r\n    ie_result = ie.extract(url)\r\n  File \"/usr/bin/youtube-dl/youtube_dl/extractor/common.py\", line 433, in extract\r\n    ie_result = self._real_extract(url)\r\n  File \"/usr/bin/youtube-dl/youtube_dl/extractor/cda.py\", line 180, in _real_extract\r\n    self._sort_formats(formats)\r\n  File \"/usr/bin/youtube-dl/youtube_dl/extractor/common.py\", line 1057, in _sort_formats\r\n    raise ExtractorError('No video formats found')\r\nExtractorError: No video formats found; please report this issue on https://yt-dl.org/bug . Make sure you are using the latest version; type  youtube-dl -U  to update. Be sure to call youtube-dl with the --verbose flag and include its complete output.\r\n\r\n```\r\n\r\n---\r\n\r\n\n",
    "desc_source": "github_issue"
  },
  "17": {
    "description": "[YoutubeDL] Fix typo in string negation implementation and add more tests (closes #18961)",
    "desc_source": "commit_msg"
  },
  "18": {
    "description": "[utils] Fix urljoin for paths with non-http(s) schemes",
    "desc_source": "commit_msg"
  },
  "19": {
    "description": "[utils] js_to_json can't handle scientific notations (was: [twitch:clips] JSONDecodeError for certain clips)\n\nERROR: CarelessZealousKangarooNerfBlueBlaster: Failed to parse JSON  (caused by JSONDecodeError(\"Expecting ',' delimiter: line 38 column 78 (char 2178)\",)); please report this issue on https://yt-dl.org/bug . Make sure you are using the latest version; see  https://yt-dl.org/update  on how to update. Be sure to call youtube-dl with the --verbose flag and include its complete output.\nTraceback (most recent call last):\n  File \"/tmp/youtube-dl/youtube_dl/extractor/common.py\", line 686, in _parse_json\n    return json.loads(json_string)\n  File \"/usr/lib/python3.6/json/__init__.py\", line 354, in loads\n    return _default_decoder.decode(s)\n  File \"/usr/lib/python3.6/json/decoder.py\", line 339, in decode\n    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n  File \"/usr/lib/python3.6/json/decoder.py\", line 355, in raw_decode\n    obj, end = self.scan_once(s, idx)\njson.decoder.JSONDecodeError: Expecting ',' delimiter: line 38 column 78 (char 2178)\nTraceback (most recent call last):\n  File \"/tmp/youtube-dl/youtube_dl/extractor/common.py\", line 686, in _parse_json\n    return json.loads(json_string)\n  File \"/usr/lib/python3.6/json/__init__.py\", line 354, in loads\n    return _default_decoder.decode(s)\n  File \"/usr/lib/python3.6/json/decoder.py\", line 339, in decode\n    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n  File \"/usr/lib/python3.6/json/decoder.py\", line 355, in raw_decode\n    obj, end = self.scan_once(s, idx)\njson.decoder.JSONDecodeError: Expecting ',' delimiter: line 38 column 78 (char 2178)\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/tmp/youtube-dl/youtube_dl/YoutubeDL.py\", line 784, in extract_info\n    ie_result = ie.extract(url)\n  File \"/tmp/youtube-dl/youtube_dl/extractor/common.py\", line 437, in extract\n    ie_result = self._real_extract(url)\n  File \"/tmp/youtube-dl/youtube_dl/extractor/twitch.py\", line 610, in _real_extract\n    video_id, transform_source=js_to_json)\n  File \"/tmp/youtube-dl/youtube_dl/extractor/common.py\", line 690, in _parse_json\n    raise ExtractorError(errmsg, cause=ve)\nyoutube_dl.utils.ExtractorError: CarelessZealousKangarooNerfBlueBlaster: Failed to parse JSON  (caused by JSONDecodeError(\"Expecting ',' delimiter: line 38 column 78 (char 2178)\",)); please report this issue on https://yt-dl.org/bug . Make sure you are using the latest version; see  https://yt-dl.org/update  on how to update. Be sure to call youtube-dl with the --verbose flag and include its complete output.\n",
    "desc_source": "github_issue"
  },
  "20": {
    "description": "[YoutubeDL] Do not override id, extractor and extractor_key in url_transparent\nAll these meta fields must be borrowed from final extractor that actually performs extraction.\nThis commit fixes extractor id in download archives for url_transparent downloads. Previously, 'transparent' extractor was erroneously\nused for extractor archive id, e.g. 'eggheadlesson 4n8ugwwj5t' instead of 'wistia 4n8ugwwj5t'.",
    "desc_source": "commit_msg"
  },
  "21": {
    "description": "[utils] Don't transform numbers not starting with a zero\n\nFix test_Viidea and maybe others",
    "desc_source": "commit_msg"
  },
  "22": {
    "description": "[utils] Fix url_basename",
    "desc_source": "commit_msg"
  },
  "23": {
    "description": "Fix and add tests for some module_utils.common.validation (#67771)\n\n* Fix test_check_mutually_exclusive exception-checking\r\n\r\nAsserting inside of the `with` context of `pytest.raises`\r\ndoesn't actually have any effect. So we move the assert\r\nout, using the exception that gets placed into the scope\r\nafter we leave the context, and ensure that it actually gets\r\nchecked.\r\n\r\nThis is also what the pytest documentation says to do:\r\nhttps://docs.pytest.org/en/latest/assert.html#assertions-about-expected-exceptions\r\n\r\nSigned-off-by: Rick Elrod <rick@elrod.me>\r\n\r\n* Add some tests for check_required_together\r\n\r\nSigned-off-by: Rick Elrod <rick@elrod.me>\r\n\r\n* use to_native instead of str, for consistency\r\n\r\nSigned-off-by: Rick Elrod <rick@elrod.me>\r\n\r\n* Add newlines for pep8\r\n\r\nSigned-off-by: Rick Elrod <rick@elrod.me>\r\n\r\n* Add tests for check_required_arguments\r\n\r\nSigned-off-by: Rick Elrod <rick@elrod.me>\r\n\r\n* Sort missing keys in error message, since hashes are unsorted and this can be random\r\n\r\nSigned-off-by: Rick Elrod <rick@elrod.me>\r\n\r\n* Add changelog entry\r\n\r\nSigned-off-by: Rick Elrod <rick@elrod.me>",
    "desc_source": "commit_msg"
  },
  "24": {
    "description": "eos_eapi does not turn on eapi by default\n##### SUMMARY\r\neos_eapi does not turn on the Arista eAPI, it is missing an important command\r\n\r\ntoday if I have\r\n\r\n```\r\n- eos_eapi:\r\n```\r\n\r\nit will render this config on the Arista EOS device->\r\n\r\n```\r\nmanagement api http-commands\r\n   vrf default\r\n      no shutdown\r\n!\r\n```\r\n\r\nwhat it \"should\" render is this->\r\n\r\n```\r\nmanagement api http-commands\r\n   no shutdown\r\n   !\r\n   vrf default\r\n      no shutdown\r\n!\r\n```\r\n\r\nthe vrf default no shutdown is not even needed if you are truly just using the default vrf\r\nyou will get YES for the top line here->\r\n\r\n```\r\nrtr2#show management api http-commands\r\nEnabled:            Yes\r\nHTTPS server:       running, set to use port 443\r\nHTTP server:        shutdown, set to use port 80\r\nLocal HTTP server:  shutdown, no authentication, set to use port 8080\r\nUnix Socket server: shutdown, no authentication\r\nVRFs:               default\r\nHits:               13\r\nLast hit:           865 seconds ago\r\nBytes in:           2090\r\nBytes out:          3712\r\nRequests:           9\r\nCommands:           18\r\nDuration:           0.187 seconds\r\nSSL Profile:        none\r\nFIPS Mode:          No\r\nQoS DSCP:           0\r\nLog Level:          none\r\nCSP Frame Ancestor: None\r\nTLS Protocols:      1.0 1.1 1.2\r\n   User        Requests       Bytes in       Bytes out    Last hit\r\n----------- -------------- -------------- --------------- ---------------\r\n   admin       9              2090           3712         865 seconds ago\r\n\r\nURLs\r\n----------------------------------------\r\n```\r\n\r\nthe default behavior leaves you with no\n\n",
    "desc_source": "github_issue"
  },
  "25": {
    "description": "Fix contract errback",
    "desc_source": "commit_msg"
  },
  "26": {
    "description": "PY3: SitemapSpider fail to extract sitemap URLs from robots.txt in Scrapy 1.1.0rc1\n## Environment\n- Mac OS X 10.10.5\n- Python 3.4.2\n- Scrapy 1.1.0rc1\n## Steps to Reproduce\n1. Save the following spider as `sitemap_spider.py`.\n   \n   ``` py\n   from scrapy.spiders import SitemapSpider\n   \n   \n   class BlogSitemapSpider(SitemapSpider):\n      name = \"blog_sitemap\"\n      allowed_domains = [\"blog.scrapinghub.com\"]\n   \n      sitemap_urls = [\n          'https://blog.scrapinghub.com/robots.txt',\n      ]\n      sitemap_rules = [\n          (r'/2016/', 'parse'),\n      ]\n   \n      def parse(self, response):\n          pass\n   ```\n2. Run the following command.\n   \n   ```\n   $ scrapy runspider sitemap_spider.py\n   ```\n## Expected Results\n\nThe spider crawl several pages according to the sitemaps without error.\n## Actual Results\n\nThe spider fail to extract sitemap URLs from robots.txt. No pages are crawled.\n\n",
    "desc_source": "github_issue"
  },
  "27": {
    "description": "Do not fail representing non-http requests",
    "desc_source": "commit_msg"
  },
  "28": {
    "description": "scrapy.Request no init error on invalid url\nI stumbled on some weird issue, spider got some invalid url, but instead of crashing loudly when trying to create scrapy.Request() with invalid url it just silently ignored this error. Sample to reproduce\r\n\r\n```python\r\nfrom scrapy.spiders import Spider\r\nfrom scrapy import Request\r\n\r\n\r\nclass DmozSpider(Spider):\r\n    name = \"dmoz\"\r\n    allowed_domains = [\"dmoz.org\"]\r\n    start_urls = [\r\n        \"http://www.dmoz.org/Computers/Programming/Languages/Python/Books/\",\r\n    ]\r\n\r\n    def parse(self, response):\r\n        invalid_url = \"/container.productlist.productslist.productthumbnail.articledetaillink.layerlink:open-layer/0/CLASSIC/-1/WEB$007cARBO$007c13263065/null$007cDisplay$0020Product$002f111499$002fAil$0020blanc$007c?t:ac=13263065\"\r\n        yield Request(invalid_url)\r\n```\r\n\r\nthis generates following output:\r\n\r\n```\r\n2017-02-09 12:21:04 [scrapy.core.engine] INFO: Spider opened\r\n2017-02-09 12:21:04 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\r\n2017-02-09 12:21:04 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6024\r\n2017-02-09 12:21:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www.dmoz.org/Computers/Programming/Languages/Python/Books/> (referer: None)\r\n2017-02-09 12:21:04 [scrapy.core.engine] INFO: Closing spider (finished)\r\n```\r\n\r\nthere is no information about trying to generate this Request with invalid_url, no stacktrace, no error info from middleware. Why?\n",
    "desc_source": "github_issue"
  },
  "29": {
    "description": "BUG: `is_string_dtype` incorrectly identifies categorical data\n- [X] I have checked that this issue has not already been reported.\r\n\r\n- [X] I have confirmed this bug exists on the latest version of pandas.\r\n\r\n- [X] (optional) I have confirmed this bug exists on the master branch of pandas.\r\n\r\n---\r\n\r\n```python\r\ns = pd.Series(['a', 'b', 'c'], dtype='category')\r\npdt.is_string_dtype(s)\r\n>>> True\r\n```\r\n\r\n#### Problem description\r\n\r\nThe current implementation of `is_string_dtype` incorrectly evaluates to True for categorical series.\r\n\r\n#### Expected Output\r\n\r\n```python\r\ns = pd.Series(['a', 'b', 'c'], dtype='category')\r\npdt.is_string_dtype(s)\r\n>>> False\r\n```\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit           : None\r\npython           : 3.8.0.final.0\r\npython-bits      : 64\r\nOS               : Darwin\r\nOS-release       : 19.4.0\r\nmachine          : x86_64\r\nprocessor        : i386\r\nbyteorder        : little\r\nLC_ALL           : None\r\nLANG             : en_US.UTF-8\r\nLOCALE           : en_US.UTF-8\r\n\r\npandas           : 1.0.3\r\nnumpy            : 1.17.3\r\npytz             : 2019.3\r\ndateutil         : 2.8.1\r\npip              : 19.3.1\r\nsetuptools       : 44.0.0.post20200106\r\nCython           : 0.29.14\r\npytest           : 5.3.2\r\nhypothesis       : None\r\nsphinx           : 2.3.1\r\nblosc            : None\r\nfeather          : None\r\nxlsxwriter       : None\r\nlxml.etree       : None\r\nhtml5lib         : None\r\npymysql          : None\r\npsycopg2         : None\r\njinja2           : 2.10.3\r\nIPython          : 7.11.1\r\npandas_datareader: None\r\nbs4              : None\r\nbottleneck       : None\r\nfastparquet      : None\r\ngcsfs            : None\r\nlxml.etree       : None\r\nmatplotlib       : 3.1.2\r\nnumexpr          : None\r\nodfpy            : None\r\nopenpyxl         : None\r\npandas_gbq       : None\r\npyarrow          : None\r\npytables         : None\r\npytest           : 5.3.2\r\npyxlsb           : None\r\ns3fs             : None\r\nscipy            : 1.4.1\r\nsqlalchemy       : None\r\ntables           : None\r\ntabulate         : None\r\nxarray           : None\r\nxlrd             : None\r\nxlwt             : None\r\nxlsxwriter       : None\r\nnumba            : 0.48.0\r\n\r\n</details>\r\n\n",
    "desc_source": "github_issue"
  },
  "30": {
    "description": "BUG: Series.update() raises ValueError if dtype=\"string\"\n- [x] I have checked that this issue has not already been reported.\r\n\r\n- [x] I have confirmed this bug exists on the latest version of pandas.\r\n\r\n- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.\r\n\r\n---\r\n\r\n#### Code Sample, a copy-pastable example\r\n\r\n```python\r\nimport pandas as pd\r\na = pd.Series([\"a\", None, \"c\"], dtype=\"string\")\r\nb = pd.Series([None, \"b\", None], dtype=\"string\")\r\na.update(b)\r\n\r\n```\r\nresults in:\r\n```python-traceback\r\nTraceback (most recent call last):\r\n\r\n  File \"<ipython-input-15-b9da8f25067a>\", line 1, in <module>\r\n    a.update(b)\r\n\r\n  File \"C:\\tools\\anaconda3\\envs\\Simple\\lib\\site-packages\\pandas\\core\\series.py\", line 2810, in update\r\n    self._data = self._data.putmask(mask=mask, new=other, inplace=True)\r\n\r\n  File \"C:\\tools\\anaconda3\\envs\\Simple\\lib\\site-packages\\pandas\\core\\internals\\managers.py\", line 564, in putmask\r\n    return self.apply(\"putmask\", **kwargs)\r\n\r\n  File \"C:\\tools\\anaconda3\\envs\\Simple\\lib\\site-packages\\pandas\\core\\internals\\managers.py\", line 442, in apply\r\n    applied = getattr(b, f)(**kwargs)\r\n\r\n  File \"C:\\tools\\anaconda3\\envs\\Simple\\lib\\site-packages\\pandas\\core\\internals\\blocks.py\", line 1676, in putmask\r\n    new_values[mask] = new\r\n\r\n  File \"C:\\tools\\anaconda3\\envs\\Simple\\lib\\site-packages\\pandas\\core\\arrays\\string_.py\", line 248, in __setitem__\r\n    super().__setitem__(key, value)\r\n\r\n  File \"C:\\tools\\anaconda3\\envs\\Simple\\lib\\site-packages\\pandas\\core\\arrays\\numpy_.py\", line 252, in __setitem__\r\n    self._ndarray[key] = value\r\n\r\nValueError: NumPy boolean array indexing assignment cannot assign 3 input values to the 1 output values where the mask is true\r\n\r\n```\r\n\r\n\r\n#### Problem description\r\n\r\nThe example works if I leave off the `dtype=\"string\"` (resulting in the implicit dtype `object`). \r\nIMO update should work for all dtypes, not only the \"old\" ones.\r\n\r\n`a = pd.Series([1, None, 3], dtype=\"Int16\")` etc. also raises ValueError, while the same with `dtype=\"float64\"`works. \r\n\r\nIt looks as if update doesn't work with the new nullable dtypes (the ones with `pd.NA`).\r\n\r\n#### Expected Output\r\n\r\nThe expected result is that `a.update(b)` updates `a` without raising an exception, not only for `object` and `float64`, but also for `string` and `Int16` etc..\r\n",
    "desc_source": "github_issue"
  },
  "31": {
    "description": "BUG: str.cat produces NaNs when others is an Index\n- [ ] I have checked that this issue has not already been reported.\r\n\r\n- [ ] I have confirmed this bug exists on the latest version of pandas.\r\n\r\n- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.\r\n\r\n---\r\n\r\n#### Code Sample, a copy-pastable example\r\n\r\n```python\r\nimport pandas as pd\r\nprint(pd.__version__)\r\n\r\nsr = pd.Series(['a','b','c','d','e'])\r\nothers = pd.Index(['a','b','c','d','e'])\r\n\r\nresult = sr.str.cat(others=others)\r\nprint(result)\r\n\r\n1.0.3\r\n0    NaN\r\n1    NaN\r\n2    NaN\r\n3    NaN\r\n4    NaN\r\ndtype: object\r\n\r\n\r\n```\r\n\r\n#### Problem description\r\n\r\nThe result should be the same as when `others` is a list or numpy array with the same values. The result is correct for pandas < 1.0. \r\n\r\n#### Expected Output\r\n\r\n```\r\n0    aa\r\n1    bb\r\n2    cc\r\n3    dd\r\n4    ee\r\ndtype: object\r\n```\r\n",
    "desc_source": "github_issue"
  },
  "32": {
    "description": "read_json with typ=\"series\" of json list of bools results in timestamps/Exception\n#### Code Sample, a copy-pastable example if possible\r\n\r\n```python\r\nimport pandas as pd\r\npd.read_json('[true, true, false]', typ=\"series\")\r\n```\r\nresults in the following Pandas Series object in older Pandas versions:\r\n```\r\n0   1970-01-01 00:00:01\r\n1   1970-01-01 00:00:01\r\n2   1970-01-01 00:00:00\r\ndtype: datetime64[ns]\r\n```\r\nSince 1.0.0 it raises TypeError: <class 'bool'> is not convertible to datetime\r\n\r\n\r\n#### Problem description\r\nThe expected output would be a Pandas Series of bools. Note that\r\n* with typ=\"frame\" it works and the result is a dataframe with one column with bool values\r\n* with convert_dates set to False correctly outputs a Series of boolean values\r\n\r\nThis is a problem because\r\n* users would expect a Series of bools (and neither an exception nor a series of timestamps)\r\n* it is inconsistent with the \"frame\" case\r\n",
    "desc_source": "github_issue"
  },
  "33": {
    "description": "calling `mean` on a `DataFrameGroupBy` with `Int64` dtype results in `TypeError` \n```python\r\nimport pandas as pd\r\n\r\ndf = pd.DataFrame({\r\n    'a' : [0,0,1,1,2,2,3,3],\r\n    'b' : [1,2,3,4,5,6,7,8]\r\n},\r\ndtype='Int64')\r\n\r\ndf.groupby('a').mean()\r\n\r\n```\r\n#### Problem description\r\n\r\nUsing the new nullable integer data type, calling `mean` after grouping results in a `TypeError`. Using `int64` dtype it works:\r\n\r\n```python\r\nimport pandas as pd\r\n\r\ndf = pd.DataFrame({\r\n    'a' : [0,0,1,1,2,2,3,3],\r\n    'b' : [1,2,3,4,5,6,7,8]\r\n},\r\ndtype='int64')\r\n\r\nprint(df.groupby('a').mean())\r\n```\r\n\r\nas does keeping `Int64` dtype but taking a single column to give a `SeriesGroupBy`:\r\n\r\n```python\r\nimport pandas as pd\r\n\r\ndf = pd.DataFrame({\r\n    'a' : [0,0,1,1,2,2,3,3],\r\n    'b' : [1,2,3,4,5,6,7,8]\r\n},\r\ndtype='Int64')\r\n\r\nprint(df.groupby('a')['b'].mean())\r\n```\r\n\r\nThe error does not occur when calling `min`, `max` or `first`, but does also occur with `median` and `std`.\r\n\r\n#### Expected Output\r\n\r\n```\r\n     b\r\na     \r\n0  1.5\r\n1  3.5\r\n2  5.5\r\n3  7.5\r\n```\r\n\r\n\r\n",
    "desc_source": "github_issue"
  },
  "34": {
    "description": "MemoryError when using series.rolling().corr(other) with >1.0\n#### Code Sample, a copy-pastable example if possible\r\n\r\n```python\r\nsrs1 = pd.Series(np.random.rand(11521),pd.date_range('2019-08-15', '2019-08-23',freq='1T'))\r\nsrs2 = pd.Series(np.random.rand(11521),pd.date_range('2019-08-15', '2019-08-23',freq='1T'))\r\nsrs1.rolling(pd.to_timedelta(\"12H\")).corr(srs2)\r\n\r\n```\r\n#### Problem description\r\n\r\n\r\nRunning the code above results in the following error `Unable to allocate 314. TiB for an array with shape (43200000000000,) and data type int64` on pandas 1.0.1. Confirmed that this used to work on pandas 0.25.3.\r\n\r\n#### Expected Output\r\nThe correct calculations\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit           : None\r\npython           : 3.7.4.final.0\r\npython-bits      : 64\r\nOS               : Darwin\r\nOS-release       : 19.2.0\r\nmachine          : x86_64\r\nprocessor        : i386\r\nbyteorder        : little\r\nLC_ALL           : None\r\nLANG             : en_US.UTF-8\r\nLOCALE           : en_US.UTF-8\r\n\r\npandas           : 1.0.1\r\nnumpy            : 1.18.1\r\npytz             : 2019.3\r\ndateutil         : 2.8.1\r\npip              : 20.0.2\r\nsetuptools       : 45.1.0.post20200127\r\nCython           : 0.29.14\r\npytest           : 5.3.5\r\nhypothesis       : 5.4.1\r\nsphinx           : 2.3.1\r\nblosc            : None\r\nfeather          : None\r\nxlsxwriter       : 1.2.7\r\nlxml.etree       : 4.5.0\r\nhtml5lib         : 1.0.1\r\npymysql          : None\r\npsycopg2         : None\r\njinja2           : 2.11.1\r\nIPython          : 7.12.0\r\npandas_datareader: None\r\nbs4              : 4.8.2\r\nbottleneck       : 1.3.1\r\nfastparquet      : 0.3.2\r\ngcsfs            : None\r\nlxml.etree       : 4.5.0\r\nmatplotlib       : 3.1.3\r\nnumexpr          : 2.7.1\r\nodfpy            : None\r\nopenpyxl         : 3.0.3\r\npandas_gbq       : None\r\npyarrow          : 0.13.0\r\npytables         : None\r\npytest           : 5.3.5\r\npyxlsb           : None\r\ns3fs             : None\r\nscipy            : 1.4.1\r\nsqlalchemy       : 1.3.13\r\ntables           : 3.6.1\r\ntabulate         : None\r\nxarray           : 0.13.0\r\nxlrd             : 1.2.0\r\nxlwt             : 1.3.0\r\nxlsxwriter       : 1.2.7\r\nnumba            : 0.48.0\r\n\r\n</details>\n",
    "desc_source": "github_issue"
  },
  "35": {
    "description": "BUG: Series[dim3array] failing to raise ValueError for some Index subclasses (#31816)",
    "desc_source": "commit_msg"
  },
  "36": {
    "description": "Pandas excel output specify column names to write is broken in 1.0.0\n#### Example code:\r\ndf = pd.DataFrame(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]),columns=['col_a', 'col_b', 'col_c'])\r\nexcelwritename = '/tmp/Test%s.xlsx' %pd.datetime.now(pytz.timezone('US/Pacific')).strftime('%b%d_%I_%M %p')\r\nwith pd.ExcelWriter(excelwritename, engine='xlsxwriter',datetime_format='mmm dd yyyy hh:mm AM/PM') as writer:\r\n    df.to_excel(writer,columns=['col_a','col_b'],sheet_name = 'xyz')\r\n\r\n#### Problem description:\r\nspecifying column names to output to excel is no longer working in pandas 1.0.0 and it outputs all columns in the df. \r\nChecked different versions of xlsxwriter, but pandas 0.25.3 works fine, while 1.0.0 does not.\r\n\r\n\r\n#### Expected Output\r\nIt should only output columns that are specified, but it outputs all columns of df\r\n",
    "desc_source": "github_issue"
  },
  "37": {
    "description": "BUG: ser.at match ser.loc with Float64Index (#31329)",
    "desc_source": "commit_msg"
  },
  "38": {
    "description": "BUG: passing TDA and wrong freq to TimedeltaIndex (#31268)",
    "desc_source": "commit_msg"
  },
  "39": {
    "description": "Read_json overflow error when json contains big number strings \n#### Code Sample, a copy-pastable example if possible\r\n\r\n```python\r\nimport json\r\nimport pandas as pd\r\n\r\ntest_data = [{\"col\": \"31900441201190696999\"}, {\"col\": \"Text\"}]\r\ntest_json = json.dumps(test_data)\r\npd.read_json(test_json)\r\n\r\n```\r\n#### Problem description\r\n\r\nThe current behaviour doesn't return a dateframe for a valid JSON. Note when the number is smaller, it works fine. It also works when only big numbers are present. It would be cool to have it work with big numbers as it works for small numbers.\r\n\r\n#### Expected Output\r\nA dataframe with a number and string\r\n```\r\n       col\r\n0  3.190044e+19\r\n1     Text\r\n```\r\n\r\n#### Output of ``pd.read_json()``\r\n\r\n<details>\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \".../.venv/lib/python3.6/site-packages/pandas/io/json/_json.py\", line 592, in read_json\r\n    result = json_reader.read()\r\n  File \".../.venv/lib/python3.6/site-packages/pandas/io/json/_json.py\", line 717, in read\r\n    obj = self._get_object_parser(self.data)\r\n  File \".../.venv/lib/python3.6/site-packages/pandas/io/json/_json.py\", line 739, in _get_object_parser\r\n    obj = FrameParser(json, **kwargs).parse()\r\n  File \".../.venv/lib/python3.6/site-packages/pandas/io/json/_json.py\", line 855, in parse\r\n    self._try_convert_types()\r\n  File \".../.venv/lib/python3.6/site-packages/pandas/io/json/_json.py\", line 1151, in _try_convert_types\r\n    lambda col, c: self._try_convert_data(col, c, convert_dates=False)\r\n  File \".../.venv/lib/python3.6/site-packages/pandas/io/json/_json.py\", line 1131, in _process_converter\r\n    new_data, result = f(col, c)\r\n  File \".../.venv/lib/python3.6/site-packages/pandas/io/json/_json.py\", line 1151, in <lambda>\r\n    lambda col, c: self._try_convert_data(col, c, convert_dates=False)\r\n  File \".../.venv/lib/python3.6/site-packages/pandas/io/json/_json.py\", line 927, in _try_convert_data\r\n    new_data = data.astype(\"int64\")\r\n  File \".../.venv/lib/python3.6/site-packages/pandas/core/generic.py\", line 5882, in astype\r\n    dtype=dtype, copy=copy, errors=errors, **kwargs\r\n  File \".../.venv/lib/python3.6/site-packages/pandas/core/internals/managers.py\", line 581, in astype\r\n    return self.apply(\"astype\", dtype=dtype, **kwargs)\r\n  File \".../.venv/lib/python3.6/site-packages/pandas/core/internals/managers.py\", line 438, in apply\r\n    applied = getattr(b, f)(**kwargs)\r\n  File \".../.venv/lib/python3.6/site-packages/pandas/core/internals/blocks.py\", line 559, in astype\r\n    return self._astype(dtype, copy=copy, errors=errors, values=values, **kwargs)\r\n  File \".../.venv/lib/python3.6/site-packages/pandas/core/internals/blocks.py\", line 643, in _astype\r\n    values = astype_nansafe(vals1d, dtype, copy=True, **kwargs)\r\n  File \".../.venv/lib/python3.6/site-packages/pandas/core/dtypes/cast.py\", line 707, in astype_nansafe\r\n    return lib.astype_intsafe(arr.ravel(), dtype).reshape(arr.shape)\r\n  File \"pandas/_libs/lib.pyx\", line 547, in pandas._libs.lib.astype_intsafe\r\nOverflowError: Python int too large to convert to C long\r\n\r\n</details>\r\n\n",
    "desc_source": "github_issue"
  },
  "40": {
    "description": "pandas.DataFrame.sum() returns wrong type for subclassed pandas DataFrame\n#### Code Sample, a copy-pastable example if possible\r\n\r\n```python\r\n\r\n# the following code is obtained from the documentation\r\n# https://pandas.pydata.org/pandas-docs/stable/development/extending.html\r\n\r\nimport pandas as pd\r\n\r\nclass SubclassedSeries(pd.Series):\r\n    @property\r\n    def _constructor(self):\r\n        return SubclassedSeries\r\n    @property\r\n    def _constructor_expanddim(self):\r\n        return SubclassedDataFrame\r\n\r\n\r\nclass SubclassedDataFrame(pd.DataFrame):\r\n    @property\r\n    def _constructor(self):\r\n        return SubclassedDataFrame\r\n    @property\r\n    def _constructor_sliced(self):\r\n        return SubclassedSeries\r\n\r\n# create a class instance as in the example of the documentation\r\n\r\ndf = SubclassedDataFrame({'A': [1, 2, 3], 'B': [4, 5, 6], 'C': [7, 8, 9]})\r\n>>> df\r\n   A  B  C\r\n0  1  4  7\r\n1  2  5  8\r\n2  3  6  9\r\n\r\n# this works just fine\r\n\r\n>>> type(df)\r\n<class '__main__.SubclassedDataFrame'>\r\n\r\n# slicing also works fine\r\n\r\n>>> sliced2 = df['A']\r\n>>> sliced2\r\n0    1\r\n1    2\r\n2    3\r\nName: A, dtype: int64\r\n\r\n>>> type(sliced2)\r\n<class '__main__.SubclassedSeries'>\r\n\r\n# however, the sum operation returns a pandas.Series, not SubclassedSeries\r\n\r\n>>> sliced3 = df.sum()\r\n>>> sliced3\r\n0    1\r\n1    2\r\n2    3\r\nName: A, dtype: int64\r\n\r\n>>> type(sliced3)\r\n<class 'pandas.core.series.Series'>\r\n\r\n```\r\n#### Problem description\r\n\r\nIn our project, we extend pandas as described in the documentation and implement our own kind of DataFrame and Series, similar to the geopandas project (if you apply sum on their DataFrame, the same problem appears). If you want to use _reduce operations like sum, it is important that the correct SubclassedSeries is returned. Otherwise, inheritance from pandas.DataFrames is not possible.\r\n\r\n#### Expected Output\r\n```python\r\n>>> type(sliced3)\r\n<class '__main__.SubclassedSeries'>\r\n\r\n```\r\n\r\nI think I can provide a possible fix of this problem: The relevant code is contained in core/frame.py just before the return statement of the _reduce function:\r\n\r\n```python\r\n\r\n# this is the code in core/frame.py:\r\ndef _reduce(...):\r\n        # .... left out\r\n        if constructor is not None:\r\n            result = Series(result, index=labels)\r\n        return result\r\n\r\n# I suggest the following change:\r\ndef _reduce(...):\r\n        # .... left out\r\n        if constructor is None:\r\n            result = Series(result, index=labels)\r\n        else:\r\n            result = constructor(result, index=labels)\r\n        # alternative (since constructor will create a SubclassedDataFrame):\r\n            result = self._constructor_sliced(result, index=labels)\r\n        return result\r\n\r\n```\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\n\r\n[paste the output of ``pd.show_versions()`` here below this line]\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 3.7.2.final.0\r\npython-bits: 64\r\nOS: Windows\r\nOS-release: 10\r\nmachine: AMD64\r\nprocessor: Intel64 Family 6 Model 142 Stepping 10, GenuineIntel\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: None\r\nLOCALE: None.None\r\n\r\npandas: 0.24.1\r\npytest: None\r\npip: 19.0.3\r\nsetuptools: 40.8.0\r\nCython: None\r\nnumpy: 1.16.2\r\nscipy: None\r\npyarrow: None\r\nxarray: None\r\nIPython: None\r\nsphinx: None\r\npatsy: None\r\ndateutil: 2.8.0\r\npytz: 2018.9\r\nblosc: None\r\nbottleneck: None\r\ntables: None\r\nnumexpr: None\r\nfeather: None\r\nmatplotlib: None\r\nopenpyxl: None\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml.etree: None\r\nbs4: None\r\nhtml5lib: None\r\nsqlalchemy: None\r\npymysql: None\r\npsycopg2: None\r\njinja2: None\r\ns3fs: None\r\nfastparquet: None\r\npandas_gbq: None\r\npandas_datareader: None\r\ngcsfs: None\r\n\r\n</details>\r\n\n",
    "desc_source": "github_issue"
  },
  "41": {
    "description": "BUG: Fixed upcast dtype for datetime64 in merge (#31211)",
    "desc_source": "commit_msg"
  },
  "42": {
    "description": "DataFrame.unstack() with list of levels ignores fill_value\n#### Code Sample, a copy-pastable example if possible\r\n\r\n```python\r\n>>> import pandas as pd\r\n>>> df = (\r\n...     pd.DataFrame(\r\n...         {\r\n...             \"name\": [\"Alice\", \"Bob\"],\r\n...             \"score\": [9.5, 8],\r\n...             \"employed\": [False, True],\r\n...             \"kids\": [0, 0],\r\n...             \"gender\": [\"female\", \"male\"],\r\n...         }\r\n...     )\r\n...     .set_index([\"name\", \"employed\", \"kids\", \"gender\"])\r\n...     .unstack([\"gender\"], fill_value=0)\r\n... )\r\n>>> df.unstack([\"employed\", \"kids\"], fill_value=0)\r\n          score\r\ngender   female        male\r\nemployed  False True  False True\r\nkids          0     0     0     0\r\nname\r\nAlice       9.5   NaN   0.0   NaN\r\nBob         NaN   0.0   NaN   8.0\r\n```\r\n#### Problem description\r\n\r\nwhen unstacking with a list of levels on a DataFrame that already has a columns MultiIndex, fill_value is ignored.\r\n\r\n#### Expected Output\r\n\r\n```python\r\n>>> df.unstack(\"employed\", fill_value=0).unstack(\"kids\", fill_value=0)\r\n          score\r\ngender   female        male\r\nemployed  False True  False True\r\nkids          0     0     0     0\r\nname\r\nAlice       9.5   0.0   0.0   0.0\r\nBob         0.0   0.0   0.0   8.0\r\n>>>\r\n```\r\n",
    "desc_source": "github_issue"
  },
  "43": {
    "description": "BUG/ERR: wrong error in DataFrame.drop with non-unique datetime index + invalid keys\nConsider this example, where there is a DataFrame with a non-unique DatetimeIndex:\r\n\r\n```\r\nIn [8]: df = pd.DataFrame(np.random.randn(5, 3), columns=['a', 'b', 'c'], index=pd.date_range(\"2012\", freq='H', periods=5))\r\n\r\nIn [9]: df = df.iloc[[0, 2, 2, 3]] \r\n\r\nIn [10]: df\r\nOut[10]: \r\n                            a         b         c\r\n2012-01-01 00:00:00 -1.534726 -0.559295  0.207194\r\n2012-01-01 02:00:00 -1.072027  0.376595  0.407512\r\n2012-01-01 02:00:00 -1.072027  0.376595  0.407512\r\n2012-01-01 03:00:00  0.581614  1.782635 -0.678197\r\n```\r\n\r\nIf you then use `drop` to drop some columns, but forget to specify `columns=` or `axis=1` (so you are actually dropping rows), you get a wrong error and very confusing error message:\r\n\r\n```\r\nIn [10]: df.drop(['a', 'b'])\r\n...\r\n\r\n~/scipy/pandas/pandas/core/indexes/base.py in get_indexer_non_unique(self, target)\r\n   4559             tgt_values = target._ndarray_values\r\n   4560 \r\n-> 4561         indexer, missing = self._engine.get_indexer_non_unique(tgt_values)\r\n   4562         return ensure_platform_int(indexer), missing\r\n   4563 \r\n\r\n~/scipy/pandas/pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_indexer_non_unique()\r\n\r\nTypeError: 'NoneType' object is not iterable\r\n```\r\n\r\nTested with pandas 0.25 and pandas master.\n",
    "desc_source": "github_issue"
  },
  "44": {
    "description": "BUG: merge_asof with tz_aware left index and right column (#29940)",
    "desc_source": "commit_msg"
  },
  "45": {
    "description": "BUG: Series.count() raises exception after upgrading from v0.24.1 to v0.25.3  if use_inf_as_na is enabled for a DateTime series.\n#### Code Sample, a copy-pastable example if possible\r\n\r\n```python\r\n# Your code here\r\nimport pandas as pd\r\nfrom datetime import datetime\r\n\r\nif __name__ == '__main__':\r\n    s = pd.Series([datetime.now()])\r\n    with pd.option_context('use_inf_as_na', True):\r\n        s.count()\r\n```\r\n#### Problem description\r\n\r\nPreviously with pandas v0.24.1, the above code works well,\r\nhowever, after I upgrade the version to v0.25.3,\r\nthe above code raises an exception:\r\n`AttributeError: 'DatetimeArray' object has no attribute '_constructor'`\r\n\r\nIt seems that something goes wrong when passing a `DatetimeArray` to `_isna_old`.\r\nI think it is a bug introduced in some recent update.\r\n",
    "desc_source": "github_issue"
  },
  "46": {
    "description": "CLN/BUG: array_equivalent on nested objects (#28347)",
    "desc_source": "commit_msg"
  },
  "47": {
    "description": "No fucks given\n\n",
    "desc_source": "github_issue"
  },
  "48": {
    "description": "Crash in git module\nI was pushing a branch, and executed `fuck` afterwards. Here is the stacktrace:\r\n\r\n```Traceback (most recent call last):\r\n  File \"/usr/local/bin/thefuck\", line 11, in <module>\r\n    sys.exit(main())\r\n  File \"/usr/local/lib/python3.5/dist-packages/thefuck/entrypoints/main.py\", line 26, in main\r\n    fix_command(known_args)\r\n  File \"/usr/local/lib/python3.5/dist-packages/thefuck/entrypoints/fix_command.py\", line 42, in fix_command\r\n    selected_command = select_command(corrected_commands)\r\n  File \"/usr/local/lib/python3.5/dist-packages/thefuck/ui.py\", line 71, in select_command\r\n    selector = CommandSelector(corrected_commands)\r\n  File \"/usr/local/lib/python3.5/dist-packages/thefuck/ui.py\", line 34, in __init__\r\n    self._commands = [next(self._commands_gen)]\r\n  File \"/usr/local/lib/python3.5/dist-packages/thefuck/corrector.py\", line 43, in organize_commands\r\n    first_command = next(corrected_commands)\r\n  File \"/usr/local/lib/python3.5/dist-packages/thefuck/corrector.py\", line 74, in <genexpr>\r\n    for corrected in rule.get_corrected_commands(command))\r\n  File \"/usr/local/lib/python3.5/dist-packages/thefuck/types.py\", line 189, in get_corrected_commands\r\n    new_commands = self.get_new_command(command)\r\n  File \"<decorator-gen-104>\", line 2, in get_new_command\r\n  File \"/usr/local/lib/python3.5/dist-packages/thefuck/specific/git.py\", line 32, in git_support\r\n    return fn(command)\r\n  File \"/usr/local/lib/python3.5/dist-packages/thefuck/rules/git_push.py\", line 36, in get_new_command\r\n    arguments = re.findall(r'git push (.*)', command.output)[0].strip()\r\nIndexError: list index out of range\r\n```\r\n\r\nThis is the output from the command before (which was only `git push`):\r\n```git push --set-upstream origin feature/5_exec_command [enter/\u2191/\u2193/ctrl+c]\r\nTotal 0 (delta 0), reused 0 (delta 0)\r\nremote: \r\nremote: Create pull request for feature/[...]:\r\nremote:   https://bitbucket.org/[...]\r\nremote: \r\nTo git@bitbucket.org:[...].git\r\n   e5e7fbb..700d998  feature/[...] -> feature/[...]\r\nBranch feature/[...] set up to track remote branch feature/[...] from origin.\r\n```\n",
    "desc_source": "github_issue"
  },
  "49": {
    "description": "Use --force-with-lease instead of --force for git push\n\n--force flag can be very dangerous, because it unconditionally\noverwrites remote branch - if someone pushed new commits to the remote\nrepo after you last fetched/pulled, and you do push --force, you will\noverwrite his commits without even knowing that you did that.  Using\n--force-with-lease is much safer because it only overwrites remote\nbranch when it points to the same commit that you think it points to.\n\nRead more:\nhttps://developer.atlassian.com/blog/2015/04/force-with-lease/",
    "desc_source": "commit_msg"
  },
  "50": {
    "description": "Support for hdfs dfs -mkdir -p /directory/subdirectory",
    "desc_source": "commit_msg"
  },
  "51": {
    "description": "Fix the open rule\n\nIt was simply wrong with `xdg-`, `gnome-` and `kde-open`.",
    "desc_source": "commit_msg"
  },
  "52": {
    "description": "Fix the `git_diff_staged` rule\n\nThe problem was:\n```\n% git add foo\n% git diff foo\n% fuck\ngit diff foo --staged [enter/ctrl+c]\nfatal: bad flag '--staged' used after filename\n```",
    "desc_source": "commit_msg"
  },
  "53": {
    "description": "Fix handling of chunked-encoded requests with uppercase \"Transfer-Encoding\" value\n\n`HTTP1Connection._can_keep_alive` applied lower() to the header's value, but\n`_read_body` didn't.",
    "desc_source": "commit_msg"
  },
  "54": {
    "description": "ioloop.py(line 252) is None or not None\n![qq20150724-1](https://cloud.githubusercontent.com/assets/1759875/8869574/e79c044c-3217-11e5-96ec-3d3d553e5b5d.png)\n\nline 252\uff0c IOLoop.current(instance=False) is None\u3002why then raise \"already exists\"?\n\n",
    "desc_source": "github_issue"
  },
  "55": {
    "description": "Use var_list argument in TFOptimizer wrapper (#12106)\n\n* Update optimizers.py\r\n\r\nfix bug in TFOptimizer, this optimizer should call tensorflow native optimizer with the named param var_list\r\n\r\n* add test_tfoptimizer_pass_correct_named_params_to_compute_gradient\r\n\r\n* fix formating\r\n\r\n* fix test\r\n\r\n* fix test\r\n\r\n* fix formating\r\n\r\n* fix test",
    "desc_source": "commit_msg"
  },
  "56": {
    "description": "[P] Fixed NaN loss when given a mask with all zeros (#11643)",
    "desc_source": "commit_msg"
  },
  "57": {
    "description": "Scikit Learn wrapper predict() inappropriately squashes size-1 batch dimension\nPlease make sure that the boxes below are checked before you submit your issue.\r\nIf your issue is an **implementation question**, please ask your question on [StackOverflow](http://stackoverflow.com/questions/tagged/keras) or [on the Keras Slack channel](https://keras-slack-autojoin.herokuapp.com/) instead of opening a GitHub issue.\r\n\r\nThank you!\r\n\r\n- [X] Check that you are up-to-date with the master branch of Keras. You can update with:\r\n`pip install git+git://github.com/keras-team/keras.git --upgrade --no-deps`\r\nUsing Keras version 2.2.4\r\n\r\n- [X] Check that your version of TensorFlow is up-to-date. The installation instructions can be found [here](https://www.tensorflow.org/get_started/os_setup).\r\nUsing Tensorflow version 1.12.0\r\n\r\n- [X] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).\r\n\r\nIf predict() is called on input with shape `(1, num_features)`, then the output is a 0-dimensional array instead of a 1-dimensional array with 1 element.\r\n\r\n```python\r\nimport keras\r\nimport keras.wrappers.scikit_learn\r\nimport numpy as np\r\nimport sklearn.linear_model\r\nimport sklearn.metrics\r\n\r\ndef build_net():\r\n    model = keras.models.Sequential([keras.layers.Dense(units=1, input_dim=2)])\r\n    model.compile(loss=keras.losses.mean_squared_error, optimizer=\"sgd\")\r\n    return model\r\n\r\nregressor = keras.wrappers.scikit_learn.KerasRegressor(build_fn=build_net)\r\n# Works with the sklearn regressors\r\n# regressor = sklearn.linear_model.LinearRegression()\r\nX = np.zeros((1, 2))\r\nY = np.zeros((1,))\r\nregressor.fit(X, Y)\r\nY_pred = regressor.predict(X)\r\nprint(Y_pred.shape)  # Is (), should be (1,)\r\n# As a result, this fails with an exception\r\n# TypeError: Singleton array array(0., dtype=float32) cannot be considered a valid collection.\r\nprint(sklearn.metrics.mean_squared_error(y_true=Y, y_pred=Y_pred))\r\n```\n",
    "desc_source": "github_issue"
  },
  "58": {
    "description": "Fix StackedRNNCells step input shape (#9090)\n\n* Fix StackedRNNCells step input shape\r\n\r\n* Add test for input shape in MinimalRNNCell.build",
    "desc_source": "commit_msg"
  },
  "59": {
    "description": "Fix progress bar when total number of steps is unknown (#9052)\n\n* Fix get_file when total size is unknown\r\n\r\n* Update test case",
    "desc_source": "commit_msg"
  },
  "60": {
    "description": "lock: Fix clone field implementation to handle sub-models in response_model (#889)",
    "desc_source": "commit_msg"
  },
  "61": {
    "description": "Tests fail on windows\nI suspected this had more to do with my git client setup than an actual bug, but since I'm just using the stock GitHub Windows GUI and these files open just fine in notepad, I thought I'd file an issue here.\r\n\r\nThe problem is caused by these characters: https://github.com/ambv/black/blob/master/tests/expression.py#L138-L139\r\n\r\n```\r\nERROR: test_expression_diff (tests.test_black.BlackTestCase)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"c:\\users\\zsolz\\documents\\github\\black\\tests\\test_black.py\", line 165, in test_expression_diff\r\n    tmp_file = Path(black.dump_to_file(source))\r\n  File \"c:\\users\\zsolz\\documents\\github\\black\\black.py\", line 2161, in dump_to_file\r\n    f.write(lines)\r\n  File \"C:\\Users\\zsolz\\.virtualenvs\\black-TlIYXM7K\\lib\\tempfile.py\", line 483, in func_wrapper\r\n    return func(*args, **kwargs)\r\n  File \"C:\\Users\\zsolz\\.virtualenvs\\black-TlIYXM7K\\lib\\encodings\\cp1252.py\", line 19, in encode\r\n    return codecs.charmap_encode(input,self.errors,encoding_table)[0]\r\nUnicodeEncodeError: 'charmap' codec can't encode character '\\u0142' in position 4011: character maps to <undefined>\r\n```\r\n\r\n```\r\nERROR: test_expression_ff (tests.test_black.BlackTestCase)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"c:\\users\\zsolz\\documents\\github\\black\\tests\\test_black.py\", line 150, in test_expression_ff\r\n    tmp_file = Path(black.dump_to_file(source))\r\n  File \"c:\\users\\zsolz\\documents\\github\\black\\black.py\", line 2161, in dump_to_file\r\n    f.write(lines)\r\n  File \"C:\\Users\\zsolz\\.virtualenvs\\black-TlIYXM7K\\lib\\tempfile.py\", line 483, in func_wrapper\r\n    return func(*args, **kwargs)\r\n  File \"C:\\Users\\zsolz\\.virtualenvs\\black-TlIYXM7K\\lib\\encodings\\cp1252.py\", line 19, in encode\r\n    return codecs.charmap_encode(input,self.errors,encoding_table)[0]\r\nUnicodeEncodeError: 'charmap' codec can't encode character '\\u0142' in position 4011: character maps to <undefined>\r\n```\r\n\r\n\n",
    "desc_source": "github_issue"
  },
  "62": {
    "description": "LightSource.shade fails on a masked array\n<!--To help us understand and resolve your issue, please fill out the form to the best of your ability.-->\r\n<!--You can feel free to delete the sections that do not apply.-->\r\n\r\n### Bug report\r\n\r\n**Bug summary**\r\n\r\n`shade` fails on a masked array. The offender seems to be `intensity = np.clip(intensity, 0, 1)` in `shade_normals`, as `np.clip` returns a masked array with attribute `mask=False` and `shade_rgb` expects the mask attribute to be 2D.\r\n\r\n<!--A short 1-2 sentences that succinctly describes the bug-->\r\n\r\n**Code for reproduction**\r\n\r\n<!--A minimum code snippet required to reproduce the bug.\r\nPlease make sure to minimize the number of dependencies required, and provide\r\nany necessary plotted data.\r\nAvoid using threads, as Matplotlib is (explicitly) not thread-safe.-->\r\n\r\n```python\r\nimport numpy as np\r\nfrom matplotlib.colors import LightSource\r\nimport matplotlib.cm as cm\r\n\r\ndelta = 0.5\r\nx = np.arange(-3.0, 4.001, delta)\r\ny = np.arange(-4.0, 3.001, delta)\r\nX, Y = np.meshgrid(x, y)\r\nZ1 = np.exp(-X**2 - Y**2)\r\nZ2 = np.exp(-(X - 1)**2 - (Y - 1)**2)\r\nZ = Z1 - Z2\r\n\r\nmask = np.zeros(Z.shape, dtype = bool)\r\n\r\nZ = np.ma.masked_array(Z, mask = mask)\r\n\r\nls = LightSource(azdeg = 45., altdeg = 0.)\r\n\r\nrgba = ls.shade(Z, cmap = cm.get_cmap())\r\n```\r\n\r\n**Actual outcome**\r\n\r\n<!--The output produced by the above code, which may be a screenshot, console output, etc.-->\r\n\r\n```\r\n  File \"/Users/kpenner/anaconda3/envs/mpl/lib/python3.8/site-packages/matplotlib/colors.py\", line 1869, in shade\r\n    rgb1 = self.shade_rgb(rgb0, elevation=data, blend_mode=blend_mode,\r\n  File \"/Users/kpenner/anaconda3/envs/mpl/lib/python3.8/site-packages/matplotlib/colors.py\", line 1943, in shade_rgb\r\n    mask = intensity.mask[..., 0]\r\nIndexError: invalid index to scalar variable.\r\n```\r\n\r\n**Expected outcome**\r\n\r\n<!--A description of the expected outcome from the code snippet-->\r\n<!--If this used to work in an earlier version of Matplotlib, please note the version it used to work on-->\r\n\r\n**Matplotlib version**\r\n<!--Please specify your platform and versions of the relevant libraries you are using:-->\r\n  * Matplotlib version: 3.2.1+2358.g9e20541c9\r\n  * Python version: 3.8\r\n  * numpy version: 1.18.4\r\n<!--Please tell us how you installed matplotlib and python e.g., from source, pip, conda-->\r\n<!--If you installed from conda, please specify which channel you used if not the default-->\r\n\r\n\n",
    "desc_source": "github_issue"
  },
  "63": {
    "description": "Heatmaps are being truncated when using with seaborn\n### Bug report\r\n\r\n**Bug summary**\r\n\r\nThe very top and bottom of the heatmaps are getting truncated to 1/2 height in version `3.1.1`. This does not happen for version `3.0.3`.\r\n\r\nThis is the code from a Jupyter Notebook\r\n```python\r\nimport matplotlib\r\nimport pandas as pd\r\nimport numpy as np\r\nimport seaborn as sb\r\n%pylab inline\r\n\r\nprint(matplotlib.__version__)\r\nprint(sb.__version__)\r\n\r\ngrid = pd.DataFrame(np.arange(9).reshape((3,3))/10)\r\nfig, ax = subplots(1, 1, figsize=(5, 5))\r\n\r\nsb.heatmap(grid, annot=True, fmt=\".0f\", ax=ax, cmap='RdBu',  vmin=0, vmax=1, cbar=True);\r\n```\r\n**Actual outcome**\r\n\r\n```\r\nPopulating the interactive namespace from numpy and matplotlib\r\n3.1.1\r\n0.9.0\r\n```\r\n\r\n\r\n![download](https://user-images.githubusercontent.com/1696066/60530345-87f5c780-9cc6-11e9-9e31-a251283f3f7e.png)\r\n\r\n**Matplotlib version**\r\n<!--Please specify your platform and versions of the relevant libraries you are using:-->\r\n  * Operating system: Ubuntu Linux\r\n  * Matplotlib version: 3.1.1\r\n  * Matplotlib backend: module://ipykernel.pylab.backend_inline\r\n  * Python version: 3.6\r\n  * Jupyter version (if applicable): 4.4.0\r\n  * Other libraries: \r\n\n",
    "desc_source": "github_issue"
  },
  "64": {
    "description": "Don't misclip axis when calling set_ticks on inverted axes.\n\nA small inversion in 0e41317...",
    "desc_source": "commit_msg"
  },
  "65": {
    "description": "colorbar(label=None) should give an empty label",
    "desc_source": "commit_msg"
  },
  "66": {
    "description": "fix `tenumerate(start)`",
    "desc_source": "commit_msg"
  },
  "67": {
    "description": "'tqdm' object has no attribute 'total' when using 'disable' and multiprocessing 'Pool'\nThe following MWE shows how to get the error message described in the title:\r\n\r\n```python\r\nfrom tqdm import tqdm\r\nfrom multiprocessing import Pool\r\n\r\ndef f(arg):\r\n    pass\r\n\r\nif __name__ == '__main__':\r\n    pool = Pool()\r\n    list(tqdm(pool.imap(f, range(100)), disable=True))\r\n```\r\n\r\n- tqdm Version: 4.22.0\r\n- python 3.6.0\r\n- Win64\r\n\r\nIt works, when the `disable=True` is removed or set to `False`. But it should also work for the disabled case I would guess.\n",
    "desc_source": "github_issue"
  },
  "68": {
    "description": "Fix a bug with new bar_format + fix unit test\n\nSigned-off-by: Stephen L. <lrq3000@gmail.com>",
    "desc_source": "commit_msg"
  }
}