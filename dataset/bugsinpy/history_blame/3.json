{
  "id": "3",
  "blame_commit": {
    "commit": {
      "commit_id": "9cbf18822c74790009841c3305c0ae9d77726726",
      "commit_message": "Use columns defined for COPY to Redshift so we never rely on order (#2245)\n\n* Use columns defined for COPY to Redshift so we never rely on order",
      "commit_author": "James McMurray",
      "commit_date": "2017-10-03 09:06:41",
      "commit_parent": "68d75990dbdaeb7c2a5cfd6aeb0c616cda840542"
    },
    "function": {
      "function_name": "copy",
      "function_code_before": "def copy(self, cursor, f):\n    \"\"\"\n        Defines copying from s3 into redshift.\n\n        If both key-based and role-based credentials are provided, role-based will be used.\n        \"\"\"\n    logger.info('Inserting file: %s', f)\n    cursor.execute(\"\\n         COPY {table} from '{source}'\\n         CREDENTIALS '{creds}'\\n         {options}\\n         ;\".format(table=self.table, source=f, creds=self._credentials(), options=self.copy_options))",
      "function_code_after": "def copy(self, cursor, f):\n    \"\"\"\n        Defines copying from s3 into redshift.\n\n        If both key-based and role-based credentials are provided, role-based will be used.\n        \"\"\"\n    logger.info('Inserting file: %s', f)\n    colnames = ''\n    if len(self.columns) > 0:\n        colnames = ','.join([x[0] for x in self.columns])\n        colnames = '({})'.format(colnames)\n    cursor.execute(\"\\n         COPY {table} {colnames} from '{source}'\\n         CREDENTIALS '{creds}'\\n         {options}\\n         ;\".format(table=self.table, colnames=colnames, source=f, creds=self._credentials(), options=self.copy_options))",
      "function_before_start_line": 330,
      "function_before_end_line": 346,
      "function_after_start_line": 330,
      "function_after_end_line": 352,
      "function_before_token_count": 51,
      "function_after_token_count": 95,
      "functions_name_modified_file": [
        "prune",
        "output",
        "configuration_section",
        "post_copy",
        "_credentials",
        "user",
        "create_table",
        "s3_load_path",
        "do_prune",
        "_get_configuration_attribute",
        "do_truncate_table",
        "s3_unload_path",
        "aws_account_id",
        "host",
        "aws_session_token",
        "prune_column",
        "queries",
        "prune_date",
        "copy_json_options",
        "truncate_table",
        "aws_secret_access_key",
        "aws_arn_role_name",
        "table_type",
        "password",
        "run",
        "unload_options",
        "prune_table",
        "copy_options",
        "table_attributes",
        "does_table_exist",
        "init_copy",
        "jsonpath",
        "aws_access_key_id",
        "unload_query",
        "copy",
        "update_id",
        "database"
      ],
      "functions_name_all_files": [
        "prune",
        "output",
        "configuration_section",
        "post_copy",
        "_credentials",
        "user",
        "create_table",
        "s3_load_path",
        "do_prune",
        "_get_configuration_attribute",
        "do_truncate_table",
        "s3_unload_path",
        "aws_account_id",
        "host",
        "aws_session_token",
        "prune_column",
        "queries",
        "prune_date",
        "copy_json_options",
        "truncate_table",
        "aws_secret_access_key",
        "aws_arn_role_name",
        "table_type",
        "password",
        "run",
        "unload_options",
        "prune_table",
        "copy_options",
        "table_attributes",
        "does_table_exist",
        "init_copy",
        "jsonpath",
        "aws_access_key_id",
        "unload_query",
        "copy",
        "update_id",
        "database"
      ],
      "functions_name_co_evolved_modified_file": [],
      "functions_name_co_evolved_all_files": []
    },
    "file": {
      "file_name": "redshift.py",
      "file_nloc": 449,
      "file_complexity": 78,
      "file_token_count": 1907,
      "file_before": "# -*- coding: utf-8 -*-\n#\n# Copyright 2012-2015 Spotify AB\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport abc\nimport json\nimport logging\nimport time\nimport os\n\nimport luigi\nfrom luigi.contrib import postgres\nfrom luigi.contrib import rdbms\nfrom luigi.contrib.s3 import S3PathTask, S3Target\n\nlogger = logging.getLogger('luigi-interface')\n\n\ntry:\n    import psycopg2\n    import psycopg2.errorcodes\nexcept ImportError:\n    logger.warning(\"Loading postgres module without psycopg2 installed. \"\n                   \"Will crash at runtime if postgres functionality is used.\")\n\n\nclass _CredentialsMixin():\n    \"\"\"\n    This mixin is used to provide the same credential properties\n    for AWS to all Redshift tasks. It also provides a helper method\n    to generate the credentials string for the task.\n    \"\"\"\n\n    @property\n    def configuration_section(self):\n        \"\"\"\n        Override to change the configuration section used\n        to obtain default credentials.\n        \"\"\"\n        return 'redshift'\n\n    @property\n    def aws_access_key_id(self):\n        \"\"\"\n        Override to return the key id.\n        \"\"\"\n        return self._get_configuration_attribute('aws_access_key_id')\n\n    @property\n    def aws_secret_access_key(self):\n        \"\"\"\n        Override to return the secret access key.\n        \"\"\"\n        return self._get_configuration_attribute('aws_secret_access_key')\n\n    @property\n    def aws_account_id(self):\n        \"\"\"\n        Override to return the account id.\n        \"\"\"\n        return self._get_configuration_attribute('aws_account_id')\n\n    @property\n    def aws_arn_role_name(self):\n        \"\"\"\n        Override to return the arn role name.\n        \"\"\"\n        return self._get_configuration_attribute('aws_arn_role_name')\n\n    @property\n    def aws_session_token(self):\n        \"\"\"\n        Override to return the session token.\n        \"\"\"\n        return self._get_configuration_attribute('aws_session_token')\n\n    def _get_configuration_attribute(self, attribute):\n        config = luigi.configuration.get_config()\n\n        value = config.get(self.configuration_section, attribute, default=None)\n\n        if not value:\n            value = os.environ.get(attribute.upper(), None)\n\n        return value\n\n    def _credentials(self):\n        \"\"\"\n        Return a credential string for the provided task. If no valid\n        credentials are set, raise a NotImplementedError.\n        \"\"\"\n\n        if self.aws_account_id and self.aws_arn_role_name:\n            return 'aws_iam_role=arn:aws:iam::{id}:role/{role}'.format(\n                id=self.aws_account_id,\n                role=self.aws_arn_role_name\n            )\n        elif self.aws_access_key_id and self.aws_secret_access_key:\n            return 'aws_access_key_id={key};aws_secret_access_key={secret}{opt}'.format(\n                key=self.aws_access_key_id,\n                secret=self.aws_secret_access_key,\n                opt=';token={}'.format(self.aws_session_token) if self.aws_session_token else ''\n            )\n        else:\n            raise NotImplementedError(\"Missing Credentials. \"\n                                      \"Ensure one of the pairs of auth args below are set \"\n                                      \"in a configuration file, environment variables or by \"\n                                      \"being overridden in the task: \"\n                                      \"'aws_access_key_id' AND 'aws_secret_access_key' OR \"\n                                      \"'aws_account_id' AND 'aws_arn_role_name'\")\n\n\nclass RedshiftTarget(postgres.PostgresTarget):\n    \"\"\"\n    Target for a resource in Redshift.\n\n    Redshift is similar to postgres with a few adjustments\n    required by redshift.\n    \"\"\"\n    marker_table = luigi.configuration.get_config().get(\n        'redshift',\n        'marker-table',\n        'table_updates')\n\n    use_db_timestamps = False\n\n\nclass S3CopyToTable(rdbms.CopyToTable, _CredentialsMixin):\n    \"\"\"\n    Template task for inserting a data set into Redshift from s3.\n\n    Usage:\n\n    * Subclass and override the required attributes:\n\n      * `host`,\n      * `database`,\n      * `user`,\n      * `password`,\n      * `table`,\n      * `columns`,\n      * `s3_load_path`.\n\n    * You can also override the attributes provided by the\n      CredentialsMixin if they are not supplied by your\n      configuration or environment variables.\n    \"\"\"\n\n    @abc.abstractmethod\n    def s3_load_path(self):\n        \"\"\"\n        Override to return the load path.\n        \"\"\"\n        return None\n\n    @abc.abstractproperty\n    def copy_options(self):\n        \"\"\"\n        Add extra copy options, for example:\n\n        * TIMEFORMAT 'auto'\n        * IGNOREHEADER 1\n        * TRUNCATECOLUMNS\n        * IGNOREBLANKLINES\n        * DELIMITER '\\t'\n        \"\"\"\n        return ''\n\n    @property\n    def prune_table(self):\n        \"\"\"\n        Override to set equal to the name of the table which is to be pruned.\n        Intended to be used in conjunction with prune_column and prune_date\n        i.e. copy to temp table, prune production table to prune_column with a date greater than prune_date, then insert into production table from temp table\n        \"\"\"\n        return None\n\n    @property\n    def prune_column(self):\n        \"\"\"\n        Override to set equal to the column of the prune_table which is to be compared\n        Intended to be used in conjunction with prune_table and prune_date\n        i.e. copy to temp table, prune production table to prune_column with a date greater than prune_date, then insert into production table from temp table\n        \"\"\"\n        return None\n\n    @property\n    def prune_date(self):\n        \"\"\"\n        Override to set equal to the date by which prune_column is to be compared\n        Intended to be used in conjunction with prune_table and prune_column\n        i.e. copy to temp table, prune production table to prune_column with a date greater than prune_date, then insert into production table from temp table\n        \"\"\"\n        return None\n\n    @property\n    def table_attributes(self):\n        \"\"\"\n        Add extra table attributes, for example:\n\n        DISTSTYLE KEY\n        DISTKEY (MY_FIELD)\n        SORTKEY (MY_FIELD_2, MY_FIELD_3)\n        \"\"\"\n        return ''\n\n    @property\n    def do_truncate_table(self):\n        \"\"\"\n        Return True if table should be truncated before copying new data in.\n        \"\"\"\n        return False\n\n    def do_prune(self):\n        \"\"\"\n        Return True if prune_table, prune_column, and prune_date are implemented.\n        If only a subset of prune variables are override, an exception is raised to remind the user to implement all or none.\n        Prune (data newer than prune_date deleted) before copying new data in.\n        \"\"\"\n        if self.prune_table and self.prune_column and self.prune_date:\n            return True\n        elif self.prune_table or self.prune_column or self.prune_date:\n            raise Exception('override zero or all prune variables')\n        else:\n            return False\n\n    @property\n    def table_type(self):\n        \"\"\"\n        Return table type (i.e. 'temp').\n        \"\"\"\n        return ''\n\n    @property\n    def queries(self):\n        \"\"\"\n        Override to return a list of queries to be executed in order.\n        \"\"\"\n        return []\n\n    def truncate_table(self, connection):\n        query = \"truncate %s\" % self.table\n        cursor = connection.cursor()\n        try:\n            cursor.execute(query)\n        finally:\n            cursor.close()\n\n    def prune(self, connection):\n        query = \"delete from %s where %s >= %s\" % (self.prune_table, self.prune_column, self.prune_date)\n        cursor = connection.cursor()\n        try:\n            cursor.execute(query)\n        finally:\n            cursor.close()\n\n    def create_table(self, connection):\n        \"\"\"\n        Override to provide code for creating the target table.\n\n        By default it will be created using types (optionally)\n        specified in columns.\n\n        If overridden, use the provided connection object for\n        setting up the table in order to create the table and\n        insert data using the same transaction.\n        \"\"\"\n        if len(self.columns[0]) == 1:\n            # only names of columns specified, no types\n            raise NotImplementedError(\"create_table() not implemented \"\n                                      \"for %r and columns types not \"\n                                      \"specified\" % self.table)\n        elif len(self.columns[0]) == 2:\n            # if columns is specified as (name, type) tuples\n            coldefs = ','.join(\n                '{name} {type}'.format(\n                    name=name,\n                    type=type) for name, type in self.columns\n            )\n            query = (\"CREATE {type} TABLE \"\n                     \"{table} ({coldefs}) \"\n                     \"{table_attributes}\").format(\n                type=self.table_type,\n                table=self.table,\n                coldefs=coldefs,\n                table_attributes=self.table_attributes)\n\n            connection.cursor().execute(query)\n        else:\n            raise ValueError(\"create_table() found no columns for %r\"\n                             % self.table)\n\n    def run(self):\n        \"\"\"\n        If the target table doesn't exist, self.create_table\n        will be called to attempt to create the table.\n        \"\"\"\n        if not (self.table):\n            raise Exception(\"table need to be specified\")\n\n        path = self.s3_load_path()\n        output = self.output()\n        connection = output.connect()\n        cursor = connection.cursor()\n\n        self.init_copy(connection)\n        self.copy(cursor, path)\n        self.post_copy(cursor)\n\n        # update marker table\n        output.touch(connection)\n        connection.commit()\n\n        # commit and clean up\n        connection.close()\n\n    def copy(self, cursor, f):\n        \"\"\"\n        Defines copying from s3 into redshift.\n\n        If both key-based and role-based credentials are provided, role-based will be used.\n        \"\"\"\n        logger.info(\"Inserting file: %s\", f)\n        cursor.execute(\"\"\"\n         COPY {table} from '{source}'\n         CREDENTIALS '{creds}'\n         {options}\n         ;\"\"\".format(\n            table=self.table,\n            source=f,\n            creds=self._credentials(),\n            options=self.copy_options)\n        )\n\n    def output(self):\n        \"\"\"\n        Returns a RedshiftTarget representing the inserted dataset.\n\n        Normally you don't override this.\n        \"\"\"\n        return RedshiftTarget(\n            host=self.host,\n            database=self.database,\n            user=self.user,\n            password=self.password,\n            table=self.table,\n            update_id=self.update_id)\n\n    def does_table_exist(self, connection):\n        \"\"\"\n        Determine whether the table already exists.\n        \"\"\"\n\n        if '.' in self.table:\n            query = (\"select 1 as table_exists \"\n                     \"from information_schema.tables \"\n                     \"where table_schema = lower(%s) and table_name = lower(%s) limit 1\")\n        else:\n            query = (\"select 1 as table_exists \"\n                     \"from pg_table_def \"\n                     \"where tablename = lower(%s) limit 1\")\n        cursor = connection.cursor()\n        try:\n            cursor.execute(query, tuple(self.table.split('.')))\n            result = cursor.fetchone()\n            return bool(result)\n        finally:\n            cursor.close()\n\n    def init_copy(self, connection):\n        \"\"\"\n        Perform pre-copy sql - such as creating table, truncating, or removing data older than x.\n        \"\"\"\n        if not self.does_table_exist(connection):\n            logger.info(\"Creating table %s\", self.table)\n            connection.reset()\n            self.create_table(connection)\n\n        if self.do_truncate_table:\n            logger.info(\"Truncating table %s\", self.table)\n            self.truncate_table(connection)\n\n        if self.do_prune():\n            logger.info(\"Removing %s older than %s from %s\", self.prune_column, self.prune_date, self.prune_table)\n            self.prune(connection)\n\n    def post_copy(self, cursor):\n        \"\"\"\n        Performs post-copy sql - such as cleansing data, inserting into production table (if copied to temp table), etc.\n        \"\"\"\n        logger.info('Executing post copy queries')\n        for query in self.queries:\n            cursor.execute(query)\n\n\nclass S3CopyJSONToTable(S3CopyToTable, _CredentialsMixin):\n    \"\"\"\n    Template task for inserting a JSON data set into Redshift from s3.\n\n    Usage:\n\n        * Subclass and override the required attributes:\n\n            * `host`,\n            * `database`,\n            * `user`,\n            * `password`,\n            * `table`,\n            * `columns`,\n            * `s3_load_path`,\n            * `jsonpath`,\n            * `copy_json_options`.\n\n    * You can also override the attributes provided by the\n      CredentialsMixin if they are not supplied by your\n      configuration or environment variables.\n    \"\"\"\n\n    @abc.abstractproperty\n    def jsonpath(self):\n        \"\"\"\n        Override the jsonpath schema location for the table.\n        \"\"\"\n        return ''\n\n    @abc.abstractproperty\n    def copy_json_options(self):\n        \"\"\"\n        Add extra copy options, for example:\n\n        * GZIP\n        * LZOP\n        \"\"\"\n        return ''\n\n    def copy(self, cursor, f):\n        \"\"\"\n        Defines copying JSON from s3 into redshift.\n        \"\"\"\n\n        logger.info(\"Inserting file: %s\", f)\n        cursor.execute(\"\"\"\n         COPY %s from '%s'\n         CREDENTIALS '%s'\n         JSON AS '%s' %s\n         %s\n         ;\"\"\" % (self.table, f, self._credentials(),\n                 self.jsonpath, self.copy_json_options, self.copy_options))\n\n\nclass RedshiftManifestTask(S3PathTask):\n    \"\"\"\n    Generic task to generate a manifest file that can be used\n    in S3CopyToTable in order to copy multiple files from your\n    s3 folder into a redshift table at once.\n\n    For full description on how to use the manifest file see\n    http://docs.aws.amazon.com/redshift/latest/dg/loading-data-files-using-manifest.html\n\n    Usage:\n\n        * requires parameters\n            * path - s3 path to the generated manifest file, including the\n                     name of the generated file\n                     to be copied into a redshift table\n            * folder_paths - s3 paths to the folders containing files you wish to be copied\n\n    Output:\n\n        * generated manifest file\n    \"\"\"\n\n    # should be over ridden to point to a variety\n    # of folders you wish to copy from\n    folder_paths = luigi.Parameter()\n    text_target = True\n\n    def run(self):\n        entries = []\n        for folder_path in self.folder_paths:\n            s3 = S3Target(folder_path)\n            client = s3.fs\n            for file_name in client.list(s3.path):\n                entries.append({\n                    'url': '%s/%s' % (folder_path, file_name),\n                    'mandatory': True\n                })\n        manifest = {'entries': entries}\n        target = self.output().open('w')\n        dump = json.dumps(manifest)\n        if not self.text_target:\n            dump = dump.encode('utf8')\n        target.write(dump)\n        target.close()\n\n\nclass KillOpenRedshiftSessions(luigi.Task):\n    \"\"\"\n    An task for killing any open Redshift sessions\n    in a given database. This is necessary to prevent open user sessions\n    with transactions against the table from blocking drop or truncate\n    table commands.\n\n    Usage:\n\n    Subclass and override the required `host`, `database`,\n    `user`, and `password` attributes.\n    \"\"\"\n\n    # time in seconds to wait before\n    # reconnecting to Redshift if our session is killed too.\n    # 30 seconds is usually fine; 60 is conservative\n    connection_reset_wait_seconds = luigi.IntParameter(default=60)\n\n    @abc.abstractproperty\n    def host(self):\n        return None\n\n    @abc.abstractproperty\n    def database(self):\n        return None\n\n    @abc.abstractproperty\n    def user(self):\n        return None\n\n    @abc.abstractproperty\n    def password(self):\n        return None\n\n    @property\n    def update_id(self):\n        \"\"\"\n        This update id will be a unique identifier\n        for this insert on this table.\n        \"\"\"\n        return self.task_id\n\n    def output(self):\n        \"\"\"\n        Returns a RedshiftTarget representing the inserted dataset.\n\n        Normally you don't override this.\n        \"\"\"\n        # uses class name as a meta-table\n        return RedshiftTarget(\n            host=self.host,\n            database=self.database,\n            user=self.user,\n            password=self.password,\n            table=self.__class__.__name__,\n            update_id=self.update_id)\n\n    def run(self):\n        \"\"\"\n        Kill any open Redshift sessions for the given database.\n        \"\"\"\n        connection = self.output().connect()\n        # kill any sessions other than ours and\n        # internal Redshift sessions (rdsdb)\n        query = (\"select pg_terminate_backend(process) \"\n                 \"from STV_SESSIONS \"\n                 \"where db_name=%s \"\n                 \"and user_name != 'rdsdb' \"\n                 \"and process != pg_backend_pid()\")\n        cursor = connection.cursor()\n        logger.info('Killing all open Redshift sessions for database: %s', self.database)\n        try:\n            cursor.execute(query, (self.database,))\n            cursor.close()\n            connection.commit()\n        except psycopg2.DatabaseError as e:\n            if e.message and 'EOF' in e.message:\n                # sometimes this operation kills the current session.\n                # rebuild the connection. Need to pause for 30-60 seconds\n                # before Redshift will allow us back in.\n                connection.close()\n                logger.info('Pausing %s seconds for Redshift to reset connection', self.connection_reset_wait_seconds)\n                time.sleep(self.connection_reset_wait_seconds)\n                logger.info('Reconnecting to Redshift')\n                connection = self.output().connect()\n            else:\n                raise\n\n        try:\n            self.output().touch(connection)\n            connection.commit()\n        finally:\n            connection.close()\n\n        logger.info('Done killing all open Redshift sessions for database: %s', self.database)\n\n\nclass RedshiftQuery(postgres.PostgresQuery):\n    \"\"\"\n    Template task for querying an Amazon Redshift database\n\n    Usage:\n    Subclass and override the required `host`, `database`, `user`, `password`, `table`, and `query` attributes.\n\n    Override the `run` method if your use case requires some action with the query result.\n\n    Task instances require a dynamic `update_id`, e.g. via parameter(s), otherwise the query will only execute once\n\n    To customize the query signature as recorded in the database marker table, override the `update_id` property.\n    \"\"\"\n\n    def output(self):\n        \"\"\"\n        Returns a RedshiftTarget representing the executed query.\n\n        Normally you don't override this.\n        \"\"\"\n        return RedshiftTarget(\n            host=self.host,\n            database=self.database,\n            user=self.user,\n            password=self.password,\n            table=self.table,\n            update_id=self.update_id\n        )\n\n\nclass RedshiftUnloadTask(postgres.PostgresQuery, _CredentialsMixin):\n    \"\"\"\n    Template task for running UNLOAD on an Amazon Redshift database\n\n    Usage:\n    Subclass and override the required `host`, `database`, `user`, `password`, `table`, and `query` attributes.\n    Optionally, override the `autocommit` atribute to run the query in autocommit mode - this is necessary to run VACUUM for example.\n    Override the `run` method if your use case requires some action with the query result.\n    Task instances require a dynamic `update_id`, e.g. via parameter(s), otherwise the query will only execute once\n    To customize the query signature as recorded in the database marker table, override the `update_id` property.\n    You can also override the attributes provided by the CredentialsMixin if they are not supplied by your configuration or environment variables.\n    \"\"\"\n\n    @property\n    def s3_unload_path(self):\n        \"\"\"\n        Override to return the load path.\n        \"\"\"\n        return ''\n\n    @property\n    def unload_options(self):\n        \"\"\"\n        Add extra or override default unload options:\n        \"\"\"\n        return \"DELIMITER '|' ADDQUOTES GZIP ALLOWOVERWRITE PARALLEL ON\"\n\n    @property\n    def unload_query(self):\n        \"\"\"\n        Default UNLOAD command\n        \"\"\"\n        return (\"UNLOAD ( '{query}' ) TO '{s3_unload_path}' \"\n                \"credentials '{credentials}' \"\n                \"{unload_options};\")\n\n    def run(self):\n        connection = self.output().connect()\n        cursor = connection.cursor()\n\n        unload_query = self.unload_query.format(\n            query=self.query().replace(\"'\", r\"\\'\"),\n            s3_unload_path=self.s3_unload_path,\n            unload_options=self.unload_options,\n            credentials=self._credentials())\n\n        logger.info('Executing unload query from task: {name}'.format(name=self.__class__))\n        try:\n            cursor = connection.cursor()\n            cursor.execute(unload_query)\n            logger.info(cursor.statusmessage)\n        except:\n            raise\n\n        # Update marker table\n        self.output().touch(connection)\n        # commit and close connection\n        connection.commit()\n        connection.close()\n\n    def output(self):\n        \"\"\"\n        Returns a RedshiftTarget representing the executed query.\n\n        Normally you don't override this.\n        \"\"\"\n        return RedshiftTarget(\n            host=self.host,\n            database=self.database,\n            user=self.user,\n            password=self.password,\n            table=self.table,\n            update_id=self.update_id\n        )\n",
      "file_after": "# -*- coding: utf-8 -*-\n#\n# Copyright 2012-2015 Spotify AB\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport abc\nimport json\nimport logging\nimport time\nimport os\n\nimport luigi\nfrom luigi.contrib import postgres\nfrom luigi.contrib import rdbms\nfrom luigi.contrib.s3 import S3PathTask, S3Target\n\nlogger = logging.getLogger('luigi-interface')\n\n\ntry:\n    import psycopg2\n    import psycopg2.errorcodes\nexcept ImportError:\n    logger.warning(\"Loading postgres module without psycopg2 installed. \"\n                   \"Will crash at runtime if postgres functionality is used.\")\n\n\nclass _CredentialsMixin():\n    \"\"\"\n    This mixin is used to provide the same credential properties\n    for AWS to all Redshift tasks. It also provides a helper method\n    to generate the credentials string for the task.\n    \"\"\"\n\n    @property\n    def configuration_section(self):\n        \"\"\"\n        Override to change the configuration section used\n        to obtain default credentials.\n        \"\"\"\n        return 'redshift'\n\n    @property\n    def aws_access_key_id(self):\n        \"\"\"\n        Override to return the key id.\n        \"\"\"\n        return self._get_configuration_attribute('aws_access_key_id')\n\n    @property\n    def aws_secret_access_key(self):\n        \"\"\"\n        Override to return the secret access key.\n        \"\"\"\n        return self._get_configuration_attribute('aws_secret_access_key')\n\n    @property\n    def aws_account_id(self):\n        \"\"\"\n        Override to return the account id.\n        \"\"\"\n        return self._get_configuration_attribute('aws_account_id')\n\n    @property\n    def aws_arn_role_name(self):\n        \"\"\"\n        Override to return the arn role name.\n        \"\"\"\n        return self._get_configuration_attribute('aws_arn_role_name')\n\n    @property\n    def aws_session_token(self):\n        \"\"\"\n        Override to return the session token.\n        \"\"\"\n        return self._get_configuration_attribute('aws_session_token')\n\n    def _get_configuration_attribute(self, attribute):\n        config = luigi.configuration.get_config()\n\n        value = config.get(self.configuration_section, attribute, default=None)\n\n        if not value:\n            value = os.environ.get(attribute.upper(), None)\n\n        return value\n\n    def _credentials(self):\n        \"\"\"\n        Return a credential string for the provided task. If no valid\n        credentials are set, raise a NotImplementedError.\n        \"\"\"\n\n        if self.aws_account_id and self.aws_arn_role_name:\n            return 'aws_iam_role=arn:aws:iam::{id}:role/{role}'.format(\n                id=self.aws_account_id,\n                role=self.aws_arn_role_name\n            )\n        elif self.aws_access_key_id and self.aws_secret_access_key:\n            return 'aws_access_key_id={key};aws_secret_access_key={secret}{opt}'.format(\n                key=self.aws_access_key_id,\n                secret=self.aws_secret_access_key,\n                opt=';token={}'.format(self.aws_session_token) if self.aws_session_token else ''\n            )\n        else:\n            raise NotImplementedError(\"Missing Credentials. \"\n                                      \"Ensure one of the pairs of auth args below are set \"\n                                      \"in a configuration file, environment variables or by \"\n                                      \"being overridden in the task: \"\n                                      \"'aws_access_key_id' AND 'aws_secret_access_key' OR \"\n                                      \"'aws_account_id' AND 'aws_arn_role_name'\")\n\n\nclass RedshiftTarget(postgres.PostgresTarget):\n    \"\"\"\n    Target for a resource in Redshift.\n\n    Redshift is similar to postgres with a few adjustments\n    required by redshift.\n    \"\"\"\n    marker_table = luigi.configuration.get_config().get(\n        'redshift',\n        'marker-table',\n        'table_updates')\n\n    use_db_timestamps = False\n\n\nclass S3CopyToTable(rdbms.CopyToTable, _CredentialsMixin):\n    \"\"\"\n    Template task for inserting a data set into Redshift from s3.\n\n    Usage:\n\n    * Subclass and override the required attributes:\n\n      * `host`,\n      * `database`,\n      * `user`,\n      * `password`,\n      * `table`,\n      * `columns`,\n      * `s3_load_path`.\n\n    * You can also override the attributes provided by the\n      CredentialsMixin if they are not supplied by your\n      configuration or environment variables.\n    \"\"\"\n\n    @abc.abstractmethod\n    def s3_load_path(self):\n        \"\"\"\n        Override to return the load path.\n        \"\"\"\n        return None\n\n    @abc.abstractproperty\n    def copy_options(self):\n        \"\"\"\n        Add extra copy options, for example:\n\n        * TIMEFORMAT 'auto'\n        * IGNOREHEADER 1\n        * TRUNCATECOLUMNS\n        * IGNOREBLANKLINES\n        * DELIMITER '\\t'\n        \"\"\"\n        return ''\n\n    @property\n    def prune_table(self):\n        \"\"\"\n        Override to set equal to the name of the table which is to be pruned.\n        Intended to be used in conjunction with prune_column and prune_date\n        i.e. copy to temp table, prune production table to prune_column with a date greater than prune_date, then insert into production table from temp table\n        \"\"\"\n        return None\n\n    @property\n    def prune_column(self):\n        \"\"\"\n        Override to set equal to the column of the prune_table which is to be compared\n        Intended to be used in conjunction with prune_table and prune_date\n        i.e. copy to temp table, prune production table to prune_column with a date greater than prune_date, then insert into production table from temp table\n        \"\"\"\n        return None\n\n    @property\n    def prune_date(self):\n        \"\"\"\n        Override to set equal to the date by which prune_column is to be compared\n        Intended to be used in conjunction with prune_table and prune_column\n        i.e. copy to temp table, prune production table to prune_column with a date greater than prune_date, then insert into production table from temp table\n        \"\"\"\n        return None\n\n    @property\n    def table_attributes(self):\n        \"\"\"\n        Add extra table attributes, for example:\n\n        DISTSTYLE KEY\n        DISTKEY (MY_FIELD)\n        SORTKEY (MY_FIELD_2, MY_FIELD_3)\n        \"\"\"\n        return ''\n\n    @property\n    def do_truncate_table(self):\n        \"\"\"\n        Return True if table should be truncated before copying new data in.\n        \"\"\"\n        return False\n\n    def do_prune(self):\n        \"\"\"\n        Return True if prune_table, prune_column, and prune_date are implemented.\n        If only a subset of prune variables are override, an exception is raised to remind the user to implement all or none.\n        Prune (data newer than prune_date deleted) before copying new data in.\n        \"\"\"\n        if self.prune_table and self.prune_column and self.prune_date:\n            return True\n        elif self.prune_table or self.prune_column or self.prune_date:\n            raise Exception('override zero or all prune variables')\n        else:\n            return False\n\n    @property\n    def table_type(self):\n        \"\"\"\n        Return table type (i.e. 'temp').\n        \"\"\"\n        return ''\n\n    @property\n    def queries(self):\n        \"\"\"\n        Override to return a list of queries to be executed in order.\n        \"\"\"\n        return []\n\n    def truncate_table(self, connection):\n        query = \"truncate %s\" % self.table\n        cursor = connection.cursor()\n        try:\n            cursor.execute(query)\n        finally:\n            cursor.close()\n\n    def prune(self, connection):\n        query = \"delete from %s where %s >= %s\" % (self.prune_table, self.prune_column, self.prune_date)\n        cursor = connection.cursor()\n        try:\n            cursor.execute(query)\n        finally:\n            cursor.close()\n\n    def create_table(self, connection):\n        \"\"\"\n        Override to provide code for creating the target table.\n\n        By default it will be created using types (optionally)\n        specified in columns.\n\n        If overridden, use the provided connection object for\n        setting up the table in order to create the table and\n        insert data using the same transaction.\n        \"\"\"\n        if len(self.columns[0]) == 1:\n            # only names of columns specified, no types\n            raise NotImplementedError(\"create_table() not implemented \"\n                                      \"for %r and columns types not \"\n                                      \"specified\" % self.table)\n        elif len(self.columns[0]) == 2:\n            # if columns is specified as (name, type) tuples\n            coldefs = ','.join(\n                '{name} {type}'.format(\n                    name=name,\n                    type=type) for name, type in self.columns\n            )\n            query = (\"CREATE {type} TABLE \"\n                     \"{table} ({coldefs}) \"\n                     \"{table_attributes}\").format(\n                type=self.table_type,\n                table=self.table,\n                coldefs=coldefs,\n                table_attributes=self.table_attributes)\n\n            connection.cursor().execute(query)\n        else:\n            raise ValueError(\"create_table() found no columns for %r\"\n                             % self.table)\n\n    def run(self):\n        \"\"\"\n        If the target table doesn't exist, self.create_table\n        will be called to attempt to create the table.\n        \"\"\"\n        if not (self.table):\n            raise Exception(\"table need to be specified\")\n\n        path = self.s3_load_path()\n        output = self.output()\n        connection = output.connect()\n        cursor = connection.cursor()\n\n        self.init_copy(connection)\n        self.copy(cursor, path)\n        self.post_copy(cursor)\n\n        # update marker table\n        output.touch(connection)\n        connection.commit()\n\n        # commit and clean up\n        connection.close()\n\n    def copy(self, cursor, f):\n        \"\"\"\n        Defines copying from s3 into redshift.\n\n        If both key-based and role-based credentials are provided, role-based will be used.\n        \"\"\"\n        logger.info(\"Inserting file: %s\", f)\n        colnames = ''\n        if len(self.columns) > 0:\n            colnames = \",\".join([x[0] for x in self.columns])\n            colnames = '({})'.format(colnames)\n\n        cursor.execute(\"\"\"\n         COPY {table} {colnames} from '{source}'\n         CREDENTIALS '{creds}'\n         {options}\n         ;\"\"\".format(\n            table=self.table,\n            colnames=colnames,\n            source=f,\n            creds=self._credentials(),\n            options=self.copy_options)\n        )\n\n    def output(self):\n        \"\"\"\n        Returns a RedshiftTarget representing the inserted dataset.\n\n        Normally you don't override this.\n        \"\"\"\n        return RedshiftTarget(\n            host=self.host,\n            database=self.database,\n            user=self.user,\n            password=self.password,\n            table=self.table,\n            update_id=self.update_id)\n\n    def does_table_exist(self, connection):\n        \"\"\"\n        Determine whether the table already exists.\n        \"\"\"\n\n        if '.' in self.table:\n            query = (\"select 1 as table_exists \"\n                     \"from information_schema.tables \"\n                     \"where table_schema = lower(%s) and table_name = lower(%s) limit 1\")\n        else:\n            query = (\"select 1 as table_exists \"\n                     \"from pg_table_def \"\n                     \"where tablename = lower(%s) limit 1\")\n        cursor = connection.cursor()\n        try:\n            cursor.execute(query, tuple(self.table.split('.')))\n            result = cursor.fetchone()\n            return bool(result)\n        finally:\n            cursor.close()\n\n    def init_copy(self, connection):\n        \"\"\"\n        Perform pre-copy sql - such as creating table, truncating, or removing data older than x.\n        \"\"\"\n        if not self.does_table_exist(connection):\n            logger.info(\"Creating table %s\", self.table)\n            connection.reset()\n            self.create_table(connection)\n\n        if self.do_truncate_table:\n            logger.info(\"Truncating table %s\", self.table)\n            self.truncate_table(connection)\n\n        if self.do_prune():\n            logger.info(\"Removing %s older than %s from %s\", self.prune_column, self.prune_date, self.prune_table)\n            self.prune(connection)\n\n    def post_copy(self, cursor):\n        \"\"\"\n        Performs post-copy sql - such as cleansing data, inserting into production table (if copied to temp table), etc.\n        \"\"\"\n        logger.info('Executing post copy queries')\n        for query in self.queries:\n            cursor.execute(query)\n\n\nclass S3CopyJSONToTable(S3CopyToTable, _CredentialsMixin):\n    \"\"\"\n    Template task for inserting a JSON data set into Redshift from s3.\n\n    Usage:\n\n        * Subclass and override the required attributes:\n\n            * `host`,\n            * `database`,\n            * `user`,\n            * `password`,\n            * `table`,\n            * `columns`,\n            * `s3_load_path`,\n            * `jsonpath`,\n            * `copy_json_options`.\n\n    * You can also override the attributes provided by the\n      CredentialsMixin if they are not supplied by your\n      configuration or environment variables.\n    \"\"\"\n\n    @abc.abstractproperty\n    def jsonpath(self):\n        \"\"\"\n        Override the jsonpath schema location for the table.\n        \"\"\"\n        return ''\n\n    @abc.abstractproperty\n    def copy_json_options(self):\n        \"\"\"\n        Add extra copy options, for example:\n\n        * GZIP\n        * LZOP\n        \"\"\"\n        return ''\n\n    def copy(self, cursor, f):\n        \"\"\"\n        Defines copying JSON from s3 into redshift.\n        \"\"\"\n\n        logger.info(\"Inserting file: %s\", f)\n        cursor.execute(\"\"\"\n         COPY %s from '%s'\n         CREDENTIALS '%s'\n         JSON AS '%s' %s\n         %s\n         ;\"\"\" % (self.table, f, self._credentials(),\n                 self.jsonpath, self.copy_json_options, self.copy_options))\n\n\nclass RedshiftManifestTask(S3PathTask):\n    \"\"\"\n    Generic task to generate a manifest file that can be used\n    in S3CopyToTable in order to copy multiple files from your\n    s3 folder into a redshift table at once.\n\n    For full description on how to use the manifest file see\n    http://docs.aws.amazon.com/redshift/latest/dg/loading-data-files-using-manifest.html\n\n    Usage:\n\n        * requires parameters\n            * path - s3 path to the generated manifest file, including the\n                     name of the generated file\n                     to be copied into a redshift table\n            * folder_paths - s3 paths to the folders containing files you wish to be copied\n\n    Output:\n\n        * generated manifest file\n    \"\"\"\n\n    # should be over ridden to point to a variety\n    # of folders you wish to copy from\n    folder_paths = luigi.Parameter()\n    text_target = True\n\n    def run(self):\n        entries = []\n        for folder_path in self.folder_paths:\n            s3 = S3Target(folder_path)\n            client = s3.fs\n            for file_name in client.list(s3.path):\n                entries.append({\n                    'url': '%s/%s' % (folder_path, file_name),\n                    'mandatory': True\n                })\n        manifest = {'entries': entries}\n        target = self.output().open('w')\n        dump = json.dumps(manifest)\n        if not self.text_target:\n            dump = dump.encode('utf8')\n        target.write(dump)\n        target.close()\n\n\nclass KillOpenRedshiftSessions(luigi.Task):\n    \"\"\"\n    An task for killing any open Redshift sessions\n    in a given database. This is necessary to prevent open user sessions\n    with transactions against the table from blocking drop or truncate\n    table commands.\n\n    Usage:\n\n    Subclass and override the required `host`, `database`,\n    `user`, and `password` attributes.\n    \"\"\"\n\n    # time in seconds to wait before\n    # reconnecting to Redshift if our session is killed too.\n    # 30 seconds is usually fine; 60 is conservative\n    connection_reset_wait_seconds = luigi.IntParameter(default=60)\n\n    @abc.abstractproperty\n    def host(self):\n        return None\n\n    @abc.abstractproperty\n    def database(self):\n        return None\n\n    @abc.abstractproperty\n    def user(self):\n        return None\n\n    @abc.abstractproperty\n    def password(self):\n        return None\n\n    @property\n    def update_id(self):\n        \"\"\"\n        This update id will be a unique identifier\n        for this insert on this table.\n        \"\"\"\n        return self.task_id\n\n    def output(self):\n        \"\"\"\n        Returns a RedshiftTarget representing the inserted dataset.\n\n        Normally you don't override this.\n        \"\"\"\n        # uses class name as a meta-table\n        return RedshiftTarget(\n            host=self.host,\n            database=self.database,\n            user=self.user,\n            password=self.password,\n            table=self.__class__.__name__,\n            update_id=self.update_id)\n\n    def run(self):\n        \"\"\"\n        Kill any open Redshift sessions for the given database.\n        \"\"\"\n        connection = self.output().connect()\n        # kill any sessions other than ours and\n        # internal Redshift sessions (rdsdb)\n        query = (\"select pg_terminate_backend(process) \"\n                 \"from STV_SESSIONS \"\n                 \"where db_name=%s \"\n                 \"and user_name != 'rdsdb' \"\n                 \"and process != pg_backend_pid()\")\n        cursor = connection.cursor()\n        logger.info('Killing all open Redshift sessions for database: %s', self.database)\n        try:\n            cursor.execute(query, (self.database,))\n            cursor.close()\n            connection.commit()\n        except psycopg2.DatabaseError as e:\n            if e.message and 'EOF' in e.message:\n                # sometimes this operation kills the current session.\n                # rebuild the connection. Need to pause for 30-60 seconds\n                # before Redshift will allow us back in.\n                connection.close()\n                logger.info('Pausing %s seconds for Redshift to reset connection', self.connection_reset_wait_seconds)\n                time.sleep(self.connection_reset_wait_seconds)\n                logger.info('Reconnecting to Redshift')\n                connection = self.output().connect()\n            else:\n                raise\n\n        try:\n            self.output().touch(connection)\n            connection.commit()\n        finally:\n            connection.close()\n\n        logger.info('Done killing all open Redshift sessions for database: %s', self.database)\n\n\nclass RedshiftQuery(postgres.PostgresQuery):\n    \"\"\"\n    Template task for querying an Amazon Redshift database\n\n    Usage:\n    Subclass and override the required `host`, `database`, `user`, `password`, `table`, and `query` attributes.\n\n    Override the `run` method if your use case requires some action with the query result.\n\n    Task instances require a dynamic `update_id`, e.g. via parameter(s), otherwise the query will only execute once\n\n    To customize the query signature as recorded in the database marker table, override the `update_id` property.\n    \"\"\"\n\n    def output(self):\n        \"\"\"\n        Returns a RedshiftTarget representing the executed query.\n\n        Normally you don't override this.\n        \"\"\"\n        return RedshiftTarget(\n            host=self.host,\n            database=self.database,\n            user=self.user,\n            password=self.password,\n            table=self.table,\n            update_id=self.update_id\n        )\n\n\nclass RedshiftUnloadTask(postgres.PostgresQuery, _CredentialsMixin):\n    \"\"\"\n    Template task for running UNLOAD on an Amazon Redshift database\n\n    Usage:\n    Subclass and override the required `host`, `database`, `user`, `password`, `table`, and `query` attributes.\n    Optionally, override the `autocommit` atribute to run the query in autocommit mode - this is necessary to run VACUUM for example.\n    Override the `run` method if your use case requires some action with the query result.\n    Task instances require a dynamic `update_id`, e.g. via parameter(s), otherwise the query will only execute once\n    To customize the query signature as recorded in the database marker table, override the `update_id` property.\n    You can also override the attributes provided by the CredentialsMixin if they are not supplied by your configuration or environment variables.\n    \"\"\"\n\n    @property\n    def s3_unload_path(self):\n        \"\"\"\n        Override to return the load path.\n        \"\"\"\n        return ''\n\n    @property\n    def unload_options(self):\n        \"\"\"\n        Add extra or override default unload options:\n        \"\"\"\n        return \"DELIMITER '|' ADDQUOTES GZIP ALLOWOVERWRITE PARALLEL ON\"\n\n    @property\n    def unload_query(self):\n        \"\"\"\n        Default UNLOAD command\n        \"\"\"\n        return (\"UNLOAD ( '{query}' ) TO '{s3_unload_path}' \"\n                \"credentials '{credentials}' \"\n                \"{unload_options};\")\n\n    def run(self):\n        connection = self.output().connect()\n        cursor = connection.cursor()\n\n        unload_query = self.unload_query.format(\n            query=self.query().replace(\"'\", r\"\\'\"),\n            s3_unload_path=self.s3_unload_path,\n            unload_options=self.unload_options,\n            credentials=self._credentials())\n\n        logger.info('Executing unload query from task: {name}'.format(name=self.__class__))\n        try:\n            cursor = connection.cursor()\n            cursor.execute(unload_query)\n            logger.info(cursor.statusmessage)\n        except:\n            raise\n\n        # Update marker table\n        self.output().touch(connection)\n        # commit and close connection\n        connection.commit()\n        connection.close()\n\n    def output(self):\n        \"\"\"\n        Returns a RedshiftTarget representing the executed query.\n\n        Normally you don't override this.\n        \"\"\"\n        return RedshiftTarget(\n            host=self.host,\n            database=self.database,\n            user=self.user,\n            password=self.password,\n            table=self.table,\n            update_id=self.update_id\n        )\n",
      "file_patch": "@@ -334,12 +334,18 @@ class S3CopyToTable(rdbms.CopyToTable, _CredentialsMixin):\n         If both key-based and role-based credentials are provided, role-based will be used.\n         \"\"\"\n         logger.info(\"Inserting file: %s\", f)\n+        colnames = ''\n+        if len(self.columns) > 0:\n+            colnames = \",\".join([x[0] for x in self.columns])\n+            colnames = '({})'.format(colnames)\n+\n         cursor.execute(\"\"\"\n-         COPY {table} from '{source}'\n+         COPY {table} {colnames} from '{source}'\n          CREDENTIALS '{creds}'\n          {options}\n          ;\"\"\".format(\n             table=self.table,\n+            colnames=colnames,\n             source=f,\n             creds=self._credentials(),\n             options=self.copy_options)\n",
      "files_name_in_blame_commit": [
        "redshift.py"
      ]
    }
  },
  "commits_modify_file_before_fix": {
    "size": 54
  },
  "recursive_blame_commits": {
    "recursive_blame_function_lines": {
      "330": {
        "commit_id": "9249d68ba958560f7d371ed89f564958d2208ee0",
        "line_code": "    def copy(self, cursor, f):",
        "commit_date": "2014-02-11 20:46:09",
        "valid": 1
      },
      "331": {
        "commit_id": "580cd8deea5c37cf2bb21b2cb156560c3bc4c6b2",
        "line_code": "        \"\"\"",
        "commit_date": "2015-02-03 10:48:30",
        "valid": 1
      },
      "332": {
        "commit_id": "580cd8deea5c37cf2bb21b2cb156560c3bc4c6b2",
        "line_code": "        Defines copying from s3 into redshift.",
        "commit_date": "2015-02-03 10:48:30",
        "valid": 1
      },
      "333": {
        "commit_id": "b86166cc1a46af90a78c91528f1e18d66760b659",
        "line_code": "",
        "commit_date": "2017-01-26 07:01:18",
        "valid": 0
      },
      "334": {
        "commit_id": "b86166cc1a46af90a78c91528f1e18d66760b659",
        "line_code": "        If both key-based and role-based credentials are provided, role-based will be used.",
        "commit_date": "2017-01-26 07:01:18",
        "valid": 1
      },
      "335": {
        "commit_id": "580cd8deea5c37cf2bb21b2cb156560c3bc4c6b2",
        "line_code": "        \"\"\"",
        "commit_date": "2015-02-03 10:48:30",
        "valid": 1
      },
      "336": {
        "commit_id": "f5700be541d1331f476b0b2da1f10f2fd0c6a0e2",
        "line_code": "        logger.info(\"Inserting file: %s\", f)",
        "commit_date": "2015-10-26 14:22:31",
        "valid": 1
      },
      "337": {
        "commit_id": "9249d68ba958560f7d371ed89f564958d2208ee0",
        "line_code": "        cursor.execute(\"\"\"",
        "commit_date": "2014-02-11 20:46:09",
        "valid": 1
      },
      "338": {
        "commit_id": "b86166cc1a46af90a78c91528f1e18d66760b659",
        "line_code": "         COPY {table} from '{source}'",
        "commit_date": "2017-01-26 07:01:18",
        "valid": 1
      },
      "339": {
        "commit_id": "b86166cc1a46af90a78c91528f1e18d66760b659",
        "line_code": "         CREDENTIALS '{creds}'",
        "commit_date": "2017-01-26 07:01:18",
        "valid": 1
      },
      "340": {
        "commit_id": "b86166cc1a46af90a78c91528f1e18d66760b659",
        "line_code": "         {options}",
        "commit_date": "2017-01-26 07:01:18",
        "valid": 1
      },
      "341": {
        "commit_id": "b86166cc1a46af90a78c91528f1e18d66760b659",
        "line_code": "         ;\"\"\".format(",
        "commit_date": "2017-01-26 07:01:18",
        "valid": 1
      },
      "342": {
        "commit_id": "b86166cc1a46af90a78c91528f1e18d66760b659",
        "line_code": "            table=self.table,",
        "commit_date": "2017-01-26 07:01:18",
        "valid": 1
      },
      "343": {
        "commit_id": "b86166cc1a46af90a78c91528f1e18d66760b659",
        "line_code": "            source=f,",
        "commit_date": "2017-01-26 07:01:18",
        "valid": 1
      },
      "344": {
        "commit_id": "b47b2679d99980d486b1527229a9c8b4caa5a314",
        "line_code": "            creds=self._credentials(),",
        "commit_date": "2017-08-28 13:49:40",
        "valid": 1
      },
      "345": {
        "commit_id": "b86166cc1a46af90a78c91528f1e18d66760b659",
        "line_code": "            options=self.copy_options)",
        "commit_date": "2017-01-26 07:01:18",
        "valid": 1
      },
      "346": {
        "commit_id": "b86166cc1a46af90a78c91528f1e18d66760b659",
        "line_code": "        )",
        "commit_date": "2017-01-26 07:01:18",
        "valid": 1
      }
    },
    "commits": {
      "b47b2679d99980d486b1527229a9c8b4caa5a314": {
        "commit": {
          "commit_id": "b47b2679d99980d486b1527229a9c8b4caa5a314",
          "commit_message": "Standardize Redshift credential usage (#2068)\n\n* Standardize Redshift credential usage\r\n\r\nObtaining the credentials for a redshift connection is now uniform\r\nacross all of the redshift tasks. This has the added benefit of\r\nallowing role-based credentials in all of the tasks as well.",
          "commit_author": "Zachary Salzbank",
          "commit_date": "2017-08-28 13:49:40",
          "commit_parent": "102211c9ae60a78c146140c03f41f0965dcc4c10"
        },
        "function": {
          "function_name": "copy",
          "function_code_before": "def copy(self, cursor, f):\n    \"\"\"\n        Defines copying from s3 into redshift.\n\n        If both key-based and role-based credentials are provided, role-based will be used.\n        \"\"\"\n    if self.aws_account_id and self.aws_arn_role_name:\n        cred_str = 'aws_iam_role=arn:aws:iam::{id}:role/{role}'.format(id=self.aws_account_id, role=self.aws_arn_role_name)\n    elif self.aws_access_key_id and self.aws_secret_access_key:\n        cred_str = 'aws_access_key_id={key};aws_secret_access_key={secret}{opt}'.format(key=self.aws_access_key_id, secret=self.aws_secret_access_key, opt=';token={}'.format(self.aws_session_token) if self.aws_session_token else '')\n    else:\n        raise NotImplementedError(\"Missing Credentials. Override one of the following pairs of auth-args: 'aws_access_key_id' AND 'aws_secret_access_key' OR 'aws_account_id' AND 'aws_arn_role_name'\")\n    logger.info('Inserting file: %s', f)\n    cursor.execute(\"\\n         COPY {table} from '{source}'\\n         CREDENTIALS '{creds}'\\n         {options}\\n         ;\".format(table=self.table, source=f, creds=cred_str, options=self.copy_options))",
          "function_code_after": "def copy(self, cursor, f):\n    \"\"\"\n        Defines copying from s3 into redshift.\n\n        If both key-based and role-based credentials are provided, role-based will be used.\n        \"\"\"\n    logger.info('Inserting file: %s', f)\n    cursor.execute(\"\\n         COPY {table} from '{source}'\\n         CREDENTIALS '{creds}'\\n         {options}\\n         ;\".format(table=self.table, source=f, creds=self._credentials(), options=self.copy_options))",
          "function_before_start_line": 277,
          "function_before_end_line": 311,
          "function_after_start_line": 330,
          "function_after_end_line": 346,
          "function_before_token_count": 128,
          "function_after_token_count": 51,
          "functions_name_modified_file": [
            "prune",
            "output",
            "configuration_section",
            "post_copy",
            "_credentials",
            "user",
            "create_table",
            "s3_load_path",
            "do_prune",
            "_get_configuration_attribute",
            "do_truncate_table",
            "s3_unload_path",
            "aws_account_id",
            "host",
            "aws_session_token",
            "prune_column",
            "queries",
            "prune_date",
            "copy_json_options",
            "truncate_table",
            "aws_secret_access_key",
            "aws_arn_role_name",
            "table_type",
            "password",
            "run",
            "unload_options",
            "prune_table",
            "copy_options",
            "table_attributes",
            "does_table_exist",
            "init_copy",
            "jsonpath",
            "aws_access_key_id",
            "unload_query",
            "copy",
            "update_id",
            "database"
          ],
          "functions_name_all_files": [
            "prune",
            "output",
            "configuration_section",
            "post_copy",
            "_credentials",
            "user",
            "create_table",
            "s3_load_path",
            "do_prune",
            "_get_configuration_attribute",
            "do_truncate_table",
            "s3_unload_path",
            "aws_account_id",
            "host",
            "aws_session_token",
            "prune_column",
            "queries",
            "prune_date",
            "copy_json_options",
            "test_from_config",
            "truncate_table",
            "aws_secret_access_key",
            "aws_arn_role_name",
            "table_type",
            "password",
            "run",
            "unload_options",
            "query",
            "test_from_property",
            "prune_table",
            "copy_options",
            "test_s3_copy_to_temp_table",
            "test_redshift_unload_command",
            "table_attributes",
            "does_table_exist",
            "init_copy",
            "test_s3_copy_to_table",
            "jsonpath",
            "aws_access_key_id",
            "test_from_env",
            "unload_query",
            "copy",
            "test_s3_copy_to_missing_table",
            "update_id",
            "database",
            "test_copy_missing_creds"
          ],
          "functions_name_co_evolved_modified_file": [
            "aws_access_key_id",
            "configuration_section",
            "aws_secret_access_key",
            "_credentials",
            "unload_query",
            "aws_arn_role_name",
            "_get_configuration_attribute",
            "aws_account_id",
            "aws_session_token",
            "run"
          ],
          "functions_name_co_evolved_all_files": [
            "test_from_property",
            "aws_access_key_id",
            "test_from_env",
            "test_from_config",
            "configuration_section",
            "aws_secret_access_key",
            "_credentials",
            "unload_query",
            "aws_arn_role_name",
            "_get_configuration_attribute",
            "aws_account_id",
            "aws_session_token",
            "run"
          ]
        },
        "file": {
          "file_name": "redshift.py",
          "file_nloc": 443,
          "file_complexity": 76,
          "file_token_count": 1863,
          "file_before": "# -*- coding: utf-8 -*-\n#\n# Copyright 2012-2015 Spotify AB\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport abc\nimport json\nimport logging\nimport time\nimport os\n\nimport luigi\nfrom luigi.contrib import postgres\nfrom luigi.contrib import rdbms\nfrom luigi.contrib.s3 import S3PathTask, S3Target\n\nlogger = logging.getLogger('luigi-interface')\n\n\ntry:\n    import psycopg2\n    import psycopg2.errorcodes\nexcept ImportError:\n    logger.warning(\"Loading postgres module without psycopg2 installed. \"\n                   \"Will crash at runtime if postgres functionality is used.\")\n\n\nclass RedshiftTarget(postgres.PostgresTarget):\n    \"\"\"\n    Target for a resource in Redshift.\n\n    Redshift is similar to postgres with a few adjustments\n    required by redshift.\n    \"\"\"\n    marker_table = luigi.configuration.get_config().get(\n        'redshift',\n        'marker-table',\n        'table_updates')\n\n    use_db_timestamps = False\n\n\nclass S3CopyToTable(rdbms.CopyToTable):\n    \"\"\"\n    Template task for inserting a data set into Redshift from s3.\n\n    Usage:\n\n    * Subclass and override the required attributes:\n\n      * `host`,\n      * `database`,\n      * `user`,\n      * `password`,\n      * `table`,\n      * `columns`,\n      * `aws_access_key_id`,\n      * `aws_secret_access_key`,\n      * `s3_load_path`.\n    \"\"\"\n\n    @abc.abstractmethod\n    def s3_load_path(self):\n        \"\"\"\n        Override to return the load path.\n        \"\"\"\n        return None\n\n    @property\n    def aws_access_key_id(self):\n        \"\"\"\n        Override to return the key id.\n        \"\"\"\n        return None\n\n    @property\n    def aws_secret_access_key(self):\n        \"\"\"\n        Override to return the secret access key.\n        \"\"\"\n        return None\n\n    @property\n    def aws_account_id(self):\n        \"\"\"\n        Override to return the account id.\n        \"\"\"\n        return None\n\n    @property\n    def aws_arn_role_name(self):\n        \"\"\"\n        Override to return the arn role name.\n        \"\"\"\n        return None\n\n    @property\n    def aws_session_token(self):\n        \"\"\"\n        Override to return the session token.\n        \"\"\"\n        return None\n\n    @abc.abstractproperty\n    def copy_options(self):\n        \"\"\"\n        Add extra copy options, for example:\n\n        * TIMEFORMAT 'auto'\n        * IGNOREHEADER 1\n        * TRUNCATECOLUMNS\n        * IGNOREBLANKLINES\n        * DELIMITER '\\t'\n        \"\"\"\n        return ''\n\n    @property\n    def prune_table(self):\n        \"\"\"\n        Override to set equal to the name of the table which is to be pruned.\n        Intended to be used in conjunction with prune_column and prune_date\n        i.e. copy to temp table, prune production table to prune_column with a date greater than prune_date, then insert into production table from temp table\n        \"\"\"\n        return None\n\n    @property\n    def prune_column(self):\n        \"\"\"\n        Override to set equal to the column of the prune_table which is to be compared\n        Intended to be used in conjunction with prune_table and prune_date\n        i.e. copy to temp table, prune production table to prune_column with a date greater than prune_date, then insert into production table from temp table\n        \"\"\"\n        return None\n\n    @property\n    def prune_date(self):\n        \"\"\"\n        Override to set equal to the date by which prune_column is to be compared\n        Intended to be used in conjunction with prune_table and prune_column\n        i.e. copy to temp table, prune production table to prune_column with a date greater than prune_date, then insert into production table from temp table\n        \"\"\"\n        return None\n\n    @property\n    def table_attributes(self):\n        \"\"\"\n        Add extra table attributes, for example:\n\n        DISTSTYLE KEY\n        DISTKEY (MY_FIELD)\n        SORTKEY (MY_FIELD_2, MY_FIELD_3)\n        \"\"\"\n        return ''\n\n    @property\n    def do_truncate_table(self):\n        \"\"\"\n        Return True if table should be truncated before copying new data in.\n        \"\"\"\n        return False\n\n    def do_prune(self):\n        \"\"\"\n        Return True if prune_table, prune_column, and prune_date are implemented.\n        If only a subset of prune variables are override, an exception is raised to remind the user to implement all or none.\n        Prune (data newer than prune_date deleted) before copying new data in.\n        \"\"\"\n        if self.prune_table and self.prune_column and self.prune_date:\n            return True\n        elif self.prune_table or self.prune_column or self.prune_date:\n            raise Exception('override zero or all prune variables')\n        else:\n            return False\n\n    @property\n    def table_type(self):\n        \"\"\"\n        Return table type (i.e. 'temp').\n        \"\"\"\n        return ''\n\n    @property\n    def queries(self):\n        \"\"\"\n        Override to return a list of queries to be executed in order.\n        \"\"\"\n        return []\n\n    def truncate_table(self, connection):\n        query = \"truncate %s\" % self.table\n        cursor = connection.cursor()\n        try:\n            cursor.execute(query)\n        finally:\n            cursor.close()\n\n    def prune(self, connection):\n        query = \"delete from %s where %s >= %s\" % (self.prune_table, self.prune_column, self.prune_date)\n        cursor = connection.cursor()\n        try:\n            cursor.execute(query)\n        finally:\n            cursor.close()\n\n    def create_table(self, connection):\n        \"\"\"\n        Override to provide code for creating the target table.\n\n        By default it will be created using types (optionally)\n        specified in columns.\n\n        If overridden, use the provided connection object for\n        setting up the table in order to create the table and\n        insert data using the same transaction.\n        \"\"\"\n        if len(self.columns[0]) == 1:\n            # only names of columns specified, no types\n            raise NotImplementedError(\"create_table() not implemented \"\n                                      \"for %r and columns types not \"\n                                      \"specified\" % self.table)\n        elif len(self.columns[0]) == 2:\n            # if columns is specified as (name, type) tuples\n            coldefs = ','.join(\n                '{name} {type}'.format(\n                    name=name,\n                    type=type) for name, type in self.columns\n            )\n            query = (\"CREATE {type} TABLE \"\n                     \"{table} ({coldefs}) \"\n                     \"{table_attributes}\").format(\n                type=self.table_type,\n                table=self.table,\n                coldefs=coldefs,\n                table_attributes=self.table_attributes)\n\n            connection.cursor().execute(query)\n        else:\n            raise ValueError(\"create_table() found no columns for %r\"\n                             % self.table)\n\n    def run(self):\n        \"\"\"\n        If the target table doesn't exist, self.create_table\n        will be called to attempt to create the table.\n        \"\"\"\n        if not (self.table):\n            raise Exception(\"table need to be specified\")\n\n        path = self.s3_load_path()\n        output = self.output()\n        connection = output.connect()\n        cursor = connection.cursor()\n\n        self.init_copy(connection)\n        self.copy(cursor, path)\n        self.post_copy(cursor)\n\n        # update marker table\n        output.touch(connection)\n        connection.commit()\n\n        # commit and clean up\n        connection.close()\n\n    def copy(self, cursor, f):\n        \"\"\"\n        Defines copying from s3 into redshift.\n\n        If both key-based and role-based credentials are provided, role-based will be used.\n        \"\"\"\n        # format the credentials string dependent upon which type of credentials were provided\n        if self.aws_account_id and self.aws_arn_role_name:\n            cred_str = 'aws_iam_role=arn:aws:iam::{id}:role/{role}'.format(\n                id=self.aws_account_id,\n                role=self.aws_arn_role_name\n            )\n        elif self.aws_access_key_id and self.aws_secret_access_key:\n            cred_str = 'aws_access_key_id={key};aws_secret_access_key={secret}{opt}'.format(\n                key=self.aws_access_key_id,\n                secret=self.aws_secret_access_key,\n                opt=';token={}'.format(self.aws_session_token) if self.aws_session_token else ''\n            )\n        else:\n            raise NotImplementedError(\"Missing Credentials. \"\n                                      \"Override one of the following pairs of auth-args: \"\n                                      \"'aws_access_key_id' AND 'aws_secret_access_key' OR \"\n                                      \"'aws_account_id' AND 'aws_arn_role_name'\")\n\n        logger.info(\"Inserting file: %s\", f)\n        cursor.execute(\"\"\"\n         COPY {table} from '{source}'\n         CREDENTIALS '{creds}'\n         {options}\n         ;\"\"\".format(\n            table=self.table,\n            source=f,\n            creds=cred_str,\n            options=self.copy_options)\n        )\n\n    def output(self):\n        \"\"\"\n        Returns a RedshiftTarget representing the inserted dataset.\n\n        Normally you don't override this.\n        \"\"\"\n        return RedshiftTarget(\n            host=self.host,\n            database=self.database,\n            user=self.user,\n            password=self.password,\n            table=self.table,\n            update_id=self.update_id)\n\n    def does_table_exist(self, connection):\n        \"\"\"\n        Determine whether the table already exists.\n        \"\"\"\n\n        if '.' in self.table:\n            query = (\"select 1 as table_exists \"\n                     \"from information_schema.tables \"\n                     \"where table_schema = lower(%s) and table_name = lower(%s) limit 1\")\n        else:\n            query = (\"select 1 as table_exists \"\n                     \"from pg_table_def \"\n                     \"where tablename = lower(%s) limit 1\")\n        cursor = connection.cursor()\n        try:\n            cursor.execute(query, tuple(self.table.split('.')))\n            result = cursor.fetchone()\n            return bool(result)\n        finally:\n            cursor.close()\n\n    def init_copy(self, connection):\n        \"\"\"\n        Perform pre-copy sql - such as creating table, truncating, or removing data older than x.\n        \"\"\"\n        if not self.does_table_exist(connection):\n            logger.info(\"Creating table %s\", self.table)\n            connection.reset()\n            self.create_table(connection)\n\n        if self.do_truncate_table:\n            logger.info(\"Truncating table %s\", self.table)\n            self.truncate_table(connection)\n\n        if self.do_prune():\n            logger.info(\"Removing %s older than %s from %s\", self.prune_column, self.prune_date, self.prune_table)\n            self.prune(connection)\n\n    def post_copy(self, cursor):\n        \"\"\"\n        Performs post-copy sql - such as cleansing data, inserting into production table (if copied to temp table), etc.\n        \"\"\"\n        logger.info('Executing post copy queries')\n        for query in self.queries:\n            cursor.execute(query)\n\n\nclass S3CopyJSONToTable(S3CopyToTable):\n    \"\"\"\n    Template task for inserting a JSON data set into Redshift from s3.\n\n    Usage:\n\n        * Subclass and override the required attributes:\n\n            * `host`,\n            * `database`,\n            * `user`,\n            * `password`,\n            * `table`,\n            * `columns`,\n            * `aws_access_key_id`,\n            * `aws_secret_access_key`,\n            * `s3_load_path`,\n            * `jsonpath`,\n            * `copy_json_options`.\n    \"\"\"\n\n    @abc.abstractproperty\n    def jsonpath(self):\n        \"\"\"\n        Override the jsonpath schema location for the table.\n        \"\"\"\n        return ''\n\n    @abc.abstractproperty\n    def copy_json_options(self):\n        \"\"\"\n        Add extra copy options, for example:\n\n        * GZIP\n        * LZOP\n        \"\"\"\n        return ''\n\n    def copy(self, cursor, f):\n        \"\"\"\n        Defines copying JSON from s3 into redshift.\n        \"\"\"\n        # if session token is set, create token string\n        if self.aws_session_token:\n            token = ';token=%s' % self.aws_session_token\n        # otherwise, leave token string empty\n        else:\n            token = ''\n\n        logger.info(\"Inserting file: %s\", f)\n        cursor.execute(\"\"\"\n         COPY %s from '%s'\n         CREDENTIALS 'aws_access_key_id=%s;aws_secret_access_key=%s%s'\n         JSON AS '%s' %s\n         %s\n         ;\"\"\" % (self.table, f, self.aws_access_key_id,\n                 self.aws_secret_access_key, token,\n                 self.jsonpath, self.copy_json_options, self.copy_options))\n\n\nclass RedshiftManifestTask(S3PathTask):\n    \"\"\"\n    Generic task to generate a manifest file that can be used\n    in S3CopyToTable in order to copy multiple files from your\n    s3 folder into a redshift table at once.\n\n    For full description on how to use the manifest file see\n    http://docs.aws.amazon.com/redshift/latest/dg/loading-data-files-using-manifest.html\n\n    Usage:\n\n        * requires parameters\n            * path - s3 path to the generated manifest file, including the\n                     name of the generated file\n                     to be copied into a redshift table\n            * folder_paths - s3 paths to the folders containing files you wish to be copied\n\n    Output:\n\n        * generated manifest file\n    \"\"\"\n\n    # should be over ridden to point to a variety\n    # of folders you wish to copy from\n    folder_paths = luigi.Parameter()\n    text_target = True\n\n    def run(self):\n        entries = []\n        for folder_path in self.folder_paths:\n            s3 = S3Target(folder_path)\n            client = s3.fs\n            for file_name in client.list(s3.path):\n                entries.append({\n                    'url': '%s/%s' % (folder_path, file_name),\n                    'mandatory': True\n                })\n        manifest = {'entries': entries}\n        target = self.output().open('w')\n        dump = json.dumps(manifest)\n        if not self.text_target:\n            dump = dump.encode('utf8')\n        target.write(dump)\n        target.close()\n\n\nclass KillOpenRedshiftSessions(luigi.Task):\n    \"\"\"\n    An task for killing any open Redshift sessions\n    in a given database. This is necessary to prevent open user sessions\n    with transactions against the table from blocking drop or truncate\n    table commands.\n\n    Usage:\n\n    Subclass and override the required `host`, `database`,\n    `user`, and `password` attributes.\n    \"\"\"\n\n    # time in seconds to wait before\n    # reconnecting to Redshift if our session is killed too.\n    # 30 seconds is usually fine; 60 is conservative\n    connection_reset_wait_seconds = luigi.IntParameter(default=60)\n\n    @abc.abstractproperty\n    def host(self):\n        return None\n\n    @abc.abstractproperty\n    def database(self):\n        return None\n\n    @abc.abstractproperty\n    def user(self):\n        return None\n\n    @abc.abstractproperty\n    def password(self):\n        return None\n\n    @property\n    def update_id(self):\n        \"\"\"\n        This update id will be a unique identifier\n        for this insert on this table.\n        \"\"\"\n        return self.task_id\n\n    def output(self):\n        \"\"\"\n        Returns a RedshiftTarget representing the inserted dataset.\n\n        Normally you don't override this.\n        \"\"\"\n        # uses class name as a meta-table\n        return RedshiftTarget(\n            host=self.host,\n            database=self.database,\n            user=self.user,\n            password=self.password,\n            table=self.__class__.__name__,\n            update_id=self.update_id)\n\n    def run(self):\n        \"\"\"\n        Kill any open Redshift sessions for the given database.\n        \"\"\"\n        connection = self.output().connect()\n        # kill any sessions other than ours and\n        # internal Redshift sessions (rdsdb)\n        query = (\"select pg_terminate_backend(process) \"\n                 \"from STV_SESSIONS \"\n                 \"where db_name=%s \"\n                 \"and user_name != 'rdsdb' \"\n                 \"and process != pg_backend_pid()\")\n        cursor = connection.cursor()\n        logger.info('Killing all open Redshift sessions for database: %s', self.database)\n        try:\n            cursor.execute(query, (self.database,))\n            cursor.close()\n            connection.commit()\n        except psycopg2.DatabaseError as e:\n            if e.message and 'EOF' in e.message:\n                # sometimes this operation kills the current session.\n                # rebuild the connection. Need to pause for 30-60 seconds\n                # before Redshift will allow us back in.\n                connection.close()\n                logger.info('Pausing %s seconds for Redshift to reset connection', self.connection_reset_wait_seconds)\n                time.sleep(self.connection_reset_wait_seconds)\n                logger.info('Reconnecting to Redshift')\n                connection = self.output().connect()\n            else:\n                raise\n\n        try:\n            self.output().touch(connection)\n            connection.commit()\n        finally:\n            connection.close()\n\n        logger.info('Done killing all open Redshift sessions for database: %s', self.database)\n\n\nclass RedshiftQuery(postgres.PostgresQuery):\n    \"\"\"\n    Template task for querying an Amazon Redshift database\n\n    Usage:\n    Subclass and override the required `host`, `database`, `user`, `password`, `table`, and `query` attributes.\n\n    Override the `run` method if your use case requires some action with the query result.\n\n    Task instances require a dynamic `update_id`, e.g. via parameter(s), otherwise the query will only execute once\n\n    To customize the query signature as recorded in the database marker table, override the `update_id` property.\n    \"\"\"\n\n    def output(self):\n        \"\"\"\n        Returns a RedshiftTarget representing the executed query.\n\n        Normally you don't override this.\n        \"\"\"\n        return RedshiftTarget(\n            host=self.host,\n            database=self.database,\n            user=self.user,\n            password=self.password,\n            table=self.table,\n            update_id=self.update_id\n        )\n\n\nclass RedshiftUnloadTask(postgres.PostgresQuery):\n    \"\"\"\n    Template task for running UNLOAD on an Amazon Redshift database\n\n    Usage:\n    Subclass and override the required `host`, `database`, `user`, `password`, `table`, and `query` attributes.\n    Override the `run` method if your use case requires some action with the query result.\n    Task instances require a dynamic `update_id`, e.g. via parameter(s), otherwise the query will only execute once\n    To customize the query signature as recorded in the database marker table, override the `update_id` property.\n    \"\"\"\n\n    @abc.abstractproperty\n    def aws_access_key_id(self):\n        \"\"\"\n        Override to return the key id.\n        \"\"\"\n        return None\n\n    @abc.abstractproperty\n    def aws_secret_access_key(self):\n        \"\"\"\n        Override to return the secret access key.\n        \"\"\"\n        return None\n\n    @property\n    def s3_unload_path(self):\n        \"\"\"\n        Override to return the load path.\n        \"\"\"\n        return ''\n\n    @property\n    def unload_options(self):\n        \"\"\"\n        Add extra or override default unload options:\n        \"\"\"\n        return \"DELIMITER '|' ADDQUOTES GZIP ALLOWOVERWRITE PARALLEL ON\"\n\n    @property\n    def unload_query(self):\n        \"\"\"\n        Default UNLOAD command\n        \"\"\"\n        return (\"UNLOAD ( '{query}' ) TO '{s3_unload_path}' \"\n                \"credentials 'aws_access_key_id={s3_access_key};aws_secret_access_key={s3_security_key}' \"\n                \"{unload_options};\")\n\n    def run(self):\n        connection = self.output().connect()\n        cursor = connection.cursor()\n\n        # Retrieve AWS s3 credentials\n        config = luigi.configuration.get_config()\n        if self.aws_access_key_id is None or self.aws_secret_access_key is None:\n            self.aws_access_key_id = config.get('s3', 'aws_access_key_id')\n            self.aws_secret_access_key = config.get('s3', 'aws_secret_access_key')\n        # Optionally we can access env variables to get the keys\n        if self.aws_access_key_id is None or self.aws_access_key_id.strip() == '' \\\n                or self.aws_secret_access_key is None or self.aws_secret_access_key.strip() == '':\n            self.aws_access_key_id = os.environ['AWS_ACCESS_KEY_ID']\n            self.aws_secret_access_key = os.environ['AWS_SECRET_ACCESS_KEY']\n\n        unload_query = self.unload_query.format(\n            query=self.query().replace(\"'\", r\"\\'\"),\n            s3_unload_path=self.s3_unload_path,\n            unload_options=self.unload_options,\n            s3_access_key=self.aws_access_key_id,\n            s3_security_key=self.aws_secret_access_key)\n\n        logger.info('Executing unload query from task: {name}'.format(name=self.__class__))\n        try:\n            cursor = connection.cursor()\n            cursor.execute(unload_query)\n            logger.info(cursor.statusmessage)\n        except:\n            raise\n\n        # Update marker table\n        self.output().touch(connection)\n        # commit and close connection\n        connection.commit()\n        connection.close()\n\n    def output(self):\n        \"\"\"\n        Returns a RedshiftTarget representing the executed query.\n\n        Normally you don't override this.\n        \"\"\"\n        return RedshiftTarget(\n            host=self.host,\n            database=self.database,\n            user=self.user,\n            password=self.password,\n            table=self.table,\n            update_id=self.update_id\n        )\n",
          "file_after": "# -*- coding: utf-8 -*-\n#\n# Copyright 2012-2015 Spotify AB\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport abc\nimport json\nimport logging\nimport time\nimport os\n\nimport luigi\nfrom luigi.contrib import postgres\nfrom luigi.contrib import rdbms\nfrom luigi.contrib.s3 import S3PathTask, S3Target\n\nlogger = logging.getLogger('luigi-interface')\n\n\ntry:\n    import psycopg2\n    import psycopg2.errorcodes\nexcept ImportError:\n    logger.warning(\"Loading postgres module without psycopg2 installed. \"\n                   \"Will crash at runtime if postgres functionality is used.\")\n\n\nclass _CredentialsMixin():\n    \"\"\"\n    This mixin is used to provide the same credential properties\n    for AWS to all Redshift tasks. It also provides a helper method\n    to generate the credentials string for the task.\n    \"\"\"\n\n    @property\n    def configuration_section(self):\n        \"\"\"\n        Override to change the configuration section used\n        to obtain default credentials.\n        \"\"\"\n        return 'redshift'\n\n    @property\n    def aws_access_key_id(self):\n        \"\"\"\n        Override to return the key id.\n        \"\"\"\n        return self._get_configuration_attribute('aws_access_key_id')\n\n    @property\n    def aws_secret_access_key(self):\n        \"\"\"\n        Override to return the secret access key.\n        \"\"\"\n        return self._get_configuration_attribute('aws_secret_access_key')\n\n    @property\n    def aws_account_id(self):\n        \"\"\"\n        Override to return the account id.\n        \"\"\"\n        return self._get_configuration_attribute('aws_account_id')\n\n    @property\n    def aws_arn_role_name(self):\n        \"\"\"\n        Override to return the arn role name.\n        \"\"\"\n        return self._get_configuration_attribute('aws_arn_role_name')\n\n    @property\n    def aws_session_token(self):\n        \"\"\"\n        Override to return the session token.\n        \"\"\"\n        return self._get_configuration_attribute('aws_session_token')\n\n    def _get_configuration_attribute(self, attribute):\n        config = luigi.configuration.get_config()\n\n        value = config.get(self.configuration_section, attribute, default=None)\n\n        if not value:\n            value = os.environ.get(attribute.upper(), None)\n\n        return value\n\n    def _credentials(self):\n        \"\"\"\n        Return a credential string for the provided task. If no valid\n        credentials are set, raise a NotImplementedError.\n        \"\"\"\n\n        if self.aws_account_id and self.aws_arn_role_name:\n            return 'aws_iam_role=arn:aws:iam::{id}:role/{role}'.format(\n                id=self.aws_account_id,\n                role=self.aws_arn_role_name\n            )\n        elif self.aws_access_key_id and self.aws_secret_access_key:\n            return 'aws_access_key_id={key};aws_secret_access_key={secret}{opt}'.format(\n                key=self.aws_access_key_id,\n                secret=self.aws_secret_access_key,\n                opt=';token={}'.format(self.aws_session_token) if self.aws_session_token else ''\n            )\n        else:\n            raise NotImplementedError(\"Missing Credentials. \"\n                                      \"Ensure one of the pairs of auth args below are set \"\n                                      \"in a configuration file, environment variables or by \"\n                                      \"being overridden in the task: \"\n                                      \"'aws_access_key_id' AND 'aws_secret_access_key' OR \"\n                                      \"'aws_account_id' AND 'aws_arn_role_name'\")\n\n\nclass RedshiftTarget(postgres.PostgresTarget):\n    \"\"\"\n    Target for a resource in Redshift.\n\n    Redshift is similar to postgres with a few adjustments\n    required by redshift.\n    \"\"\"\n    marker_table = luigi.configuration.get_config().get(\n        'redshift',\n        'marker-table',\n        'table_updates')\n\n    use_db_timestamps = False\n\n\nclass S3CopyToTable(rdbms.CopyToTable, _CredentialsMixin):\n    \"\"\"\n    Template task for inserting a data set into Redshift from s3.\n\n    Usage:\n\n    * Subclass and override the required attributes:\n\n      * `host`,\n      * `database`,\n      * `user`,\n      * `password`,\n      * `table`,\n      * `columns`,\n      * `s3_load_path`.\n\n    * You can also override the attributes provided by the\n      CredentialsMixin if they are not supplied by your\n      configuration or environment variables.\n    \"\"\"\n\n    @abc.abstractmethod\n    def s3_load_path(self):\n        \"\"\"\n        Override to return the load path.\n        \"\"\"\n        return None\n\n    @abc.abstractproperty\n    def copy_options(self):\n        \"\"\"\n        Add extra copy options, for example:\n\n        * TIMEFORMAT 'auto'\n        * IGNOREHEADER 1\n        * TRUNCATECOLUMNS\n        * IGNOREBLANKLINES\n        * DELIMITER '\\t'\n        \"\"\"\n        return ''\n\n    @property\n    def prune_table(self):\n        \"\"\"\n        Override to set equal to the name of the table which is to be pruned.\n        Intended to be used in conjunction with prune_column and prune_date\n        i.e. copy to temp table, prune production table to prune_column with a date greater than prune_date, then insert into production table from temp table\n        \"\"\"\n        return None\n\n    @property\n    def prune_column(self):\n        \"\"\"\n        Override to set equal to the column of the prune_table which is to be compared\n        Intended to be used in conjunction with prune_table and prune_date\n        i.e. copy to temp table, prune production table to prune_column with a date greater than prune_date, then insert into production table from temp table\n        \"\"\"\n        return None\n\n    @property\n    def prune_date(self):\n        \"\"\"\n        Override to set equal to the date by which prune_column is to be compared\n        Intended to be used in conjunction with prune_table and prune_column\n        i.e. copy to temp table, prune production table to prune_column with a date greater than prune_date, then insert into production table from temp table\n        \"\"\"\n        return None\n\n    @property\n    def table_attributes(self):\n        \"\"\"\n        Add extra table attributes, for example:\n\n        DISTSTYLE KEY\n        DISTKEY (MY_FIELD)\n        SORTKEY (MY_FIELD_2, MY_FIELD_3)\n        \"\"\"\n        return ''\n\n    @property\n    def do_truncate_table(self):\n        \"\"\"\n        Return True if table should be truncated before copying new data in.\n        \"\"\"\n        return False\n\n    def do_prune(self):\n        \"\"\"\n        Return True if prune_table, prune_column, and prune_date are implemented.\n        If only a subset of prune variables are override, an exception is raised to remind the user to implement all or none.\n        Prune (data newer than prune_date deleted) before copying new data in.\n        \"\"\"\n        if self.prune_table and self.prune_column and self.prune_date:\n            return True\n        elif self.prune_table or self.prune_column or self.prune_date:\n            raise Exception('override zero or all prune variables')\n        else:\n            return False\n\n    @property\n    def table_type(self):\n        \"\"\"\n        Return table type (i.e. 'temp').\n        \"\"\"\n        return ''\n\n    @property\n    def queries(self):\n        \"\"\"\n        Override to return a list of queries to be executed in order.\n        \"\"\"\n        return []\n\n    def truncate_table(self, connection):\n        query = \"truncate %s\" % self.table\n        cursor = connection.cursor()\n        try:\n            cursor.execute(query)\n        finally:\n            cursor.close()\n\n    def prune(self, connection):\n        query = \"delete from %s where %s >= %s\" % (self.prune_table, self.prune_column, self.prune_date)\n        cursor = connection.cursor()\n        try:\n            cursor.execute(query)\n        finally:\n            cursor.close()\n\n    def create_table(self, connection):\n        \"\"\"\n        Override to provide code for creating the target table.\n\n        By default it will be created using types (optionally)\n        specified in columns.\n\n        If overridden, use the provided connection object for\n        setting up the table in order to create the table and\n        insert data using the same transaction.\n        \"\"\"\n        if len(self.columns[0]) == 1:\n            # only names of columns specified, no types\n            raise NotImplementedError(\"create_table() not implemented \"\n                                      \"for %r and columns types not \"\n                                      \"specified\" % self.table)\n        elif len(self.columns[0]) == 2:\n            # if columns is specified as (name, type) tuples\n            coldefs = ','.join(\n                '{name} {type}'.format(\n                    name=name,\n                    type=type) for name, type in self.columns\n            )\n            query = (\"CREATE {type} TABLE \"\n                     \"{table} ({coldefs}) \"\n                     \"{table_attributes}\").format(\n                type=self.table_type,\n                table=self.table,\n                coldefs=coldefs,\n                table_attributes=self.table_attributes)\n\n            connection.cursor().execute(query)\n        else:\n            raise ValueError(\"create_table() found no columns for %r\"\n                             % self.table)\n\n    def run(self):\n        \"\"\"\n        If the target table doesn't exist, self.create_table\n        will be called to attempt to create the table.\n        \"\"\"\n        if not (self.table):\n            raise Exception(\"table need to be specified\")\n\n        path = self.s3_load_path()\n        output = self.output()\n        connection = output.connect()\n        cursor = connection.cursor()\n\n        self.init_copy(connection)\n        self.copy(cursor, path)\n        self.post_copy(cursor)\n\n        # update marker table\n        output.touch(connection)\n        connection.commit()\n\n        # commit and clean up\n        connection.close()\n\n    def copy(self, cursor, f):\n        \"\"\"\n        Defines copying from s3 into redshift.\n\n        If both key-based and role-based credentials are provided, role-based will be used.\n        \"\"\"\n        logger.info(\"Inserting file: %s\", f)\n        cursor.execute(\"\"\"\n         COPY {table} from '{source}'\n         CREDENTIALS '{creds}'\n         {options}\n         ;\"\"\".format(\n            table=self.table,\n            source=f,\n            creds=self._credentials(),\n            options=self.copy_options)\n        )\n\n    def output(self):\n        \"\"\"\n        Returns a RedshiftTarget representing the inserted dataset.\n\n        Normally you don't override this.\n        \"\"\"\n        return RedshiftTarget(\n            host=self.host,\n            database=self.database,\n            user=self.user,\n            password=self.password,\n            table=self.table,\n            update_id=self.update_id)\n\n    def does_table_exist(self, connection):\n        \"\"\"\n        Determine whether the table already exists.\n        \"\"\"\n\n        if '.' in self.table:\n            query = (\"select 1 as table_exists \"\n                     \"from information_schema.tables \"\n                     \"where table_schema = lower(%s) and table_name = lower(%s) limit 1\")\n        else:\n            query = (\"select 1 as table_exists \"\n                     \"from pg_table_def \"\n                     \"where tablename = lower(%s) limit 1\")\n        cursor = connection.cursor()\n        try:\n            cursor.execute(query, tuple(self.table.split('.')))\n            result = cursor.fetchone()\n            return bool(result)\n        finally:\n            cursor.close()\n\n    def init_copy(self, connection):\n        \"\"\"\n        Perform pre-copy sql - such as creating table, truncating, or removing data older than x.\n        \"\"\"\n        if not self.does_table_exist(connection):\n            logger.info(\"Creating table %s\", self.table)\n            connection.reset()\n            self.create_table(connection)\n\n        if self.do_truncate_table:\n            logger.info(\"Truncating table %s\", self.table)\n            self.truncate_table(connection)\n\n        if self.do_prune():\n            logger.info(\"Removing %s older than %s from %s\", self.prune_column, self.prune_date, self.prune_table)\n            self.prune(connection)\n\n    def post_copy(self, cursor):\n        \"\"\"\n        Performs post-copy sql - such as cleansing data, inserting into production table (if copied to temp table), etc.\n        \"\"\"\n        logger.info('Executing post copy queries')\n        for query in self.queries:\n            cursor.execute(query)\n\n\nclass S3CopyJSONToTable(S3CopyToTable, _CredentialsMixin):\n    \"\"\"\n    Template task for inserting a JSON data set into Redshift from s3.\n\n    Usage:\n\n        * Subclass and override the required attributes:\n\n            * `host`,\n            * `database`,\n            * `user`,\n            * `password`,\n            * `table`,\n            * `columns`,\n            * `s3_load_path`,\n            * `jsonpath`,\n            * `copy_json_options`.\n\n    * You can also override the attributes provided by the\n      CredentialsMixin if they are not supplied by your\n      configuration or environment variables.\n    \"\"\"\n\n    @abc.abstractproperty\n    def jsonpath(self):\n        \"\"\"\n        Override the jsonpath schema location for the table.\n        \"\"\"\n        return ''\n\n    @abc.abstractproperty\n    def copy_json_options(self):\n        \"\"\"\n        Add extra copy options, for example:\n\n        * GZIP\n        * LZOP\n        \"\"\"\n        return ''\n\n    def copy(self, cursor, f):\n        \"\"\"\n        Defines copying JSON from s3 into redshift.\n        \"\"\"\n\n        logger.info(\"Inserting file: %s\", f)\n        cursor.execute(\"\"\"\n         COPY %s from '%s'\n         CREDENTIALS '%s'\n         JSON AS '%s' %s\n         %s\n         ;\"\"\" % (self.table, f, self._credentials(),\n                 self.jsonpath, self.copy_json_options, self.copy_options))\n\n\nclass RedshiftManifestTask(S3PathTask):\n    \"\"\"\n    Generic task to generate a manifest file that can be used\n    in S3CopyToTable in order to copy multiple files from your\n    s3 folder into a redshift table at once.\n\n    For full description on how to use the manifest file see\n    http://docs.aws.amazon.com/redshift/latest/dg/loading-data-files-using-manifest.html\n\n    Usage:\n\n        * requires parameters\n            * path - s3 path to the generated manifest file, including the\n                     name of the generated file\n                     to be copied into a redshift table\n            * folder_paths - s3 paths to the folders containing files you wish to be copied\n\n    Output:\n\n        * generated manifest file\n    \"\"\"\n\n    # should be over ridden to point to a variety\n    # of folders you wish to copy from\n    folder_paths = luigi.Parameter()\n    text_target = True\n\n    def run(self):\n        entries = []\n        for folder_path in self.folder_paths:\n            s3 = S3Target(folder_path)\n            client = s3.fs\n            for file_name in client.list(s3.path):\n                entries.append({\n                    'url': '%s/%s' % (folder_path, file_name),\n                    'mandatory': True\n                })\n        manifest = {'entries': entries}\n        target = self.output().open('w')\n        dump = json.dumps(manifest)\n        if not self.text_target:\n            dump = dump.encode('utf8')\n        target.write(dump)\n        target.close()\n\n\nclass KillOpenRedshiftSessions(luigi.Task):\n    \"\"\"\n    An task for killing any open Redshift sessions\n    in a given database. This is necessary to prevent open user sessions\n    with transactions against the table from blocking drop or truncate\n    table commands.\n\n    Usage:\n\n    Subclass and override the required `host`, `database`,\n    `user`, and `password` attributes.\n    \"\"\"\n\n    # time in seconds to wait before\n    # reconnecting to Redshift if our session is killed too.\n    # 30 seconds is usually fine; 60 is conservative\n    connection_reset_wait_seconds = luigi.IntParameter(default=60)\n\n    @abc.abstractproperty\n    def host(self):\n        return None\n\n    @abc.abstractproperty\n    def database(self):\n        return None\n\n    @abc.abstractproperty\n    def user(self):\n        return None\n\n    @abc.abstractproperty\n    def password(self):\n        return None\n\n    @property\n    def update_id(self):\n        \"\"\"\n        This update id will be a unique identifier\n        for this insert on this table.\n        \"\"\"\n        return self.task_id\n\n    def output(self):\n        \"\"\"\n        Returns a RedshiftTarget representing the inserted dataset.\n\n        Normally you don't override this.\n        \"\"\"\n        # uses class name as a meta-table\n        return RedshiftTarget(\n            host=self.host,\n            database=self.database,\n            user=self.user,\n            password=self.password,\n            table=self.__class__.__name__,\n            update_id=self.update_id)\n\n    def run(self):\n        \"\"\"\n        Kill any open Redshift sessions for the given database.\n        \"\"\"\n        connection = self.output().connect()\n        # kill any sessions other than ours and\n        # internal Redshift sessions (rdsdb)\n        query = (\"select pg_terminate_backend(process) \"\n                 \"from STV_SESSIONS \"\n                 \"where db_name=%s \"\n                 \"and user_name != 'rdsdb' \"\n                 \"and process != pg_backend_pid()\")\n        cursor = connection.cursor()\n        logger.info('Killing all open Redshift sessions for database: %s', self.database)\n        try:\n            cursor.execute(query, (self.database,))\n            cursor.close()\n            connection.commit()\n        except psycopg2.DatabaseError as e:\n            if e.message and 'EOF' in e.message:\n                # sometimes this operation kills the current session.\n                # rebuild the connection. Need to pause for 30-60 seconds\n                # before Redshift will allow us back in.\n                connection.close()\n                logger.info('Pausing %s seconds for Redshift to reset connection', self.connection_reset_wait_seconds)\n                time.sleep(self.connection_reset_wait_seconds)\n                logger.info('Reconnecting to Redshift')\n                connection = self.output().connect()\n            else:\n                raise\n\n        try:\n            self.output().touch(connection)\n            connection.commit()\n        finally:\n            connection.close()\n\n        logger.info('Done killing all open Redshift sessions for database: %s', self.database)\n\n\nclass RedshiftQuery(postgres.PostgresQuery):\n    \"\"\"\n    Template task for querying an Amazon Redshift database\n\n    Usage:\n    Subclass and override the required `host`, `database`, `user`, `password`, `table`, and `query` attributes.\n\n    Override the `run` method if your use case requires some action with the query result.\n\n    Task instances require a dynamic `update_id`, e.g. via parameter(s), otherwise the query will only execute once\n\n    To customize the query signature as recorded in the database marker table, override the `update_id` property.\n    \"\"\"\n\n    def output(self):\n        \"\"\"\n        Returns a RedshiftTarget representing the executed query.\n\n        Normally you don't override this.\n        \"\"\"\n        return RedshiftTarget(\n            host=self.host,\n            database=self.database,\n            user=self.user,\n            password=self.password,\n            table=self.table,\n            update_id=self.update_id\n        )\n\n\nclass RedshiftUnloadTask(postgres.PostgresQuery, _CredentialsMixin):\n    \"\"\"\n    Template task for running UNLOAD on an Amazon Redshift database\n\n    Usage:\n    Subclass and override the required `host`, `database`, `user`, `password`, `table`, and `query` attributes.\n    Override the `run` method if your use case requires some action with the query result.\n    Task instances require a dynamic `update_id`, e.g. via parameter(s), otherwise the query will only execute once\n    To customize the query signature as recorded in the database marker table, override the `update_id` property.\n    You can also override the attributes provided by the CredentialsMixin if they are not supplied by your configuration or environment variables.\n    \"\"\"\n\n    @property\n    def s3_unload_path(self):\n        \"\"\"\n        Override to return the load path.\n        \"\"\"\n        return ''\n\n    @property\n    def unload_options(self):\n        \"\"\"\n        Add extra or override default unload options:\n        \"\"\"\n        return \"DELIMITER '|' ADDQUOTES GZIP ALLOWOVERWRITE PARALLEL ON\"\n\n    @property\n    def unload_query(self):\n        \"\"\"\n        Default UNLOAD command\n        \"\"\"\n        return (\"UNLOAD ( '{query}' ) TO '{s3_unload_path}' \"\n                \"credentials '{credentials}' \"\n                \"{unload_options};\")\n\n    def run(self):\n        connection = self.output().connect()\n        cursor = connection.cursor()\n\n        unload_query = self.unload_query.format(\n            query=self.query().replace(\"'\", r\"\\'\"),\n            s3_unload_path=self.s3_unload_path,\n            unload_options=self.unload_options,\n            credentials=self._credentials())\n\n        logger.info('Executing unload query from task: {name}'.format(name=self.__class__))\n        try:\n            cursor = connection.cursor()\n            cursor.execute(unload_query)\n            logger.info(cursor.statusmessage)\n        except:\n            raise\n\n        # Update marker table\n        self.output().touch(connection)\n        # commit and close connection\n        connection.commit()\n        connection.close()\n\n    def output(self):\n        \"\"\"\n        Returns a RedshiftTarget representing the executed query.\n\n        Normally you don't override this.\n        \"\"\"\n        return RedshiftTarget(\n            host=self.host,\n            database=self.database,\n            user=self.user,\n            password=self.password,\n            table=self.table,\n            update_id=self.update_id\n        )\n",
          "file_patch": "@@ -37,6 +37,92 @@ except ImportError:\n                    \"Will crash at runtime if postgres functionality is used.\")\n \n \n+class _CredentialsMixin():\n+    \"\"\"\n+    This mixin is used to provide the same credential properties\n+    for AWS to all Redshift tasks. It also provides a helper method\n+    to generate the credentials string for the task.\n+    \"\"\"\n+\n+    @property\n+    def configuration_section(self):\n+        \"\"\"\n+        Override to change the configuration section used\n+        to obtain default credentials.\n+        \"\"\"\n+        return 'redshift'\n+\n+    @property\n+    def aws_access_key_id(self):\n+        \"\"\"\n+        Override to return the key id.\n+        \"\"\"\n+        return self._get_configuration_attribute('aws_access_key_id')\n+\n+    @property\n+    def aws_secret_access_key(self):\n+        \"\"\"\n+        Override to return the secret access key.\n+        \"\"\"\n+        return self._get_configuration_attribute('aws_secret_access_key')\n+\n+    @property\n+    def aws_account_id(self):\n+        \"\"\"\n+        Override to return the account id.\n+        \"\"\"\n+        return self._get_configuration_attribute('aws_account_id')\n+\n+    @property\n+    def aws_arn_role_name(self):\n+        \"\"\"\n+        Override to return the arn role name.\n+        \"\"\"\n+        return self._get_configuration_attribute('aws_arn_role_name')\n+\n+    @property\n+    def aws_session_token(self):\n+        \"\"\"\n+        Override to return the session token.\n+        \"\"\"\n+        return self._get_configuration_attribute('aws_session_token')\n+\n+    def _get_configuration_attribute(self, attribute):\n+        config = luigi.configuration.get_config()\n+\n+        value = config.get(self.configuration_section, attribute, default=None)\n+\n+        if not value:\n+            value = os.environ.get(attribute.upper(), None)\n+\n+        return value\n+\n+    def _credentials(self):\n+        \"\"\"\n+        Return a credential string for the provided task. If no valid\n+        credentials are set, raise a NotImplementedError.\n+        \"\"\"\n+\n+        if self.aws_account_id and self.aws_arn_role_name:\n+            return 'aws_iam_role=arn:aws:iam::{id}:role/{role}'.format(\n+                id=self.aws_account_id,\n+                role=self.aws_arn_role_name\n+            )\n+        elif self.aws_access_key_id and self.aws_secret_access_key:\n+            return 'aws_access_key_id={key};aws_secret_access_key={secret}{opt}'.format(\n+                key=self.aws_access_key_id,\n+                secret=self.aws_secret_access_key,\n+                opt=';token={}'.format(self.aws_session_token) if self.aws_session_token else ''\n+            )\n+        else:\n+            raise NotImplementedError(\"Missing Credentials. \"\n+                                      \"Ensure one of the pairs of auth args below are set \"\n+                                      \"in a configuration file, environment variables or by \"\n+                                      \"being overridden in the task: \"\n+                                      \"'aws_access_key_id' AND 'aws_secret_access_key' OR \"\n+                                      \"'aws_account_id' AND 'aws_arn_role_name'\")\n+\n+\n class RedshiftTarget(postgres.PostgresTarget):\n     \"\"\"\n     Target for a resource in Redshift.\n@@ -52,7 +138,7 @@ class RedshiftTarget(postgres.PostgresTarget):\n     use_db_timestamps = False\n \n \n-class S3CopyToTable(rdbms.CopyToTable):\n+class S3CopyToTable(rdbms.CopyToTable, _CredentialsMixin):\n     \"\"\"\n     Template task for inserting a data set into Redshift from s3.\n \n@@ -66,9 +152,11 @@ class S3CopyToTable(rdbms.CopyToTable):\n       * `password`,\n       * `table`,\n       * `columns`,\n-      * `aws_access_key_id`,\n-      * `aws_secret_access_key`,\n       * `s3_load_path`.\n+\n+    * You can also override the attributes provided by the\n+      CredentialsMixin if they are not supplied by your\n+      configuration or environment variables.\n     \"\"\"\n \n     @abc.abstractmethod\n@@ -78,41 +166,6 @@ class S3CopyToTable(rdbms.CopyToTable):\n         \"\"\"\n         return None\n \n-    @property\n-    def aws_access_key_id(self):\n-        \"\"\"\n-        Override to return the key id.\n-        \"\"\"\n-        return None\n-\n-    @property\n-    def aws_secret_access_key(self):\n-        \"\"\"\n-        Override to return the secret access key.\n-        \"\"\"\n-        return None\n-\n-    @property\n-    def aws_account_id(self):\n-        \"\"\"\n-        Override to return the account id.\n-        \"\"\"\n-        return None\n-\n-    @property\n-    def aws_arn_role_name(self):\n-        \"\"\"\n-        Override to return the arn role name.\n-        \"\"\"\n-        return None\n-\n-    @property\n-    def aws_session_token(self):\n-        \"\"\"\n-        Override to return the session token.\n-        \"\"\"\n-        return None\n-\n     @abc.abstractproperty\n     def copy_options(self):\n         \"\"\"\n@@ -280,24 +333,6 @@ class S3CopyToTable(rdbms.CopyToTable):\n \n         If both key-based and role-based credentials are provided, role-based will be used.\n         \"\"\"\n-        # format the credentials string dependent upon which type of credentials were provided\n-        if self.aws_account_id and self.aws_arn_role_name:\n-            cred_str = 'aws_iam_role=arn:aws:iam::{id}:role/{role}'.format(\n-                id=self.aws_account_id,\n-                role=self.aws_arn_role_name\n-            )\n-        elif self.aws_access_key_id and self.aws_secret_access_key:\n-            cred_str = 'aws_access_key_id={key};aws_secret_access_key={secret}{opt}'.format(\n-                key=self.aws_access_key_id,\n-                secret=self.aws_secret_access_key,\n-                opt=';token={}'.format(self.aws_session_token) if self.aws_session_token else ''\n-            )\n-        else:\n-            raise NotImplementedError(\"Missing Credentials. \"\n-                                      \"Override one of the following pairs of auth-args: \"\n-                                      \"'aws_access_key_id' AND 'aws_secret_access_key' OR \"\n-                                      \"'aws_account_id' AND 'aws_arn_role_name'\")\n-\n         logger.info(\"Inserting file: %s\", f)\n         cursor.execute(\"\"\"\n          COPY {table} from '{source}'\n@@ -306,7 +341,7 @@ class S3CopyToTable(rdbms.CopyToTable):\n          ;\"\"\".format(\n             table=self.table,\n             source=f,\n-            creds=cred_str,\n+            creds=self._credentials(),\n             options=self.copy_options)\n         )\n \n@@ -371,7 +406,7 @@ class S3CopyToTable(rdbms.CopyToTable):\n             cursor.execute(query)\n \n \n-class S3CopyJSONToTable(S3CopyToTable):\n+class S3CopyJSONToTable(S3CopyToTable, _CredentialsMixin):\n     \"\"\"\n     Template task for inserting a JSON data set into Redshift from s3.\n \n@@ -385,11 +420,13 @@ class S3CopyJSONToTable(S3CopyToTable):\n             * `password`,\n             * `table`,\n             * `columns`,\n-            * `aws_access_key_id`,\n-            * `aws_secret_access_key`,\n             * `s3_load_path`,\n             * `jsonpath`,\n             * `copy_json_options`.\n+\n+    * You can also override the attributes provided by the\n+      CredentialsMixin if they are not supplied by your\n+      configuration or environment variables.\n     \"\"\"\n \n     @abc.abstractproperty\n@@ -413,21 +450,14 @@ class S3CopyJSONToTable(S3CopyToTable):\n         \"\"\"\n         Defines copying JSON from s3 into redshift.\n         \"\"\"\n-        # if session token is set, create token string\n-        if self.aws_session_token:\n-            token = ';token=%s' % self.aws_session_token\n-        # otherwise, leave token string empty\n-        else:\n-            token = ''\n \n         logger.info(\"Inserting file: %s\", f)\n         cursor.execute(\"\"\"\n          COPY %s from '%s'\n-         CREDENTIALS 'aws_access_key_id=%s;aws_secret_access_key=%s%s'\n+         CREDENTIALS '%s'\n          JSON AS '%s' %s\n          %s\n-         ;\"\"\" % (self.table, f, self.aws_access_key_id,\n-                 self.aws_secret_access_key, token,\n+         ;\"\"\" % (self.table, f, self._credentials(),\n                  self.jsonpath, self.copy_json_options, self.copy_options))\n \n \n@@ -604,7 +634,7 @@ class RedshiftQuery(postgres.PostgresQuery):\n         )\n \n \n-class RedshiftUnloadTask(postgres.PostgresQuery):\n+class RedshiftUnloadTask(postgres.PostgresQuery, _CredentialsMixin):\n     \"\"\"\n     Template task for running UNLOAD on an Amazon Redshift database\n \n@@ -613,22 +643,9 @@ class RedshiftUnloadTask(postgres.PostgresQuery):\n     Override the `run` method if your use case requires some action with the query result.\n     Task instances require a dynamic `update_id`, e.g. via parameter(s), otherwise the query will only execute once\n     To customize the query signature as recorded in the database marker table, override the `update_id` property.\n+    You can also override the attributes provided by the CredentialsMixin if they are not supplied by your configuration or environment variables.\n     \"\"\"\n \n-    @abc.abstractproperty\n-    def aws_access_key_id(self):\n-        \"\"\"\n-        Override to return the key id.\n-        \"\"\"\n-        return None\n-\n-    @abc.abstractproperty\n-    def aws_secret_access_key(self):\n-        \"\"\"\n-        Override to return the secret access key.\n-        \"\"\"\n-        return None\n-\n     @property\n     def s3_unload_path(self):\n         \"\"\"\n@@ -649,30 +666,18 @@ class RedshiftUnloadTask(postgres.PostgresQuery):\n         Default UNLOAD command\n         \"\"\"\n         return (\"UNLOAD ( '{query}' ) TO '{s3_unload_path}' \"\n-                \"credentials 'aws_access_key_id={s3_access_key};aws_secret_access_key={s3_security_key}' \"\n+                \"credentials '{credentials}' \"\n                 \"{unload_options};\")\n \n     def run(self):\n         connection = self.output().connect()\n         cursor = connection.cursor()\n \n-        # Retrieve AWS s3 credentials\n-        config = luigi.configuration.get_config()\n-        if self.aws_access_key_id is None or self.aws_secret_access_key is None:\n-            self.aws_access_key_id = config.get('s3', 'aws_access_key_id')\n-            self.aws_secret_access_key = config.get('s3', 'aws_secret_access_key')\n-        # Optionally we can access env variables to get the keys\n-        if self.aws_access_key_id is None or self.aws_access_key_id.strip() == '' \\\n-                or self.aws_secret_access_key is None or self.aws_secret_access_key.strip() == '':\n-            self.aws_access_key_id = os.environ['AWS_ACCESS_KEY_ID']\n-            self.aws_secret_access_key = os.environ['AWS_SECRET_ACCESS_KEY']\n-\n         unload_query = self.unload_query.format(\n             query=self.query().replace(\"'\", r\"\\'\"),\n             s3_unload_path=self.s3_unload_path,\n             unload_options=self.unload_options,\n-            s3_access_key=self.aws_access_key_id,\n-            s3_security_key=self.aws_secret_access_key)\n+            credentials=self._credentials())\n \n         logger.info('Executing unload query from task: {name}'.format(name=self.__class__))\n         try:\n",
          "files_name_in_blame_commit": [
            "redshift_test.py",
            "redshift.py"
          ]
        }
      },
      "b86166cc1a46af90a78c91528f1e18d66760b659": {
        "commit": {
          "commit_id": "b86166cc1a46af90a78c91528f1e18d66760b659",
          "commit_message": "Add redshift copy support for role-based credential string (#1962)\n\n* Add support for role-based credentials and refactor key-based creds\r\n\r\n* Add test to ensure error is raised when no redshift creds are provided",
          "commit_author": "Dillon Stadther",
          "commit_date": "2017-01-26 07:01:18",
          "commit_parent": "d7d6a374b13b57acda0f45a534726d6d50c8d070"
        },
        "function": {
          "function_name": "copy",
          "function_code_before": "def copy(self, cursor, f):\n    \"\"\"\n        Defines copying from s3 into redshift.\n        \"\"\"\n    if self.aws_session_token:\n        token = ';token=%s' % self.aws_session_token\n    else:\n        token = ''\n    logger.info('Inserting file: %s', f)\n    cursor.execute(\"\\n         COPY %s from '%s'\\n         CREDENTIALS 'aws_access_key_id=%s;aws_secret_access_key=%s%s'\\n         %s\\n         ;\" % (self.table, f, self.aws_access_key_id, self.aws_secret_access_key, token, self.copy_options))",
          "function_code_after": "def copy(self, cursor, f):\n    \"\"\"\n        Defines copying from s3 into redshift.\n\n        If both key-based and role-based credentials are provided, role-based will be used.\n        \"\"\"\n    if self.aws_account_id and self.aws_arn_role_name:\n        cred_str = 'aws_iam_role=arn:aws:iam::{id}:role/{role}'.format(id=self.aws_account_id, role=self.aws_arn_role_name)\n    elif self.aws_access_key_id and self.aws_secret_access_key:\n        cred_str = 'aws_access_key_id={key};aws_secret_key={secret}{opt}'.format(key=self.aws_access_key_id, secret=self.aws_secret_access_key, opt=';token={}'.format(self.aws_session_token) if self.aws_session_token else '')\n    else:\n        raise NotImplementedError(\"Missing Credentials. Override one of the following pairs of auth-args: 'aws_access_key_id' AND 'aws_secret_access_key' OR 'aws_account_id' AND 'aws_arn_role_name'\")\n    logger.info('Inserting file: %s', f)\n    cursor.execute(\"\\n         COPY {table} from '{source}'\\n         CREDENTIALS '{creds}'\\n         {options}\\n         ;\".format(table=self.table, source=f, creds=cred_str, options=self.copy_options))",
          "function_before_start_line": 263,
          "function_before_end_line": 281,
          "function_after_start_line": 277,
          "function_after_end_line": 311,
          "function_before_token_count": 63,
          "function_after_token_count": 128,
          "functions_name_modified_file": [
            "prune",
            "output",
            "post_copy",
            "user",
            "s3_unload_path",
            "create_table",
            "s3_load_path",
            "do_prune",
            "do_truncate_table",
            "aws_account_id",
            "host",
            "aws_session_token",
            "prune_column",
            "queries",
            "prune_date",
            "copy_json_options",
            "truncate_table",
            "aws_secret_access_key",
            "aws_arn_role_name",
            "table_type",
            "password",
            "run",
            "unload_options",
            "prune_table",
            "copy_options",
            "table_attributes",
            "does_table_exist",
            "init_copy",
            "jsonpath",
            "aws_access_key_id",
            "unload_query",
            "copy",
            "update_id",
            "database"
          ],
          "functions_name_all_files": [
            "prune",
            "output",
            "post_copy",
            "user",
            "s3_unload_path",
            "create_table",
            "s3_load_path",
            "do_prune",
            "do_truncate_table",
            "aws_account_id",
            "host",
            "aws_session_token",
            "prune_column",
            "queries",
            "prune_date",
            "copy_json_options",
            "truncate_table",
            "aws_secret_access_key",
            "aws_arn_role_name",
            "table_type",
            "password",
            "run",
            "unload_options",
            "query",
            "prune_table",
            "copy_options",
            "test_s3_copy_to_temp_table",
            "test_redshift_unload_command",
            "table_attributes",
            "does_table_exist",
            "init_copy",
            "test_s3_copy_to_table",
            "jsonpath",
            "aws_access_key_id",
            "unload_query",
            "copy",
            "test_s3_copy_to_missing_table",
            "update_id",
            "database",
            "test_copy_missing_creds"
          ],
          "functions_name_co_evolved_modified_file": [
            "aws_account_id",
            "aws_arn_role_name"
          ],
          "functions_name_co_evolved_all_files": [
            "aws_arn_role_name",
            "test_s3_copy_to_missing_table",
            "aws_account_id",
            "test_s3_copy_to_table",
            "test_copy_missing_creds"
          ]
        },
        "file": {
          "file_name": "redshift.py",
          "file_nloc": 440,
          "file_complexity": 81,
          "file_token_count": 1896,
          "file_before": "# -*- coding: utf-8 -*-\n#\n# Copyright 2012-2015 Spotify AB\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport abc\nimport json\nimport logging\nimport time\nimport os\n\nimport luigi\nfrom luigi import postgres\nfrom luigi.contrib import rdbms\nfrom luigi.s3 import S3PathTask, S3Target\n\nlogger = logging.getLogger('luigi-interface')\n\n\ntry:\n    import psycopg2\n    import psycopg2.errorcodes\nexcept ImportError:\n    logger.warning(\"Loading postgres module without psycopg2 installed. \"\n                   \"Will crash at runtime if postgres functionality is used.\")\n\n\nclass RedshiftTarget(postgres.PostgresTarget):\n    \"\"\"\n    Target for a resource in Redshift.\n\n    Redshift is similar to postgres with a few adjustments\n    required by redshift.\n    \"\"\"\n    marker_table = luigi.configuration.get_config().get(\n        'redshift',\n        'marker-table',\n        'table_updates')\n\n    use_db_timestamps = False\n\n\nclass S3CopyToTable(rdbms.CopyToTable):\n    \"\"\"\n    Template task for inserting a data set into Redshift from s3.\n\n    Usage:\n\n    * Subclass and override the required attributes:\n\n      * `host`,\n      * `database`,\n      * `user`,\n      * `password`,\n      * `table`,\n      * `columns`,\n      * `aws_access_key_id`,\n      * `aws_secret_access_key`,\n      * `s3_load_path`.\n    \"\"\"\n\n    @abc.abstractmethod\n    def s3_load_path(self):\n        \"\"\"\n        Override to return the load path.\n        \"\"\"\n        return None\n\n    @abc.abstractproperty\n    def aws_access_key_id(self):\n        \"\"\"\n        Override to return the key id.\n        \"\"\"\n        return None\n\n    @abc.abstractproperty\n    def aws_secret_access_key(self):\n        \"\"\"\n        Override to return the secret access key.\n        \"\"\"\n        return None\n\n    @property\n    def aws_session_token(self):\n        \"\"\"\n        Override to return the session token.\n        \"\"\"\n        return None\n\n    @abc.abstractproperty\n    def copy_options(self):\n        \"\"\"\n        Add extra copy options, for example:\n\n        * TIMEFORMAT 'auto'\n        * IGNOREHEADER 1\n        * TRUNCATECOLUMNS\n        * IGNOREBLANKLINES\n        * DELIMITER '\\t'\n        \"\"\"\n        return ''\n\n    @property\n    def prune_table(self):\n        \"\"\"\n        Override to set equal to the name of the table which is to be pruned.\n        Intended to be used in conjunction with prune_column and prune_date\n        i.e. copy to temp table, prune production table to prune_column with a date greater than prune_date, then insert into production table from temp table\n        \"\"\"\n        return None\n\n    @property\n    def prune_column(self):\n        \"\"\"\n        Override to set equal to the column of the prune_table which is to be compared\n        Intended to be used in conjunction with prune_table and prune_date\n        i.e. copy to temp table, prune production table to prune_column with a date greater than prune_date, then insert into production table from temp table\n        \"\"\"\n        return None\n\n    @property\n    def prune_date(self):\n        \"\"\"\n        Override to set equal to the date by which prune_column is to be compared\n        Intended to be used in conjunction with prune_table and prune_column\n        i.e. copy to temp table, prune production table to prune_column with a date greater than prune_date, then insert into production table from temp table\n        \"\"\"\n        return None\n\n    @property\n    def table_attributes(self):\n        \"\"\"\n        Add extra table attributes, for example:\n\n        DISTSTYLE KEY\n        DISTKEY (MY_FIELD)\n        SORTKEY (MY_FIELD_2, MY_FIELD_3)\n        \"\"\"\n        return ''\n\n    @property\n    def do_truncate_table(self):\n        \"\"\"\n        Return True if table should be truncated before copying new data in.\n        \"\"\"\n        return False\n\n    def do_prune(self):\n        \"\"\"\n        Return True if prune_table, prune_column, and prune_date are implemented.\n        If only a subset of prune variables are override, an exception is raised to remind the user to implement all or none.\n        Prune (data newer than prune_date deleted) before copying new data in.\n        \"\"\"\n        if self.prune_table and self.prune_column and self.prune_date:\n            return True\n        elif self.prune_table or self.prune_column or self.prune_date:\n            raise Exception('override zero or all prune variables')\n        else:\n            return False\n\n    @property\n    def table_type(self):\n        \"\"\"\n        Return table type (i.e. 'temp').\n        \"\"\"\n        return ''\n\n    @property\n    def queries(self):\n        \"\"\"\n        Override to return a list of queries to be executed in order.\n        \"\"\"\n        return []\n\n    def truncate_table(self, connection):\n        query = \"truncate %s\" % self.table\n        cursor = connection.cursor()\n        try:\n            cursor.execute(query)\n        finally:\n            cursor.close()\n\n    def prune(self, connection):\n        query = \"delete from %s where %s >= %s\" % (self.prune_table, self.prune_column, self.prune_date)\n        cursor = connection.cursor()\n        try:\n            cursor.execute(query)\n        finally:\n            cursor.close()\n\n    def create_table(self, connection):\n        \"\"\"\n        Override to provide code for creating the target table.\n\n        By default it will be created using types (optionally)\n        specified in columns.\n\n        If overridden, use the provided connection object for\n        setting up the table in order to create the table and\n        insert data using the same transaction.\n        \"\"\"\n        if len(self.columns[0]) == 1:\n            # only names of columns specified, no types\n            raise NotImplementedError(\"create_table() not implemented \"\n                                      \"for %r and columns types not \"\n                                      \"specified\" % self.table)\n        elif len(self.columns[0]) == 2:\n            # if columns is specified as (name, type) tuples\n            coldefs = ','.join(\n                '{name} {type}'.format(\n                    name=name,\n                    type=type) for name, type in self.columns\n            )\n            query = (\"CREATE {type} TABLE \"\n                     \"{table} ({coldefs}) \"\n                     \"{table_attributes}\").format(\n                type=self.table_type,\n                table=self.table,\n                coldefs=coldefs,\n                table_attributes=self.table_attributes)\n\n            connection.cursor().execute(query)\n        else:\n            raise ValueError(\"create_table() found no columns for %r\"\n                             % self.table)\n\n    def run(self):\n        \"\"\"\n        If the target table doesn't exist, self.create_table\n        will be called to attempt to create the table.\n        \"\"\"\n        if not (self.table):\n            raise Exception(\"table need to be specified\")\n\n        path = self.s3_load_path()\n        output = self.output()\n        connection = output.connect()\n        cursor = connection.cursor()\n\n        self.init_copy(connection)\n        self.copy(cursor, path)\n        self.post_copy(cursor)\n\n        # update marker table\n        output.touch(connection)\n        connection.commit()\n\n        # commit and clean up\n        connection.close()\n\n    def copy(self, cursor, f):\n        \"\"\"\n        Defines copying from s3 into redshift.\n        \"\"\"\n        # if session token is set, create token string\n        if self.aws_session_token:\n            token = ';token=%s' % self.aws_session_token\n        # otherwise, leave token string empty\n        else:\n            token = ''\n\n        logger.info(\"Inserting file: %s\", f)\n        cursor.execute(\"\"\"\n         COPY %s from '%s'\n         CREDENTIALS 'aws_access_key_id=%s;aws_secret_access_key=%s%s'\n         %s\n         ;\"\"\" % (self.table, f, self.aws_access_key_id,\n                 self.aws_secret_access_key, token,\n                 self.copy_options))\n\n    def output(self):\n        \"\"\"\n        Returns a RedshiftTarget representing the inserted dataset.\n\n        Normally you don't override this.\n        \"\"\"\n        return RedshiftTarget(\n            host=self.host,\n            database=self.database,\n            user=self.user,\n            password=self.password,\n            table=self.table,\n            update_id=self.update_id)\n\n    def does_table_exist(self, connection):\n        \"\"\"\n        Determine whether the table already exists.\n        \"\"\"\n\n        if '.' in self.table:\n            query = (\"select 1 as table_exists \"\n                     \"from information_schema.tables \"\n                     \"where table_schema = %s and table_name = %s limit 1\")\n        else:\n            query = (\"select 1 as table_exists \"\n                     \"from pg_table_def \"\n                     \"where tablename = %s limit 1\")\n        cursor = connection.cursor()\n        try:\n            cursor.execute(query, tuple(self.table.split('.')))\n            result = cursor.fetchone()\n            return bool(result)\n        finally:\n            cursor.close()\n\n    def init_copy(self, connection):\n        \"\"\"\n        Perform pre-copy sql - such as creating table, truncating, or removing data older than x.\n        \"\"\"\n        if not self.does_table_exist(connection):\n            logger.info(\"Creating table %s\", self.table)\n            connection.reset()\n            self.create_table(connection)\n\n        if self.do_truncate_table:\n            logger.info(\"Truncating table %s\", self.table)\n            self.truncate_table(connection)\n\n        if self.do_prune():\n            logger.info(\"Removing %s older than %s from %s\", self.prune_column, self.prune_date, self.prune_table)\n            self.prune(connection)\n\n    def post_copy(self, cursor):\n        \"\"\"\n        Performs post-copy sql - such as cleansing data, inserting into production table (if copied to temp table), etc.\n        \"\"\"\n        logger.info('Executing post copy queries')\n        for query in self.queries:\n            cursor.execute(query)\n\n\nclass S3CopyJSONToTable(S3CopyToTable):\n    \"\"\"\n    Template task for inserting a JSON data set into Redshift from s3.\n\n    Usage:\n\n        * Subclass and override the required attributes:\n\n            * `host`,\n            * `database`,\n            * `user`,\n            * `password`,\n            * `table`,\n            * `columns`,\n            * `aws_access_key_id`,\n            * `aws_secret_access_key`,\n            * `s3_load_path`,\n            * `jsonpath`,\n            * `copy_json_options`.\n    \"\"\"\n\n    @abc.abstractproperty\n    def jsonpath(self):\n        \"\"\"\n        Override the jsonpath schema location for the table.\n        \"\"\"\n        return ''\n\n    @abc.abstractproperty\n    def copy_json_options(self):\n        \"\"\"\n        Add extra copy options, for example:\n\n        * GZIP\n        * LZOP\n        \"\"\"\n        return ''\n\n    def copy(self, cursor, f):\n        \"\"\"\n        Defines copying JSON from s3 into redshift.\n        \"\"\"\n        # if session token is set, create token string\n        if self.aws_session_token:\n            token = ';token=%s' % self.aws_session_token\n        # otherwise, leave token string empty\n        else:\n            token = ''\n\n        logger.info(\"Inserting file: %s\", f)\n        cursor.execute(\"\"\"\n         COPY %s from '%s'\n         CREDENTIALS 'aws_access_key_id=%s;aws_secret_access_key=%s%s'\n         JSON AS '%s' %s\n         %s\n         ;\"\"\" % (self.table, f, self.aws_access_key_id,\n                 self.aws_secret_access_key, token,\n                 self.jsonpath, self.copy_json_options, self.copy_options))\n\n\nclass RedshiftManifestTask(S3PathTask):\n    \"\"\"\n    Generic task to generate a manifest file that can be used\n    in S3CopyToTable in order to copy multiple files from your\n    s3 folder into a redshift table at once.\n\n    For full description on how to use the manifest file see\n    http://docs.aws.amazon.com/redshift/latest/dg/loading-data-files-using-manifest.html\n\n    Usage:\n\n        * requires parameters\n            * path - s3 path to the generated manifest file, including the\n                     name of the generated file\n                     to be copied into a redshift table\n            * folder_paths - s3 paths to the folders containing files you wish to be copied\n\n    Output:\n\n        * generated manifest file\n    \"\"\"\n\n    # should be over ridden to point to a variety\n    # of folders you wish to copy from\n    folder_paths = luigi.Parameter()\n    text_target = True\n\n    def run(self):\n        entries = []\n        for folder_path in self.folder_paths:\n            s3 = S3Target(folder_path)\n            client = s3.fs\n            for file_name in client.list(s3.path):\n                entries.append({\n                    'url': '%s/%s' % (folder_path, file_name),\n                    'mandatory': True\n                })\n        manifest = {'entries': entries}\n        target = self.output().open('w')\n        dump = json.dumps(manifest)\n        if not self.text_target:\n            dump = dump.encode('utf8')\n        target.write(dump)\n        target.close()\n\n\nclass KillOpenRedshiftSessions(luigi.Task):\n    \"\"\"\n    An task for killing any open Redshift sessions\n    in a given database. This is necessary to prevent open user sessions\n    with transactions against the table from blocking drop or truncate\n    table commands.\n\n    Usage:\n\n    Subclass and override the required `host`, `database`,\n    `user`, and `password` attributes.\n    \"\"\"\n\n    # time in seconds to wait before\n    # reconnecting to Redshift if our session is killed too.\n    # 30 seconds is usually fine; 60 is conservative\n    connection_reset_wait_seconds = luigi.IntParameter(default=60)\n\n    @abc.abstractproperty\n    def host(self):\n        return None\n\n    @abc.abstractproperty\n    def database(self):\n        return None\n\n    @abc.abstractproperty\n    def user(self):\n        return None\n\n    @abc.abstractproperty\n    def password(self):\n        return None\n\n    @property\n    def update_id(self):\n        \"\"\"\n        This update id will be a unique identifier\n        for this insert on this table.\n        \"\"\"\n        return self.task_id\n\n    def output(self):\n        \"\"\"\n        Returns a RedshiftTarget representing the inserted dataset.\n\n        Normally you don't override this.\n        \"\"\"\n        # uses class name as a meta-table\n        return RedshiftTarget(\n            host=self.host,\n            database=self.database,\n            user=self.user,\n            password=self.password,\n            table=self.__class__.__name__,\n            update_id=self.update_id)\n\n    def run(self):\n        \"\"\"\n        Kill any open Redshift sessions for the given database.\n        \"\"\"\n        connection = self.output().connect()\n        # kill any sessions other than ours and\n        # internal Redshift sessions (rdsdb)\n        query = (\"select pg_terminate_backend(process) \"\n                 \"from STV_SESSIONS \"\n                 \"where db_name=%s \"\n                 \"and user_name != 'rdsdb' \"\n                 \"and process != pg_backend_pid()\")\n        cursor = connection.cursor()\n        logger.info('Killing all open Redshift sessions for database: %s', self.database)\n        try:\n            cursor.execute(query, (self.database,))\n            cursor.close()\n            connection.commit()\n        except psycopg2.DatabaseError as e:\n            if e.message and 'EOF' in e.message:\n                # sometimes this operation kills the current session.\n                # rebuild the connection. Need to pause for 30-60 seconds\n                # before Redshift will allow us back in.\n                connection.close()\n                logger.info('Pausing %s seconds for Redshift to reset connection', self.connection_reset_wait_seconds)\n                time.sleep(self.connection_reset_wait_seconds)\n                logger.info('Reconnecting to Redshift')\n                connection = self.output().connect()\n            else:\n                raise\n\n        try:\n            self.output().touch(connection)\n            connection.commit()\n        finally:\n            connection.close()\n\n        logger.info('Done killing all open Redshift sessions for database: %s', self.database)\n\n\nclass RedshiftQuery(postgres.PostgresQuery):\n    \"\"\"\n    Template task for querying an Amazon Redshift database\n\n    Usage:\n    Subclass and override the required `host`, `database`, `user`, `password`, `table`, and `query` attributes.\n\n    Override the `run` method if your use case requires some action with the query result.\n\n    Task instances require a dynamic `update_id`, e.g. via parameter(s), otherwise the query will only execute once\n\n    To customize the query signature as recorded in the database marker table, override the `update_id` property.\n    \"\"\"\n\n    def output(self):\n        \"\"\"\n        Returns a RedshiftTarget representing the executed query.\n\n        Normally you don't override this.\n        \"\"\"\n        return RedshiftTarget(\n            host=self.host,\n            database=self.database,\n            user=self.user,\n            password=self.password,\n            table=self.table,\n            update_id=self.update_id\n        )\n\n\nclass RedshiftUnloadTask(postgres.PostgresQuery):\n    \"\"\"\n    Template task for running UNLOAD on an Amazon Redshift database\n\n    Usage:\n    Subclass and override the required `host`, `database`, `user`, `password`, `table`, and `query` attributes.\n    Override the `run` method if your use case requires some action with the query result.\n    Task instances require a dynamic `update_id`, e.g. via parameter(s), otherwise the query will only execute once\n    To customize the query signature as recorded in the database marker table, override the `update_id` property.\n    \"\"\"\n\n    @abc.abstractproperty\n    def aws_access_key_id(self):\n        \"\"\"\n        Override to return the key id.\n        \"\"\"\n        return None\n\n    @abc.abstractproperty\n    def aws_secret_access_key(self):\n        \"\"\"\n        Override to return the secret access key.\n        \"\"\"\n        return None\n\n    @property\n    def s3_unload_path(self):\n        \"\"\"\n        Override to return the load path.\n        \"\"\"\n        return ''\n\n    @property\n    def unload_options(self):\n        \"\"\"\n        Add extra or override default unload options:\n        \"\"\"\n        return \"DELIMITER '|' ADDQUOTES GZIP ALLOWOVERWRITE PARALLEL ON\"\n\n    @property\n    def unload_query(self):\n        \"\"\"\n        Default UNLOAD command\n        \"\"\"\n        return (\"UNLOAD ( '{query}' ) TO '{s3_unload_path}' \"\n                \"credentials 'aws_access_key_id={s3_access_key};aws_secret_access_key={s3_security_key}' \"\n                \"{unload_options};\")\n\n    def run(self):\n        connection = self.output().connect()\n        cursor = connection.cursor()\n\n        # Retrieve AWS s3 credentials\n        config = luigi.configuration.get_config()\n        if self.aws_access_key_id is None or self.aws_secret_access_key is None:\n            self.aws_access_key_id = config.get('s3', 'aws_access_key_id')\n            self.aws_secret_access_key = config.get('s3', 'aws_secret_access_key')\n        # Optionally we can access env variables to get the keys\n        if self.aws_access_key_id is None or self.aws_access_key_id.strip() == '' \\\n                or self.aws_secret_access_key is None or self.aws_secret_access_key.strip() == '':\n            self.aws_access_key_id = os.environ['AWS_ACCESS_KEY_ID']\n            self.aws_secret_access_key = os.environ['AWS_SECRET_ACCESS_KEY']\n\n        unload_query = self.unload_query.format(\n            query=self.query().replace(\"'\", r\"\\'\"),\n            s3_unload_path=self.s3_unload_path,\n            unload_options=self.unload_options,\n            s3_access_key=self.aws_access_key_id,\n            s3_security_key=self.aws_secret_access_key)\n\n        logger.info('Executing unload query from task: {name}'.format(name=self.__class__))\n        try:\n            cursor = connection.cursor()\n            cursor.execute(unload_query)\n            logger.info(cursor.statusmessage)\n        except:\n            raise\n\n        # Update marker table\n        self.output().touch(connection)\n        # commit and close connection\n        connection.commit()\n        connection.close()\n\n    def output(self):\n        \"\"\"\n        Returns a RedshiftTarget representing the executed query.\n\n        Normally you don't override this.\n        \"\"\"\n        return RedshiftTarget(\n            host=self.host,\n            database=self.database,\n            user=self.user,\n            password=self.password,\n            table=self.table,\n            update_id=self.update_id\n        )\n",
          "file_after": "# -*- coding: utf-8 -*-\n#\n# Copyright 2012-2015 Spotify AB\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport abc\nimport json\nimport logging\nimport time\nimport os\n\nimport luigi\nfrom luigi import postgres\nfrom luigi.contrib import rdbms\nfrom luigi.s3 import S3PathTask, S3Target\n\nlogger = logging.getLogger('luigi-interface')\n\n\ntry:\n    import psycopg2\n    import psycopg2.errorcodes\nexcept ImportError:\n    logger.warning(\"Loading postgres module without psycopg2 installed. \"\n                   \"Will crash at runtime if postgres functionality is used.\")\n\n\nclass RedshiftTarget(postgres.PostgresTarget):\n    \"\"\"\n    Target for a resource in Redshift.\n\n    Redshift is similar to postgres with a few adjustments\n    required by redshift.\n    \"\"\"\n    marker_table = luigi.configuration.get_config().get(\n        'redshift',\n        'marker-table',\n        'table_updates')\n\n    use_db_timestamps = False\n\n\nclass S3CopyToTable(rdbms.CopyToTable):\n    \"\"\"\n    Template task for inserting a data set into Redshift from s3.\n\n    Usage:\n\n    * Subclass and override the required attributes:\n\n      * `host`,\n      * `database`,\n      * `user`,\n      * `password`,\n      * `table`,\n      * `columns`,\n      * `aws_access_key_id`,\n      * `aws_secret_access_key`,\n      * `s3_load_path`.\n    \"\"\"\n\n    @abc.abstractmethod\n    def s3_load_path(self):\n        \"\"\"\n        Override to return the load path.\n        \"\"\"\n        return None\n\n    @property\n    def aws_access_key_id(self):\n        \"\"\"\n        Override to return the key id.\n        \"\"\"\n        return None\n\n    @property\n    def aws_secret_access_key(self):\n        \"\"\"\n        Override to return the secret access key.\n        \"\"\"\n        return None\n\n    @property\n    def aws_account_id(self):\n        \"\"\"\n        Override to return the account id.\n        \"\"\"\n        return None\n\n    @property\n    def aws_arn_role_name(self):\n        \"\"\"\n        Override to return the arn role name.\n        \"\"\"\n        return None\n\n    @property\n    def aws_session_token(self):\n        \"\"\"\n        Override to return the session token.\n        \"\"\"\n        return None\n\n    @abc.abstractproperty\n    def copy_options(self):\n        \"\"\"\n        Add extra copy options, for example:\n\n        * TIMEFORMAT 'auto'\n        * IGNOREHEADER 1\n        * TRUNCATECOLUMNS\n        * IGNOREBLANKLINES\n        * DELIMITER '\\t'\n        \"\"\"\n        return ''\n\n    @property\n    def prune_table(self):\n        \"\"\"\n        Override to set equal to the name of the table which is to be pruned.\n        Intended to be used in conjunction with prune_column and prune_date\n        i.e. copy to temp table, prune production table to prune_column with a date greater than prune_date, then insert into production table from temp table\n        \"\"\"\n        return None\n\n    @property\n    def prune_column(self):\n        \"\"\"\n        Override to set equal to the column of the prune_table which is to be compared\n        Intended to be used in conjunction with prune_table and prune_date\n        i.e. copy to temp table, prune production table to prune_column with a date greater than prune_date, then insert into production table from temp table\n        \"\"\"\n        return None\n\n    @property\n    def prune_date(self):\n        \"\"\"\n        Override to set equal to the date by which prune_column is to be compared\n        Intended to be used in conjunction with prune_table and prune_column\n        i.e. copy to temp table, prune production table to prune_column with a date greater than prune_date, then insert into production table from temp table\n        \"\"\"\n        return None\n\n    @property\n    def table_attributes(self):\n        \"\"\"\n        Add extra table attributes, for example:\n\n        DISTSTYLE KEY\n        DISTKEY (MY_FIELD)\n        SORTKEY (MY_FIELD_2, MY_FIELD_3)\n        \"\"\"\n        return ''\n\n    @property\n    def do_truncate_table(self):\n        \"\"\"\n        Return True if table should be truncated before copying new data in.\n        \"\"\"\n        return False\n\n    def do_prune(self):\n        \"\"\"\n        Return True if prune_table, prune_column, and prune_date are implemented.\n        If only a subset of prune variables are override, an exception is raised to remind the user to implement all or none.\n        Prune (data newer than prune_date deleted) before copying new data in.\n        \"\"\"\n        if self.prune_table and self.prune_column and self.prune_date:\n            return True\n        elif self.prune_table or self.prune_column or self.prune_date:\n            raise Exception('override zero or all prune variables')\n        else:\n            return False\n\n    @property\n    def table_type(self):\n        \"\"\"\n        Return table type (i.e. 'temp').\n        \"\"\"\n        return ''\n\n    @property\n    def queries(self):\n        \"\"\"\n        Override to return a list of queries to be executed in order.\n        \"\"\"\n        return []\n\n    def truncate_table(self, connection):\n        query = \"truncate %s\" % self.table\n        cursor = connection.cursor()\n        try:\n            cursor.execute(query)\n        finally:\n            cursor.close()\n\n    def prune(self, connection):\n        query = \"delete from %s where %s >= %s\" % (self.prune_table, self.prune_column, self.prune_date)\n        cursor = connection.cursor()\n        try:\n            cursor.execute(query)\n        finally:\n            cursor.close()\n\n    def create_table(self, connection):\n        \"\"\"\n        Override to provide code for creating the target table.\n\n        By default it will be created using types (optionally)\n        specified in columns.\n\n        If overridden, use the provided connection object for\n        setting up the table in order to create the table and\n        insert data using the same transaction.\n        \"\"\"\n        if len(self.columns[0]) == 1:\n            # only names of columns specified, no types\n            raise NotImplementedError(\"create_table() not implemented \"\n                                      \"for %r and columns types not \"\n                                      \"specified\" % self.table)\n        elif len(self.columns[0]) == 2:\n            # if columns is specified as (name, type) tuples\n            coldefs = ','.join(\n                '{name} {type}'.format(\n                    name=name,\n                    type=type) for name, type in self.columns\n            )\n            query = (\"CREATE {type} TABLE \"\n                     \"{table} ({coldefs}) \"\n                     \"{table_attributes}\").format(\n                type=self.table_type,\n                table=self.table,\n                coldefs=coldefs,\n                table_attributes=self.table_attributes)\n\n            connection.cursor().execute(query)\n        else:\n            raise ValueError(\"create_table() found no columns for %r\"\n                             % self.table)\n\n    def run(self):\n        \"\"\"\n        If the target table doesn't exist, self.create_table\n        will be called to attempt to create the table.\n        \"\"\"\n        if not (self.table):\n            raise Exception(\"table need to be specified\")\n\n        path = self.s3_load_path()\n        output = self.output()\n        connection = output.connect()\n        cursor = connection.cursor()\n\n        self.init_copy(connection)\n        self.copy(cursor, path)\n        self.post_copy(cursor)\n\n        # update marker table\n        output.touch(connection)\n        connection.commit()\n\n        # commit and clean up\n        connection.close()\n\n    def copy(self, cursor, f):\n        \"\"\"\n        Defines copying from s3 into redshift.\n\n        If both key-based and role-based credentials are provided, role-based will be used.\n        \"\"\"\n        # format the credentials string dependent upon which type of credentials were provided\n        if self.aws_account_id and self.aws_arn_role_name:\n            cred_str = 'aws_iam_role=arn:aws:iam::{id}:role/{role}'.format(\n                id=self.aws_account_id,\n                role=self.aws_arn_role_name\n            )\n        elif self.aws_access_key_id and self.aws_secret_access_key:\n            cred_str = 'aws_access_key_id={key};aws_secret_key={secret}{opt}'.format(\n                key=self.aws_access_key_id,\n                secret=self.aws_secret_access_key,\n                opt=';token={}'.format(self.aws_session_token) if self.aws_session_token else ''\n            )\n        else:\n            raise NotImplementedError(\"Missing Credentials. \"\n                                      \"Override one of the following pairs of auth-args: \"\n                                      \"'aws_access_key_id' AND 'aws_secret_access_key' OR \"\n                                      \"'aws_account_id' AND 'aws_arn_role_name'\")\n\n        logger.info(\"Inserting file: %s\", f)\n        cursor.execute(\"\"\"\n         COPY {table} from '{source}'\n         CREDENTIALS '{creds}'\n         {options}\n         ;\"\"\".format(\n            table=self.table,\n            source=f,\n            creds=cred_str,\n            options=self.copy_options)\n        )\n\n    def output(self):\n        \"\"\"\n        Returns a RedshiftTarget representing the inserted dataset.\n\n        Normally you don't override this.\n        \"\"\"\n        return RedshiftTarget(\n            host=self.host,\n            database=self.database,\n            user=self.user,\n            password=self.password,\n            table=self.table,\n            update_id=self.update_id)\n\n    def does_table_exist(self, connection):\n        \"\"\"\n        Determine whether the table already exists.\n        \"\"\"\n\n        if '.' in self.table:\n            query = (\"select 1 as table_exists \"\n                     \"from information_schema.tables \"\n                     \"where table_schema = %s and table_name = %s limit 1\")\n        else:\n            query = (\"select 1 as table_exists \"\n                     \"from pg_table_def \"\n                     \"where tablename = %s limit 1\")\n        cursor = connection.cursor()\n        try:\n            cursor.execute(query, tuple(self.table.split('.')))\n            result = cursor.fetchone()\n            return bool(result)\n        finally:\n            cursor.close()\n\n    def init_copy(self, connection):\n        \"\"\"\n        Perform pre-copy sql - such as creating table, truncating, or removing data older than x.\n        \"\"\"\n        if not self.does_table_exist(connection):\n            logger.info(\"Creating table %s\", self.table)\n            connection.reset()\n            self.create_table(connection)\n\n        if self.do_truncate_table:\n            logger.info(\"Truncating table %s\", self.table)\n            self.truncate_table(connection)\n\n        if self.do_prune():\n            logger.info(\"Removing %s older than %s from %s\", self.prune_column, self.prune_date, self.prune_table)\n            self.prune(connection)\n\n    def post_copy(self, cursor):\n        \"\"\"\n        Performs post-copy sql - such as cleansing data, inserting into production table (if copied to temp table), etc.\n        \"\"\"\n        logger.info('Executing post copy queries')\n        for query in self.queries:\n            cursor.execute(query)\n\n\nclass S3CopyJSONToTable(S3CopyToTable):\n    \"\"\"\n    Template task for inserting a JSON data set into Redshift from s3.\n\n    Usage:\n\n        * Subclass and override the required attributes:\n\n            * `host`,\n            * `database`,\n            * `user`,\n            * `password`,\n            * `table`,\n            * `columns`,\n            * `aws_access_key_id`,\n            * `aws_secret_access_key`,\n            * `s3_load_path`,\n            * `jsonpath`,\n            * `copy_json_options`.\n    \"\"\"\n\n    @abc.abstractproperty\n    def jsonpath(self):\n        \"\"\"\n        Override the jsonpath schema location for the table.\n        \"\"\"\n        return ''\n\n    @abc.abstractproperty\n    def copy_json_options(self):\n        \"\"\"\n        Add extra copy options, for example:\n\n        * GZIP\n        * LZOP\n        \"\"\"\n        return ''\n\n    def copy(self, cursor, f):\n        \"\"\"\n        Defines copying JSON from s3 into redshift.\n        \"\"\"\n        # if session token is set, create token string\n        if self.aws_session_token:\n            token = ';token=%s' % self.aws_session_token\n        # otherwise, leave token string empty\n        else:\n            token = ''\n\n        logger.info(\"Inserting file: %s\", f)\n        cursor.execute(\"\"\"\n         COPY %s from '%s'\n         CREDENTIALS 'aws_access_key_id=%s;aws_secret_access_key=%s%s'\n         JSON AS '%s' %s\n         %s\n         ;\"\"\" % (self.table, f, self.aws_access_key_id,\n                 self.aws_secret_access_key, token,\n                 self.jsonpath, self.copy_json_options, self.copy_options))\n\n\nclass RedshiftManifestTask(S3PathTask):\n    \"\"\"\n    Generic task to generate a manifest file that can be used\n    in S3CopyToTable in order to copy multiple files from your\n    s3 folder into a redshift table at once.\n\n    For full description on how to use the manifest file see\n    http://docs.aws.amazon.com/redshift/latest/dg/loading-data-files-using-manifest.html\n\n    Usage:\n\n        * requires parameters\n            * path - s3 path to the generated manifest file, including the\n                     name of the generated file\n                     to be copied into a redshift table\n            * folder_paths - s3 paths to the folders containing files you wish to be copied\n\n    Output:\n\n        * generated manifest file\n    \"\"\"\n\n    # should be over ridden to point to a variety\n    # of folders you wish to copy from\n    folder_paths = luigi.Parameter()\n    text_target = True\n\n    def run(self):\n        entries = []\n        for folder_path in self.folder_paths:\n            s3 = S3Target(folder_path)\n            client = s3.fs\n            for file_name in client.list(s3.path):\n                entries.append({\n                    'url': '%s/%s' % (folder_path, file_name),\n                    'mandatory': True\n                })\n        manifest = {'entries': entries}\n        target = self.output().open('w')\n        dump = json.dumps(manifest)\n        if not self.text_target:\n            dump = dump.encode('utf8')\n        target.write(dump)\n        target.close()\n\n\nclass KillOpenRedshiftSessions(luigi.Task):\n    \"\"\"\n    An task for killing any open Redshift sessions\n    in a given database. This is necessary to prevent open user sessions\n    with transactions against the table from blocking drop or truncate\n    table commands.\n\n    Usage:\n\n    Subclass and override the required `host`, `database`,\n    `user`, and `password` attributes.\n    \"\"\"\n\n    # time in seconds to wait before\n    # reconnecting to Redshift if our session is killed too.\n    # 30 seconds is usually fine; 60 is conservative\n    connection_reset_wait_seconds = luigi.IntParameter(default=60)\n\n    @abc.abstractproperty\n    def host(self):\n        return None\n\n    @abc.abstractproperty\n    def database(self):\n        return None\n\n    @abc.abstractproperty\n    def user(self):\n        return None\n\n    @abc.abstractproperty\n    def password(self):\n        return None\n\n    @property\n    def update_id(self):\n        \"\"\"\n        This update id will be a unique identifier\n        for this insert on this table.\n        \"\"\"\n        return self.task_id\n\n    def output(self):\n        \"\"\"\n        Returns a RedshiftTarget representing the inserted dataset.\n\n        Normally you don't override this.\n        \"\"\"\n        # uses class name as a meta-table\n        return RedshiftTarget(\n            host=self.host,\n            database=self.database,\n            user=self.user,\n            password=self.password,\n            table=self.__class__.__name__,\n            update_id=self.update_id)\n\n    def run(self):\n        \"\"\"\n        Kill any open Redshift sessions for the given database.\n        \"\"\"\n        connection = self.output().connect()\n        # kill any sessions other than ours and\n        # internal Redshift sessions (rdsdb)\n        query = (\"select pg_terminate_backend(process) \"\n                 \"from STV_SESSIONS \"\n                 \"where db_name=%s \"\n                 \"and user_name != 'rdsdb' \"\n                 \"and process != pg_backend_pid()\")\n        cursor = connection.cursor()\n        logger.info('Killing all open Redshift sessions for database: %s', self.database)\n        try:\n            cursor.execute(query, (self.database,))\n            cursor.close()\n            connection.commit()\n        except psycopg2.DatabaseError as e:\n            if e.message and 'EOF' in e.message:\n                # sometimes this operation kills the current session.\n                # rebuild the connection. Need to pause for 30-60 seconds\n                # before Redshift will allow us back in.\n                connection.close()\n                logger.info('Pausing %s seconds for Redshift to reset connection', self.connection_reset_wait_seconds)\n                time.sleep(self.connection_reset_wait_seconds)\n                logger.info('Reconnecting to Redshift')\n                connection = self.output().connect()\n            else:\n                raise\n\n        try:\n            self.output().touch(connection)\n            connection.commit()\n        finally:\n            connection.close()\n\n        logger.info('Done killing all open Redshift sessions for database: %s', self.database)\n\n\nclass RedshiftQuery(postgres.PostgresQuery):\n    \"\"\"\n    Template task for querying an Amazon Redshift database\n\n    Usage:\n    Subclass and override the required `host`, `database`, `user`, `password`, `table`, and `query` attributes.\n\n    Override the `run` method if your use case requires some action with the query result.\n\n    Task instances require a dynamic `update_id`, e.g. via parameter(s), otherwise the query will only execute once\n\n    To customize the query signature as recorded in the database marker table, override the `update_id` property.\n    \"\"\"\n\n    def output(self):\n        \"\"\"\n        Returns a RedshiftTarget representing the executed query.\n\n        Normally you don't override this.\n        \"\"\"\n        return RedshiftTarget(\n            host=self.host,\n            database=self.database,\n            user=self.user,\n            password=self.password,\n            table=self.table,\n            update_id=self.update_id\n        )\n\n\nclass RedshiftUnloadTask(postgres.PostgresQuery):\n    \"\"\"\n    Template task for running UNLOAD on an Amazon Redshift database\n\n    Usage:\n    Subclass and override the required `host`, `database`, `user`, `password`, `table`, and `query` attributes.\n    Override the `run` method if your use case requires some action with the query result.\n    Task instances require a dynamic `update_id`, e.g. via parameter(s), otherwise the query will only execute once\n    To customize the query signature as recorded in the database marker table, override the `update_id` property.\n    \"\"\"\n\n    @abc.abstractproperty\n    def aws_access_key_id(self):\n        \"\"\"\n        Override to return the key id.\n        \"\"\"\n        return None\n\n    @abc.abstractproperty\n    def aws_secret_access_key(self):\n        \"\"\"\n        Override to return the secret access key.\n        \"\"\"\n        return None\n\n    @property\n    def s3_unload_path(self):\n        \"\"\"\n        Override to return the load path.\n        \"\"\"\n        return ''\n\n    @property\n    def unload_options(self):\n        \"\"\"\n        Add extra or override default unload options:\n        \"\"\"\n        return \"DELIMITER '|' ADDQUOTES GZIP ALLOWOVERWRITE PARALLEL ON\"\n\n    @property\n    def unload_query(self):\n        \"\"\"\n        Default UNLOAD command\n        \"\"\"\n        return (\"UNLOAD ( '{query}' ) TO '{s3_unload_path}' \"\n                \"credentials 'aws_access_key_id={s3_access_key};aws_secret_access_key={s3_security_key}' \"\n                \"{unload_options};\")\n\n    def run(self):\n        connection = self.output().connect()\n        cursor = connection.cursor()\n\n        # Retrieve AWS s3 credentials\n        config = luigi.configuration.get_config()\n        if self.aws_access_key_id is None or self.aws_secret_access_key is None:\n            self.aws_access_key_id = config.get('s3', 'aws_access_key_id')\n            self.aws_secret_access_key = config.get('s3', 'aws_secret_access_key')\n        # Optionally we can access env variables to get the keys\n        if self.aws_access_key_id is None or self.aws_access_key_id.strip() == '' \\\n                or self.aws_secret_access_key is None or self.aws_secret_access_key.strip() == '':\n            self.aws_access_key_id = os.environ['AWS_ACCESS_KEY_ID']\n            self.aws_secret_access_key = os.environ['AWS_SECRET_ACCESS_KEY']\n\n        unload_query = self.unload_query.format(\n            query=self.query().replace(\"'\", r\"\\'\"),\n            s3_unload_path=self.s3_unload_path,\n            unload_options=self.unload_options,\n            s3_access_key=self.aws_access_key_id,\n            s3_security_key=self.aws_secret_access_key)\n\n        logger.info('Executing unload query from task: {name}'.format(name=self.__class__))\n        try:\n            cursor = connection.cursor()\n            cursor.execute(unload_query)\n            logger.info(cursor.statusmessage)\n        except:\n            raise\n\n        # Update marker table\n        self.output().touch(connection)\n        # commit and close connection\n        connection.commit()\n        connection.close()\n\n    def output(self):\n        \"\"\"\n        Returns a RedshiftTarget representing the executed query.\n\n        Normally you don't override this.\n        \"\"\"\n        return RedshiftTarget(\n            host=self.host,\n            database=self.database,\n            user=self.user,\n            password=self.password,\n            table=self.table,\n            update_id=self.update_id\n        )\n",
          "file_patch": "@@ -78,20 +78,34 @@ class S3CopyToTable(rdbms.CopyToTable):\n         \"\"\"\n         return None\n \n-    @abc.abstractproperty\n+    @property\n     def aws_access_key_id(self):\n         \"\"\"\n         Override to return the key id.\n         \"\"\"\n         return None\n \n-    @abc.abstractproperty\n+    @property\n     def aws_secret_access_key(self):\n         \"\"\"\n         Override to return the secret access key.\n         \"\"\"\n         return None\n \n+    @property\n+    def aws_account_id(self):\n+        \"\"\"\n+        Override to return the account id.\n+        \"\"\"\n+        return None\n+\n+    @property\n+    def aws_arn_role_name(self):\n+        \"\"\"\n+        Override to return the arn role name.\n+        \"\"\"\n+        return None\n+\n     @property\n     def aws_session_token(self):\n         \"\"\"\n@@ -263,22 +277,38 @@ class S3CopyToTable(rdbms.CopyToTable):\n     def copy(self, cursor, f):\n         \"\"\"\n         Defines copying from s3 into redshift.\n+\n+        If both key-based and role-based credentials are provided, role-based will be used.\n         \"\"\"\n-        # if session token is set, create token string\n-        if self.aws_session_token:\n-            token = ';token=%s' % self.aws_session_token\n-        # otherwise, leave token string empty\n+        # format the credentials string dependent upon which type of credentials were provided\n+        if self.aws_account_id and self.aws_arn_role_name:\n+            cred_str = 'aws_iam_role=arn:aws:iam::{id}:role/{role}'.format(\n+                id=self.aws_account_id,\n+                role=self.aws_arn_role_name\n+            )\n+        elif self.aws_access_key_id and self.aws_secret_access_key:\n+            cred_str = 'aws_access_key_id={key};aws_secret_key={secret}{opt}'.format(\n+                key=self.aws_access_key_id,\n+                secret=self.aws_secret_access_key,\n+                opt=';token={}'.format(self.aws_session_token) if self.aws_session_token else ''\n+            )\n         else:\n-            token = ''\n+            raise NotImplementedError(\"Missing Credentials. \"\n+                                      \"Override one of the following pairs of auth-args: \"\n+                                      \"'aws_access_key_id' AND 'aws_secret_access_key' OR \"\n+                                      \"'aws_account_id' AND 'aws_arn_role_name'\")\n \n         logger.info(\"Inserting file: %s\", f)\n         cursor.execute(\"\"\"\n-         COPY %s from '%s'\n-         CREDENTIALS 'aws_access_key_id=%s;aws_secret_access_key=%s%s'\n-         %s\n-         ;\"\"\" % (self.table, f, self.aws_access_key_id,\n-                 self.aws_secret_access_key, token,\n-                 self.copy_options))\n+         COPY {table} from '{source}'\n+         CREDENTIALS '{creds}'\n+         {options}\n+         ;\"\"\".format(\n+            table=self.table,\n+            source=f,\n+            creds=cred_str,\n+            options=self.copy_options)\n+        )\n \n     def output(self):\n         \"\"\"\n",
          "files_name_in_blame_commit": [
            "redshift_test.py",
            "redshift.py"
          ]
        }
      },
      "f5700be541d1331f476b0b2da1f10f2fd0c6a0e2": {
        "commit": {
          "commit_id": "f5700be541d1331f476b0b2da1f10f2fd0c6a0e2",
          "commit_message": "Added support for redshift copy to temp, prune table (delete some amount of records based on date), init copy, post copy (also added to rdbms.py)\n\nAdded docstrings to prune variables. Updated do_prune() to raise an exception when only a subset of prune variables are implemented (needs to be all or none). Updated docstring accordingly.\n\nAdded decorator @property to prune_*, table_type, and table_attributes.",
          "commit_author": "Dillon Stadther",
          "commit_date": "2015-10-26 14:22:31",
          "commit_parent": "de0421b422ed3047107530d1ce73cd19ad58dbdf"
        },
        "function": {
          "function_name": "copy",
          "function_code_before": "def copy(self, cursor, f):\n    \"\"\"\n        Defines copying from s3 into redshift.\n        \"\"\"\n    cursor.execute(\"\\n         COPY %s from '%s'\\n         CREDENTIALS 'aws_access_key_id=%s;aws_secret_access_key=%s'\\n         %s\\n         ;\" % (self.table, f, self.aws_access_key_id, self.aws_secret_access_key, self.copy_options))",
          "function_code_after": "def copy(self, cursor, f):\n    \"\"\"\n        Defines copying from s3 into redshift.\n        \"\"\"\n    logger.info('Inserting file: %s', f)\n    cursor.execute(\"\\n         COPY %s from '%s'\\n         CREDENTIALS 'aws_access_key_id=%s;aws_secret_access_key=%s'\\n         %s\\n         ;\" % (self.table, f, self.aws_access_key_id, self.aws_secret_access_key, self.copy_options))",
          "function_before_start_line": 190,
          "function_before_end_line": 199,
          "function_after_start_line": 249,
          "function_after_end_line": 259,
          "function_before_token_count": 36,
          "function_after_token_count": 44,
          "functions_name_modified_file": [
            "prune",
            "output",
            "post_copy",
            "user",
            "create_table",
            "s3_load_path",
            "do_prune",
            "do_truncate_table",
            "host",
            "prune_column",
            "queries",
            "prune_date",
            "copy_json_options",
            "truncate_table",
            "aws_secret_access_key",
            "table_type",
            "password",
            "run",
            "prune_table",
            "copy_options",
            "table_attributes",
            "does_table_exist",
            "init_copy",
            "jsonpath",
            "aws_access_key_id",
            "copy",
            "update_id",
            "database"
          ],
          "functions_name_all_files": [
            "prune",
            "output",
            "table",
            "post_copy",
            "user",
            "create_table",
            "s3_load_path",
            "do_prune",
            "do_truncate_table",
            "host",
            "prune_column",
            "prune_date",
            "queries",
            "copy_json_options",
            "truncate_table",
            "aws_secret_access_key",
            "table_type",
            "password",
            "run",
            "prune_table",
            "copy_options",
            "test_s3_copy_to_temp_table",
            "table_attributes",
            "init_copy",
            "does_table_exist",
            "test_s3_copy_to_table",
            "jsonpath",
            "aws_access_key_id",
            "copy",
            "update_id",
            "database"
          ],
          "functions_name_co_evolved_modified_file": [
            "prune_date",
            "prune",
            "post_copy",
            "create_table",
            "do_prune",
            "prune_table",
            "table_type",
            "init_copy",
            "run",
            "prune_column",
            "queries"
          ],
          "functions_name_co_evolved_all_files": [
            "prune_date",
            "prune",
            "post_copy",
            "create_table",
            "do_prune",
            "s3_load_path",
            "prune_table",
            "table_type",
            "init_copy",
            "test_s3_copy_to_temp_table",
            "run",
            "prune_column",
            "queries"
          ]
        },
        "file": {
          "file_name": "redshift.py",
          "file_nloc": 312,
          "file_complexity": 57,
          "file_token_count": 1344,
          "file_before": "# -*- coding: utf-8 -*-\n#\n# Copyright 2012-2015 Spotify AB\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport abc\nimport json\nimport logging\nimport time\n\nimport luigi\nfrom luigi import postgres\nfrom luigi.contrib import rdbms\nfrom luigi.s3 import S3PathTask, S3Target\n\nlogger = logging.getLogger('luigi-interface')\n\n\ntry:\n    import psycopg2\n    import psycopg2.errorcodes\nexcept ImportError:\n    logger.warning(\"Loading postgres module without psycopg2 installed. \"\n                   \"Will crash at runtime if postgres functionality is used.\")\n\n\nclass RedshiftTarget(postgres.PostgresTarget):\n    \"\"\"\n    Target for a resource in Redshift.\n\n    Redshift is similar to postgres with a few adjustments\n    required by redshift.\n    \"\"\"\n    marker_table = luigi.configuration.get_config().get(\n        'redshift',\n        'marker-table',\n        'table_updates')\n\n    use_db_timestamps = False\n\n\nclass S3CopyToTable(rdbms.CopyToTable):\n    \"\"\"\n    Template task for inserting a data set into Redshift from s3.\n\n    Usage:\n\n    * Subclass and override the required attributes:\n      * `host`,\n      * `database`,\n      * `user`,\n      * `password`,\n      * `table`,\n      * `columns`,\n      * `aws_access_key_id`,\n      * `aws_secret_access_key`,\n      * `s3_load_path`.\n    \"\"\"\n\n    @abc.abstractproperty\n    def s3_load_path(self):\n        \"\"\"\n        Override to return the load path.\n        \"\"\"\n        return None\n\n    @abc.abstractproperty\n    def aws_access_key_id(self):\n        \"\"\"\n        Override to return the key id.\n        \"\"\"\n        return None\n\n    @abc.abstractproperty\n    def aws_secret_access_key(self):\n        \"\"\"\n        Override to return the secret access key.\n        \"\"\"\n        return None\n\n    @abc.abstractproperty\n    def copy_options(self):\n        \"\"\"\n        Add extra copy options, for example:\n\n        * TIMEFORMAT 'auto'\n        * IGNOREHEADER 1\n        * TRUNCATECOLUMNS\n        * IGNOREBLANKLINES\n        * DELIMITER '\\t'\n        \"\"\"\n        return ''\n\n    def table_attributes(self):\n        \"\"\"\n        Add extra table attributes, for example:\n\n        DISTSTYLE KEY\n        DISTKEY (MY_FIELD)\n        SORTKEY (MY_FIELD_2, MY_FIELD_3)\n        \"\"\"\n        return ''\n\n    def do_truncate_table(self):\n        \"\"\"\n        Return True if table should be truncated before copying new data in.\n        \"\"\"\n        return False\n\n    def truncate_table(self, connection):\n        query = \"truncate %s\" % self.table\n        cursor = connection.cursor()\n        try:\n            cursor.execute(query)\n        finally:\n            cursor.close()\n\n    def create_table(self, connection):\n        \"\"\"\n        Override to provide code for creating the target table.\n\n        By default it will be created using types (optionally)\n        specified in columns.\n\n        If overridden, use the provided connection object for\n        setting up the table in order to create the table and\n        insert data using the same transaction.\n        \"\"\"\n        if len(self.columns[0]) == 1:\n            # only names of columns specified, no types\n            raise NotImplementedError(\"create_table() not implemented \"\n                                      \"for %r and columns types not \"\n                                      \"specified\" % self.table)\n        elif len(self.columns[0]) == 2:\n            # if columns is specified as (name, type) tuples\n            coldefs = ','.join(\n                '{name} {type}'.format(\n                    name=name,\n                    type=type) for name, type in self.columns\n            )\n            query = (\"CREATE TABLE \"\n                     \"{table} ({coldefs}) \"\n                     \"{table_attributes}\").format(\n                table=self.table,\n                coldefs=coldefs,\n                table_attributes=self.table_attributes())\n            connection.cursor().execute(query)\n\n    def run(self):\n        \"\"\"\n        If the target table doesn't exist, self.create_table\n        will be called to attempt to create the table.\n        \"\"\"\n        if not (self.table):\n            raise Exception(\"table need to be specified\")\n\n        path = self.s3_load_path()\n        connection = self.output().connect()\n        if not self.does_table_exist(connection):\n            # try creating table\n            logger.info(\"Creating table %s\", self.table)\n            connection.reset()\n            self.create_table(connection)\n        elif self.do_truncate_table():\n            logger.info(\"Truncating table %s\", self.table)\n            self.truncate_table(connection)\n\n        logger.info(\"Inserting file: %s\", path)\n        cursor = connection.cursor()\n        self.init_copy(connection)\n        self.copy(cursor, path)\n        self.output().touch(connection)\n        connection.commit()\n\n        # commit and clean up\n        connection.close()\n\n    def copy(self, cursor, f):\n        \"\"\"\n        Defines copying from s3 into redshift.\n        \"\"\"\n        cursor.execute(\"\"\"\n         COPY %s from '%s'\n         CREDENTIALS 'aws_access_key_id=%s;aws_secret_access_key=%s'\n         %s\n         ;\"\"\" % (self.table, f, self.aws_access_key_id,\n                 self.aws_secret_access_key, self.copy_options))\n\n    def output(self):\n        \"\"\"\n        Returns a RedshiftTarget representing the inserted dataset.\n\n        Normally you don't override this.\n        \"\"\"\n        return RedshiftTarget(\n            host=self.host,\n            database=self.database,\n            user=self.user,\n            password=self.password,\n            table=self.table,\n            update_id=self.update_id())\n\n    def does_table_exist(self, connection):\n        \"\"\"\n        Determine whether the table already exists.\n        \"\"\"\n\n        if '.' in self.table:\n            query = (\"select 1 as table_exists \"\n                     \"from information_schema.tables \"\n                     \"where table_schema = %s and table_name = %s limit 1\")\n        else:\n            query = (\"select 1 as table_exists \"\n                     \"from pg_table_def \"\n                     \"where tablename = %s limit 1\")\n        cursor = connection.cursor()\n        try:\n            cursor.execute(query, tuple(self.table.split('.')))\n            result = cursor.fetchone()\n            return bool(result)\n        finally:\n            cursor.close()\n\n\nclass S3CopyJSONToTable(S3CopyToTable):\n    \"\"\"\n    Template task for inserting a JSON data set into Redshift from s3.\n\n    Usage:\n\n        * Subclass and override the required attributes:\n\n            * `host`,\n            * `database`,\n            * `user`,\n            * `password`,\n            * `table`,\n            * `columns`,\n            * `aws_access_key_id`,\n            * `aws_secret_access_key`,\n            * `s3_load_path`,\n            * `jsonpath`,\n            * `copy_json_options`.\n    \"\"\"\n\n    @abc.abstractproperty\n    def jsonpath(self):\n        \"\"\"\n        Override the jsonpath schema location for the table.\n        \"\"\"\n        return ''\n\n    @abc.abstractproperty\n    def copy_json_options(self):\n        \"\"\"\n        Add extra copy options, for example:\n\n        * GZIP\n        * LZOP\n        \"\"\"\n        return ''\n\n    def copy(self, cursor, f):\n        \"\"\"\n        Defines copying JSON from s3 into redshift.\n        \"\"\"\n\n        cursor.execute(\"\"\"\n         COPY %s from '%s'\n         CREDENTIALS 'aws_access_key_id=%s;aws_secret_access_key=%s'\n         JSON AS '%s' %s\n         %s\n         ;\"\"\" % (self.table, f, self.aws_access_key_id,\n                 self.aws_secret_access_key, self.jsonpath,\n                 self.copy_json_options, self.copy_options))\n\n\nclass RedshiftManifestTask(S3PathTask):\n    \"\"\"\n    Generic task to generate a manifest file that can be used\n    in S3CopyToTable in order to copy multiple files from your\n    s3 folder into a redshift table at once.\n\n    For full description on how to use the manifest file see\n    http://docs.aws.amazon.com/redshift/latest/dg/loading-data-files-using-manifest.html\n\n    Usage:\n\n        * requires parameters\n            * path - s3 path to the generated manifest file, including the\n                     name of the generated file\n                     to be copied into a redshift table\n            * folder_paths - s3 paths to the folders containing files you wish to be copied\n\n    Output:\n\n        * generated manifest file\n    \"\"\"\n\n    # should be over ridden to point to a variety\n    # of folders you wish to copy from\n    folder_paths = luigi.Parameter()\n    text_target = True\n\n    def run(self):\n        entries = []\n        for folder_path in self.folder_paths:\n            s3 = S3Target(folder_path)\n            client = s3.fs\n            for file_name in client.list(s3.path):\n                entries.append({\n                    'url': '%s/%s' % (folder_path, file_name),\n                    'mandatory': True\n                })\n        manifest = {'entries': entries}\n        target = self.output().open('w')\n        dump = json.dumps(manifest)\n        if not self.text_target:\n            dump = dump.encode('utf8')\n        target.write(dump)\n        target.close()\n\n\nclass KillOpenRedshiftSessions(luigi.Task):\n    \"\"\"\n    An task for killing any open Redshift sessions\n    in a given database. This is necessary to prevent open user sessions\n    with transactions against the table from blocking drop or truncate\n    table commands.\n\n    Usage:\n\n    Subclass and override the required `host`, `database`,\n    `user`, and `password` attributes.\n    \"\"\"\n\n    # time in seconds to wait before\n    # reconnecting to Redshift if our session is killed too.\n    # 30 seconds is usually fine; 60 is conservative\n    connection_reset_wait_seconds = luigi.IntParameter(default=60)\n\n    @abc.abstractproperty\n    def host(self):\n        return None\n\n    @abc.abstractproperty\n    def database(self):\n        return None\n\n    @abc.abstractproperty\n    def user(self):\n        return None\n\n    @abc.abstractproperty\n    def password(self):\n        return None\n\n    def update_id(self):\n        \"\"\"\n        This update id will be a unique identifier\n        for this insert on this table.\n        \"\"\"\n        return self.task_id\n\n    def output(self):\n        \"\"\"\n        Returns a RedshiftTarget representing the inserted dataset.\n\n        Normally you don't override this.\n        \"\"\"\n        # uses class name as a meta-table\n        return RedshiftTarget(\n            host=self.host,\n            database=self.database,\n            user=self.user,\n            password=self.password,\n            table=self.__class__.__name__,\n            update_id=self.update_id())\n\n    def run(self):\n        \"\"\"\n        Kill any open Redshift sessions for the given database.\n        \"\"\"\n        connection = self.output().connect()\n        # kill any sessions other than ours and\n        # internal Redshift sessions (rdsdb)\n        query = (\"select pg_terminate_backend(process) \"\n                 \"from STV_SESSIONS \"\n                 \"where db_name=%s \"\n                 \"and user_name != 'rdsdb' \"\n                 \"and process != pg_backend_pid()\")\n        cursor = connection.cursor()\n        logger.info('Killing all open Redshift sessions for database: %s', self.database)\n        try:\n            cursor.execute(query, (self.database,))\n            cursor.close()\n            connection.commit()\n        except psycopg2.DatabaseError as e:\n            if e.message and 'EOF' in e.message:\n                # sometimes this operation kills the current session.\n                # rebuild the connection. Need to pause for 30-60 seconds\n                # before Redshift will allow us back in.\n                connection.close()\n                logger.info('Pausing %s seconds for Redshift to reset connection', self.connection_reset_wait_seconds)\n                time.sleep(self.connection_reset_wait_seconds)\n                logger.info('Reconnecting to Redshift')\n                connection = self.output().connect()\n            else:\n                raise\n\n        try:\n            self.output().touch(connection)\n            connection.commit()\n        finally:\n            connection.close()\n\n        logger.info('Done killing all open Redshift sessions for database: %s', self.database)\n",
          "file_after": "# -*- coding: utf-8 -*-\n#\n# Copyright 2012-2015 Spotify AB\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport abc\nimport json\nimport logging\nimport time\n\nimport luigi\nfrom luigi import postgres\nfrom luigi.contrib import rdbms\nfrom luigi.s3 import S3PathTask, S3Target\n\nlogger = logging.getLogger('luigi-interface')\n\n\ntry:\n    import psycopg2\n    import psycopg2.errorcodes\nexcept ImportError:\n    logger.warning(\"Loading postgres module without psycopg2 installed. \"\n                   \"Will crash at runtime if postgres functionality is used.\")\n\n\nclass RedshiftTarget(postgres.PostgresTarget):\n    \"\"\"\n    Target for a resource in Redshift.\n\n    Redshift is similar to postgres with a few adjustments\n    required by redshift.\n    \"\"\"\n    marker_table = luigi.configuration.get_config().get(\n        'redshift',\n        'marker-table',\n        'table_updates')\n\n    use_db_timestamps = False\n\n\nclass S3CopyToTable(rdbms.CopyToTable):\n    \"\"\"\n    Template task for inserting a data set into Redshift from s3.\n\n    Usage:\n\n    * Subclass and override the required attributes:\n      * `host`,\n      * `database`,\n      * `user`,\n      * `password`,\n      * `table`,\n      * `columns`,\n      * `aws_access_key_id`,\n      * `aws_secret_access_key`,\n      * `s3_load_path`.\n    \"\"\"\n\n    @abc.abstractproperty\n    def s3_load_path(self):\n        \"\"\"\n        Override to return the load path.\n        \"\"\"\n        return None\n\n    @abc.abstractproperty\n    def aws_access_key_id(self):\n        \"\"\"\n        Override to return the key id.\n        \"\"\"\n        return None\n\n    @abc.abstractproperty\n    def aws_secret_access_key(self):\n        \"\"\"\n        Override to return the secret access key.\n        \"\"\"\n        return None\n\n    @abc.abstractproperty\n    def copy_options(self):\n        \"\"\"\n        Add extra copy options, for example:\n\n        * TIMEFORMAT 'auto'\n        * IGNOREHEADER 1\n        * TRUNCATECOLUMNS\n        * IGNOREBLANKLINES\n        * DELIMITER '\\t'\n        \"\"\"\n        return ''\n\n    @property    \n    def prune_table(self):\n        \"\"\"\n        Override to set equal to the name of the table which is to be pruned.\n        Intended to be used in conjunction with prune_column and prune_date\n        i.e. copy to temp table, prune production table to prune_column with a date greater than prune_date, then insert into production table from temp table\n        \"\"\"\n        return None\n\n    @property\n    def prune_column(self):\n        \"\"\"\n        Override to set equal to the column of the prune_table which is to be compared\n        Intended to be used in conjunction with prune_table and prune_date\n        i.e. copy to temp table, prune production table to prune_column with a date greater than prune_date, then insert into production table from temp table\n        \"\"\"\n        return None\n\n    @property\n    def prune_date(self):\n        \"\"\"\n        Override to set equal to the date by which prune_column is to be compared\n        Intended to be used in conjunction with prune_table and prune_column\n        i.e. copy to temp table, prune production table to prune_column with a date greater than prune_date, then insert into production table from temp table\n        \"\"\"\n        return None\n\n    @property\n    def table_attributes(self):\n        \"\"\"\n        Add extra table attributes, for example:\n\n        DISTSTYLE KEY\n        DISTKEY (MY_FIELD)\n        SORTKEY (MY_FIELD_2, MY_FIELD_3)\n        \"\"\"\n        return ''\n\n    def do_truncate_table(self):\n        \"\"\"\n        Return True if table should be truncated before copying new data in.\n        \"\"\"\n        return False\n\n    def do_prune(self):\n        \"\"\"\n        Return True if prune_table, prune_column, and prune_date are implemented.\n        If only a subset of prune variables are override, an exception is raised to remind the user to implement all or none.\n        Prune (data newer than prune_date deleted) before copying new data in.\n        \"\"\"\n        if self.prune_table and self.prune_column and self.prune_date:\n            return True\n        elif self.prune_table or self.prune_column or self.prune_date:\n            raise Exception('override zero or all prune variables')\n        else:\n            return False\n\n    @property\n    def table_type(self):\n        \"\"\"\n        Return table type (i.e. 'temp').\n        \"\"\"\n        return ''\n\n    def queries(self):\n        \"\"\"\n        Override to return a list of queries to be executed in order.\n        \"\"\"\n        return []\n\n    def truncate_table(self, connection):\n        query = \"truncate %s\" % self.table\n        cursor = connection.cursor()\n        try:\n            cursor.execute(query)\n        finally:\n            cursor.close()\n\n    def prune(self, connection):\n        query = \"delete from %s where %s >= %s\" % (self.prune_table, self.prune_column, self.prune_date)\n        cursor = connection.cursor()\n        try:\n            cursor.execute(query)\n        finally:\n            cursor.close()\n\n    def create_table(self, connection):\n        \"\"\"\n        Override to provide code for creating the target table.\n\n        By default it will be created using types (optionally)\n        specified in columns.\n\n        If overridden, use the provided connection object for\n        setting up the table in order to create the table and\n        insert data using the same transaction.\n        \"\"\"\n        if len(self.columns[0]) == 1:\n            # only names of columns specified, no types\n            raise NotImplementedError(\"create_table() not implemented \"\n                                      \"for %r and columns types not \"\n                                      \"specified\" % self.table)\n        elif len(self.columns[0]) == 2:\n            # if columns is specified as (name, type) tuples\n            coldefs = ','.join(\n                '{name} {type}'.format(\n                    name=name,\n                    type=type) for name, type in self.columns\n            )\n\n            query = (\"CREATE {type} TABLE \"\n                     \"{table} ({coldefs}) \"\n                     \"{table_attributes}\").format(\n                type=self.table_type(),\n                table=self.table,\n                coldefs=coldefs,\n                table_attributes=self.table_attributes())\n\n            connection.cursor().execute(query)\n\n    def run(self):\n        \"\"\"\n        If the target table doesn't exist, self.create_table\n        will be called to attempt to create the table.\n        \"\"\"\n        if not (self.table):\n            raise Exception(\"table need to be specified\")\n\n        path = self.s3_load_path()\n        connection = self.output().connect()\n        cursor = connection.cursor()\n\n        self.init_copy(connection)\n        self.copy(cursor, path)\n        self.post_copy(cursor)\n\n        # update marker table\n        self.output().touch(connection)\n        connection.commit()\n\n        # commit and clean up\n        connection.close()\n\n    def copy(self, cursor, f):\n        \"\"\"\n        Defines copying from s3 into redshift.\n        \"\"\"\n        logger.info(\"Inserting file: %s\", f)\n        cursor.execute(\"\"\"\n         COPY %s from '%s'\n         CREDENTIALS 'aws_access_key_id=%s;aws_secret_access_key=%s'\n         %s\n         ;\"\"\" % (self.table, f, self.aws_access_key_id,\n                 self.aws_secret_access_key, self.copy_options))\n\n    def output(self):\n        \"\"\"\n        Returns a RedshiftTarget representing the inserted dataset.\n\n        Normally you don't override this.\n        \"\"\"\n        return RedshiftTarget(\n            host=self.host,\n            database=self.database,\n            user=self.user,\n            password=self.password,\n            table=self.table,\n            update_id=self.update_id())\n\n    def does_table_exist(self, connection):\n        \"\"\"\n        Determine whether the table already exists.\n        \"\"\"\n\n        if '.' in self.table:\n            query = (\"select 1 as table_exists \"\n                     \"from information_schema.tables \"\n                     \"where table_schema = %s and table_name = %s limit 1\")\n        else:\n            query = (\"select 1 as table_exists \"\n                     \"from pg_table_def \"\n                     \"where tablename = %s limit 1\")\n        cursor = connection.cursor()\n        try:\n            cursor.execute(query, tuple(self.table.split('.')))\n            result = cursor.fetchone()\n            return bool(result)\n        finally:\n            cursor.close()\n\n    def init_copy(self, connection):\n        \"\"\"\n        Perform pre-copy sql - such as creating table, truncating, or removing data older than x.\n        \"\"\"\n        if not self.does_table_exist(connection):\n            logger.info(\"Creating table %s\", self.table)\n            connection.reset()\n            self.create_table(connection)\n        elif self.do_truncate_table():\n            logger.info(\"Truncating table %s\", self.table)\n            self.truncate_table(connection)\n        elif self.do_prune():\n            logger.info(\"Removing %s older than %s from %s\", self.prune_column, self.prune_date, self.prune_table)\n            self.prune(connection)\n\n    def post_copy(self, cursor):\n        \"\"\"\n        Performs post-copy sql - such as cleansing data, inserting into production table (if copied to temp table), etc.\n        \"\"\"\n        logger.info('Executing post copy queries')\n        for query in self.queries():\n            cursor.execute(query)\n\n\nclass S3CopyJSONToTable(S3CopyToTable):\n    \"\"\"\n    Template task for inserting a JSON data set into Redshift from s3.\n\n    Usage:\n\n        * Subclass and override the required attributes:\n\n            * `host`,\n            * `database`,\n            * `user`,\n            * `password`,\n            * `table`,\n            * `columns`,\n            * `aws_access_key_id`,\n            * `aws_secret_access_key`,\n            * `s3_load_path`,\n            * `jsonpath`,\n            * `copy_json_options`.\n    \"\"\"\n\n    @abc.abstractproperty\n    def jsonpath(self):\n        \"\"\"\n        Override the jsonpath schema location for the table.\n        \"\"\"\n        return ''\n\n    @abc.abstractproperty\n    def copy_json_options(self):\n        \"\"\"\n        Add extra copy options, for example:\n\n        * GZIP\n        * LZOP\n        \"\"\"\n        return ''\n\n    def copy(self, cursor, f):\n        \"\"\"\n        Defines copying JSON from s3 into redshift.\n        \"\"\"\n\n        cursor.execute(\"\"\"\n         COPY %s from '%s'\n         CREDENTIALS 'aws_access_key_id=%s;aws_secret_access_key=%s'\n         JSON AS '%s' %s\n         %s\n         ;\"\"\" % (self.table, f, self.aws_access_key_id,\n                 self.aws_secret_access_key, self.jsonpath,\n                 self.copy_json_options, self.copy_options))\n\n\nclass RedshiftManifestTask(S3PathTask):\n    \"\"\"\n    Generic task to generate a manifest file that can be used\n    in S3CopyToTable in order to copy multiple files from your\n    s3 folder into a redshift table at once.\n\n    For full description on how to use the manifest file see\n    http://docs.aws.amazon.com/redshift/latest/dg/loading-data-files-using-manifest.html\n\n    Usage:\n\n        * requires parameters\n            * path - s3 path to the generated manifest file, including the\n                     name of the generated file\n                     to be copied into a redshift table\n            * folder_paths - s3 paths to the folders containing files you wish to be copied\n\n    Output:\n\n        * generated manifest file\n    \"\"\"\n\n    # should be over ridden to point to a variety\n    # of folders you wish to copy from\n    folder_paths = luigi.Parameter()\n    text_target = True\n\n    def run(self):\n        entries = []\n        for folder_path in self.folder_paths:\n            s3 = S3Target(folder_path)\n            client = s3.fs\n            for file_name in client.list(s3.path):\n                entries.append({\n                    'url': '%s/%s' % (folder_path, file_name),\n                    'mandatory': True\n                })\n        manifest = {'entries': entries}\n        target = self.output().open('w')\n        dump = json.dumps(manifest)\n        if not self.text_target:\n            dump = dump.encode('utf8')\n        target.write(dump)\n        target.close()\n\n\nclass KillOpenRedshiftSessions(luigi.Task):\n    \"\"\"\n    An task for killing any open Redshift sessions\n    in a given database. This is necessary to prevent open user sessions\n    with transactions against the table from blocking drop or truncate\n    table commands.\n\n    Usage:\n\n    Subclass and override the required `host`, `database`,\n    `user`, and `password` attributes.\n    \"\"\"\n\n    # time in seconds to wait before\n    # reconnecting to Redshift if our session is killed too.\n    # 30 seconds is usually fine; 60 is conservative\n    connection_reset_wait_seconds = luigi.IntParameter(default=60)\n\n    @abc.abstractproperty\n    def host(self):\n        return None\n\n    @abc.abstractproperty\n    def database(self):\n        return None\n\n    @abc.abstractproperty\n    def user(self):\n        return None\n\n    @abc.abstractproperty\n    def password(self):\n        return None\n\n    def update_id(self):\n        \"\"\"\n        This update id will be a unique identifier\n        for this insert on this table.\n        \"\"\"\n        return self.task_id\n\n    def output(self):\n        \"\"\"\n        Returns a RedshiftTarget representing the inserted dataset.\n\n        Normally you don't override this.\n        \"\"\"\n        # uses class name as a meta-table\n        return RedshiftTarget(\n            host=self.host,\n            database=self.database,\n            user=self.user,\n            password=self.password,\n            table=self.__class__.__name__,\n            update_id=self.update_id())\n\n    def run(self):\n        \"\"\"\n        Kill any open Redshift sessions for the given database.\n        \"\"\"\n        connection = self.output().connect()\n        # kill any sessions other than ours and\n        # internal Redshift sessions (rdsdb)\n        query = (\"select pg_terminate_backend(process) \"\n                 \"from STV_SESSIONS \"\n                 \"where db_name=%s \"\n                 \"and user_name != 'rdsdb' \"\n                 \"and process != pg_backend_pid()\")\n        cursor = connection.cursor()\n        logger.info('Killing all open Redshift sessions for database: %s', self.database)\n        try:\n            cursor.execute(query, (self.database,))\n            cursor.close()\n            connection.commit()\n        except psycopg2.DatabaseError as e:\n            if e.message and 'EOF' in e.message:\n                # sometimes this operation kills the current session.\n                # rebuild the connection. Need to pause for 30-60 seconds\n                # before Redshift will allow us back in.\n                connection.close()\n                logger.info('Pausing %s seconds for Redshift to reset connection', self.connection_reset_wait_seconds)\n                time.sleep(self.connection_reset_wait_seconds)\n                logger.info('Reconnecting to Redshift')\n                connection = self.output().connect()\n            else:\n                raise\n\n        try:\n            self.output().touch(connection)\n            connection.commit()\n        finally:\n            connection.close()\n\n        logger.info('Done killing all open Redshift sessions for database: %s', self.database)\n",
          "file_patch": "@@ -103,6 +103,34 @@ class S3CopyToTable(rdbms.CopyToTable):\n         \"\"\"\n         return ''\n \n+    @property    \n+    def prune_table(self):\n+        \"\"\"\n+        Override to set equal to the name of the table which is to be pruned.\n+        Intended to be used in conjunction with prune_column and prune_date\n+        i.e. copy to temp table, prune production table to prune_column with a date greater than prune_date, then insert into production table from temp table\n+        \"\"\"\n+        return None\n+\n+    @property\n+    def prune_column(self):\n+        \"\"\"\n+        Override to set equal to the column of the prune_table which is to be compared\n+        Intended to be used in conjunction with prune_table and prune_date\n+        i.e. copy to temp table, prune production table to prune_column with a date greater than prune_date, then insert into production table from temp table\n+        \"\"\"\n+        return None\n+\n+    @property\n+    def prune_date(self):\n+        \"\"\"\n+        Override to set equal to the date by which prune_column is to be compared\n+        Intended to be used in conjunction with prune_table and prune_column\n+        i.e. copy to temp table, prune production table to prune_column with a date greater than prune_date, then insert into production table from temp table\n+        \"\"\"\n+        return None\n+\n+    @property\n     def table_attributes(self):\n         \"\"\"\n         Add extra table attributes, for example:\n@@ -119,6 +147,32 @@ class S3CopyToTable(rdbms.CopyToTable):\n         \"\"\"\n         return False\n \n+    def do_prune(self):\n+        \"\"\"\n+        Return True if prune_table, prune_column, and prune_date are implemented.\n+        If only a subset of prune variables are override, an exception is raised to remind the user to implement all or none.\n+        Prune (data newer than prune_date deleted) before copying new data in.\n+        \"\"\"\n+        if self.prune_table and self.prune_column and self.prune_date:\n+            return True\n+        elif self.prune_table or self.prune_column or self.prune_date:\n+            raise Exception('override zero or all prune variables')\n+        else:\n+            return False\n+\n+    @property\n+    def table_type(self):\n+        \"\"\"\n+        Return table type (i.e. 'temp').\n+        \"\"\"\n+        return ''\n+\n+    def queries(self):\n+        \"\"\"\n+        Override to return a list of queries to be executed in order.\n+        \"\"\"\n+        return []\n+\n     def truncate_table(self, connection):\n         query = \"truncate %s\" % self.table\n         cursor = connection.cursor()\n@@ -127,6 +181,14 @@ class S3CopyToTable(rdbms.CopyToTable):\n         finally:\n             cursor.close()\n \n+    def prune(self, connection):\n+        query = \"delete from %s where %s >= %s\" % (self.prune_table, self.prune_column, self.prune_date)\n+        cursor = connection.cursor()\n+        try:\n+            cursor.execute(query)\n+        finally:\n+            cursor.close()\n+\n     def create_table(self, connection):\n         \"\"\"\n         Override to provide code for creating the target table.\n@@ -150,12 +212,15 @@ class S3CopyToTable(rdbms.CopyToTable):\n                     name=name,\n                     type=type) for name, type in self.columns\n             )\n-            query = (\"CREATE TABLE \"\n+\n+            query = (\"CREATE {type} TABLE \"\n                      \"{table} ({coldefs}) \"\n                      \"{table_attributes}\").format(\n+                type=self.table_type(),\n                 table=self.table,\n                 coldefs=coldefs,\n                 table_attributes=self.table_attributes())\n+\n             connection.cursor().execute(query)\n \n     def run(self):\n@@ -168,19 +233,13 @@ class S3CopyToTable(rdbms.CopyToTable):\n \n         path = self.s3_load_path()\n         connection = self.output().connect()\n-        if not self.does_table_exist(connection):\n-            # try creating table\n-            logger.info(\"Creating table %s\", self.table)\n-            connection.reset()\n-            self.create_table(connection)\n-        elif self.do_truncate_table():\n-            logger.info(\"Truncating table %s\", self.table)\n-            self.truncate_table(connection)\n-\n-        logger.info(\"Inserting file: %s\", path)\n         cursor = connection.cursor()\n+\n         self.init_copy(connection)\n         self.copy(cursor, path)\n+        self.post_copy(cursor)\n+\n+        # update marker table\n         self.output().touch(connection)\n         connection.commit()\n \n@@ -191,6 +250,7 @@ class S3CopyToTable(rdbms.CopyToTable):\n         \"\"\"\n         Defines copying from s3 into redshift.\n         \"\"\"\n+        logger.info(\"Inserting file: %s\", f)\n         cursor.execute(\"\"\"\n          COPY %s from '%s'\n          CREDENTIALS 'aws_access_key_id=%s;aws_secret_access_key=%s'\n@@ -233,6 +293,29 @@ class S3CopyToTable(rdbms.CopyToTable):\n         finally:\n             cursor.close()\n \n+    def init_copy(self, connection):\n+        \"\"\"\n+        Perform pre-copy sql - such as creating table, truncating, or removing data older than x.\n+        \"\"\"\n+        if not self.does_table_exist(connection):\n+            logger.info(\"Creating table %s\", self.table)\n+            connection.reset()\n+            self.create_table(connection)\n+        elif self.do_truncate_table():\n+            logger.info(\"Truncating table %s\", self.table)\n+            self.truncate_table(connection)\n+        elif self.do_prune():\n+            logger.info(\"Removing %s older than %s from %s\", self.prune_column, self.prune_date, self.prune_table)\n+            self.prune(connection)\n+\n+    def post_copy(self, cursor):\n+        \"\"\"\n+        Performs post-copy sql - such as cleansing data, inserting into production table (if copied to temp table), etc.\n+        \"\"\"\n+        logger.info('Executing post copy queries')\n+        for query in self.queries():\n+            cursor.execute(query)\n+\n \n class S3CopyJSONToTable(S3CopyToTable):\n     \"\"\"\n",
          "files_name_in_blame_commit": [
            "redshift.py",
            "redshift_test.py",
            "rdbms.py"
          ]
        }
      },
      "580cd8deea5c37cf2bb21b2cb156560c3bc4c6b2": {
        "commit": {
          "commit_id": "580cd8deea5c37cf2bb21b2cb156560c3bc4c6b2",
          "commit_message": "docstring standardization.\n\nadded newlines to improve readability.\nchanged the wording in the \"For the purpose of this exercise\" paragraph.\n\nfixed sphinx underline complaint.\n\nPython code now has proper syntax highlighting.\n\nadded docs env in tox.ini.\n\nadded declaration for ini code-block.\nchanged python code declaration to code-block.\n\nfixed typo.",
          "commit_author": "steenzout",
          "commit_date": "2015-02-03 10:48:30",
          "commit_parent": "90a1d44d9aaf6db446627103c24054cdf16fb243"
        },
        "function": {
          "function_name": "copy",
          "function_code_before": "def copy(self, cursor, f):\n    \"\"\"\n        Defines copying from s3 into redshift\n        \"\"\"\n    cursor.execute(\"\\n         COPY %s from '%s'\\n         CREDENTIALS 'aws_access_key_id=%s;aws_secret_access_key=%s'\\n         delimiter '%s'\\n         %s\\n         ;\" % (self.table, f, self.aws_access_key_id, self.aws_secret_access_key, self.column_separator, self.copy_options))",
          "function_code_after": "def copy(self, cursor, f):\n    \"\"\"\n        Defines copying from s3 into redshift.\n        \"\"\"\n    cursor.execute(\"\\n         COPY %s from '%s'\\n         CREDENTIALS 'aws_access_key_id=%s;aws_secret_access_key=%s'\\n         delimiter '%s'\\n         %s\\n         ;\" % (self.table, f, self.aws_access_key_id, self.aws_secret_access_key, self.column_separator, self.copy_options))",
          "function_before_start_line": 109,
          "function_before_end_line": 121,
          "function_after_start_line": 121,
          "function_after_end_line": 133,
          "function_before_token_count": 40,
          "function_after_token_count": 40,
          "functions_name_modified_file": [
            "aws_access_key_id",
            "copy_json_options",
            "output",
            "aws_secret_access_key",
            "s3_load_path",
            "copy",
            "copy_options",
            "run",
            "jsonpath"
          ],
          "functions_name_all_files": [
            "pipe_writer",
            "get_scala_jars",
            "_get_jars",
            "docs",
            "create_table",
            "s3_load_path",
            "id_to_name_and_params",
            "tmppath",
            "_list_existing",
            "mkdir",
            "walk",
            "_get_value",
            "task_started",
            "get_provided_jars",
            "get_all_params",
            "_add_task_event",
            "fix_time",
            "move",
            "parser_dest",
            "_run_task",
            "run_job",
            "_server_already_running",
            "get_data",
            "task_scheduled",
            "download",
            "_constrain_glob",
            "stop",
            "task_module",
            "init_combiner",
            "extract_packages_archive",
            "writeLine",
            "connect",
            "getpcmd",
            "_recurse_deps",
            "copy_json_options",
            "get_configured_hdfs_client",
            "aws_secret_access_key",
            "rename_dont_move",
            "update_resources",
            "_update_priority",
            "requires",
            "rows",
            "externalize",
            "fs",
            "run",
            "is_dir",
            "_get_per_location_glob",
            "create_packages_archive",
            "copy_options",
            "to_str_params",
            "create_marker_table",
            "parse_task",
            "_docs",
            "_request",
            "aws_access_key_id",
            "move_dir",
            "_flush_buffer",
            "_reduce_input",
            "get_configured_hadoop_version",
            "_flush_batch_incr_counter",
            "get_param_values",
            "_schedulable",
            "chmod",
            "check_pid",
            "_requires",
            "_email_unexpected_error",
            "has_value",
            "_validate_dependency",
            "get_active_tasks",
            "list_path",
            "on_success",
            "add_to_cmdline_parser",
            "get_scalding_core",
            "mapper",
            "serialize",
            "_is_root",
            "graph",
            "init_hadoop",
            "__call__",
            "deps",
            "_setup_links",
            "_abort",
            "_update_task_history",
            "get_spool_handler",
            "job_class",
            "incr_counter",
            "settings",
            "check_output",
            "table_location",
            "get_active_workers",
            "_host_ref",
            "has_task",
            "get_task_cls",
            "touch",
            "dep_graph",
            "put",
            "marker_index_hist_size",
            "timeout",
            "get_task_parameters",
            "clone",
            "update",
            "send_email_smtp",
            "get_scalding_jars",
            "abort",
            "call_check",
            "write_pid",
            "_get_with_default",
            "raise_on_error",
            "job_args",
            "put_multipart",
            "user",
            "listdir",
            "spark_heartbeat",
            "run_api_threaded",
            "load",
            "inverse_dependencies",
            "_add_path_delimiter",
            "_scp",
            "_has_resources",
            "wrap_traceback",
            "_serialize_task",
            "__getattr__",
            "on_failure",
            "glob_exists",
            "from_str_params",
            "_chained_call",
            "getint",
            "jsonpath",
            "atomic_output",
            "daemonize",
            "clear",
            "add_global_parameters",
            "_incr_counter",
            "getmerge",
            "inverse_dep_graph",
            "_finish",
            "generate_email",
            "jar",
            "get_template_path",
            "group",
            "create_remote_scheduler",
            "_session",
            "__str__",
            "source",
            "extra_modules",
            "port",
            "program",
            "add_worker",
            "_emit_metrics",
            "apply_async",
            "_init_connection",
            "_get",
            "disable_instance_cache",
            "process_resources",
            "_generate_worker_info",
            "instance",
            "find_all_by_parameters",
            "_is_writable",
            "ensure_hist_size",
            "spark_options",
            "writer",
            "worker_list",
            "__hash__",
            "moving_start",
            "reader",
            "trigger_event",
            "__repr__",
            "fetch_task_failures",
            "add_config_path",
            "_init_api",
            "get_task",
            "env_params",
            "_get_s3_config",
            "pipe_reader",
            "doc_type",
            "num_failures",
            "pickle_reader",
            "get_key",
            "moving_stop",
            "__iter__",
            "password",
            "load_hive_cmd",
            "hiverc",
            "exists",
            "close",
            "_process_args",
            "create_local_scheduler",
            "table_exists",
            "get",
            "hdfs_reader",
            "chown",
            "parse",
            "pickle_writer",
            "sample",
            "reload",
            "output",
            "task_finished",
            "can_disable",
            "_post",
            "get_params",
            "list",
            "check_complete",
            "hdfs_writer",
            "previous",
            "create_worker",
            "add_info",
            "finite_datetimes",
            "path",
            "requires_local",
            "send_email_sendgrid",
            "__new__",
            "missing_datetimes",
            "__init__",
            "connection_string",
            "setup_interface_logging",
            "internal_writer",
            "_map_input",
            "getintdict",
            "upload",
            "set_global",
            "isdir",
            "send_email_ses",
            "table",
            "_log_unexpected_error",
            "get_all_data",
            "_log_remote_tasks",
            "get_config",
            "open",
            "run_reducer",
            "_add_worker",
            "hiveconfs",
            "email_type",
            "attach",
            "parse_from_input",
            "task_family",
            "init_local",
            "find_task_by_id",
            "finish",
            "_parseIso8601",
            "load_task",
            "dereference",
            "input",
            "__enter__",
            "chunk_size",
            "get_libjars",
            "add_link",
            "partition_spec",
            "get_reg",
            "acquire_for",
            "delegates",
            "most_common",
            "is_writable",
            "complete",
            "getfloat",
            "update_id",
            "set_global_parameters",
            "run_hive_script",
            "kill_job",
            "set",
            "_wait",
            "_format_range",
            "extra_files",
            "internal_reader",
            "remove",
            "get_worker",
            "http_auth",
            "get_info",
            "task_list",
            "_existing_partitions",
            "rename",
            "error_task_names",
            "parameter_to_datetime",
            "_connect",
            "put_string",
            "index",
            "relpath",
            "_setup_remote",
            "common_params",
            "_replacer",
            "set_status",
            "get_extra_files",
            "_validate_task",
            "prepare_outputs",
            "get_hive_syntax",
            "set_global_from_args",
            "get_work",
            "app",
            "add_failure",
            "host",
            "readlines",
            "has_task_value",
            "marker_index_document_id",
            "makedirs",
            "create_index",
            "_rm_recursive",
            "add_task",
            "reset_global",
            "table_schema",
            "_apply_regex",
            "parse_from_args",
            "_get_work",
            "bulk_complete",
            "mapping",
            "datetime_to_parameter",
            "map_column",
            "get_previous_completed",
            "extra_jars",
            "inactivate_workers",
            "deprecate_kwarg",
            "prune",
            "fn",
            "run_hive_cmd",
            "_upstream_status",
            "create_hadoopcli_client",
            "_email_complete_error",
            "ping",
            "_editdistance",
            "tmp_path",
            "_traverse_inverse_deps",
            "print_exception",
            "send_email",
            "create_marker_index",
            "_parseSimple",
            "_add_to_buffer",
            "write",
            "getboolean",
            "main",
            "infer_bulk_complete_from_fs",
            "load_hadoop_cmd",
            "get_autoconfig_client",
            "_prepare_cmd",
            "initialized",
            "build",
            "namespace",
            "_keep_alive",
            "_create_scheduler",
            "clear_instance_cache",
            "count",
            "value",
            "purge_existing_index",
            "tasks_str",
            "_log_complete_error",
            "get_build_dir",
            "find_latest_runs",
            "_find_or_create_task",
            "_get_task",
            "_prefix",
            "has_excessive_failures",
            "build_job_jar",
            "create_subprocess",
            "read",
            "py_files",
            "skip",
            "initialize",
            "send_error_email",
            "fix_paths",
            "_path_to_bucket_and_key",
            "add_task_parameters",
            "run_combiner",
            "task_history",
            "get_job_class",
            "run_and_track_hadoop_job",
            "init_reducer",
            "_handle_next_task",
            "input_local",
            "find_all_by_name",
            "__eq__",
            "re_enable",
            "dump",
            "inactivate_tasks",
            "Popen",
            "get_tmp_job_jar",
            "dependency_jars",
            "_check_complete_value",
            "copy",
            "getpaths",
            "_rank",
            "run_hive",
            "database",
            "run_mapper",
            "flatten_output",
            "get_worker_ids",
            "_get_filesystems_and_globs",
            "job_runner",
            "task_wraps",
            "delete_index",
            "_purge_children",
            "get_pending_tasks",
            "tunnel",
            "args",
            "task_search",
            "get_bite",
            "fork_linked_workers",
            "task_value",
            "flatten",
            "event_handler",
            "get_log_format",
            "_used_resources",
            "add",
            "_write_sparkey_file",
            "track_progress",
            "query",
            "_make_method",
            "_get_value_from_config",
            "serialize_to_input",
            "init_mapper",
            "init_copy",
            "jobconfs",
            "_sleeper",
            "_get_default",
            "_format_datetime",
            "input_hadoop",
            "__del__",
            "get_running_tasks",
            "requires_hadoop",
            "_add",
            "re_enable_task",
            "fetch_error",
            "__exit__"
          ],
          "functions_name_co_evolved_modified_file": [
            "aws_access_key_id",
            "copy_json_options",
            "output",
            "aws_secret_access_key",
            "s3_load_path",
            "copy_options",
            "run",
            "jsonpath"
          ],
          "functions_name_co_evolved_all_files": [
            "_get_with_default",
            "add_failure",
            "raise_on_error",
            "output",
            "job_args",
            "docs",
            "create_table",
            "s3_load_path",
            "listdir",
            "get_params",
            "id_to_name_and_params",
            "_list_existing",
            "run_api_threaded",
            "check_complete",
            "host",
            "mkdir",
            "previous",
            "marker_index_document_id",
            "makedirs",
            "create_index",
            "add_task",
            "get_all_params",
            "finite_datetimes",
            "table_schema",
            "path",
            "on_failure",
            "from_str_params",
            "requires_local",
            "move",
            "bulk_complete",
            "__new__",
            "mapping",
            "missing_datetimes",
            "jsonpath",
            "__init__",
            "internal_writer",
            "map_column",
            "_map_input",
            "_constrain_glob",
            "stop",
            "atomic_output",
            "extra_jars",
            "set_global",
            "isdir",
            "deprecate_kwarg",
            "run_hive_cmd",
            "clear",
            "create_hadoopcli_client",
            "_incr_counter",
            "connect",
            "getpcmd",
            "_finish",
            "get_config",
            "create_marker_index",
            "copy_json_options",
            "jar",
            "get_configured_hdfs_client",
            "open",
            "aws_secret_access_key",
            "main",
            "rename_dont_move",
            "_update_priority",
            "requires",
            "rows",
            "externalize",
            "infer_bulk_complete_from_fs",
            "fs",
            "run",
            "run_reducer",
            "get_autoconfig_client",
            "_get_per_location_glob",
            "create_packages_archive",
            "initialized",
            "hiveconfs",
            "build",
            "namespace",
            "copy_options",
            "_keep_alive",
            "create_marker_table",
            "clear_instance_cache",
            "source",
            "attach",
            "value",
            "_docs",
            "purge_existing_index",
            "port",
            "aws_access_key_id",
            "move_dir",
            "task_family",
            "_emit_metrics",
            "init_local",
            "_reduce_input",
            "tasks_str",
            "get_configured_hadoop_version",
            "find_task_by_id",
            "_flush_batch_incr_counter",
            "get_param_values",
            "find_latest_runs",
            "disable_instance_cache",
            "process_resources",
            "_prefix",
            "_requires",
            "load_task",
            "find_all_by_parameters",
            "py_files",
            "skip",
            "ensure_hist_size",
            "has_value",
            "input",
            "writer",
            "send_error_email",
            "fix_paths",
            "chunk_size",
            "on_success",
            "mapper",
            "partition_spec",
            "run_and_track_hadoop_job",
            "moving_start",
            "serialize",
            "get_reg",
            "_handle_next_task",
            "find_all_by_name",
            "reader",
            "acquire_for",
            "trigger_event",
            "delegates",
            "dump",
            "most_common",
            "Popen",
            "fetch_task_failures",
            "__call__",
            "complete",
            "dependency_jars",
            "deps",
            "getpaths",
            "_rank",
            "run_hive",
            "_abort",
            "update_id",
            "run_mapper",
            "flatten_output",
            "run_hive_script",
            "_get_filesystems_and_globs",
            "job_runner",
            "job_class",
            "delete_index",
            "incr_counter",
            "_purge_children",
            "doc_type",
            "tunnel",
            "args",
            "extra_files",
            "num_failures",
            "internal_reader",
            "task_search",
            "settings",
            "remove",
            "fork_linked_workers",
            "check_output",
            "flatten",
            "event_handler",
            "http_auth",
            "moving_stop",
            "task_list",
            "rename",
            "add",
            "hiverc",
            "get_task_cls",
            "track_progress",
            "touch",
            "exists",
            "_connect",
            "marker_index_hist_size",
            "index",
            "table_exists",
            "timeout",
            "clone",
            "relpath",
            "init_copy",
            "common_params",
            "update",
            "__del__",
            "parse",
            "prepare_outputs",
            "__exit__"
          ]
        },
        "file": {
          "file_name": "redshift.py",
          "file_nloc": 160,
          "file_complexity": 18,
          "file_token_count": 564,
          "file_before": "import abc\nimport logging\nimport luigi\nimport json\nfrom luigi.contrib import rdbms\nfrom luigi import postgres\n\nfrom luigi.s3 import S3PathTask, S3Target\n\n\nlogger = logging.getLogger('luigi-interface')\n\n\ntry:\n    import psycopg2\n    import psycopg2.errorcodes\nexcept ImportError:\n    logger.warning(\"Loading postgres module without psycopg2 installed. Will crash at runtime if postgres functionality is used.\")\n\n\nclass RedshiftTarget(postgres.PostgresTarget):\n\n    \"\"\"\n    Target for a resource in Redshift.\n\n    Redshift is similar to postgres with a few adjustments required by redshift\n    \"\"\"\n    marker_table = luigi.configuration.get_config().get('redshift', 'marker-table', 'table_updates')\n\n    use_db_timestamps = False\n\n\nclass S3CopyToTable(rdbms.CopyToTable):\n\n    \"\"\"\n    Template task for inserting a data set into Redshift from s3.\n\n    Usage:\n    Subclass and override the required attributes:\n    `host`, `database`, `user`, `password`, `table`, `columns`,\n    `aws_access_key_id`, `aws_secret_access_key`, `s3_load_path`\n    \"\"\"\n\n    @abc.abstractproperty\n    def s3_load_path(self):\n        'override to return the load path'\n        return None\n\n    @abc.abstractproperty\n    def aws_access_key_id(self):\n        'override to return the key id'\n        return None\n\n    @abc.abstractproperty\n    def aws_secret_access_key(self):\n        'override to return the secret access key'\n        return None\n\n    @abc.abstractproperty\n    def copy_options(self):\n        '''Add extra copy options, for example:\n\n         TIMEFORMAT 'auto'\n         IGNOREHEADER 1\n         TRUNCATECOLUMNS\n         IGNOREBLANKLINES\n        '''\n        return ''\n\n    def run(self):\n        \"\"\"\n        If the target table doesn't exist, self.create_table will be called\n        to attempt to create the table.\n        \"\"\"\n        if not (self.table):\n            raise Exception(\"table need to be specified\")\n\n        connection = self.output().connect()\n\n        path = self.s3_load_path()\n        logger.info(\"Inserting file: %s\", path)\n\n        # attempt to copy the data into postgres\n        # if it fails because the target table doesn't exist\n        # try to create it by running self.create_table\n        for attempt in xrange(2):\n            try:\n                cursor = connection.cursor()\n                self.init_copy(connection)\n                self.copy(cursor, path)\n            except psycopg2.ProgrammingError as e:\n                if e.pgcode == psycopg2.errorcodes.UNDEFINED_TABLE and attempt == 0:\n                    # if first attempt fails with \"relation not found\",\n                    # try creating table\n                    logger.info(\"Creating table %s\", self.table)\n                    connection.reset()\n                    self.create_table(connection)\n                else:\n                    raise\n            else:\n                break\n\n        self.output().touch(connection)\n        connection.commit()\n\n        # commit and clean up\n        connection.close()\n\n    def copy(self, cursor, f):\n        '''\n        Defines copying from s3 into redshift\n        '''\n\n        cursor.execute(\"\"\"\n         COPY %s from '%s'\n         CREDENTIALS 'aws_access_key_id=%s;aws_secret_access_key=%s'\n         delimiter '%s'\n         %s\n         ;\"\"\" % (self.table, f, self.aws_access_key_id,\n                 self.aws_secret_access_key, self.column_separator,\n                 self.copy_options))\n\n    def output(self):\n        \"\"\"Returns a RedshiftTarget representing the inserted dataset.\n\n        Normally you don't override this.\n        \"\"\"\n        return RedshiftTarget(\n            host=self.host,\n            database=self.database,\n            user=self.user,\n            password=self.password,\n            table=self.table,\n            update_id=self.update_id())\n\n\nclass S3CopyJSONToTable(S3CopyToTable):\n\n    \"\"\"\n    Template task for inserting a JSON data set into Redshift from s3.\n\n    Usage:\n    Subclass and override the required attributes:\n    `host`, `database`, `user`, `password`, `table`, `columns`,\n    `aws_access_key_id`, `aws_secret_access_key`, `s3_load_path`,\n    `jsonpath`, `copy_json_options`\n    \"\"\"\n\n    @abc.abstractproperty\n    def jsonpath(self):\n        'override the jsonpath schema location for the table'\n        return ''\n\n    @abc.abstractproperty\n    def copy_json_options(self):\n        '''Add extra copy options, for example:\n        GZIP\n        LZOP\n        '''\n        return ''\n\n    def copy(self, cursor, f):\n        '''\n        Defines copying JSON from s3 into redshift\n        '''\n\n        cursor.execute(\"\"\"\n         COPY %s from '%s'\n         CREDENTIALS 'aws_access_key_id=%s;aws_secret_access_key=%s'\n         JSON AS '%s' %s\n         %s\n         ;\"\"\" % (self.table, f, self.aws_access_key_id,\n                 self.aws_secret_access_key, self.jsonpath,\n                 self.copy_json_options, self.copy_options))\n\n\nclass RedshiftManifestTask(S3PathTask):\n\n    \"\"\"\n    Generic task to generate a manifest file that can be used\n    in S3CopyToTable in order to copy multiple files from your\n    s3 folder into a redshift table at once\n\n    For full description on how to use the manifest file see:\n    http://docs.aws.amazon.com/redshift/latest/dg/loading-data-files-using-manifest.html\n\n    Usage:\n    Requires parameters\n        path - s3 path to the generated manifest file, including the\n               name of the generated file\n                      to be copied into a redshift table\n        folder_paths - s3 paths to the folders containing files you wish to be copied\n    Output:\n        generated manifest file\n    \"\"\"\n\n    # should be over ridden to point to a variety of folders you wish to copy from\n    folder_paths = luigi.Parameter()\n\n    def run(self):\n        entries = []\n        for folder_path in self.folder_paths:\n            s3 = S3Target(folder_path)\n            client = s3.fs\n            for file_name in client.list(s3.path):\n                entries.append({\n                    'url': '%s/%s' % (folder_path, file_name),\n                    'mandatory': True\n                })\n        manifest = {'entries': entries}\n        target = self.output().open('w')\n        target.write(json.dumps(manifest))\n        target.close()\n",
          "file_after": "import abc\nimport logging\nimport luigi\nimport json\nfrom luigi.contrib import rdbms\nfrom luigi import postgres\n\nfrom luigi.s3 import S3PathTask, S3Target\n\n\nlogger = logging.getLogger('luigi-interface')\n\n\ntry:\n    import psycopg2\n    import psycopg2.errorcodes\nexcept ImportError:\n    logger.warning(\"Loading postgres module without psycopg2 installed. Will crash at runtime if postgres functionality is used.\")\n\n\nclass RedshiftTarget(postgres.PostgresTarget):\n    \"\"\"\n    Target for a resource in Redshift.\n\n    Redshift is similar to postgres with a few adjustments required by redshift.\n    \"\"\"\n    marker_table = luigi.configuration.get_config().get('redshift', 'marker-table', 'table_updates')\n\n    use_db_timestamps = False\n\n\nclass S3CopyToTable(rdbms.CopyToTable):\n    \"\"\"\n    Template task for inserting a data set into Redshift from s3.\n\n    Usage:\n        Subclass and override the required attributes:\n\n        * `host`,\n        * `database`,\n        * `user`,\n        * `password`,\n        * `table`,\n        * `columns`,\n        * `aws_access_key_id`,\n        * `aws_secret_access_key`,\n        * `s3_load_path`.\n    \"\"\"\n\n    @abc.abstractproperty\n    def s3_load_path(self):\n        \"\"\"\n        Override to return the load path.\n        \"\"\"\n        return None\n\n    @abc.abstractproperty\n    def aws_access_key_id(self):\n        \"\"\"\n        Override to return the key id.\n        \"\"\"\n        return None\n\n    @abc.abstractproperty\n    def aws_secret_access_key(self):\n        \"\"\"\n        Override to return the secret access key.\n        \"\"\"\n        return None\n\n    @abc.abstractproperty\n    def copy_options(self):\n        \"\"\"\n        Add extra copy options, for example:\n\n        * TIMEFORMAT 'auto'\n        * IGNOREHEADER 1\n        * TRUNCATECOLUMNS\n        * IGNOREBLANKLINES\n        \"\"\"\n        return ''\n\n    def run(self):\n        \"\"\"\n        If the target table doesn't exist, self.create_table will be called to attempt to create the table.\n        \"\"\"\n        if not (self.table):\n            raise Exception(\"table need to be specified\")\n\n        connection = self.output().connect()\n\n        path = self.s3_load_path()\n        logger.info(\"Inserting file: %s\", path)\n\n        # attempt to copy the data into postgres\n        # if it fails because the target table doesn't exist\n        # try to create it by running self.create_table\n        for attempt in xrange(2):\n            try:\n                cursor = connection.cursor()\n                self.init_copy(connection)\n                self.copy(cursor, path)\n            except psycopg2.ProgrammingError as e:\n                if e.pgcode == psycopg2.errorcodes.UNDEFINED_TABLE and attempt == 0:\n                    # if first attempt fails with \"relation not found\",\n                    # try creating table\n                    logger.info(\"Creating table %s\", self.table)\n                    connection.reset()\n                    self.create_table(connection)\n                else:\n                    raise\n            else:\n                break\n\n        self.output().touch(connection)\n        connection.commit()\n\n        # commit and clean up\n        connection.close()\n\n    def copy(self, cursor, f):\n        \"\"\"\n        Defines copying from s3 into redshift.\n        \"\"\"\n\n        cursor.execute(\"\"\"\n         COPY %s from '%s'\n         CREDENTIALS 'aws_access_key_id=%s;aws_secret_access_key=%s'\n         delimiter '%s'\n         %s\n         ;\"\"\" % (self.table, f, self.aws_access_key_id,\n                 self.aws_secret_access_key, self.column_separator,\n                 self.copy_options))\n\n    def output(self):\n        \"\"\"\n        Returns a RedshiftTarget representing the inserted dataset.\n\n        Normally you don't override this.\n        \"\"\"\n        return RedshiftTarget(\n            host=self.host,\n            database=self.database,\n            user=self.user,\n            password=self.password,\n            table=self.table,\n            update_id=self.update_id())\n\n\nclass S3CopyJSONToTable(S3CopyToTable):\n    \"\"\"\n    Template task for inserting a JSON data set into Redshift from s3.\n\n    Usage:\n\n        Subclass and override the required attributes:\n\n        * `host`,\n        * `database`,\n        * `user`,\n        * `password`,\n        * `table`,\n        * `columns`,\n        * `aws_access_key_id`,\n        * `aws_secret_access_key`,\n        * `s3_load_path`,\n        * `jsonpath`,\n        * `copy_json_options`.\n    \"\"\"\n\n    @abc.abstractproperty\n    def jsonpath(self):\n        \"\"\"\n        Override the jsonpath schema location for the table.\n        \"\"\"\n        return ''\n\n    @abc.abstractproperty\n    def copy_json_options(self):\n        \"\"\"\n        Add extra copy options, for example:\n\n        * GZIP\n        * LZOP\n        \"\"\"\n        return ''\n\n    def copy(self, cursor, f):\n        \"\"\"\n        Defines copying JSON from s3 into redshift.\n        \"\"\"\n\n        cursor.execute(\"\"\"\n         COPY %s from '%s'\n         CREDENTIALS 'aws_access_key_id=%s;aws_secret_access_key=%s'\n         JSON AS '%s' %s\n         %s\n         ;\"\"\" % (self.table, f, self.aws_access_key_id,\n                 self.aws_secret_access_key, self.jsonpath,\n                 self.copy_json_options, self.copy_options))\n\n\nclass RedshiftManifestTask(S3PathTask):\n    \"\"\"\n    Generic task to generate a manifest file that can be used\n    in S3CopyToTable in order to copy multiple files from your\n    s3 folder into a redshift table at once.\n\n    For full description on how to use the manifest file see:\n    http://docs.aws.amazon.com/redshift/latest/dg/loading-data-files-using-manifest.html\n\n    Usage:\n\n    Requires parameters\n        path - s3 path to the generated manifest file, including the\n               name of the generated file\n                      to be copied into a redshift table\n        folder_paths - s3 paths to the folders containing files you wish to be copied\n\n    Output:\n        generated manifest file\n    \"\"\"\n\n    # should be over ridden to point to a variety of folders you wish to copy from\n    folder_paths = luigi.Parameter()\n\n    def run(self):\n        entries = []\n        for folder_path in self.folder_paths:\n            s3 = S3Target(folder_path)\n            client = s3.fs\n            for file_name in client.list(s3.path):\n                entries.append({\n                    'url': '%s/%s' % (folder_path, file_name),\n                    'mandatory': True\n                })\n        manifest = {'entries': entries}\n        target = self.output().open('w')\n        target.write(json.dumps(manifest))\n        target.close()\n",
          "file_patch": "@@ -19,11 +19,10 @@ except ImportError:\n \n \n class RedshiftTarget(postgres.PostgresTarget):\n-\n     \"\"\"\n     Target for a resource in Redshift.\n \n-    Redshift is similar to postgres with a few adjustments required by redshift\n+    Redshift is similar to postgres with a few adjustments required by redshift.\n     \"\"\"\n     marker_table = luigi.configuration.get_config().get('redshift', 'marker-table', 'table_updates')\n \n@@ -31,46 +30,59 @@ class RedshiftTarget(postgres.PostgresTarget):\n \n \n class S3CopyToTable(rdbms.CopyToTable):\n-\n     \"\"\"\n     Template task for inserting a data set into Redshift from s3.\n \n     Usage:\n-    Subclass and override the required attributes:\n-    `host`, `database`, `user`, `password`, `table`, `columns`,\n-    `aws_access_key_id`, `aws_secret_access_key`, `s3_load_path`\n+        Subclass and override the required attributes:\n+\n+        * `host`,\n+        * `database`,\n+        * `user`,\n+        * `password`,\n+        * `table`,\n+        * `columns`,\n+        * `aws_access_key_id`,\n+        * `aws_secret_access_key`,\n+        * `s3_load_path`.\n     \"\"\"\n \n     @abc.abstractproperty\n     def s3_load_path(self):\n-        'override to return the load path'\n+        \"\"\"\n+        Override to return the load path.\n+        \"\"\"\n         return None\n \n     @abc.abstractproperty\n     def aws_access_key_id(self):\n-        'override to return the key id'\n+        \"\"\"\n+        Override to return the key id.\n+        \"\"\"\n         return None\n \n     @abc.abstractproperty\n     def aws_secret_access_key(self):\n-        'override to return the secret access key'\n+        \"\"\"\n+        Override to return the secret access key.\n+        \"\"\"\n         return None\n \n     @abc.abstractproperty\n     def copy_options(self):\n-        '''Add extra copy options, for example:\n+        \"\"\"\n+        Add extra copy options, for example:\n \n-         TIMEFORMAT 'auto'\n-         IGNOREHEADER 1\n-         TRUNCATECOLUMNS\n-         IGNOREBLANKLINES\n-        '''\n+        * TIMEFORMAT 'auto'\n+        * IGNOREHEADER 1\n+        * TRUNCATECOLUMNS\n+        * IGNOREBLANKLINES\n+        \"\"\"\n         return ''\n \n     def run(self):\n         \"\"\"\n-        If the target table doesn't exist, self.create_table will be called\n-        to attempt to create the table.\n+        If the target table doesn't exist, self.create_table will be called to attempt to create the table.\n         \"\"\"\n         if not (self.table):\n             raise Exception(\"table need to be specified\")\n@@ -107,9 +119,9 @@ class S3CopyToTable(rdbms.CopyToTable):\n         connection.close()\n \n     def copy(self, cursor, f):\n-        '''\n-        Defines copying from s3 into redshift\n-        '''\n+        \"\"\"\n+        Defines copying from s3 into redshift.\n+        \"\"\"\n \n         cursor.execute(\"\"\"\n          COPY %s from '%s'\n@@ -121,7 +133,8 @@ class S3CopyToTable(rdbms.CopyToTable):\n                  self.copy_options))\n \n     def output(self):\n-        \"\"\"Returns a RedshiftTarget representing the inserted dataset.\n+        \"\"\"\n+        Returns a RedshiftTarget representing the inserted dataset.\n \n         Normally you don't override this.\n         \"\"\"\n@@ -135,34 +148,47 @@ class S3CopyToTable(rdbms.CopyToTable):\n \n \n class S3CopyJSONToTable(S3CopyToTable):\n-\n     \"\"\"\n     Template task for inserting a JSON data set into Redshift from s3.\n \n     Usage:\n-    Subclass and override the required attributes:\n-    `host`, `database`, `user`, `password`, `table`, `columns`,\n-    `aws_access_key_id`, `aws_secret_access_key`, `s3_load_path`,\n-    `jsonpath`, `copy_json_options`\n+\n+        Subclass and override the required attributes:\n+\n+        * `host`,\n+        * `database`,\n+        * `user`,\n+        * `password`,\n+        * `table`,\n+        * `columns`,\n+        * `aws_access_key_id`,\n+        * `aws_secret_access_key`,\n+        * `s3_load_path`,\n+        * `jsonpath`,\n+        * `copy_json_options`.\n     \"\"\"\n \n     @abc.abstractproperty\n     def jsonpath(self):\n-        'override the jsonpath schema location for the table'\n+        \"\"\"\n+        Override the jsonpath schema location for the table.\n+        \"\"\"\n         return ''\n \n     @abc.abstractproperty\n     def copy_json_options(self):\n-        '''Add extra copy options, for example:\n-        GZIP\n-        LZOP\n-        '''\n+        \"\"\"\n+        Add extra copy options, for example:\n+\n+        * GZIP\n+        * LZOP\n+        \"\"\"\n         return ''\n \n     def copy(self, cursor, f):\n-        '''\n-        Defines copying JSON from s3 into redshift\n-        '''\n+        \"\"\"\n+        Defines copying JSON from s3 into redshift.\n+        \"\"\"\n \n         cursor.execute(\"\"\"\n          COPY %s from '%s'\n@@ -175,21 +201,22 @@ class S3CopyJSONToTable(S3CopyToTable):\n \n \n class RedshiftManifestTask(S3PathTask):\n-\n     \"\"\"\n     Generic task to generate a manifest file that can be used\n     in S3CopyToTable in order to copy multiple files from your\n-    s3 folder into a redshift table at once\n+    s3 folder into a redshift table at once.\n \n     For full description on how to use the manifest file see:\n     http://docs.aws.amazon.com/redshift/latest/dg/loading-data-files-using-manifest.html\n \n     Usage:\n+\n     Requires parameters\n         path - s3 path to the generated manifest file, including the\n                name of the generated file\n                       to be copied into a redshift table\n         folder_paths - s3 paths to the folders containing files you wish to be copied\n+\n     Output:\n         generated manifest file\n     \"\"\"\n",
          "files_name_in_blame_commit": [
            "hive.py",
            "target.py",
            "ssh.py",
            "notifications.py",
            "format.py",
            "process.py",
            "__init__.py",
            "deprecate_kwarg.py",
            "hadoop.py",
            "task_history.py",
            "ftp.py",
            "interface.py",
            "redshift.py",
            "postgres.py",
            "mysqldb.py",
            "db_task_history.py",
            "parameter.py",
            "task.py",
            "task_status.py",
            "util.py",
            "hadoop_jar.py",
            "spark.py",
            "parse_task.py",
            "hdfs.py",
            "range.py",
            "rdbms.py",
            "esindex.py",
            "mrrunner.py",
            "mock.py",
            "scheduler.py",
            "sqla.py",
            "webhdfs.py",
            "scalding.py",
            "server.py",
            "worker.py",
            "s3.py",
            "configuration.py",
            "file.py",
            "rpc.py",
            "sparkey.py",
            "lock.py"
          ]
        }
      },
      "9249d68ba958560f7d371ed89f564958d2208ee0": {
        "commit": {
          "commit_id": "9249d68ba958560f7d371ed89f564958d2208ee0",
          "commit_message": "Add support for redshift.\n\nCreate a common class for redshift and postgres (since they are similar).\nIn the future postres should more to contrib as well.\nCreate both a Redshift target and an S3CopyTask",
          "commit_author": "Ran Tavory",
          "commit_date": "2014-02-11 20:46:09",
          "commit_parent": "c5f93a8a129f6f81c5f64d7a834cd0bdeb295eb9"
        },
        "function": {
          "function_name": "copy",
          "function_code_before": "",
          "function_code_after": "",
          "function_before_start_line": "",
          "function_before_end_line": "",
          "function_after_start_line": "",
          "function_after_end_line": "",
          "function_before_token_count": 0,
          "function_after_token_count": 0,
          "functions_name_modified_file": [
            "aws_access_key_id",
            "output",
            "aws_secret_access_key",
            "s3_load_path",
            "copy",
            "copy_options",
            "create_marker_table",
            "run",
            "touch"
          ],
          "functions_name_all_files": [
            "output",
            "table",
            "user",
            "create_table",
            "s3_load_path",
            "connect",
            "host",
            "open",
            "aws_secret_access_key",
            "password",
            "rows",
            "run",
            "touch",
            "exists",
            "copy_options",
            "create_marker_table",
            "init_copy",
            "_replacer",
            "__init__",
            "aws_access_key_id",
            "__call__",
            "map_column",
            "copy",
            "update_id",
            "database"
          ],
          "functions_name_co_evolved_modified_file": [
            "aws_access_key_id",
            "output",
            "aws_secret_access_key",
            "s3_load_path",
            "copy_options",
            "create_marker_table",
            "run",
            "touch"
          ],
          "functions_name_co_evolved_all_files": [
            "output",
            "table",
            "user",
            "create_table",
            "s3_load_path",
            "connect",
            "host",
            "open",
            "aws_secret_access_key",
            "password",
            "run",
            "touch",
            "exists",
            "copy_options",
            "create_marker_table",
            "init_copy",
            "_replacer",
            "__init__",
            "aws_access_key_id",
            "__call__",
            "update_id",
            "database"
          ]
        },
        "file": {
          "file_name": "redshift.py",
          "file_nloc": 116,
          "file_complexity": 17,
          "file_token_count": 517,
          "file_before": null,
          "file_after": "import datetime\nimport abc\nimport logging\nimport luigi.postgres\nimport luigi\nfrom luigi.contrib import postgreslike\n\nlogger = logging.getLogger('luigi-interface')\n\ntry:\n    import psycopg2\n    import psycopg2.errorcodes\n    import psycopg2.extensions\nexcept ImportError:\n    logger.warning(\"Loading postgres module without psycopg2 installed. Will crash at runtime if postgres functionality is used.\")\n\n\nclass RedshiftTarget(postgreslike.PostgreslikeTarget):\n    \"\"\"\n    Target for a resource in Redshift.\n\n    Redshift is similar to postgres with a few adjustments required by redshift\n    \"\"\"\n    marker_table = luigi.configuration.get_config().get('redshift', 'marker-table', 'table_updates')\n\n    def touch(self, connection=None):\n        \"\"\"Mark this update as complete.\n\n        Important: If the marker table doesn't exist,\n        the connection transaction will be aborted\n        and the connection reset. Then the marker table will be created.\n        \"\"\"\n        self.create_marker_table()\n\n        if connection is None:\n            connection = self.connect()\n            # if connection created here, we commit it here\n            connection.autocommit = True\n\n        # Automatic timestamps aren't supported by redshift, so we\n        # send the insertion date explicitly\n        connection.cursor().execute(\n                \"\"\"INSERT INTO {marker_table} (update_id, target_table, inserted)\n                     VALUES (%s, %s, %s);\n                \"\"\".format(marker_table=self.marker_table),\n                        (self.update_id, self.table,\n                        datetime.datetime.now())\n        )\n        # make sure update is properly marked\n        assert self.exists(connection)\n\n    def create_marker_table(self):\n        \"\"\"Create marker table if it doesn't exist.\n\n        Using a separate connection since the transaction might have to be reset\n        \"\"\"\n        connection = self.connect()\n        connection.autocommit = True\n        cursor = connection.cursor()\n        try:\n                cursor.execute(\n                        \"\"\" CREATE TABLE {marker_table} (\n                                        update_id TEXT PRIMARY KEY,\n                                        target_table TEXT,\n                                        inserted TIMESTAMP);\n                        \"\"\"\n                        .format(marker_table=self.marker_table)\n                )\n        except psycopg2.ProgrammingError, e:\n                if e.pgcode == psycopg2.errorcodes.DUPLICATE_TABLE:\n                        pass\n                else:\n                        raise\n        connection.close()\n\n\nclass S3CopyToTable(postgreslike.CopyToTable):\n    \"\"\"\n    Template task for inserting a data set into Redshift from s3.\n\n    Usage:\n    Subclass and override the required attributes:\n    `host`, `database`, `user`, `password`, `table`, `columns`,\n    `aws_access_key_id`, `aws_secret_access_key`, `s3_load_path`\n    \"\"\"\n\n    @abc.abstractproperty\n    def s3_load_path(self):\n        'override to return the load path'\n        return None\n\n    @abc.abstractproperty\n    def aws_access_key_id(self):\n        'override to return the key id'\n        return None\n\n    @abc.abstractproperty\n    def aws_secret_access_key(self):\n        'override to return the secret access key'\n        return None\n\n    @abc.abstractproperty\n    def copy_options(self):\n        '''Add extra copy options, for example:\n\n         TIMEFORMAT 'auto'\n         IGNOREHEADER 1\n         TRUNCATECOLUMNS\n         IGNOREBLANKLINES\n        '''\n        return ''\n\n    def run(self):\n        \"\"\"\n        If the target table doesn't exist, self.create_table will be called\n        to attempt to create the table.\n        \"\"\"\n        if not (self.table):\n            raise Exception(\"table need to be specified\")\n\n        connection = self.output().connect()\n\n        path = self.s3_load_path()\n        logger.info(\"Inserting file: %s\", path)\n\n        # attempt to copy the data into postgres\n        # if it fails because the target table doesn't exist\n        # try to create it by running self.create_table\n        for attempt in xrange(2):\n            try:\n                cursor = connection.cursor()\n                self.init_copy(connection)\n                self.copy(cursor, path)\n            except psycopg2.ProgrammingError, e:\n                if e.pgcode == psycopg2.errorcodes.UNDEFINED_TABLE and attempt == 0:\n                    # if first attempt fails with \"relation not found\",\n                    # try creating table\n                    logger.info(\"Creating table %s\", self.table)\n                    connection.reset()\n                    self.create_table(connection)\n                else:\n                    raise\n            else:\n                break\n\n        self.output().touch(connection)\n        connection.commit()\n\n        # commit and clean up\n        connection.close()\n\n    def copy(self, cursor, f):\n        '''\n        Defines copying from s3 into redshift\n        '''\n\n        cursor.execute(\"\"\"\n         COPY %s from '%s'\n         CREDENTIALS 'aws_access_key_id=%s;aws_secret_access_key=%s'\n         delimiter '%s'\n         %s\n         ;\"\"\" % (self.table, f, self.aws_access_key_id,\n                 self.aws_secret_access_key, self.column_separator,\n                 self.copy_options))\n\n    def output(self):\n        \"\"\"Returns a RedshiftTarget representing the inserted dataset.\n\n        Normally you don't override this.\n        \"\"\"\n        return RedshiftTarget(\n                host=self.host,\n                database=self.database,\n                user=self.user,\n                password=self.password,\n                table=self.table,\n                update_id=self.update_id())\n",
          "file_patch": "@@ -0,0 +1,177 @@\n+import datetime\n+import abc\n+import logging\n+import luigi.postgres\n+import luigi\n+from luigi.contrib import postgreslike\n+\n+logger = logging.getLogger('luigi-interface')\n+\n+try:\n+    import psycopg2\n+    import psycopg2.errorcodes\n+    import psycopg2.extensions\n+except ImportError:\n+    logger.warning(\"Loading postgres module without psycopg2 installed. Will crash at runtime if postgres functionality is used.\")\n+\n+\n+class RedshiftTarget(postgreslike.PostgreslikeTarget):\n+    \"\"\"\n+    Target for a resource in Redshift.\n+\n+    Redshift is similar to postgres with a few adjustments required by redshift\n+    \"\"\"\n+    marker_table = luigi.configuration.get_config().get('redshift', 'marker-table', 'table_updates')\n+\n+    def touch(self, connection=None):\n+        \"\"\"Mark this update as complete.\n+\n+        Important: If the marker table doesn't exist,\n+        the connection transaction will be aborted\n+        and the connection reset. Then the marker table will be created.\n+        \"\"\"\n+        self.create_marker_table()\n+\n+        if connection is None:\n+            connection = self.connect()\n+            # if connection created here, we commit it here\n+            connection.autocommit = True\n+\n+        # Automatic timestamps aren't supported by redshift, so we\n+        # send the insertion date explicitly\n+        connection.cursor().execute(\n+                \"\"\"INSERT INTO {marker_table} (update_id, target_table, inserted)\n+                     VALUES (%s, %s, %s);\n+                \"\"\".format(marker_table=self.marker_table),\n+                        (self.update_id, self.table,\n+                        datetime.datetime.now())\n+        )\n+        # make sure update is properly marked\n+        assert self.exists(connection)\n+\n+    def create_marker_table(self):\n+        \"\"\"Create marker table if it doesn't exist.\n+\n+        Using a separate connection since the transaction might have to be reset\n+        \"\"\"\n+        connection = self.connect()\n+        connection.autocommit = True\n+        cursor = connection.cursor()\n+        try:\n+                cursor.execute(\n+                        \"\"\" CREATE TABLE {marker_table} (\n+                                        update_id TEXT PRIMARY KEY,\n+                                        target_table TEXT,\n+                                        inserted TIMESTAMP);\n+                        \"\"\"\n+                        .format(marker_table=self.marker_table)\n+                )\n+        except psycopg2.ProgrammingError, e:\n+                if e.pgcode == psycopg2.errorcodes.DUPLICATE_TABLE:\n+                        pass\n+                else:\n+                        raise\n+        connection.close()\n+\n+\n+class S3CopyToTable(postgreslike.CopyToTable):\n+    \"\"\"\n+    Template task for inserting a data set into Redshift from s3.\n+\n+    Usage:\n+    Subclass and override the required attributes:\n+    `host`, `database`, `user`, `password`, `table`, `columns`,\n+    `aws_access_key_id`, `aws_secret_access_key`, `s3_load_path`\n+    \"\"\"\n+\n+    @abc.abstractproperty\n+    def s3_load_path(self):\n+        'override to return the load path'\n+        return None\n+\n+    @abc.abstractproperty\n+    def aws_access_key_id(self):\n+        'override to return the key id'\n+        return None\n+\n+    @abc.abstractproperty\n+    def aws_secret_access_key(self):\n+        'override to return the secret access key'\n+        return None\n+\n+    @abc.abstractproperty\n+    def copy_options(self):\n+        '''Add extra copy options, for example:\n+\n+         TIMEFORMAT 'auto'\n+         IGNOREHEADER 1\n+         TRUNCATECOLUMNS\n+         IGNOREBLANKLINES\n+        '''\n+        return ''\n+\n+    def run(self):\n+        \"\"\"\n+        If the target table doesn't exist, self.create_table will be called\n+        to attempt to create the table.\n+        \"\"\"\n+        if not (self.table):\n+            raise Exception(\"table need to be specified\")\n+\n+        connection = self.output().connect()\n+\n+        path = self.s3_load_path()\n+        logger.info(\"Inserting file: %s\", path)\n+\n+        # attempt to copy the data into postgres\n+        # if it fails because the target table doesn't exist\n+        # try to create it by running self.create_table\n+        for attempt in xrange(2):\n+            try:\n+                cursor = connection.cursor()\n+                self.init_copy(connection)\n+                self.copy(cursor, path)\n+            except psycopg2.ProgrammingError, e:\n+                if e.pgcode == psycopg2.errorcodes.UNDEFINED_TABLE and attempt == 0:\n+                    # if first attempt fails with \"relation not found\",\n+                    # try creating table\n+                    logger.info(\"Creating table %s\", self.table)\n+                    connection.reset()\n+                    self.create_table(connection)\n+                else:\n+                    raise\n+            else:\n+                break\n+\n+        self.output().touch(connection)\n+        connection.commit()\n+\n+        # commit and clean up\n+        connection.close()\n+\n+    def copy(self, cursor, f):\n+        '''\n+        Defines copying from s3 into redshift\n+        '''\n+\n+        cursor.execute(\"\"\"\n+         COPY %s from '%s'\n+         CREDENTIALS 'aws_access_key_id=%s;aws_secret_access_key=%s'\n+         delimiter '%s'\n+         %s\n+         ;\"\"\" % (self.table, f, self.aws_access_key_id,\n+                 self.aws_secret_access_key, self.column_separator,\n+                 self.copy_options))\n+\n+    def output(self):\n+        \"\"\"Returns a RedshiftTarget representing the inserted dataset.\n+\n+        Normally you don't override this.\n+        \"\"\"\n+        return RedshiftTarget(\n+                host=self.host,\n+                database=self.database,\n+                user=self.user,\n+                password=self.password,\n+                table=self.table,\n+                update_id=self.update_id())\n",
          "files_name_in_blame_commit": [
            "postgres.py",
            "postgreslike.py",
            "redshift.py"
          ]
        }
      }
    }
  }
}