{
  "id": "8",
  "blame_commit": {
    "commit": {
      "commit_id": "a045647a7f8e8ce3b501fb72899636a29bb2abf8",
      "commit_message": "Fixed various things that Landscape was complaining about",
      "commit_author": "Erik Bernhardsson",
      "commit_date": "2015-01-27 21:45:33",
      "commit_parent": "1939a06c1b205fcb20ba80223537345333f45c6a"
    },
    "function": {
      "function_name": "__init__",
      "function_code_before": "def __init__(self, id, last_active=None):\n    self.id = id\n    self.reference = None\n    self.last_active = last_active\n    self.started = time.time()\n    self.info = {}",
      "function_code_after": "def __init__(self, worker_id, last_active=None):\n    self.id = worker_id\n    self.reference = None\n    self.last_active = last_active\n    self.started = time.time()\n    self.info = {}",
      "function_before_start_line": 156,
      "function_before_end_line": 161,
      "function_after_start_line": 163,
      "function_after_end_line": 168,
      "function_before_token_count": 41,
      "function_after_token_count": 41,
      "functions_name_modified_file": [
        "add_failure",
        "get_worker_ids",
        "prune",
        "_schedulable",
        "clear",
        "has_excessive_failures",
        "_upstream_status",
        "_update_task_history",
        "can_disable",
        "ping",
        "get_pending_tasks",
        "_traverse_inverse_deps",
        "load",
        "inverse_dependencies",
        "num_failures",
        "task_search",
        "_recurse_deps",
        "get_worker",
        "get_active_workers",
        "worker_list",
        "add_task",
        "update_resources",
        "_update_priority",
        "task_list",
        "add_info",
        "get_active_tasks",
        "_has_resources",
        "_used_resources",
        "has_task",
        "fix_time",
        "_serialize_task",
        "task_history",
        "dep_graph",
        "__str__",
        "graph",
        "_get_default",
        "dump",
        "__repr__",
        "re_enable",
        "inactivate_tasks",
        "__init__",
        "update",
        "set_status",
        "add_worker",
        "get_task",
        "get_running_tasks",
        "_rank",
        "get_work",
        "re_enable_task",
        "fetch_error",
        "inactivate_workers"
      ],
      "functions_name_all_files": [
        "_get_with_default",
        "add_failure",
        "reload",
        "output",
        "reducer",
        "listdir",
        "can_disable",
        "id_to_name_and_params",
        "get_params",
        "mkdir",
        "load",
        "inverse_dependencies",
        "previous",
        "add_task",
        "get_all_params",
        "add_info",
        "_has_resources",
        "table_schema",
        "_serialize_task",
        "path",
        "on_failure",
        "fix_time",
        "from_str_params",
        "_chained_call",
        "requires_local",
        "bulk_complete",
        "__new__",
        "run_job",
        "getint",
        "get_data",
        "__init__",
        "internal_writer",
        "get_previous_completed",
        "_map_input",
        "getintdict",
        "task_module",
        "isdir",
        "init_combiner",
        "inactivate_workers",
        "extract_packages_archive",
        "prune",
        "run_hive_cmd",
        "clear",
        "_upstream_status",
        "ping",
        "_incr_counter",
        "_traverse_inverse_deps",
        "print_exception",
        "get_all_data",
        "get_config",
        "_recurse_deps",
        "open",
        "getboolean",
        "main",
        "update_resources",
        "_update_priority",
        "requires",
        "externalize",
        "fs",
        "run",
        "group",
        "run_reducer",
        "create_packages_archive",
        "initialized",
        "hiveconfs",
        "namespace",
        "to_str_params",
        "__str__",
        "clear_instance_cache",
        "attach",
        "extra_modules",
        "add_worker",
        "task_family",
        "init_local",
        "_reduce_input",
        "tasks_str",
        "_flush_batch_incr_counter",
        "get_param_values",
        "disable_instance_cache",
        "finish",
        "process_resources",
        "_schedulable",
        "has_excessive_failures",
        "instance",
        "_requires",
        "dereference",
        "skip",
        "input",
        "writer",
        "worker_list",
        "__enter__",
        "__hash__",
        "get_active_tasks",
        "add_link",
        "run_combiner",
        "task_history",
        "on_success",
        "mapper",
        "partition_spec",
        "run_and_track_hadoop_job",
        "get_reg",
        "init_reducer",
        "input_local",
        "reader",
        "graph",
        "trigger_event",
        "re_enable",
        "init_hadoop",
        "dump",
        "__repr__",
        "inactivate_tasks",
        "__eq__",
        "delegates",
        "fetch_task_failures",
        "__call__",
        "complete",
        "add_config_path",
        "get_task",
        "deps",
        "_setup_links",
        "getpaths",
        "getfloat",
        "run_hive",
        "_rank",
        "run_mapper",
        "flatten_output",
        "run_hive_script",
        "get_worker_ids",
        "job_runner",
        "_update_task_history",
        "task_wraps",
        "incr_counter",
        "kill_job",
        "set",
        "get_pending_tasks",
        "extra_files",
        "num_failures",
        "task_search",
        "internal_reader",
        "remove",
        "pickle_reader",
        "table_location",
        "get_worker",
        "get_active_workers",
        "flatten",
        "event_handler",
        "task_list",
        "_existing_partitions",
        "load_hive_cmd",
        "rename",
        "_used_resources",
        "hiverc",
        "has_task",
        "get_task_cls",
        "query",
        "exists",
        "_make_method",
        "dep_graph",
        "table_exists",
        "clone",
        "get",
        "init_mapper",
        "_input_iterator",
        "jobconfs",
        "_setup_remote",
        "_get_default",
        "common_params",
        "input_hadoop",
        "update",
        "__del__",
        "pickle_writer",
        "set_status",
        "get_extra_files",
        "prepare_outputs",
        "get_running_tasks",
        "sample",
        "requires_hadoop",
        "get_hive_syntax",
        "get_work",
        "re_enable_task",
        "fetch_error",
        "__exit__"
      ],
      "functions_name_co_evolved_modified_file": [
        "load",
        "_get_default",
        "add_task",
        "_upstream_status"
      ],
      "functions_name_co_evolved_all_files": [
        "job_runner",
        "_upstream_status",
        "set",
        "dereference",
        "_run_combiner",
        "mkdir",
        "_run_mapper",
        "load",
        "internal_reader",
        "remove",
        "table_location",
        "open",
        "add_task",
        "main",
        "group",
        "run_reducer",
        "run",
        "add_link",
        "run_combiner",
        "from_str_params",
        "table_exists",
        "run_job",
        "_run_reducer",
        "_add_link",
        "_dump",
        "dump",
        "_get_default",
        "move_dir",
        "__call__",
        "get_previous_completed",
        "get_extra_files",
        "sample",
        "run_mapper"
      ]
    },
    "file": {
      "file_name": "scheduler.py",
      "file_nloc": 606,
      "file_complexity": 225,
      "file_token_count": 4547,
      "file_before": "# Copyright (c) 2012 Spotify AB\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"); you may not\n# use this file except in compliance with the License. You may obtain a copy of\n# the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n# License for the specific language governing permissions and limitations under\n# the License.\n\nimport collections\nimport datetime\nimport functools\nimport itertools\nimport notifications\nimport os\nimport logging\nimport time\nimport cPickle as pickle\nimport task_history as history\nlogger = logging.getLogger(\"luigi.server\")\n\nfrom task_status import PENDING, FAILED, DONE, RUNNING, SUSPENDED, UNKNOWN, DISABLED\n\n\nclass Scheduler(object):\n\n    ''' Abstract base class\n\n    Note that the methods all take string arguments, not Task objects...\n    '''\n    add_task = NotImplemented\n    get_work = NotImplemented\n    ping = NotImplemented\n\nUPSTREAM_RUNNING = 'UPSTREAM_RUNNING'\nUPSTREAM_MISSING_INPUT = 'UPSTREAM_MISSING_INPUT'\nUPSTREAM_FAILED = 'UPSTREAM_FAILED'\nUPSTREAM_DISABLED = 'UPSTREAM_DISABLED'\n\nUPSTREAM_SEVERITY_ORDER = (\n    '',\n    UPSTREAM_RUNNING,\n    UPSTREAM_MISSING_INPUT,\n    UPSTREAM_FAILED,\n    UPSTREAM_DISABLED,\n)\nUPSTREAM_SEVERITY_KEY = lambda st: UPSTREAM_SEVERITY_ORDER.index(st)\nSTATUS_TO_UPSTREAM_MAP = {\n    FAILED: UPSTREAM_FAILED,\n    RUNNING: UPSTREAM_RUNNING,\n    PENDING: UPSTREAM_MISSING_INPUT,\n    DISABLED: UPSTREAM_DISABLED,\n}\n\n\n# We're passing around this config a lot, so let's put it on an object\nSchedulerConfig = collections.namedtuple('SchedulerConfig', [\n    'retry_delay', 'remove_delay', 'worker_disconnect_delay',\n    'disable_failures', 'disable_window', 'disable_persist', 'disable_time',\n    'max_shown_tasks',\n])\n\n\ndef fix_time(x):\n    # Backwards compatibility for a fix in Dec 2014. Prior to the fix, pickled state might store datetime objects\n    # Let's remove this function soon\n    if isinstance(x, datetime.datetime):\n        return time.mktime(x.timetuple())\n    else:\n        return x\n\n\nclass Failures(object):\n\n    \"\"\" This class tracks the number of failures in a given time window\n\n    Failures added are marked with the current timestamp, and this class counts\n    the number of failures in a sliding time window ending at the present.\n\n    \"\"\"\n\n    def __init__(self, window):\n        \"\"\" Initialize with the given window\n\n        :param window: how long to track failures for, as a float (number of seconds)\n        \"\"\"\n        self.window = window\n        self.failures = collections.deque()\n\n    def add_failure(self):\n        \"\"\" Add a failure event with the current timestamp \"\"\"\n        self.failures.append(time.time())\n\n    def num_failures(self):\n        \"\"\" Return the number of failures in the window \"\"\"\n        min_time = time.time() - self.window\n\n        while self.failures and fix_time(self.failures[0]) < min_time:\n            self.failures.popleft()\n\n        return len(self.failures)\n\n    def clear(self):\n        \"\"\" Clear the failure queue \"\"\"\n        self.failures.clear()\n\n\nclass Task(object):\n\n    def __init__(self, id, status, deps, resources={}, priority=0, family='', params={},\n                 disable_failures=None, disable_window=None):\n        self.id = id\n        self.stakeholders = set()  # workers ids that are somehow related to this task (i.e. don't prune while any of these workers are still active)\n        self.workers = set()  # workers ids that can perform task - task is 'BROKEN' if none of these workers are active\n        if deps is None:\n            self.deps = set()\n        else:\n            self.deps = set(deps)\n        self.status = status  # PENDING, RUNNING, FAILED or DONE\n        self.time = time.time()  # Timestamp when task was first added\n        self.retry = None\n        self.remove = None\n        self.worker_running = None  # the worker id that is currently running the task or None\n        self.time_running = None  # Timestamp when picked up by worker\n        self.expl = None\n        self.priority = priority\n        self.resources = resources\n        self.family = family\n        self.params = params\n        self.disable_failures = disable_failures\n        self.failures = Failures(disable_window)\n        self.scheduler_disable_time = None\n\n    def __repr__(self):\n        return \"Task(%r)\" % vars(self)\n\n    def add_failure(self):\n        self.failures.add_failure()\n\n    def has_excessive_failures(self):\n        return self.failures.num_failures() >= self.disable_failures\n\n    def can_disable(self):\n        return self.disable_failures is not None\n\n\nclass Worker(object):\n\n    \"\"\" Structure for tracking worker activity and keeping their references \"\"\"\n\n    def __init__(self, id, last_active=None):\n        self.id = id\n        self.reference = None  # reference to the worker in the real world. (Currently a dict containing just the host)\n        self.last_active = last_active  # seconds since epoch\n        self.started = time.time()  # seconds since epoch\n        self.info = {}\n\n    def add_info(self, info):\n        self.info.update(info)\n\n    def update(self, worker_reference):\n        if worker_reference:\n            self.reference = worker_reference\n        self.last_active = time.time()\n\n    def prune(self, config):\n        # Delete workers that haven't said anything for a while (probably killed)\n        if self.last_active + config.worker_disconnect_delay < time.time():\n            return True\n\n    def __str__(self):\n        return self.id\n\n\nclass SimpleTaskState(object):\n\n    ''' Keep track of the current state and handle persistance\n\n    The point of this class is to enable other ways to keep state, eg. by using a database\n    These will be implemented by creating an abstract base class that this and other classes\n    inherit from.\n    '''\n\n    def __init__(self, state_path):\n        self._state_path = state_path\n        self._tasks = {}  # map from id to a Task object\n        self._status_tasks = collections.defaultdict(dict)\n        self._active_workers = {}  # map from id to a Worker object\n\n    def dump(self):\n        state = (self._tasks, self._active_workers)\n        try:\n            with open(self._state_path, 'w') as fobj:\n                pickle.dump(state, fobj)\n        except IOError:\n            logger.warning(\"Failed saving scheduler state\", exc_info=1)\n        else:\n            logger.info(\"Saved state in %s\", self._state_path)\n\n    # prone to lead to crashes when old state is unpickled with updated code. TODO some kind of version control?\n    def load(self):\n        if os.path.exists(self._state_path):\n            logger.info(\"Attempting to load state from %s\", self._state_path)\n            try:\n                with open(self._state_path) as fobj:\n                    state = pickle.load(fobj)\n            except:\n                logger.exception(\"Error when loading state. Starting from clean slate.\")\n                return\n\n            self._tasks, self._active_workers = state\n            self._status_tasks = collections.defaultdict(dict)\n            for task in self._tasks.itervalues():\n                self._status_tasks[task.status][task.id] = task\n\n            # Convert from old format\n            # TODO: this is really ugly, we need something more future-proof\n            # Every time we add an attribute to the Worker class, this code needs to be updated\n            for k, v in self._active_workers.iteritems():\n                if isinstance(v, float):\n                    self._active_workers[k] = Worker(id=k, last_active=v)\n        else:\n            logger.info(\"No prior state file exists at %s. Starting with clean slate\", self._state_path)\n\n    def get_active_tasks(self, status=None):\n        if status:\n            for task in self._status_tasks[status].itervalues():\n                yield task\n        else:\n            for task in self._tasks.itervalues():\n                yield task\n\n    def get_running_tasks(self):\n        return self._status_tasks[RUNNING].itervalues()\n\n    def get_pending_tasks(self):\n        return itertools.chain.from_iterable(self._status_tasks[status].itervalues()\n                                             for status in [PENDING, RUNNING])\n\n    def get_task(self, task_id, default=None, setdefault=None):\n        if setdefault:\n            task = self._tasks.setdefault(task_id, setdefault)\n            self._status_tasks[task.status][task.id] = task\n            return task\n        else:\n            return self._tasks.get(task_id, default)\n\n    def has_task(self, task_id):\n        return task_id in self._tasks\n\n    def re_enable(self, task, config=None):\n        task.scheduler_disable_time = None\n        task.failures.clear()\n        if config:\n            self.set_status(task, FAILED, config)\n            task.failures.clear()\n\n    def set_status(self, task, new_status, config=None):\n        if new_status == FAILED:\n            assert config is not None\n\n        # not sure why we have SUSPENDED, as it can never be set\n        if new_status == SUSPENDED:\n            new_status = PENDING\n\n        if new_status == DISABLED and task.status == RUNNING:\n            return\n\n        if task.status == DISABLED:\n            if new_status == DONE:\n                self.re_enable(task)\n\n            # don't allow workers to override a scheduler disable\n            elif task.scheduler_disable_time is not None:\n                return\n\n        if new_status == FAILED and task.can_disable():\n            task.add_failure()\n            if task.has_excessive_failures():\n                task.scheduler_disable_time = time.time()\n                new_status = DISABLED\n                notifications.send_error_email(\n                    'Luigi Scheduler: DISABLED {task} due to excessive failures'.format(task=task.id),\n                    '{task} failed {failures} times in the last {window} seconds, so it is being '\n                    'disabled for {persist} seconds'.format(\n                        failures=config.disable_failures,\n                        task=task.id,\n                        window=config.disable_window,\n                        persist=config.disable_persist,\n                    ))\n        elif new_status == DISABLED:\n            task.scheduler_disable_time = None\n\n        self._status_tasks[task.status].pop(task.id)\n        self._status_tasks[new_status][task.id] = task\n        task.status = new_status\n\n    def prune(self, task, config):\n        remove = False\n\n        # Mark tasks with no remaining active stakeholders for deletion\n        if not task.stakeholders:\n            if task.remove is None:\n                logger.info(\"Task %r has stakeholders %r but none remain connected -> will remove \"\n                            \"task in %s seconds\", task.id, task.stakeholders, config.remove_delay)\n                task.remove = time.time() + config.remove_delay\n\n        # If a running worker disconnects, tag all its jobs as FAILED and subject it to the same retry logic\n        if task.status == RUNNING and task.worker_running and task.worker_running not in task.stakeholders:\n            logger.info(\"Task %r is marked as running by disconnected worker %r -> marking as \"\n                        \"FAILED with retry delay of %rs\", task.id, task.worker_running,\n                        config.retry_delay)\n            task.worker_running = None\n            self.set_status(task, FAILED, config)\n            task.retry = time.time() + config.retry_delay\n\n        # Re-enable task after the disable time expires\n        if task.status == DISABLED and task.scheduler_disable_time:\n            if time.time() - fix_time(task.scheduler_disable_time) > config.disable_time:\n                self.re_enable(task, config)\n\n        # Remove tasks that have no stakeholders\n        if task.remove and time.time() > task.remove:\n            logger.info(\"Removing task %r (no connected stakeholders)\", task.id)\n            remove = True\n\n        # Reset FAILED tasks to PENDING if max timeout is reached, and retry delay is >= 0\n        if task.status == FAILED and config.retry_delay >= 0 and task.retry < time.time():\n            self.set_status(task, PENDING, config)\n\n        return remove\n\n    def inactivate_tasks(self, delete_tasks):\n        # The terminology is a bit confusing: we used to \"delete\" tasks when they became inactive,\n        # but with a pluggable state storage, you might very well want to keep some history of\n        # older tasks as well. That's why we call it \"inactivate\" (as in the verb)\n        for task in delete_tasks:\n            task_obj = self._tasks.pop(task)\n            self._status_tasks[task_obj.status].pop(task)\n\n    def get_active_workers(self, last_active_lt=None):\n        for worker in self._active_workers.itervalues():\n            if last_active_lt is not None and worker.last_active >= last_active_lt:\n                continue\n            yield worker\n\n    def get_worker_ids(self):\n        return self._active_workers.keys()  # only used for unit tests\n\n    def get_worker(self, worker_id):\n        return self._active_workers.setdefault(worker_id, Worker(worker_id))\n\n    def inactivate_workers(self, delete_workers):\n        # Mark workers as inactive\n        for worker in delete_workers:\n            self._active_workers.pop(worker)\n\n        # remove workers from tasks\n        for task in self.get_active_tasks():\n            task.stakeholders.difference_update(delete_workers)\n            task.workers.difference_update(delete_workers)\n\n\nclass CentralPlannerScheduler(Scheduler):\n\n    ''' Async scheduler that can handle multiple workers etc\n\n    Can be run locally or on a server (using RemoteScheduler + server.Server).\n    '''\n\n    def __init__(self, retry_delay=900.0, remove_delay=600.0, worker_disconnect_delay=60.0,\n                 state_path='/var/lib/luigi-server/state.pickle', task_history=None,\n                 resources=None, disable_persist=0, disable_window=0, disable_failures=None,\n                 max_shown_tasks=100000):\n        '''\n        (all arguments are in seconds)\n        Keyword Arguments:\n        retry_delay -- How long after a Task fails to try it again, or -1 to never retry\n        remove_delay -- How long after a Task finishes to remove it from the scheduler\n        state_path -- Path to state file (tasks and active workers)\n        worker_disconnect_delay -- If a worker hasn't communicated for this long, remove it from active workers\n        '''\n        self._config = SchedulerConfig(\n            retry_delay=retry_delay,\n            remove_delay=remove_delay,\n            worker_disconnect_delay=worker_disconnect_delay,\n            disable_failures=disable_failures,\n            disable_window=disable_window,\n            disable_persist=disable_persist,\n            disable_time=disable_persist,\n            max_shown_tasks=max_shown_tasks,\n        )\n\n        self._state = SimpleTaskState(state_path)\n        self._task_history = task_history or history.NopHistory()\n        self._resources = resources\n        self._make_task = functools.partial(\n            Task, disable_failures=disable_failures,\n            disable_window=disable_window)\n\n    def load(self):\n        self._state.load()\n\n    def dump(self):\n        self._state.dump()\n\n    def prune(self):\n        logger.info(\"Starting pruning of task graph\")\n        remove_workers = []\n        for worker in self._state.get_active_workers():\n            if worker.prune(self._config):\n                logger.info(\"Worker %s timed out (no contact for >=%ss)\", worker, self._config.worker_disconnect_delay)\n                remove_workers.append(worker.id)\n\n        self._state.inactivate_workers(remove_workers)\n\n        remove_tasks = []\n        for task in self._state.get_active_tasks():\n            if self._state.prune(task, self._config):\n                remove_tasks.append(task.id)\n\n        self._state.inactivate_tasks(remove_tasks)\n\n        logger.info(\"Done pruning task graph\")\n\n    def update(self, worker_id, worker_reference=None):\n        \"\"\" Keep track of whenever the worker was last active \"\"\"\n        worker = self._state.get_worker(worker_id)\n        worker.update(worker_reference)\n\n    def _update_priority(self, task, prio, worker):\n        \"\"\" Update priority of the given task\n\n        Priority can only be increased. If the task doesn't exist, a placeholder\n        task is created to preserve priority when the task is later scheduled.\n        \"\"\"\n        task.priority = prio = max(prio, task.priority)\n        for dep in task.deps or []:\n            t = self._state.get_task(dep)\n            if t is not None and prio > t.priority:\n                self._update_priority(t, prio, worker)\n\n    def add_task(self, worker, task_id, status=PENDING, runnable=True,\n                 deps=None, new_deps=None, expl=None, resources=None,\n                 priority=0, family='', params={}, **kwargs):\n        \"\"\"\n        * Add task identified by task_id if it doesn't exist\n        * If deps is not None, update dependency list\n        * Update status of task\n        * Add additional workers/stakeholders\n        * Update priority when needed\n        \"\"\"\n        self.update(worker)\n\n        task = self._state.get_task(task_id, setdefault=self._make_task(\n            id=task_id, status=PENDING, deps=deps, resources=resources,\n            priority=priority, family=family, params=params))\n\n        # for setting priority, we'll sometimes create tasks with unset family and params\n        if not task.family:\n            task.family = family\n        if not task.params:\n            task.params = params\n\n        if task.remove is not None:\n            task.remove = None  # unmark task for removal so it isn't removed after being added\n\n        if not (task.status == RUNNING and status == PENDING):\n            # don't allow re-scheduling of task while it is running, it must either fail or succeed first\n            if status == PENDING or status != task.status:\n                # Update the DB only if there was a acctual change, to prevent noise.\n                # We also check for status == PENDING b/c that's the default value\n                # (so checking for status != task.status woule lie)\n                self._update_task_history(task_id, status)\n            self._state.set_status(task, PENDING if status == SUSPENDED else status, self._config)\n            if status == FAILED:\n                task.retry = time.time() + self._config.retry_delay\n\n        if deps is not None:\n            task.deps = set(deps)\n\n        if new_deps is not None:\n            task.deps.update(new_deps)\n\n        task.stakeholders.add(worker)\n        task.resources = resources\n\n        # Task dependencies might not exist yet. Let's create dummy tasks for them for now.\n        # Otherwise the task dependencies might end up being pruned if scheduling takes a long time\n        for dep in task.deps or []:\n            t = self._state.get_task(dep, setdefault=self._make_task(id=dep, status=UNKNOWN, deps=None, priority=priority))\n            t.stakeholders.add(worker)\n\n        self._update_priority(task, priority, worker)\n\n        if runnable:\n            task.workers.add(worker)\n\n        if expl is not None:\n            task.expl = expl\n\n    def add_worker(self, worker, info, **kwargs):\n        self._state.get_worker(worker).add_info(info)\n\n    def update_resources(self, **resources):\n        if self._resources is None:\n            self._resources = {}\n        self._resources.update(resources)\n\n    def _has_resources(self, needed_resources, used_resources):\n        if needed_resources is None:\n            return True\n\n        available_resources = self._resources or {}\n        for resource, amount in needed_resources.iteritems():\n            if amount + used_resources[resource] > available_resources.get(resource, 1):\n                return False\n        return True\n\n    def _used_resources(self):\n        used_resources = collections.defaultdict(int)\n        if self._resources is not None:\n            for task in self._state.get_active_tasks():\n                if task.status == RUNNING and task.resources:\n                    for resource, amount in task.resources.iteritems():\n                        used_resources[resource] += amount\n        return used_resources\n\n    def _rank(self):\n        ''' Return worker's rank function for task scheduling '''\n        dependents = collections.defaultdict(int)\n\n        def not_done(t):\n            task = self._state.get_task(t, default=None)\n            return task is None or task.status != DONE\n        for task in self._state.get_pending_tasks():\n            if task.status != DONE:\n                deps = filter(not_done, task.deps)\n                inverse_num_deps = 1.0 / max(len(deps), 1)\n                for dep in deps:\n                    dependents[dep] += inverse_num_deps\n\n        return lambda task: (task.priority, dependents[task.id], -task.time)\n\n    def _schedulable(self, task):\n        if task.status != PENDING:\n            return False\n        for dep in task.deps:\n            dep_task = self._state.get_task(dep, default=None)\n            if dep_task is None or dep_task.status != DONE:\n                return False\n        return True\n\n    def get_work(self, worker, host=None, **kwargs):\n        # TODO: remove any expired nodes\n\n        # Algo: iterate over all nodes, find the highest priority node no dependencies and available\n        # resources.\n\n        # Resource checking looks both at currently available resources and at which resources would\n        # be available if all running tasks died and we rescheduled all workers greedily. We do both\n        # checks in order to prevent a worker with many low-priority tasks from starving other\n        # workers with higher priority tasks that share the same resources.\n\n        # TODO: remove tasks that can't be done, figure out if the worker has absolutely\n        # nothing it can wait for\n\n        # Return remaining tasks that have no FAILED descendents\n        self.update(worker, {'host': host})\n        best_task = None\n        best_task_id = None\n        locally_pending_tasks = 0\n        running_tasks = []\n\n        used_resources = self._used_resources()\n        greedy_resources = collections.defaultdict(int)\n        n_unique_pending = 0\n        greedy_workers = dict((worker.id, worker.info.get('workers', 1))\n                              for worker in self._state.get_active_workers())\n\n        tasks = list(self._state.get_pending_tasks())\n        tasks.sort(key=self._rank(), reverse=True)\n\n        for task in tasks:\n            if task.status == 'RUNNING' and worker in task.workers:\n                # Return a list of currently running tasks to the client,\n                # makes it easier to troubleshoot\n                other_worker = self._state.get_worker(task.worker_running)\n                more_info = {'task_id': task.id, 'worker': str(other_worker)}\n                if other_worker is not None:\n                    more_info.update(other_worker.info)\n                    running_tasks.append(more_info)\n\n            if task.status == PENDING and worker in task.workers:\n                locally_pending_tasks += 1\n                if len(task.workers) == 1:\n                    n_unique_pending += 1\n\n            if task.status == RUNNING and task.worker_running in greedy_workers:\n                greedy_workers[task.worker_running] -= 1\n                for resource, amount in (task.resources or {}).iteritems():\n                    greedy_resources[resource] += amount\n\n            if not best_task and self._schedulable(task) and self._has_resources(task.resources, greedy_resources):\n                if worker in task.workers and self._has_resources(task.resources, used_resources):\n                    best_task = task\n                    best_task_id = task.id\n                else:\n                    for task_worker in task.workers:\n                        if greedy_workers.get(task_worker, 0) > 0:\n                            # use up a worker\n                            greedy_workers[task_worker] -= 1\n\n                            # keep track of the resources used in greedy scheduling\n                            for resource, amount in (task.resources or {}).iteritems():\n                                greedy_resources[resource] += amount\n\n                            break\n\n        if best_task:\n            self._state.set_status(best_task, RUNNING, self._config)\n            best_task.worker_running = worker\n            best_task.time_running = time.time()\n            self._update_task_history(best_task.id, RUNNING, host=host)\n\n        return {'n_pending_tasks': locally_pending_tasks,\n                'n_unique_pending': n_unique_pending,\n                'task_id': best_task_id,\n                'running_tasks': running_tasks}\n\n    def ping(self, worker, **kwargs):\n        self.update(worker)\n\n    def _upstream_status(self, task_id, upstream_status_table):\n        if task_id in upstream_status_table:\n            return upstream_status_table[task_id]\n        elif self._state.has_task(task_id):\n            task_stack = [task_id]\n\n            while task_stack:\n                dep_id = task_stack.pop()\n                if self._state.has_task(dep_id):\n                    dep = self._state.get_task(dep_id)\n                    if dep_id not in upstream_status_table:\n                        if dep.status == PENDING and dep.deps:\n                            task_stack = task_stack + [dep_id] + list(dep.deps)\n                            upstream_status_table[dep_id] = ''  # will be updated postorder\n                        else:\n                            dep_status = STATUS_TO_UPSTREAM_MAP.get(dep.status, '')\n                            upstream_status_table[dep_id] = dep_status\n                    elif upstream_status_table[dep_id] == '' and dep.deps:\n                        # This is the postorder update step when we set the\n                        # status based on the previously calculated child elements\n                        upstream_status = [upstream_status_table.get(id, '') for id in dep.deps]\n                        upstream_status.append('')  # to handle empty list\n                        status = max(upstream_status, key=UPSTREAM_SEVERITY_KEY)\n                        upstream_status_table[dep_id] = status\n            return upstream_status_table[dep_id]\n\n    def _serialize_task(self, task_id, include_deps=True):\n        task = self._state.get_task(task_id)\n        ret = {\n            'deps': list(task.deps),\n            'status': task.status,\n            'workers': list(task.workers),\n            'worker_running': task.worker_running,\n            'time_running': getattr(task, \"time_running\", None),\n            'start_time': task.time,\n            'params': task.params,\n            'name': task.family,\n            'priority': task.priority,\n            'resources': task.resources,\n        }\n        if include_deps:\n            ret['deps'] = list(task.deps)\n        return ret\n\n    def graph(self, **kwargs):\n        self.prune()\n        serialized = {}\n        for task in self._state.get_active_tasks():\n            serialized[task.id] = self._serialize_task(task.id)\n        return serialized\n\n    def _recurse_deps(self, task_id, serialized):\n        if task_id not in serialized:\n            task = self._state.get_task(task_id)\n            if task is None or not task.family:\n                logger.warn('Missing task for id [%s]', task_id)\n\n                # try to infer family and params from task_id\n                try:\n                    family, _, param_str = task_id.rstrip(')').partition('(')\n                    params = dict(param.split('=') for param in param_str.split(', '))\n                except:\n                    family, params = '', {}\n                serialized[task_id] = {\n                    'deps': [],\n                    'status': UNKNOWN,\n                    'workers': [],\n                    'start_time': UNKNOWN,\n                    'params': params,\n                    'name': family,\n                    'priority': 0,\n                }\n            else:\n                serialized[task_id] = self._serialize_task(task_id)\n                for dep in task.deps:\n                    self._recurse_deps(dep, serialized)\n\n    def dep_graph(self, task_id, **kwargs):\n        self.prune()\n        serialized = {}\n        if self._state.has_task(task_id):\n            self._recurse_deps(task_id, serialized)\n        return serialized\n\n    def task_list(self, status, upstream_status, limit=True, **kwargs):\n        ''' query for a subset of tasks by status '''\n        self.prune()\n        result = {}\n        upstream_status_table = {}  # used to memoize upstream status\n        for task in self._state.get_active_tasks(status):\n            if (task.status != PENDING or not upstream_status or\n                    upstream_status == self._upstream_status(task.id, upstream_status_table)):\n                serialized = self._serialize_task(task.id, False)\n                result[task.id] = serialized\n        if limit and len(result) > self._config.max_shown_tasks:\n            return {'num_tasks': len(result)}\n        return result\n\n    def worker_list(self, include_running=True, **kwargs):\n        self.prune()\n        workers = [\n            dict(\n                name=worker.id,\n                last_active=worker.last_active,\n                started=getattr(worker, 'started', None),\n                **worker.info\n            ) for worker in self._state.get_active_workers()]\n        workers.sort(key=lambda worker: worker['started'], reverse=True)\n        if include_running:\n            running = collections.defaultdict(dict)\n            num_pending = collections.defaultdict(int)\n            num_uniques = collections.defaultdict(int)\n            for task in self._state.get_pending_tasks():\n                if task.status == RUNNING and task.worker_running:\n                    running[task.worker_running][task.id] = self._serialize_task(task.id, False)\n                elif task.status == PENDING:\n                    for worker in task.workers:\n                        num_pending[worker] += 1\n                    if len(task.workers) == 1:\n                        num_uniques[list(task.workers)[0]] += 1\n            for worker in workers:\n                tasks = running[worker['name']]\n                worker['num_running'] = len(tasks)\n                worker['num_pending'] = num_pending[worker['name']]\n                worker['num_uniques'] = num_uniques[worker['name']]\n                worker['running'] = tasks\n        return workers\n\n    def inverse_dependencies(self, task_id, **kwargs):\n        self.prune()\n        serialized = {}\n        if self._state.has_task(task_id):\n            self._traverse_inverse_deps(task_id, serialized)\n        return serialized\n\n    def _traverse_inverse_deps(self, task_id, serialized):\n        stack = [task_id]\n        serialized[task_id] = self._serialize_task(task_id)\n        while len(stack) > 0:\n            curr_id = stack.pop()\n            for task in self._state.get_active_tasks():\n                if curr_id in task.deps:\n                    serialized[curr_id][\"deps\"].append(task.id)\n                    if task.id not in serialized:\n                        serialized[task.id] = self._serialize_task(task.id)\n                        serialized[task.id][\"deps\"] = []\n                        stack.append(task.id)\n\n    def task_search(self, task_str, **kwargs):\n        ''' query for a subset of tasks by task_id '''\n        self.prune()\n        result = collections.defaultdict(dict)\n        for task in self._state.get_active_tasks():\n            if task.id.find(task_str) != -1:\n                serialized = self._serialize_task(task.id, False)\n                result[task.status][task.id] = serialized\n        return result\n\n    def re_enable_task(self, task_id):\n        serialized = {}\n        task = self._state.get_task(task_id)\n        if task and task.status == DISABLED and task.scheduler_disable_time:\n            self._state.re_enable(task, self._config)\n            serialized = self._serialize_task(task_id)\n        return serialized\n\n    def fetch_error(self, task_id, **kwargs):\n        if self._state.has_task(task_id):\n            return {\"taskId\": task_id, \"error\": self._state.get_task(task_id).expl}\n        else:\n            return {\"taskId\": task_id, \"error\": \"\"}\n\n    def _update_task_history(self, task_id, status, host=None):\n        try:\n            if status == DONE or status == FAILED:\n                successful = (status == DONE)\n                self._task_history.task_finished(task_id, successful)\n            elif status == PENDING:\n                self._task_history.task_scheduled(task_id)\n            elif status == RUNNING:\n                self._task_history.task_started(task_id, host)\n        except:\n            logger.warning(\"Error saving Task history\", exc_info=1)\n\n    @property\n    def task_history(self):\n        # Used by server.py to expose the calls\n        return self._task_history\n",
      "file_after": "# Copyright (c) 2012 Spotify AB\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"); you may not\n# use this file except in compliance with the License. You may obtain a copy of\n# the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n# License for the specific language governing permissions and limitations under\n# the License.\n\nimport collections\nimport datetime\nimport functools\nimport itertools\nimport notifications\nimport os\nimport logging\nimport time\nimport cPickle as pickle\nimport task_history as history\nlogger = logging.getLogger(\"luigi.server\")\n\nfrom task_status import PENDING, FAILED, DONE, RUNNING, SUSPENDED, UNKNOWN, DISABLED\n\n\nclass Scheduler(object):\n\n    ''' Abstract base class\n\n    Note that the methods all take string arguments, not Task objects...\n    '''\n    add_task = NotImplemented\n    get_work = NotImplemented\n    ping = NotImplemented\n\nUPSTREAM_RUNNING = 'UPSTREAM_RUNNING'\nUPSTREAM_MISSING_INPUT = 'UPSTREAM_MISSING_INPUT'\nUPSTREAM_FAILED = 'UPSTREAM_FAILED'\nUPSTREAM_DISABLED = 'UPSTREAM_DISABLED'\n\nUPSTREAM_SEVERITY_ORDER = (\n    '',\n    UPSTREAM_RUNNING,\n    UPSTREAM_MISSING_INPUT,\n    UPSTREAM_FAILED,\n    UPSTREAM_DISABLED,\n)\nUPSTREAM_SEVERITY_KEY = UPSTREAM_SEVERITY_ORDER.index\nSTATUS_TO_UPSTREAM_MAP = {\n    FAILED: UPSTREAM_FAILED,\n    RUNNING: UPSTREAM_RUNNING,\n    PENDING: UPSTREAM_MISSING_INPUT,\n    DISABLED: UPSTREAM_DISABLED,\n}\n\n\n# We're passing around this config a lot, so let's put it on an object\nSchedulerConfig = collections.namedtuple('SchedulerConfig', [\n    'retry_delay', 'remove_delay', 'worker_disconnect_delay',\n    'disable_failures', 'disable_window', 'disable_persist', 'disable_time',\n    'max_shown_tasks',\n])\n\n\ndef fix_time(x):\n    # Backwards compatibility for a fix in Dec 2014. Prior to the fix, pickled state might store datetime objects\n    # Let's remove this function soon\n    if isinstance(x, datetime.datetime):\n        return time.mktime(x.timetuple())\n    else:\n        return x\n\n\nclass Failures(object):\n\n    \"\"\" This class tracks the number of failures in a given time window\n\n    Failures added are marked with the current timestamp, and this class counts\n    the number of failures in a sliding time window ending at the present.\n\n    \"\"\"\n\n    def __init__(self, window):\n        \"\"\" Initialize with the given window\n\n        :param window: how long to track failures for, as a float (number of seconds)\n        \"\"\"\n        self.window = window\n        self.failures = collections.deque()\n\n    def add_failure(self):\n        \"\"\" Add a failure event with the current timestamp \"\"\"\n        self.failures.append(time.time())\n\n    def num_failures(self):\n        \"\"\" Return the number of failures in the window \"\"\"\n        min_time = time.time() - self.window\n\n        while self.failures and fix_time(self.failures[0]) < min_time:\n            self.failures.popleft()\n\n        return len(self.failures)\n\n    def clear(self):\n        \"\"\" Clear the failure queue \"\"\"\n        self.failures.clear()\n\n\ndef _get_default(x, default):\n    if x is not None:\n        return x\n    else:\n        return default\n\n\nclass Task(object):\n\n    def __init__(self, task_id, status, deps, resources=None, priority=0, family='', params=None,\n                 disable_failures=None, disable_window=None):\n        self.id = task_id\n        self.stakeholders = set()  # workers ids that are somehow related to this task (i.e. don't prune while any of these workers are still active)\n        self.workers = set()  # workers ids that can perform task - task is 'BROKEN' if none of these workers are active\n        if deps is None:\n            self.deps = set()\n        else:\n            self.deps = set(deps)\n        self.status = status  # PENDING, RUNNING, FAILED or DONE\n        self.time = time.time()  # Timestamp when task was first added\n        self.retry = None\n        self.remove = None\n        self.worker_running = None  # the worker id that is currently running the task or None\n        self.time_running = None  # Timestamp when picked up by worker\n        self.expl = None\n        self.priority = priority\n        self.resources = _get_default(resources, {})\n        self.family = family\n        self.params = _get_default(params, {})\n        self.disable_failures = disable_failures\n        self.failures = Failures(disable_window)\n        self.scheduler_disable_time = None\n\n    def __repr__(self):\n        return \"Task(%r)\" % vars(self)\n\n    def add_failure(self):\n        self.failures.add_failure()\n\n    def has_excessive_failures(self):\n        return self.failures.num_failures() >= self.disable_failures\n\n    def can_disable(self):\n        return self.disable_failures is not None\n\n\nclass Worker(object):\n\n    \"\"\" Structure for tracking worker activity and keeping their references \"\"\"\n\n    def __init__(self, worker_id, last_active=None):\n        self.id = worker_id\n        self.reference = None  # reference to the worker in the real world. (Currently a dict containing just the host)\n        self.last_active = last_active  # seconds since epoch\n        self.started = time.time()  # seconds since epoch\n        self.info = {}\n\n    def add_info(self, info):\n        self.info.update(info)\n\n    def update(self, worker_reference):\n        if worker_reference:\n            self.reference = worker_reference\n        self.last_active = time.time()\n\n    def prune(self, config):\n        # Delete workers that haven't said anything for a while (probably killed)\n        if self.last_active + config.worker_disconnect_delay < time.time():\n            return True\n\n    def __str__(self):\n        return self.id\n\n\nclass SimpleTaskState(object):\n\n    ''' Keep track of the current state and handle persistance\n\n    The point of this class is to enable other ways to keep state, eg. by using a database\n    These will be implemented by creating an abstract base class that this and other classes\n    inherit from.\n    '''\n\n    def __init__(self, state_path):\n        self._state_path = state_path\n        self._tasks = {}  # map from id to a Task object\n        self._status_tasks = collections.defaultdict(dict)\n        self._active_workers = {}  # map from id to a Worker object\n\n    def dump(self):\n        state = (self._tasks, self._active_workers)\n        try:\n            with open(self._state_path, 'w') as fobj:\n                pickle.dump(state, fobj)\n        except IOError:\n            logger.warning(\"Failed saving scheduler state\", exc_info=1)\n        else:\n            logger.info(\"Saved state in %s\", self._state_path)\n\n    # prone to lead to crashes when old state is unpickled with updated code. TODO some kind of version control?\n    def load(self):\n        if os.path.exists(self._state_path):\n            logger.info(\"Attempting to load state from %s\", self._state_path)\n            try:\n                with open(self._state_path) as fobj:\n                    state = pickle.load(fobj)\n            except:\n                logger.exception(\"Error when loading state. Starting from clean slate.\")\n                return\n\n            self._tasks, self._active_workers = state\n            self._status_tasks = collections.defaultdict(dict)\n            for task in self._tasks.itervalues():\n                self._status_tasks[task.status][task.id] = task\n\n            # Convert from old format\n            # TODO: this is really ugly, we need something more future-proof\n            # Every time we add an attribute to the Worker class, this code needs to be updated\n            for k, v in self._active_workers.iteritems():\n                if isinstance(v, float):\n                    self._active_workers[k] = Worker(worker_id=k, last_active=v)\n        else:\n            logger.info(\"No prior state file exists at %s. Starting with clean slate\", self._state_path)\n\n    def get_active_tasks(self, status=None):\n        if status:\n            for task in self._status_tasks[status].itervalues():\n                yield task\n        else:\n            for task in self._tasks.itervalues():\n                yield task\n\n    def get_running_tasks(self):\n        return self._status_tasks[RUNNING].itervalues()\n\n    def get_pending_tasks(self):\n        return itertools.chain.from_iterable(self._status_tasks[status].itervalues()\n                                             for status in [PENDING, RUNNING])\n\n    def get_task(self, task_id, default=None, setdefault=None):\n        if setdefault:\n            task = self._tasks.setdefault(task_id, setdefault)\n            self._status_tasks[task.status][task.id] = task\n            return task\n        else:\n            return self._tasks.get(task_id, default)\n\n    def has_task(self, task_id):\n        return task_id in self._tasks\n\n    def re_enable(self, task, config=None):\n        task.scheduler_disable_time = None\n        task.failures.clear()\n        if config:\n            self.set_status(task, FAILED, config)\n            task.failures.clear()\n\n    def set_status(self, task, new_status, config=None):\n        if new_status == FAILED:\n            assert config is not None\n\n        # not sure why we have SUSPENDED, as it can never be set\n        if new_status == SUSPENDED:\n            new_status = PENDING\n\n        if new_status == DISABLED and task.status == RUNNING:\n            return\n\n        if task.status == DISABLED:\n            if new_status == DONE:\n                self.re_enable(task)\n\n            # don't allow workers to override a scheduler disable\n            elif task.scheduler_disable_time is not None:\n                return\n\n        if new_status == FAILED and task.can_disable():\n            task.add_failure()\n            if task.has_excessive_failures():\n                task.scheduler_disable_time = time.time()\n                new_status = DISABLED\n                notifications.send_error_email(\n                    'Luigi Scheduler: DISABLED {task} due to excessive failures'.format(task=task.id),\n                    '{task} failed {failures} times in the last {window} seconds, so it is being '\n                    'disabled for {persist} seconds'.format(\n                        failures=config.disable_failures,\n                        task=task.id,\n                        window=config.disable_window,\n                        persist=config.disable_persist,\n                    ))\n        elif new_status == DISABLED:\n            task.scheduler_disable_time = None\n\n        self._status_tasks[task.status].pop(task.id)\n        self._status_tasks[new_status][task.id] = task\n        task.status = new_status\n\n    def prune(self, task, config):\n        remove = False\n\n        # Mark tasks with no remaining active stakeholders for deletion\n        if not task.stakeholders:\n            if task.remove is None:\n                logger.info(\"Task %r has stakeholders %r but none remain connected -> will remove \"\n                            \"task in %s seconds\", task.id, task.stakeholders, config.remove_delay)\n                task.remove = time.time() + config.remove_delay\n\n        # If a running worker disconnects, tag all its jobs as FAILED and subject it to the same retry logic\n        if task.status == RUNNING and task.worker_running and task.worker_running not in task.stakeholders:\n            logger.info(\"Task %r is marked as running by disconnected worker %r -> marking as \"\n                        \"FAILED with retry delay of %rs\", task.id, task.worker_running,\n                        config.retry_delay)\n            task.worker_running = None\n            self.set_status(task, FAILED, config)\n            task.retry = time.time() + config.retry_delay\n\n        # Re-enable task after the disable time expires\n        if task.status == DISABLED and task.scheduler_disable_time:\n            if time.time() - fix_time(task.scheduler_disable_time) > config.disable_time:\n                self.re_enable(task, config)\n\n        # Remove tasks that have no stakeholders\n        if task.remove and time.time() > task.remove:\n            logger.info(\"Removing task %r (no connected stakeholders)\", task.id)\n            remove = True\n\n        # Reset FAILED tasks to PENDING if max timeout is reached, and retry delay is >= 0\n        if task.status == FAILED and config.retry_delay >= 0 and task.retry < time.time():\n            self.set_status(task, PENDING, config)\n\n        return remove\n\n    def inactivate_tasks(self, delete_tasks):\n        # The terminology is a bit confusing: we used to \"delete\" tasks when they became inactive,\n        # but with a pluggable state storage, you might very well want to keep some history of\n        # older tasks as well. That's why we call it \"inactivate\" (as in the verb)\n        for task in delete_tasks:\n            task_obj = self._tasks.pop(task)\n            self._status_tasks[task_obj.status].pop(task)\n\n    def get_active_workers(self, last_active_lt=None):\n        for worker in self._active_workers.itervalues():\n            if last_active_lt is not None and worker.last_active >= last_active_lt:\n                continue\n            yield worker\n\n    def get_worker_ids(self):\n        return self._active_workers.keys()  # only used for unit tests\n\n    def get_worker(self, worker_id):\n        return self._active_workers.setdefault(worker_id, Worker(worker_id))\n\n    def inactivate_workers(self, delete_workers):\n        # Mark workers as inactive\n        for worker in delete_workers:\n            self._active_workers.pop(worker)\n\n        # remove workers from tasks\n        for task in self.get_active_tasks():\n            task.stakeholders.difference_update(delete_workers)\n            task.workers.difference_update(delete_workers)\n\n\nclass CentralPlannerScheduler(Scheduler):\n\n    ''' Async scheduler that can handle multiple workers etc\n\n    Can be run locally or on a server (using RemoteScheduler + server.Server).\n    '''\n\n    def __init__(self, retry_delay=900.0, remove_delay=600.0, worker_disconnect_delay=60.0,\n                 state_path='/var/lib/luigi-server/state.pickle', task_history=None,\n                 resources=None, disable_persist=0, disable_window=0, disable_failures=None,\n                 max_shown_tasks=100000):\n        '''\n        (all arguments are in seconds)\n        Keyword Arguments:\n        retry_delay -- How long after a Task fails to try it again, or -1 to never retry\n        remove_delay -- How long after a Task finishes to remove it from the scheduler\n        state_path -- Path to state file (tasks and active workers)\n        worker_disconnect_delay -- If a worker hasn't communicated for this long, remove it from active workers\n        '''\n        self._config = SchedulerConfig(\n            retry_delay=retry_delay,\n            remove_delay=remove_delay,\n            worker_disconnect_delay=worker_disconnect_delay,\n            disable_failures=disable_failures,\n            disable_window=disable_window,\n            disable_persist=disable_persist,\n            disable_time=disable_persist,\n            max_shown_tasks=max_shown_tasks,\n        )\n\n        self._state = SimpleTaskState(state_path)\n        self._task_history = task_history or history.NopHistory()\n        self._resources = resources\n        self._make_task = functools.partial(\n            Task, disable_failures=disable_failures,\n            disable_window=disable_window)\n\n    def load(self):\n        self._state.load()\n\n    def dump(self):\n        self._state.dump()\n\n    def prune(self):\n        logger.info(\"Starting pruning of task graph\")\n        remove_workers = []\n        for worker in self._state.get_active_workers():\n            if worker.prune(self._config):\n                logger.info(\"Worker %s timed out (no contact for >=%ss)\", worker, self._config.worker_disconnect_delay)\n                remove_workers.append(worker.id)\n\n        self._state.inactivate_workers(remove_workers)\n\n        remove_tasks = []\n        for task in self._state.get_active_tasks():\n            if self._state.prune(task, self._config):\n                remove_tasks.append(task.id)\n\n        self._state.inactivate_tasks(remove_tasks)\n\n        logger.info(\"Done pruning task graph\")\n\n    def update(self, worker_id, worker_reference=None):\n        \"\"\" Keep track of whenever the worker was last active \"\"\"\n        worker = self._state.get_worker(worker_id)\n        worker.update(worker_reference)\n\n    def _update_priority(self, task, prio, worker):\n        \"\"\" Update priority of the given task\n\n        Priority can only be increased. If the task doesn't exist, a placeholder\n        task is created to preserve priority when the task is later scheduled.\n        \"\"\"\n        task.priority = prio = max(prio, task.priority)\n        for dep in task.deps or []:\n            t = self._state.get_task(dep)\n            if t is not None and prio > t.priority:\n                self._update_priority(t, prio, worker)\n\n    def add_task(self, worker, task_id, status=PENDING, runnable=True,\n                 deps=None, new_deps=None, expl=None, resources=None,\n                 priority=0, family='', params=None, **kwargs):\n        \"\"\"\n        * Add task identified by task_id if it doesn't exist\n        * If deps is not None, update dependency list\n        * Update status of task\n        * Add additional workers/stakeholders\n        * Update priority when needed\n        \"\"\"\n        self.update(worker)\n\n        task = self._state.get_task(task_id, setdefault=self._make_task(\n            task_id=task_id, status=PENDING, deps=deps, resources=resources,\n            priority=priority, family=family, params=params))\n\n        # for setting priority, we'll sometimes create tasks with unset family and params\n        if not task.family:\n            task.family = family\n        if not task.params:\n            task.params = _get_default(params, {})\n\n        if task.remove is not None:\n            task.remove = None  # unmark task for removal so it isn't removed after being added\n\n        if not (task.status == RUNNING and status == PENDING):\n            # don't allow re-scheduling of task while it is running, it must either fail or succeed first\n            if status == PENDING or status != task.status:\n                # Update the DB only if there was a acctual change, to prevent noise.\n                # We also check for status == PENDING b/c that's the default value\n                # (so checking for status != task.status woule lie)\n                self._update_task_history(task_id, status)\n            self._state.set_status(task, PENDING if status == SUSPENDED else status, self._config)\n            if status == FAILED:\n                task.retry = time.time() + self._config.retry_delay\n\n        if deps is not None:\n            task.deps = set(deps)\n\n        if new_deps is not None:\n            task.deps.update(new_deps)\n\n        task.stakeholders.add(worker)\n        task.resources = resources\n\n        # Task dependencies might not exist yet. Let's create dummy tasks for them for now.\n        # Otherwise the task dependencies might end up being pruned if scheduling takes a long time\n        for dep in task.deps or []:\n            t = self._state.get_task(dep, setdefault=self._make_task(task_id=dep, status=UNKNOWN, deps=None, priority=priority))\n            t.stakeholders.add(worker)\n\n        self._update_priority(task, priority, worker)\n\n        if runnable:\n            task.workers.add(worker)\n\n        if expl is not None:\n            task.expl = expl\n\n    def add_worker(self, worker, info, **kwargs):\n        self._state.get_worker(worker).add_info(info)\n\n    def update_resources(self, **resources):\n        if self._resources is None:\n            self._resources = {}\n        self._resources.update(resources)\n\n    def _has_resources(self, needed_resources, used_resources):\n        if needed_resources is None:\n            return True\n\n        available_resources = self._resources or {}\n        for resource, amount in needed_resources.iteritems():\n            if amount + used_resources[resource] > available_resources.get(resource, 1):\n                return False\n        return True\n\n    def _used_resources(self):\n        used_resources = collections.defaultdict(int)\n        if self._resources is not None:\n            for task in self._state.get_active_tasks():\n                if task.status == RUNNING and task.resources:\n                    for resource, amount in task.resources.iteritems():\n                        used_resources[resource] += amount\n        return used_resources\n\n    def _rank(self):\n        ''' Return worker's rank function for task scheduling '''\n        dependents = collections.defaultdict(int)\n\n        def not_done(t):\n            task = self._state.get_task(t, default=None)\n            return task is None or task.status != DONE\n        for task in self._state.get_pending_tasks():\n            if task.status != DONE:\n                deps = filter(not_done, task.deps)\n                inverse_num_deps = 1.0 / max(len(deps), 1)\n                for dep in deps:\n                    dependents[dep] += inverse_num_deps\n\n        return lambda task: (task.priority, dependents[task.id], -task.time)\n\n    def _schedulable(self, task):\n        if task.status != PENDING:\n            return False\n        for dep in task.deps:\n            dep_task = self._state.get_task(dep, default=None)\n            if dep_task is None or dep_task.status != DONE:\n                return False\n        return True\n\n    def get_work(self, worker, host=None, **kwargs):\n        # TODO: remove any expired nodes\n\n        # Algo: iterate over all nodes, find the highest priority node no dependencies and available\n        # resources.\n\n        # Resource checking looks both at currently available resources and at which resources would\n        # be available if all running tasks died and we rescheduled all workers greedily. We do both\n        # checks in order to prevent a worker with many low-priority tasks from starving other\n        # workers with higher priority tasks that share the same resources.\n\n        # TODO: remove tasks that can't be done, figure out if the worker has absolutely\n        # nothing it can wait for\n\n        # Return remaining tasks that have no FAILED descendents\n        self.update(worker, {'host': host})\n        best_task = None\n        best_task_id = None\n        locally_pending_tasks = 0\n        running_tasks = []\n\n        used_resources = self._used_resources()\n        greedy_resources = collections.defaultdict(int)\n        n_unique_pending = 0\n        greedy_workers = dict((worker.id, worker.info.get('workers', 1))\n                              for worker in self._state.get_active_workers())\n\n        tasks = list(self._state.get_pending_tasks())\n        tasks.sort(key=self._rank(), reverse=True)\n\n        for task in tasks:\n            if task.status == 'RUNNING' and worker in task.workers:\n                # Return a list of currently running tasks to the client,\n                # makes it easier to troubleshoot\n                other_worker = self._state.get_worker(task.worker_running)\n                more_info = {'task_id': task.id, 'worker': str(other_worker)}\n                if other_worker is not None:\n                    more_info.update(other_worker.info)\n                    running_tasks.append(more_info)\n\n            if task.status == PENDING and worker in task.workers:\n                locally_pending_tasks += 1\n                if len(task.workers) == 1:\n                    n_unique_pending += 1\n\n            if task.status == RUNNING and task.worker_running in greedy_workers:\n                greedy_workers[task.worker_running] -= 1\n                for resource, amount in (task.resources or {}).iteritems():\n                    greedy_resources[resource] += amount\n\n            if not best_task and self._schedulable(task) and self._has_resources(task.resources, greedy_resources):\n                if worker in task.workers and self._has_resources(task.resources, used_resources):\n                    best_task = task\n                    best_task_id = task.id\n                else:\n                    for task_worker in task.workers:\n                        if greedy_workers.get(task_worker, 0) > 0:\n                            # use up a worker\n                            greedy_workers[task_worker] -= 1\n\n                            # keep track of the resources used in greedy scheduling\n                            for resource, amount in (task.resources or {}).iteritems():\n                                greedy_resources[resource] += amount\n\n                            break\n\n        if best_task:\n            self._state.set_status(best_task, RUNNING, self._config)\n            best_task.worker_running = worker\n            best_task.time_running = time.time()\n            self._update_task_history(best_task.id, RUNNING, host=host)\n\n        return {'n_pending_tasks': locally_pending_tasks,\n                'n_unique_pending': n_unique_pending,\n                'task_id': best_task_id,\n                'running_tasks': running_tasks}\n\n    def ping(self, worker, **kwargs):\n        self.update(worker)\n\n    def _upstream_status(self, task_id, upstream_status_table):\n        if task_id in upstream_status_table:\n            return upstream_status_table[task_id]\n        elif self._state.has_task(task_id):\n            task_stack = [task_id]\n\n            while task_stack:\n                dep_id = task_stack.pop()\n                if self._state.has_task(dep_id):\n                    dep = self._state.get_task(dep_id)\n                    if dep_id not in upstream_status_table:\n                        if dep.status == PENDING and dep.deps:\n                            task_stack = task_stack + [dep_id] + list(dep.deps)\n                            upstream_status_table[dep_id] = ''  # will be updated postorder\n                        else:\n                            dep_status = STATUS_TO_UPSTREAM_MAP.get(dep.status, '')\n                            upstream_status_table[dep_id] = dep_status\n                    elif upstream_status_table[dep_id] == '' and dep.deps:\n                        # This is the postorder update step when we set the\n                        # status based on the previously calculated child elements\n                        upstream_status = [upstream_status_table.get(task_id, '') for task_id in dep.deps]\n                        upstream_status.append('')  # to handle empty list\n                        status = max(upstream_status, key=UPSTREAM_SEVERITY_KEY)\n                        upstream_status_table[dep_id] = status\n            return upstream_status_table[dep_id]\n\n    def _serialize_task(self, task_id, include_deps=True):\n        task = self._state.get_task(task_id)\n        ret = {\n            'deps': list(task.deps),\n            'status': task.status,\n            'workers': list(task.workers),\n            'worker_running': task.worker_running,\n            'time_running': getattr(task, \"time_running\", None),\n            'start_time': task.time,\n            'params': task.params,\n            'name': task.family,\n            'priority': task.priority,\n            'resources': task.resources,\n        }\n        if include_deps:\n            ret['deps'] = list(task.deps)\n        return ret\n\n    def graph(self, **kwargs):\n        self.prune()\n        serialized = {}\n        for task in self._state.get_active_tasks():\n            serialized[task.id] = self._serialize_task(task.id)\n        return serialized\n\n    def _recurse_deps(self, task_id, serialized):\n        if task_id not in serialized:\n            task = self._state.get_task(task_id)\n            if task is None or not task.family:\n                logger.warn('Missing task for id [%s]', task_id)\n\n                # try to infer family and params from task_id\n                try:\n                    family, _, param_str = task_id.rstrip(')').partition('(')\n                    params = dict(param.split('=') for param in param_str.split(', '))\n                except:\n                    family, params = '', {}\n                serialized[task_id] = {\n                    'deps': [],\n                    'status': UNKNOWN,\n                    'workers': [],\n                    'start_time': UNKNOWN,\n                    'params': params,\n                    'name': family,\n                    'priority': 0,\n                }\n            else:\n                serialized[task_id] = self._serialize_task(task_id)\n                for dep in task.deps:\n                    self._recurse_deps(dep, serialized)\n\n    def dep_graph(self, task_id, **kwargs):\n        self.prune()\n        serialized = {}\n        if self._state.has_task(task_id):\n            self._recurse_deps(task_id, serialized)\n        return serialized\n\n    def task_list(self, status, upstream_status, limit=True, **kwargs):\n        ''' query for a subset of tasks by status '''\n        self.prune()\n        result = {}\n        upstream_status_table = {}  # used to memoize upstream status\n        for task in self._state.get_active_tasks(status):\n            if (task.status != PENDING or not upstream_status or\n                    upstream_status == self._upstream_status(task.id, upstream_status_table)):\n                serialized = self._serialize_task(task.id, False)\n                result[task.id] = serialized\n        if limit and len(result) > self._config.max_shown_tasks:\n            return {'num_tasks': len(result)}\n        return result\n\n    def worker_list(self, include_running=True, **kwargs):\n        self.prune()\n        workers = [\n            dict(\n                name=worker.id,\n                last_active=worker.last_active,\n                started=getattr(worker, 'started', None),\n                **worker.info\n            ) for worker in self._state.get_active_workers()]\n        workers.sort(key=lambda worker: worker['started'], reverse=True)\n        if include_running:\n            running = collections.defaultdict(dict)\n            num_pending = collections.defaultdict(int)\n            num_uniques = collections.defaultdict(int)\n            for task in self._state.get_pending_tasks():\n                if task.status == RUNNING and task.worker_running:\n                    running[task.worker_running][task.id] = self._serialize_task(task.id, False)\n                elif task.status == PENDING:\n                    for worker in task.workers:\n                        num_pending[worker] += 1\n                    if len(task.workers) == 1:\n                        num_uniques[list(task.workers)[0]] += 1\n            for worker in workers:\n                tasks = running[worker['name']]\n                worker['num_running'] = len(tasks)\n                worker['num_pending'] = num_pending[worker['name']]\n                worker['num_uniques'] = num_uniques[worker['name']]\n                worker['running'] = tasks\n        return workers\n\n    def inverse_dependencies(self, task_id, **kwargs):\n        self.prune()\n        serialized = {}\n        if self._state.has_task(task_id):\n            self._traverse_inverse_deps(task_id, serialized)\n        return serialized\n\n    def _traverse_inverse_deps(self, task_id, serialized):\n        stack = [task_id]\n        serialized[task_id] = self._serialize_task(task_id)\n        while len(stack) > 0:\n            curr_id = stack.pop()\n            for task in self._state.get_active_tasks():\n                if curr_id in task.deps:\n                    serialized[curr_id][\"deps\"].append(task.id)\n                    if task.id not in serialized:\n                        serialized[task.id] = self._serialize_task(task.id)\n                        serialized[task.id][\"deps\"] = []\n                        stack.append(task.id)\n\n    def task_search(self, task_str, **kwargs):\n        ''' query for a subset of tasks by task_id '''\n        self.prune()\n        result = collections.defaultdict(dict)\n        for task in self._state.get_active_tasks():\n            if task.id.find(task_str) != -1:\n                serialized = self._serialize_task(task.id, False)\n                result[task.status][task.id] = serialized\n        return result\n\n    def re_enable_task(self, task_id):\n        serialized = {}\n        task = self._state.get_task(task_id)\n        if task and task.status == DISABLED and task.scheduler_disable_time:\n            self._state.re_enable(task, self._config)\n            serialized = self._serialize_task(task_id)\n        return serialized\n\n    def fetch_error(self, task_id, **kwargs):\n        if self._state.has_task(task_id):\n            return {\"taskId\": task_id, \"error\": self._state.get_task(task_id).expl}\n        else:\n            return {\"taskId\": task_id, \"error\": \"\"}\n\n    def _update_task_history(self, task_id, status, host=None):\n        try:\n            if status == DONE or status == FAILED:\n                successful = (status == DONE)\n                self._task_history.task_finished(task_id, successful)\n            elif status == PENDING:\n                self._task_history.task_scheduled(task_id)\n            elif status == RUNNING:\n                self._task_history.task_started(task_id, host)\n        except:\n            logger.warning(\"Error saving Task history\", exc_info=1)\n\n    @property\n    def task_history(self):\n        # Used by server.py to expose the calls\n        return self._task_history\n",
      "file_patch": "@@ -49,7 +49,7 @@ UPSTREAM_SEVERITY_ORDER = (\n     UPSTREAM_FAILED,\n     UPSTREAM_DISABLED,\n )\n-UPSTREAM_SEVERITY_KEY = lambda st: UPSTREAM_SEVERITY_ORDER.index(st)\n+UPSTREAM_SEVERITY_KEY = UPSTREAM_SEVERITY_ORDER.index\n STATUS_TO_UPSTREAM_MAP = {\n     FAILED: UPSTREAM_FAILED,\n     RUNNING: UPSTREAM_RUNNING,\n@@ -110,11 +110,18 @@ class Failures(object):\n         self.failures.clear()\n \n \n+def _get_default(x, default):\n+    if x is not None:\n+        return x\n+    else:\n+        return default\n+\n+\n class Task(object):\n \n-    def __init__(self, id, status, deps, resources={}, priority=0, family='', params={},\n+    def __init__(self, task_id, status, deps, resources=None, priority=0, family='', params=None,\n                  disable_failures=None, disable_window=None):\n-        self.id = id\n+        self.id = task_id\n         self.stakeholders = set()  # workers ids that are somehow related to this task (i.e. don't prune while any of these workers are still active)\n         self.workers = set()  # workers ids that can perform task - task is 'BROKEN' if none of these workers are active\n         if deps is None:\n@@ -129,9 +136,9 @@ class Task(object):\n         self.time_running = None  # Timestamp when picked up by worker\n         self.expl = None\n         self.priority = priority\n-        self.resources = resources\n+        self.resources = _get_default(resources, {})\n         self.family = family\n-        self.params = params\n+        self.params = _get_default(params, {})\n         self.disable_failures = disable_failures\n         self.failures = Failures(disable_window)\n         self.scheduler_disable_time = None\n@@ -153,8 +160,8 @@ class Worker(object):\n \n     \"\"\" Structure for tracking worker activity and keeping their references \"\"\"\n \n-    def __init__(self, id, last_active=None):\n-        self.id = id\n+    def __init__(self, worker_id, last_active=None):\n+        self.id = worker_id\n         self.reference = None  # reference to the worker in the real world. (Currently a dict containing just the host)\n         self.last_active = last_active  # seconds since epoch\n         self.started = time.time()  # seconds since epoch\n@@ -223,7 +230,7 @@ class SimpleTaskState(object):\n             # Every time we add an attribute to the Worker class, this code needs to be updated\n             for k, v in self._active_workers.iteritems():\n                 if isinstance(v, float):\n-                    self._active_workers[k] = Worker(id=k, last_active=v)\n+                    self._active_workers[k] = Worker(worker_id=k, last_active=v)\n         else:\n             logger.info(\"No prior state file exists at %s. Starting with clean slate\", self._state_path)\n \n@@ -447,7 +454,7 @@ class CentralPlannerScheduler(Scheduler):\n \n     def add_task(self, worker, task_id, status=PENDING, runnable=True,\n                  deps=None, new_deps=None, expl=None, resources=None,\n-                 priority=0, family='', params={}, **kwargs):\n+                 priority=0, family='', params=None, **kwargs):\n         \"\"\"\n         * Add task identified by task_id if it doesn't exist\n         * If deps is not None, update dependency list\n@@ -458,14 +465,14 @@ class CentralPlannerScheduler(Scheduler):\n         self.update(worker)\n \n         task = self._state.get_task(task_id, setdefault=self._make_task(\n-            id=task_id, status=PENDING, deps=deps, resources=resources,\n+            task_id=task_id, status=PENDING, deps=deps, resources=resources,\n             priority=priority, family=family, params=params))\n \n         # for setting priority, we'll sometimes create tasks with unset family and params\n         if not task.family:\n             task.family = family\n         if not task.params:\n-            task.params = params\n+            task.params = _get_default(params, {})\n \n         if task.remove is not None:\n             task.remove = None  # unmark task for removal so it isn't removed after being added\n@@ -493,7 +500,7 @@ class CentralPlannerScheduler(Scheduler):\n         # Task dependencies might not exist yet. Let's create dummy tasks for them for now.\n         # Otherwise the task dependencies might end up being pruned if scheduling takes a long time\n         for dep in task.deps or []:\n-            t = self._state.get_task(dep, setdefault=self._make_task(id=dep, status=UNKNOWN, deps=None, priority=priority))\n+            t = self._state.get_task(dep, setdefault=self._make_task(task_id=dep, status=UNKNOWN, deps=None, priority=priority))\n             t.stakeholders.add(worker)\n \n         self._update_priority(task, priority, worker)\n@@ -656,7 +663,7 @@ class CentralPlannerScheduler(Scheduler):\n                     elif upstream_status_table[dep_id] == '' and dep.deps:\n                         # This is the postorder update step when we set the\n                         # status based on the previously calculated child elements\n-                        upstream_status = [upstream_status_table.get(id, '') for id in dep.deps]\n+                        upstream_status = [upstream_status_table.get(task_id, '') for task_id in dep.deps]\n                         upstream_status.append('')  # to handle empty list\n                         status = max(upstream_status, key=UPSTREAM_SEVERITY_KEY)\n                         upstream_status_table[dep_id] = status\n",
      "files_name_in_blame_commit": [
        "hive.py",
        "hadoop.py",
        "scheduler.py",
        "wordcount.py",
        "target.py",
        "task.py",
        "util.py",
        "top_artists.py",
        "configuration.py",
        "mrrunner.py",
        "mock.py"
      ]
    }
  },
  "commits_modify_file_before_fix": {
    "size": 126
  },
  "recursive_blame_commits": {
    "recursive_blame_function_lines": {
      "156": {
        "commit_id": "ea8476655ba625d0e2d51cf86a179603bf49c707",
        "line_code": "    def __init__(self, id, last_active=None):",
        "commit_date": "2014-06-02 08:28:56",
        "valid": 1
      },
      "157": {
        "commit_id": "c75085b3c812c8054ef76d39676ab473e45b9600",
        "line_code": "        self.id = id",
        "commit_date": "2014-05-08 16:30:46",
        "valid": 1
      },
      "158": {
        "commit_id": "c75085b3c812c8054ef76d39676ab473e45b9600",
        "line_code": "        self.reference = None  # reference to the worker in the real world. (Currently a dict containing just the host)",
        "commit_date": "2014-05-08 16:30:46",
        "valid": 1
      },
      "159": {
        "commit_id": "ea8476655ba625d0e2d51cf86a179603bf49c707",
        "line_code": "        self.last_active = last_active  # seconds since epoch",
        "commit_date": "2014-06-02 08:28:56",
        "valid": 1
      },
      "160": {
        "commit_id": "d1faaee79913174df556f1b1c2791c11a7f6739b",
        "line_code": "        self.started = time.time()  # seconds since epoch",
        "commit_date": "2014-10-07 14:33:56",
        "valid": 1
      },
      "161": {
        "commit_id": "77e027b52ea8b0645f33ff90b7ad9371a707c13b",
        "line_code": "        self.info = {}",
        "commit_date": "2014-06-29 22:31:19",
        "valid": 1
      }
    },
    "commits": {
      "d1faaee79913174df556f1b1c2791c11a7f6739b": {
        "commit": {
          "commit_id": "d1faaee79913174df556f1b1c2791c11a7f6739b",
          "commit_message": "Adds a worker list view to the visualizer",
          "commit_author": "Dave Buchfuhrer",
          "commit_date": "2014-10-07 14:33:56",
          "commit_parent": "fe96e485225e91fa1682650386b9f4990944d434"
        },
        "function": {
          "function_name": "__init__",
          "function_code_before": "",
          "function_code_after": "",
          "function_before_start_line": "",
          "function_before_end_line": "",
          "function_after_start_line": "",
          "function_after_end_line": "",
          "function_before_token_count": 0,
          "function_after_token_count": 0,
          "functions_name_modified_file": [
            "_update_task_history",
            "prune",
            "_upstream_status",
            "ping",
            "_traverse_inverse_deps",
            "load",
            "inverse_dependencies",
            "task_search",
            "_recurse_deps",
            "worker_list",
            "add_task",
            "update_resources",
            "_update_priority",
            "task_list",
            "add_info",
            "_has_resources",
            "_used_resources",
            "_serialize_task",
            "task_history",
            "dep_graph",
            "__str__",
            "graph",
            "dump",
            "__repr__",
            "__init__",
            "update",
            "_not_schedulable",
            "add_worker",
            "_rank",
            "get_work",
            "fetch_error"
          ],
          "functions_name_all_files": [
            "_post",
            "indexByProperty",
            "load",
            "inverse_dependencies",
            "add_task",
            "add_info",
            "_has_resources",
            "showErrorTrace",
            "_serialize_task",
            "_get_work",
            "_run_task",
            "bindListEvents",
            "__init__",
            "stop",
            "prune",
            "_upstream_status",
            "_email_complete_error",
            "ping",
            "inverse_dep_graph",
            "_traverse_inverse_deps",
            "_log_unexpected_error",
            "_log_remote_tasks",
            "_recurse_deps",
            "LuigiAPI",
            "update_resources",
            "_update_priority",
            "renderTemplate",
            "run",
            "_add_worker",
            "loadTemplates",
            "_keep_alive",
            "_check_complete",
            "__str__",
            "jsonRPC",
            "switchTab",
            "_request",
            "processWorker",
            "add_worker",
            "_log_complete_error",
            "_get",
            "_generate_worker_info",
            "flatten_running",
            "(anonymous)",
            "_email_unexpected_error",
            "worker_list",
            "_validate_dependency",
            "depGraphCallback",
            "formatTime",
            "task_history",
            "_handle_next_task",
            "graph",
            "dump",
            "__repr__",
            "_check_complete_value",
            "renderTasks",
            "_rank",
            "_update_task_history",
            "_purge_children",
            "processHashChange",
            "_wait",
            "bindTaskEvents",
            "renderWorkers",
            "task_search",
            "entryList",
            "flatten",
            "task_list",
            "_used_resources",
            "add",
            "dep_graph",
            "bindGraphEvents",
            "_sleeper",
            "update",
            "_not_schedulable",
            "_validate_task",
            "_add",
            "get_work",
            "fetch_error"
          ],
          "functions_name_co_evolved_modified_file": [
            "worker_list"
          ],
          "functions_name_co_evolved_all_files": [
            "renderWorkers",
            "_add_worker",
            "processWorker",
            "worker_list",
            "renderTasks",
            "processHashChange",
            "add",
            "flatten_running",
            "(anonymous)",
            "jsonRPC"
          ]
        },
        "file": {
          "file_name": "scheduler.py",
          "file_nloc": 408,
          "file_complexity": 154,
          "file_token_count": 3192,
          "file_before": "# Copyright (c) 2012 Spotify AB\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"); you may not\n# use this file except in compliance with the License. You may obtain a copy of\n# the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n# License for the specific language governing permissions and limitations under\n# the License.\n\nimport collections\nimport datetime\nimport os\nimport logging\nimport time\nimport cPickle as pickle\nimport task_history as history\nlogger = logging.getLogger(\"luigi.server\")\n\nfrom task_status import PENDING, FAILED, DONE, RUNNING, SUSPENDED, UNKNOWN\n\n\nclass Scheduler(object):\n    ''' Abstract base class\n\n    Note that the methods all take string arguments, not Task objects...\n    '''\n    add_task = NotImplemented\n    get_work = NotImplemented\n    ping = NotImplemented\n\nUPSTREAM_RUNNING = 'UPSTREAM_RUNNING'\nUPSTREAM_MISSING_INPUT = 'UPSTREAM_MISSING_INPUT'\nUPSTREAM_FAILED = 'UPSTREAM_FAILED'\n\nUPSTREAM_SEVERITY_ORDER = ('', UPSTREAM_RUNNING, UPSTREAM_MISSING_INPUT, UPSTREAM_FAILED)\nUPSTREAM_SEVERITY_KEY = lambda st: UPSTREAM_SEVERITY_ORDER.index(st)\nSTATUS_TO_UPSTREAM_MAP = {FAILED: UPSTREAM_FAILED, RUNNING: UPSTREAM_RUNNING, PENDING: UPSTREAM_MISSING_INPUT}\n\n\nclass Task(object):\n    def __init__(self, status, deps, resources={}, priority=0, family='', params={}):\n        self.stakeholders = set()  # workers ids that are somehow related to this task (i.e. don't prune while any of these workers are still active)\n        self.workers = set()  # workers ids that can perform task - task is 'BROKEN' if none of these workers are active\n        if deps is None:\n            self.deps = set()\n        else:\n            self.deps = set(deps)\n        self.status = status  # PENDING, RUNNING, FAILED or DONE\n        self.time = time.time()  # Timestamp when task was first added\n        self.retry = None\n        self.remove = None\n        self.worker_running = None  # the worker id that is currently running the task or None\n        self.time_running = None  # Timestamp when picked up by worker\n        self.expl = None\n        self.priority = priority\n        self.resources = resources\n        self.family = family\n        self.params = params\n\n    def __repr__(self):\n        return \"Task(%r)\" % vars(self)\n\n\nclass Worker(object):\n    \"\"\" Structure for tracking worker activity and keeping their references \"\"\"\n    def __init__(self, id, last_active=None):\n        self.id = id\n        self.reference = None  # reference to the worker in the real world. (Currently a dict containing just the host)\n        self.last_active = last_active  # seconds since epoch\n        self.info = {}\n\n    def add_info(self, info):\n        self.info.update(info)\n\n    def __str__(self):\n        return self.id\n\n\nclass CentralPlannerScheduler(Scheduler):\n    ''' Async scheduler that can handle multiple workers etc\n\n    Can be run locally or on a server (using RemoteScheduler + server.Server).\n    '''\n\n    def __init__(self, retry_delay=900.0, remove_delay=600.0, worker_disconnect_delay=60.0,\n                 state_path='/var/lib/luigi-server/state.pickle', task_history=None,\n                 resources=None):\n        '''\n        (all arguments are in seconds)\n        Keyword Arguments:\n        retry_delay -- How long after a Task fails to try it again, or -1 to never retry\n        remove_delay -- How long after a Task finishes to remove it from the scheduler\n        state_path -- Path to state file (tasks and active workers)\n        worker_disconnect_delay -- If a worker hasn't communicated for this long, remove it from active workers\n        '''\n        self._state_path = state_path\n        self._tasks = {}  # map from id to a Task object\n        self._retry_delay = retry_delay\n        self._remove_delay = remove_delay\n        self._worker_disconnect_delay = worker_disconnect_delay\n        self._active_workers = {}  # map from id to a Worker object\n        self._task_history = task_history or history.NopHistory()\n        self._resources = resources\n\n    def dump(self):\n        state = (self._tasks, self._active_workers)\n        try:\n            with open(self._state_path, 'w') as fobj:\n                pickle.dump(state, fobj)\n        except IOError:\n            logger.warning(\"Failed saving scheduler state\", exc_info=1)\n        else:\n            logger.info(\"Saved state in %s\", self._state_path)\n\n    # prone to lead to crashes when old state is unpickled with updated code. TODO some kind of version control?\n    def load(self):\n        if os.path.exists(self._state_path):\n            logger.info(\"Attempting to load state from %s\", self._state_path)\n            try:\n                with open(self._state_path) as fobj:\n                    state = pickle.load(fobj)\n            except:\n                logger.exception(\"Error when loading state. Starting from clean slate.\")\n                return\n\n            self._tasks, self._active_workers = state\n\n            # Convert from old format\n            # TODO: this is really ugly, we need something more future-proof\n            # Every time we add an attribute to the Worker class, this code needs to be updated\n            for k, v in self._active_workers.iteritems():\n                if isinstance(v, float):\n                    self._active_workers[k] = Worker(id=k, last_active=v)\n        else:\n            logger.info(\"No prior state file exists at %s. Starting with clean slate\", self._state_path)\n\n    def prune(self):\n        logger.info(\"Starting pruning of task graph\")\n        # Delete workers that haven't said anything for a while (probably killed)\n        delete_workers = []\n        for worker in self._active_workers.values():\n            if worker.last_active < time.time() - self._worker_disconnect_delay:\n                logger.info(\"Worker %s timed out (no contact for >=%ss)\", worker, self._worker_disconnect_delay)\n                delete_workers.append(worker.id)\n\n        for worker in delete_workers:\n            self._active_workers.pop(worker)\n\n        remaining_workers = set(self._active_workers.keys())\n\n        # Mark tasks with no remaining active stakeholders for deletion\n        for task_id, task in self._tasks.iteritems():\n            if not task.stakeholders.intersection(remaining_workers):\n                if task.remove is None:\n                    logger.info(\"Task %r has stakeholders %r but none remain connected -> will remove task in %s seconds\", task_id, task.stakeholders, self._remove_delay)\n                    task.remove = time.time() + self._remove_delay\n\n            if task.status == RUNNING and task.worker_running and task.worker_running not in remaining_workers:\n                # If a running worker disconnects, tag all its jobs as FAILED and subject it to the same retry logic\n                logger.info(\"Task %r is marked as running by disconnected worker %r -> marking as FAILED with retry delay of %rs\", task_id, task.worker_running, self._retry_delay)\n                task.worker_running = None\n                task.status = FAILED\n                task.retry = time.time() + self._retry_delay\n\n        # Remove tasks that have no stakeholders\n        remove_tasks = []\n        for task_id, task in self._tasks.iteritems():\n            if task.remove and time.time() > task.remove:\n                logger.info(\"Removing task %r (no connected stakeholders)\", task_id)\n                remove_tasks.append(task_id)\n\n        for task_id in remove_tasks:\n            self._tasks.pop(task_id)\n\n        # Reset FAILED tasks to PENDING if max timeout is reached, and retry delay is >= 0\n        for task in self._tasks.values():\n            if task.status == FAILED and self._retry_delay >= 0 and task.retry < time.time():\n                task.status = PENDING\n        logger.info(\"Done pruning task graph\")\n\n    def update(self, worker_id, worker_reference=None):\n        \"\"\" Keep track of whenever the worker was last active \"\"\"\n        worker = self._active_workers.setdefault(worker_id, Worker(worker_id))\n        if worker_reference:\n            worker.reference = worker_reference\n        worker.last_active = time.time()\n\n    def _update_priority(self, task, prio, worker):\n        \"\"\" Update priority of the given task\n\n        Priority can only be increased. If the task doesn't exist, a placeholder\n        task is created to preserve priority when the task is later scheduled.\n        \"\"\"\n        task.priority = prio = max(prio, task.priority)\n        for dep in task.deps or []:\n            t = self._tasks[dep] # This should always exist, see add_task\n            if prio > t.priority:\n                self._update_priority(t, prio, worker)\n\n    def add_task(self, worker, task_id, status=PENDING, runnable=True,\n                 deps=None, new_deps=None, expl=None, resources=None,\n                 priority=0, family='', params={}):\n        \"\"\"\n        * Add task identified by task_id if it doesn't exist\n        * If deps is not None, update dependency list\n        * Update status of task\n        * Add additional workers/stakeholders\n        * Update priority when needed\n        \"\"\"\n        self.update(worker)\n\n        task = self._tasks.setdefault(task_id, Task(\n            status=PENDING, deps=deps, resources=resources, priority=priority, family=family,\n            params=params))\n\n        # for setting priority, we'll sometimes create tasks with unset family and params\n        if not task.family:\n            task.family = family\n        if not task.params:\n            task.params = params\n\n        if task.remove is not None:\n            task.remove = None  # unmark task for removal so it isn't removed after being added\n\n        if not (task.status == RUNNING and status == PENDING):\n            # don't allow re-scheduling of task while it is running, it must either fail or succeed first\n            if status == PENDING or status != task.status:\n                # Update the DB only if there was a acctual change, to prevent noise.\n                # We also check for status == PENDING b/c that's the default value\n                # (so checking for status != task.status woule lie)\n                self._update_task_history(task_id, status)\n            task.status = PENDING if status == SUSPENDED else status\n            if status == FAILED:\n                task.retry = time.time() + self._retry_delay\n\n        if deps is not None:\n            task.deps = set(deps)\n\n        if new_deps is not None:\n            task.deps.update(new_deps)\n\n        task.stakeholders.add(worker)\n        task.resources = resources\n\n        # Task dependencies might not exist yet. Let's create dummy tasks for them for now.\n        # Otherwise the task dependencies might end up being pruned if scheduling takes a long time\n        for dep in task.deps or []:\n            t = self._tasks.setdefault(dep, Task(status=UNKNOWN, deps=None, priority=priority))\n            t.stakeholders.add(worker)\n\n        self._update_priority(task, priority, worker)\n\n        if runnable:\n            task.workers.add(worker)\n\n        if expl is not None:\n            task.expl = expl\n\n    def add_worker(self, worker, info):\n        self._active_workers[worker].add_info(info)\n\n    def update_resources(self, **resources):\n        if self._resources is None:\n            self._resources = {}\n        self._resources.update(resources)\n\n    def _has_resources(self, needed_resources, used_resources):\n        if needed_resources is None:\n            return True\n\n        available_resources = self._resources or {}\n        for resource, amount in needed_resources.items():\n            if amount + used_resources[resource] > available_resources.get(resource, 1):\n                return False\n        return True\n\n    def _used_resources(self):\n        used_resources = collections.defaultdict(int)\n        if self._resources is not None:\n            for task in self._tasks.itervalues():\n                if task.status == RUNNING and task.resources:\n                    for resource, amount in task.resources.items():\n                        used_resources[resource] += amount\n        return used_resources\n\n    def _rank(self, worker):\n        ''' Return worker's rank function for task scheduling '''\n        dependents = collections.defaultdict(int)\n        not_done = lambda t: t not in self._tasks or self._tasks[t].status != DONE\n        for task_id, task in self._tasks.iteritems():\n            if task.status != DONE:\n                deps = filter(not_done, task.deps)\n                inverse_num_deps = 1.0 / max(len(deps), 1)\n                for dep in deps:\n                    dependents[dep] += inverse_num_deps\n\n        return lambda (task_id, task): (task.priority, worker in task.workers, dependents[task_id], -task.time)\n\n    def _not_schedulable(self, task, used_resources):\n        return any((\n            task.status != PENDING,\n            any(dep not in self._tasks or self._tasks[dep].status != DONE for dep in task.deps),\n            not self._has_resources(task.resources, used_resources)\n        ))\n\n    def get_work(self, worker, host=None):\n        # TODO: remove any expired nodes\n\n        # Algo: iterate over all nodes, find the highest priority node no dependencies and available\n        # resources.\n\n        # Resource checking looks both at currently available resources and at which resources would\n        # be available if all running tasks died and we rescheduled all workers greedily. We do both\n        # checks in order to prevent a worker with many low-priority tasks from starving other\n        # workers with higher priority tasks that share the same resources.\n\n        # TODO: remove tasks that can't be done, figure out if the worker has absolutely\n        # nothing it can wait for\n\n        # Return remaining tasks that have no FAILED descendents\n        self.update(worker, {'host': host})\n        best_task = None\n        locally_pending_tasks = 0\n        running_tasks = []\n        used_resources = self._used_resources()\n        potential_resources = collections.defaultdict(int)\n        potential_workers = set([worker])\n        n_unique_pending = 0\n\n        for task_id, task in sorted(self._tasks.iteritems(), key=self._rank(worker), reverse=True):\n            if task.status == RUNNING and worker in task.workers:\n                # Return a list of currently running tasks to the client,\n                # makes it easier to troubleshoot\n                other_worker = self._active_workers[task.worker_running]\n                more_info = {'task_id': task_id, 'worker': str(other_worker)}\n                if other_worker is not None:\n                    more_info.update(other_worker.info)\n                running_tasks.append(more_info)\n\n            if task.status == PENDING and worker in task.workers:\n                locally_pending_tasks += 1\n                if len(task.workers) == 1:\n                    n_unique_pending += 1\n\n            if self._not_schedulable(task, potential_resources) or best_task:\n                continue\n\n            if worker in task.workers and self._has_resources(task.resources, used_resources):\n                best_task = task_id\n            else:\n                # keep track of the resources used in greedy scheduling\n                for w in filter(lambda w: w not in potential_workers, task.workers):\n                    for resource, amount in (task.resources or {}).items():\n                        potential_resources[resource] += amount\n                    potential_workers.add(w)\n\n        if best_task:\n            t = self._tasks[best_task]\n            t.status = RUNNING\n            t.worker_running = worker\n            t.time_running = time.time()\n            self._update_task_history(best_task, RUNNING, host=host)\n\n        return {'n_pending_tasks': locally_pending_tasks,\n                'n_unique_pending': n_unique_pending,\n                'task_id': best_task,\n                'running_tasks': running_tasks}\n\n    def ping(self, worker):\n        self.update(worker)\n\n    def _upstream_status(self, task_id, upstream_status_table):\n        if task_id in upstream_status_table:\n            return upstream_status_table[task_id]\n        elif task_id in self._tasks:\n            task_stack = [task_id]\n\n            while task_stack:\n                dep_id = task_stack.pop()\n                if dep_id in self._tasks:\n                    dep = self._tasks[dep_id]\n                    if dep_id not in upstream_status_table:\n                        if dep.status == PENDING and dep.deps:\n                            task_stack = task_stack + [dep_id] + list(dep.deps)\n                            upstream_status_table[dep_id] = ''  # will be updated postorder\n                        else:\n                            dep_status = STATUS_TO_UPSTREAM_MAP.get(dep.status, '')\n                            upstream_status_table[dep_id] = dep_status\n                    elif upstream_status_table[dep_id] == '' and dep.deps:\n                        # This is the postorder update step when we set the\n                        # status based on the previously calculated child elements\n                        upstream_status = [upstream_status_table.get(id, '') for id in dep.deps]\n                        upstream_status.append('')  # to handle empty list\n                        status = max(upstream_status, key=UPSTREAM_SEVERITY_KEY)\n                        upstream_status_table[dep_id] = status\n            return upstream_status_table[dep_id]\n\n    def _serialize_task(self, task_id, include_deps=True):\n        task = self._tasks[task_id]\n        ret = {\n            'status': task.status,\n            'workers': list(task.workers),\n            'worker_running': task.worker_running,\n            'time_running': getattr(task, \"time_running\", None),\n            'start_time': task.time,\n            'params': task.params,\n            'name': task.family,\n            'priority': task.priority,\n            'resources': task.resources,\n        }\n        if include_deps:\n            ret['deps'] = list(task.deps)\n        return ret\n\n    def graph(self):\n        self.prune()\n        serialized = {}\n        for task_id, task in self._tasks.iteritems():\n            serialized[task_id] = self._serialize_task(task_id)\n        return serialized\n\n    def _recurse_deps(self, task_id, serialized):\n        if task_id not in serialized:\n            task = self._tasks.get(task_id)\n            if task is None or not task.family:\n                logger.warn('Missing task for id [%s]', task_id)\n\n                # try to infer family and params from task_id\n                try:\n                    family, _, param_str = task_id.rstrip(')').partition('(')\n                    params = dict(param.split('=') for param in param_str.split(', '))\n                except:\n                    family, params = '', {}\n                serialized[task_id] = {\n                    'deps': [],\n                    'status': UNKNOWN,\n                    'workers': [],\n                    'start_time': UNKNOWN,\n                    'params': params,\n                    'name': family,\n                    'priority': 0,\n                }\n            else:\n                serialized[task_id] = self._serialize_task(task_id)\n                for dep in task.deps:\n                    self._recurse_deps(dep, serialized)\n\n    def dep_graph(self, task_id):\n        self.prune()\n        serialized = {}\n        if task_id in self._tasks:\n            self._recurse_deps(task_id, serialized)\n        return serialized\n\n    def task_list(self, status, upstream_status):\n        ''' query for a subset of tasks by status '''\n        self.prune()\n        result = {}\n        upstream_status_table = {}  # used to memoize upstream status\n        for task_id, task in self._tasks.iteritems():\n            if not status or task.status == status:\n                if (task.status != PENDING or not upstream_status or\n                    upstream_status == self._upstream_status(task_id, upstream_status_table)):\n                    serialized = self._serialize_task(task_id, False)\n                    result[task_id] = serialized\n        return result\n\n    def inverse_dependencies(self, task_id):\n        self.prune()\n        serialized = {}\n        if task_id in self._tasks:\n            self._traverse_inverse_deps(task_id, serialized)\n        return serialized\n\n    def _traverse_inverse_deps(self, task_id, serialized):\n        stack = [task_id]\n        serialized[task_id] = self._serialize_task(task_id)\n        while len(stack) > 0:\n            curr_id = stack.pop()\n            for id, task in self._tasks.iteritems():\n                if curr_id in task.deps:\n                    serialized[curr_id][\"deps\"].append(id)\n                    if id not in serialized:\n                        serialized[id] = self._serialize_task(id)\n                        serialized[id][\"deps\"] = []\n                        stack.append(id)\n\n    def task_search(self, task_str):\n        ''' query for a subset of tasks by task_id '''\n        self.prune()\n        result = collections.defaultdict(dict)\n        for task_id, task in self._tasks.iteritems():\n            if task_id.find(task_str) != -1:\n                serialized = self._serialize_task(task_id, False)\n                result[task.status][task_id] = serialized\n        return result\n\n    def fetch_error(self, task_id):\n        if self._tasks[task_id].expl is not None:\n            return {\"taskId\": task_id, \"error\": self._tasks[task_id].expl}\n        else:\n            return {\"taskId\": task_id, \"error\": \"\"}\n\n    def _update_task_history(self, task_id, status, host=None):\n        try:\n            if status == DONE or status == FAILED:\n                successful = (status == DONE)\n                self._task_history.task_finished(task_id, successful)\n            elif status == PENDING:\n                self._task_history.task_scheduled(task_id)\n            elif status == RUNNING:\n                self._task_history.task_started(task_id, host)\n        except:\n            logger.warning(\"Error saving Task history\", exc_info=1)\n\n    @property\n    def task_history(self):\n        # Used by server.py to expose the calls\n        return self._task_history\n",
          "file_after": "# Copyright (c) 2012 Spotify AB\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"); you may not\n# use this file except in compliance with the License. You may obtain a copy of\n# the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n# License for the specific language governing permissions and limitations under\n# the License.\n\nimport collections\nimport datetime\nimport os\nimport logging\nimport time\nimport cPickle as pickle\nimport task_history as history\nlogger = logging.getLogger(\"luigi.server\")\n\nfrom task_status import PENDING, FAILED, DONE, RUNNING, SUSPENDED, UNKNOWN\n\n\nclass Scheduler(object):\n    ''' Abstract base class\n\n    Note that the methods all take string arguments, not Task objects...\n    '''\n    add_task = NotImplemented\n    get_work = NotImplemented\n    ping = NotImplemented\n\nUPSTREAM_RUNNING = 'UPSTREAM_RUNNING'\nUPSTREAM_MISSING_INPUT = 'UPSTREAM_MISSING_INPUT'\nUPSTREAM_FAILED = 'UPSTREAM_FAILED'\n\nUPSTREAM_SEVERITY_ORDER = ('', UPSTREAM_RUNNING, UPSTREAM_MISSING_INPUT, UPSTREAM_FAILED)\nUPSTREAM_SEVERITY_KEY = lambda st: UPSTREAM_SEVERITY_ORDER.index(st)\nSTATUS_TO_UPSTREAM_MAP = {FAILED: UPSTREAM_FAILED, RUNNING: UPSTREAM_RUNNING, PENDING: UPSTREAM_MISSING_INPUT}\n\n\nclass Task(object):\n    def __init__(self, status, deps, resources={}, priority=0, family='', params={}):\n        self.stakeholders = set()  # workers ids that are somehow related to this task (i.e. don't prune while any of these workers are still active)\n        self.workers = set()  # workers ids that can perform task - task is 'BROKEN' if none of these workers are active\n        if deps is None:\n            self.deps = set()\n        else:\n            self.deps = set(deps)\n        self.status = status  # PENDING, RUNNING, FAILED or DONE\n        self.time = time.time()  # Timestamp when task was first added\n        self.retry = None\n        self.remove = None\n        self.worker_running = None  # the worker id that is currently running the task or None\n        self.time_running = None  # Timestamp when picked up by worker\n        self.expl = None\n        self.priority = priority\n        self.resources = resources\n        self.family = family\n        self.params = params\n\n    def __repr__(self):\n        return \"Task(%r)\" % vars(self)\n\n\nclass Worker(object):\n    \"\"\" Structure for tracking worker activity and keeping their references \"\"\"\n    def __init__(self, id, last_active=None):\n        self.id = id\n        self.reference = None  # reference to the worker in the real world. (Currently a dict containing just the host)\n        self.last_active = last_active  # seconds since epoch\n        self.started = time.time()  # seconds since epoch\n        self.info = {}\n\n    def add_info(self, info):\n        self.info.update(info)\n\n    def __str__(self):\n        return self.id\n\n\nclass CentralPlannerScheduler(Scheduler):\n    ''' Async scheduler that can handle multiple workers etc\n\n    Can be run locally or on a server (using RemoteScheduler + server.Server).\n    '''\n\n    def __init__(self, retry_delay=900.0, remove_delay=600.0, worker_disconnect_delay=60.0,\n                 state_path='/var/lib/luigi-server/state.pickle', task_history=None,\n                 resources=None):\n        '''\n        (all arguments are in seconds)\n        Keyword Arguments:\n        retry_delay -- How long after a Task fails to try it again, or -1 to never retry\n        remove_delay -- How long after a Task finishes to remove it from the scheduler\n        state_path -- Path to state file (tasks and active workers)\n        worker_disconnect_delay -- If a worker hasn't communicated for this long, remove it from active workers\n        '''\n        self._state_path = state_path\n        self._tasks = {}  # map from id to a Task object\n        self._retry_delay = retry_delay\n        self._remove_delay = remove_delay\n        self._worker_disconnect_delay = worker_disconnect_delay\n        self._active_workers = {}  # map from id to a Worker object\n        self._task_history = task_history or history.NopHistory()\n        self._resources = resources\n\n    def dump(self):\n        state = (self._tasks, self._active_workers)\n        try:\n            with open(self._state_path, 'w') as fobj:\n                pickle.dump(state, fobj)\n        except IOError:\n            logger.warning(\"Failed saving scheduler state\", exc_info=1)\n        else:\n            logger.info(\"Saved state in %s\", self._state_path)\n\n    # prone to lead to crashes when old state is unpickled with updated code. TODO some kind of version control?\n    def load(self):\n        if os.path.exists(self._state_path):\n            logger.info(\"Attempting to load state from %s\", self._state_path)\n            try:\n                with open(self._state_path) as fobj:\n                    state = pickle.load(fobj)\n            except:\n                logger.exception(\"Error when loading state. Starting from clean slate.\")\n                return\n\n            self._tasks, self._active_workers = state\n\n            # Convert from old format\n            # TODO: this is really ugly, we need something more future-proof\n            # Every time we add an attribute to the Worker class, this code needs to be updated\n            for k, v in self._active_workers.iteritems():\n                if isinstance(v, float):\n                    self._active_workers[k] = Worker(id=k, last_active=v)\n        else:\n            logger.info(\"No prior state file exists at %s. Starting with clean slate\", self._state_path)\n\n    def prune(self):\n        logger.info(\"Starting pruning of task graph\")\n        # Delete workers that haven't said anything for a while (probably killed)\n        delete_workers = []\n        for worker in self._active_workers.values():\n            if worker.last_active < time.time() - self._worker_disconnect_delay:\n                logger.info(\"Worker %s timed out (no contact for >=%ss)\", worker, self._worker_disconnect_delay)\n                delete_workers.append(worker.id)\n\n        for worker in delete_workers:\n            self._active_workers.pop(worker)\n\n        remaining_workers = set(self._active_workers.keys())\n\n        # Mark tasks with no remaining active stakeholders for deletion\n        for task_id, task in self._tasks.iteritems():\n            if not task.stakeholders.intersection(remaining_workers):\n                if task.remove is None:\n                    logger.info(\"Task %r has stakeholders %r but none remain connected -> will remove task in %s seconds\", task_id, task.stakeholders, self._remove_delay)\n                    task.remove = time.time() + self._remove_delay\n\n            if task.status == RUNNING and task.worker_running and task.worker_running not in remaining_workers:\n                # If a running worker disconnects, tag all its jobs as FAILED and subject it to the same retry logic\n                logger.info(\"Task %r is marked as running by disconnected worker %r -> marking as FAILED with retry delay of %rs\", task_id, task.worker_running, self._retry_delay)\n                task.worker_running = None\n                task.status = FAILED\n                task.retry = time.time() + self._retry_delay\n\n        # Remove tasks that have no stakeholders\n        remove_tasks = []\n        for task_id, task in self._tasks.iteritems():\n            if task.remove and time.time() > task.remove:\n                logger.info(\"Removing task %r (no connected stakeholders)\", task_id)\n                remove_tasks.append(task_id)\n\n        for task_id in remove_tasks:\n            self._tasks.pop(task_id)\n\n        # Reset FAILED tasks to PENDING if max timeout is reached, and retry delay is >= 0\n        for task in self._tasks.values():\n            if task.status == FAILED and self._retry_delay >= 0 and task.retry < time.time():\n                task.status = PENDING\n        logger.info(\"Done pruning task graph\")\n\n    def update(self, worker_id, worker_reference=None):\n        \"\"\" Keep track of whenever the worker was last active \"\"\"\n        worker = self._active_workers.setdefault(worker_id, Worker(worker_id))\n        if worker_reference:\n            worker.reference = worker_reference\n        worker.last_active = time.time()\n\n    def _update_priority(self, task, prio, worker):\n        \"\"\" Update priority of the given task\n\n        Priority can only be increased. If the task doesn't exist, a placeholder\n        task is created to preserve priority when the task is later scheduled.\n        \"\"\"\n        task.priority = prio = max(prio, task.priority)\n        for dep in task.deps or []:\n            t = self._tasks[dep] # This should always exist, see add_task\n            if prio > t.priority:\n                self._update_priority(t, prio, worker)\n\n    def add_task(self, worker, task_id, status=PENDING, runnable=True,\n                 deps=None, new_deps=None, expl=None, resources=None,\n                 priority=0, family='', params={}):\n        \"\"\"\n        * Add task identified by task_id if it doesn't exist\n        * If deps is not None, update dependency list\n        * Update status of task\n        * Add additional workers/stakeholders\n        * Update priority when needed\n        \"\"\"\n        self.update(worker)\n\n        task = self._tasks.setdefault(task_id, Task(\n            status=PENDING, deps=deps, resources=resources, priority=priority, family=family,\n            params=params))\n\n        # for setting priority, we'll sometimes create tasks with unset family and params\n        if not task.family:\n            task.family = family\n        if not task.params:\n            task.params = params\n\n        if task.remove is not None:\n            task.remove = None  # unmark task for removal so it isn't removed after being added\n\n        if not (task.status == RUNNING and status == PENDING):\n            # don't allow re-scheduling of task while it is running, it must either fail or succeed first\n            if status == PENDING or status != task.status:\n                # Update the DB only if there was a acctual change, to prevent noise.\n                # We also check for status == PENDING b/c that's the default value\n                # (so checking for status != task.status woule lie)\n                self._update_task_history(task_id, status)\n            task.status = PENDING if status == SUSPENDED else status\n            if status == FAILED:\n                task.retry = time.time() + self._retry_delay\n\n        if deps is not None:\n            task.deps = set(deps)\n\n        if new_deps is not None:\n            task.deps.update(new_deps)\n\n        task.stakeholders.add(worker)\n        task.resources = resources\n\n        # Task dependencies might not exist yet. Let's create dummy tasks for them for now.\n        # Otherwise the task dependencies might end up being pruned if scheduling takes a long time\n        for dep in task.deps or []:\n            t = self._tasks.setdefault(dep, Task(status=UNKNOWN, deps=None, priority=priority))\n            t.stakeholders.add(worker)\n\n        self._update_priority(task, priority, worker)\n\n        if runnable:\n            task.workers.add(worker)\n\n        if expl is not None:\n            task.expl = expl\n\n    def add_worker(self, worker, info):\n        self._active_workers[worker].add_info(info)\n\n    def update_resources(self, **resources):\n        if self._resources is None:\n            self._resources = {}\n        self._resources.update(resources)\n\n    def _has_resources(self, needed_resources, used_resources):\n        if needed_resources is None:\n            return True\n\n        available_resources = self._resources or {}\n        for resource, amount in needed_resources.items():\n            if amount + used_resources[resource] > available_resources.get(resource, 1):\n                return False\n        return True\n\n    def _used_resources(self):\n        used_resources = collections.defaultdict(int)\n        if self._resources is not None:\n            for task in self._tasks.itervalues():\n                if task.status == RUNNING and task.resources:\n                    for resource, amount in task.resources.items():\n                        used_resources[resource] += amount\n        return used_resources\n\n    def _rank(self, worker):\n        ''' Return worker's rank function for task scheduling '''\n        dependents = collections.defaultdict(int)\n        not_done = lambda t: t not in self._tasks or self._tasks[t].status != DONE\n        for task_id, task in self._tasks.iteritems():\n            if task.status != DONE:\n                deps = filter(not_done, task.deps)\n                inverse_num_deps = 1.0 / max(len(deps), 1)\n                for dep in deps:\n                    dependents[dep] += inverse_num_deps\n\n        return lambda (task_id, task): (task.priority, worker in task.workers, dependents[task_id], -task.time)\n\n    def _not_schedulable(self, task, used_resources):\n        return any((\n            task.status != PENDING,\n            any(dep not in self._tasks or self._tasks[dep].status != DONE for dep in task.deps),\n            not self._has_resources(task.resources, used_resources)\n        ))\n\n    def get_work(self, worker, host=None):\n        # TODO: remove any expired nodes\n\n        # Algo: iterate over all nodes, find the highest priority node no dependencies and available\n        # resources.\n\n        # Resource checking looks both at currently available resources and at which resources would\n        # be available if all running tasks died and we rescheduled all workers greedily. We do both\n        # checks in order to prevent a worker with many low-priority tasks from starving other\n        # workers with higher priority tasks that share the same resources.\n\n        # TODO: remove tasks that can't be done, figure out if the worker has absolutely\n        # nothing it can wait for\n\n        # Return remaining tasks that have no FAILED descendents\n        self.update(worker, {'host': host})\n        best_task = None\n        locally_pending_tasks = 0\n        running_tasks = []\n        used_resources = self._used_resources()\n        potential_resources = collections.defaultdict(int)\n        potential_workers = set([worker])\n        n_unique_pending = 0\n\n        for task_id, task in sorted(self._tasks.iteritems(), key=self._rank(worker), reverse=True):\n            if task.status == RUNNING and worker in task.workers:\n                # Return a list of currently running tasks to the client,\n                # makes it easier to troubleshoot\n                other_worker = self._active_workers[task.worker_running]\n                more_info = {'task_id': task_id, 'worker': str(other_worker)}\n                if other_worker is not None:\n                    more_info.update(other_worker.info)\n                running_tasks.append(more_info)\n\n            if task.status == PENDING and worker in task.workers:\n                locally_pending_tasks += 1\n                if len(task.workers) == 1:\n                    n_unique_pending += 1\n\n            if self._not_schedulable(task, potential_resources) or best_task:\n                continue\n\n            if worker in task.workers and self._has_resources(task.resources, used_resources):\n                best_task = task_id\n            else:\n                # keep track of the resources used in greedy scheduling\n                for w in filter(lambda w: w not in potential_workers, task.workers):\n                    for resource, amount in (task.resources or {}).items():\n                        potential_resources[resource] += amount\n                    potential_workers.add(w)\n\n        if best_task:\n            t = self._tasks[best_task]\n            t.status = RUNNING\n            t.worker_running = worker\n            t.time_running = time.time()\n            self._update_task_history(best_task, RUNNING, host=host)\n\n        return {'n_pending_tasks': locally_pending_tasks,\n                'n_unique_pending': n_unique_pending,\n                'task_id': best_task,\n                'running_tasks': running_tasks}\n\n    def ping(self, worker):\n        self.update(worker)\n\n    def _upstream_status(self, task_id, upstream_status_table):\n        if task_id in upstream_status_table:\n            return upstream_status_table[task_id]\n        elif task_id in self._tasks:\n            task_stack = [task_id]\n\n            while task_stack:\n                dep_id = task_stack.pop()\n                if dep_id in self._tasks:\n                    dep = self._tasks[dep_id]\n                    if dep_id not in upstream_status_table:\n                        if dep.status == PENDING and dep.deps:\n                            task_stack = task_stack + [dep_id] + list(dep.deps)\n                            upstream_status_table[dep_id] = ''  # will be updated postorder\n                        else:\n                            dep_status = STATUS_TO_UPSTREAM_MAP.get(dep.status, '')\n                            upstream_status_table[dep_id] = dep_status\n                    elif upstream_status_table[dep_id] == '' and dep.deps:\n                        # This is the postorder update step when we set the\n                        # status based on the previously calculated child elements\n                        upstream_status = [upstream_status_table.get(id, '') for id in dep.deps]\n                        upstream_status.append('')  # to handle empty list\n                        status = max(upstream_status, key=UPSTREAM_SEVERITY_KEY)\n                        upstream_status_table[dep_id] = status\n            return upstream_status_table[dep_id]\n\n    def _serialize_task(self, task_id, include_deps=True):\n        task = self._tasks[task_id]\n        ret = {\n            'status': task.status,\n            'workers': list(task.workers),\n            'worker_running': task.worker_running,\n            'time_running': getattr(task, \"time_running\", None),\n            'start_time': task.time,\n            'params': task.params,\n            'name': task.family,\n            'priority': task.priority,\n            'resources': task.resources,\n        }\n        if include_deps:\n            ret['deps'] = list(task.deps)\n        return ret\n\n    def graph(self):\n        self.prune()\n        serialized = {}\n        for task_id, task in self._tasks.iteritems():\n            serialized[task_id] = self._serialize_task(task_id)\n        return serialized\n\n    def _recurse_deps(self, task_id, serialized):\n        if task_id not in serialized:\n            task = self._tasks.get(task_id)\n            if task is None or not task.family:\n                logger.warn('Missing task for id [%s]', task_id)\n\n                # try to infer family and params from task_id\n                try:\n                    family, _, param_str = task_id.rstrip(')').partition('(')\n                    params = dict(param.split('=') for param in param_str.split(', '))\n                except:\n                    family, params = '', {}\n                serialized[task_id] = {\n                    'deps': [],\n                    'status': UNKNOWN,\n                    'workers': [],\n                    'start_time': UNKNOWN,\n                    'params': params,\n                    'name': family,\n                    'priority': 0,\n                }\n            else:\n                serialized[task_id] = self._serialize_task(task_id)\n                for dep in task.deps:\n                    self._recurse_deps(dep, serialized)\n\n    def dep_graph(self, task_id):\n        self.prune()\n        serialized = {}\n        if task_id in self._tasks:\n            self._recurse_deps(task_id, serialized)\n        return serialized\n\n    def task_list(self, status, upstream_status):\n        ''' query for a subset of tasks by status '''\n        self.prune()\n        result = {}\n        upstream_status_table = {}  # used to memoize upstream status\n        for task_id, task in self._tasks.iteritems():\n            if not status or task.status == status:\n                if (task.status != PENDING or not upstream_status or\n                    upstream_status == self._upstream_status(task_id, upstream_status_table)):\n                    serialized = self._serialize_task(task_id, False)\n                    result[task_id] = serialized\n        return result\n\n    def worker_list(self, include_running=True):\n        self.prune()\n        workers = [\n            dict(\n                name=worker.id,\n                last_active=worker.last_active,\n                started=getattr(worker, 'started', None),\n                **worker.info\n            ) for worker in self._active_workers.values()]\n        workers.sort(key=lambda worker: worker['started'], reverse=True)\n        if include_running:\n            running = collections.defaultdict(dict)\n            num_pending = collections.defaultdict(int)\n            num_uniques = collections.defaultdict(int)\n            for task_id, task in self._tasks.items():\n                if task.status == RUNNING and task.worker_running:\n                    running[task.worker_running][task_id] = self._serialize_task(task_id, False)\n                elif task.status == PENDING:\n                    for worker in task.workers:\n                        num_pending[worker] += 1\n                    if len(task.workers) == 1:\n                        num_uniques[list(task.workers)[0]] += 1\n            for worker in workers:\n                tasks = running[worker['name']]\n                worker['num_running'] = len(tasks)\n                worker['num_pending'] = num_pending[worker['name']]\n                worker['num_uniques'] = num_uniques[worker['name']]\n                worker['running'] = tasks\n        return workers\n\n    def inverse_dependencies(self, task_id):\n        self.prune()\n        serialized = {}\n        if task_id in self._tasks:\n            self._traverse_inverse_deps(task_id, serialized)\n        return serialized\n\n    def _traverse_inverse_deps(self, task_id, serialized):\n        stack = [task_id]\n        serialized[task_id] = self._serialize_task(task_id)\n        while len(stack) > 0:\n            curr_id = stack.pop()\n            for id, task in self._tasks.iteritems():\n                if curr_id in task.deps:\n                    serialized[curr_id][\"deps\"].append(id)\n                    if id not in serialized:\n                        serialized[id] = self._serialize_task(id)\n                        serialized[id][\"deps\"] = []\n                        stack.append(id)\n\n    def task_search(self, task_str):\n        ''' query for a subset of tasks by task_id '''\n        self.prune()\n        result = collections.defaultdict(dict)\n        for task_id, task in self._tasks.iteritems():\n            if task_id.find(task_str) != -1:\n                serialized = self._serialize_task(task_id, False)\n                result[task.status][task_id] = serialized\n        return result\n\n    def fetch_error(self, task_id):\n        if self._tasks[task_id].expl is not None:\n            return {\"taskId\": task_id, \"error\": self._tasks[task_id].expl}\n        else:\n            return {\"taskId\": task_id, \"error\": \"\"}\n\n    def _update_task_history(self, task_id, status, host=None):\n        try:\n            if status == DONE or status == FAILED:\n                successful = (status == DONE)\n                self._task_history.task_finished(task_id, successful)\n            elif status == PENDING:\n                self._task_history.task_scheduled(task_id)\n            elif status == RUNNING:\n                self._task_history.task_started(task_id, host)\n        except:\n            logger.warning(\"Error saving Task history\", exc_info=1)\n\n    @property\n    def task_history(self):\n        # Used by server.py to expose the calls\n        return self._task_history\n",
          "file_patch": "@@ -72,6 +72,7 @@ class Worker(object):\n         self.id = id\n         self.reference = None  # reference to the worker in the real world. (Currently a dict containing just the host)\n         self.last_active = last_active  # seconds since epoch\n+        self.started = time.time()  # seconds since epoch\n         self.info = {}\n \n     def add_info(self, info):\n@@ -470,6 +471,36 @@ class CentralPlannerScheduler(Scheduler):\n                     result[task_id] = serialized\n         return result\n \n+    def worker_list(self, include_running=True):\n+        self.prune()\n+        workers = [\n+            dict(\n+                name=worker.id,\n+                last_active=worker.last_active,\n+                started=getattr(worker, 'started', None),\n+                **worker.info\n+            ) for worker in self._active_workers.values()]\n+        workers.sort(key=lambda worker: worker['started'], reverse=True)\n+        if include_running:\n+            running = collections.defaultdict(dict)\n+            num_pending = collections.defaultdict(int)\n+            num_uniques = collections.defaultdict(int)\n+            for task_id, task in self._tasks.items():\n+                if task.status == RUNNING and task.worker_running:\n+                    running[task.worker_running][task_id] = self._serialize_task(task_id, False)\n+                elif task.status == PENDING:\n+                    for worker in task.workers:\n+                        num_pending[worker] += 1\n+                    if len(task.workers) == 1:\n+                        num_uniques[list(task.workers)[0]] += 1\n+            for worker in workers:\n+                tasks = running[worker['name']]\n+                worker['num_running'] = len(tasks)\n+                worker['num_pending'] = num_pending[worker['name']]\n+                worker['num_uniques'] = num_uniques[worker['name']]\n+                worker['running'] = tasks\n+        return workers\n+\n     def inverse_dependencies(self, task_id):\n         self.prune()\n         serialized = {}\n",
          "files_name_in_blame_commit": [
            "rpc.py",
            "worker.py",
            "scheduler.py"
          ]
        }
      },
      "77e027b52ea8b0645f33ff90b7ad9371a707c13b": {
        "commit": {
          "commit_id": "77e027b52ea8b0645f33ff90b7ad9371a707c13b",
          "commit_message": "More info for each worker",
          "commit_author": "Erik Bernhardsson",
          "commit_date": "2014-06-29 22:31:19",
          "commit_parent": "5d25c49cf4545a2b1ff7379149ec17b729ab6f65"
        },
        "function": {
          "function_name": "__init__",
          "function_code_before": "def __init__(self, id, last_active=None):\n    self.id = id\n    self.reference = None\n    self.last_active = last_active",
          "function_code_after": "def __init__(self, id, last_active=None):\n    self.id = id\n    self.reference = None\n    self.last_active = last_active\n    self.info = {}",
          "function_before_start_line": 68,
          "function_before_end_line": 71,
          "function_after_start_line": 68,
          "function_after_end_line": 72,
          "function_before_token_count": 26,
          "function_after_token_count": 32,
          "functions_name_modified_file": [
            "_update_task_history",
            "prune",
            "_upstream_status",
            "ping",
            "_traverse_inverse_deps",
            "load",
            "inverse_dependencies",
            "task_search",
            "_recurse_deps",
            "add_task",
            "task_list",
            "add_info",
            "_serialize_task",
            "task_history",
            "dep_graph",
            "__str__",
            "graph",
            "dump",
            "__repr__",
            "__init__",
            "update",
            "add_worker",
            "_get_task_params",
            "get_work",
            "_get_task_name",
            "fetch_error"
          ],
          "functions_name_all_files": [
            "_update_task_history",
            "test_remove_dep",
            "prune",
            "tearDown",
            "_upstream_status",
            "_generate_worker_info",
            "test_two_workers",
            "test_parameter_split",
            "_email_complete_error",
            "ping",
            "_fork_task",
            "test_retry",
            "inverse_dep_graph",
            "_wait",
            "_traverse_inverse_deps",
            "test_dep",
            "load",
            "inverse_dependencies",
            "_log_unexpected_error",
            "_email_unexpected_error",
            "_log_remote_tasks",
            "task_search",
            "_add_task_and_deps",
            "test_priorities_and_dependencies",
            "_recurse_deps",
            "test_timeout",
            "test_two_worker_info",
            "add_task",
            "_validate_dependency",
            "task_list",
            "add_info",
            "add",
            "run",
            "_serialize_task",
            "test_disallowed_state_changes",
            "task_history",
            "_add_worker",
            "dep_graph",
            "_run_task",
            "_get_work",
            "test_disconnect_running",
            "_check_complete",
            "__str__",
            "graph",
            "_sleeper",
            "dump",
            "__repr__",
            "setUp",
            "__init__",
            "_request",
            "update",
            "test_failed_dep",
            "add_worker",
            "_validate_task",
            "_check_complete_value",
            "test_broken_dep",
            "_get_task_params",
            "test_priorities",
            "test_priorities_default_and_negative",
            "stop",
            "_log_complete_error",
            "_add",
            "get_work",
            "_get_task_name",
            "_add_external",
            "_reap_children",
            "setTime",
            "fetch_error"
          ],
          "functions_name_co_evolved_modified_file": [
            "add_worker",
            "add_info",
            "get_work",
            "__str__"
          ],
          "functions_name_co_evolved_all_files": [
            "_add_worker",
            "add_worker",
            "_generate_worker_info",
            "test_two_worker_info",
            "add_info",
            "get_work",
            "__str__",
            "run"
          ]
        },
        "file": {
          "file_name": "scheduler.py",
          "file_nloc": 316,
          "file_complexity": 109,
          "file_token_count": 2326,
          "file_before": "# Copyright (c) 2012 Spotify AB\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"); you may not\n# use this file except in compliance with the License. You may obtain a copy of\n# the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n# License for the specific language governing permissions and limitations under\n# the License.\n\nimport collections\nimport datetime\nimport os\nimport logging\nimport time\nimport cPickle as pickle\nimport task_history as history\nlogger = logging.getLogger(\"luigi.server\")\n\nfrom task_status import PENDING, FAILED, DONE, RUNNING, UNKNOWN\n\n\nclass Scheduler(object):\n    ''' Abstract base class\n\n    Note that the methods all take string arguments, not Task objects...\n    '''\n    add_task = NotImplemented\n    get_work = NotImplemented\n    ping = NotImplemented\n\nUPSTREAM_RUNNING = 'UPSTREAM_RUNNING'\nUPSTREAM_MISSING_INPUT = 'UPSTREAM_MISSING_INPUT'\nUPSTREAM_FAILED = 'UPSTREAM_FAILED'\n\nUPSTREAM_SEVERITY_ORDER = ('', UPSTREAM_RUNNING, UPSTREAM_MISSING_INPUT, UPSTREAM_FAILED)\nUPSTREAM_SEVERITY_KEY = lambda st: UPSTREAM_SEVERITY_ORDER.index(st)\nSTATUS_TO_UPSTREAM_MAP = {FAILED: UPSTREAM_FAILED, RUNNING: UPSTREAM_RUNNING, PENDING: UPSTREAM_MISSING_INPUT}\n\n\nclass Task(object):\n    def __init__(self, status, deps, priority=0):\n        self.stakeholders = set()  # workers ids that are somehow related to this task (i.e. don't prune while any of these workers are still active)\n        self.workers = set()  # workers ids that can perform task - task is 'BROKEN' if none of these workers are active\n        if deps is None:\n            self.deps = set()\n        else:\n            self.deps = set(deps)\n        self.status = status  # PENDING, RUNNING, FAILED or DONE\n        self.time = time.time()  # Timestamp when task was first added\n        self.retry = None\n        self.remove = None\n        self.worker_running = None  # the worker id that is currently running the task or None\n        self.time_running = None  # Timestamp when picked up by worker\n        self.expl = None\n        self.priority = priority\n\n    def __repr__(self):\n        return \"Task(%r)\" % vars(self)\n\n\nclass Worker(object):\n    \"\"\" Structure for tracking worker activity and keeping their references \"\"\"\n    def __init__(self, id, last_active=None):\n        self.id = id\n        self.reference = None  # reference to the worker in the real world. (Currently a dict containing just the host)\n        self.last_active = last_active  # seconds since epoch\n\n    def __str__(self):\n        return \"%s on %s, last active %s\" % (self.id, self.reference, datetime.datetime.utcfromtimestamp(self.last_active).isoformat())\n\n\nclass CentralPlannerScheduler(Scheduler):\n    ''' Async scheduler that can handle multiple workers etc\n\n    Can be run locally or on a server (using RemoteScheduler + server.Server).\n    '''\n\n    def __init__(self, retry_delay=900.0, remove_delay=600.0, worker_disconnect_delay=60.0,\n                 state_path='/var/lib/luigi-server/state.pickle', task_history=None):\n        '''\n        (all arguments are in seconds)\n        Keyword Arguments:\n        retry_delay -- How long after a Task fails to try it again, or -1 to never retry\n        remove_delay -- How long after a Task finishes to remove it from the scheduler\n        state_path -- Path to state file (tasks and active workers)\n        worker_disconnect_delay -- If a worker hasn't communicated for this long, remove it from active workers\n        '''\n        self._state_path = state_path\n        self._tasks = {}  # map from id to a Task object\n        self._retry_delay = retry_delay\n        self._remove_delay = remove_delay\n        self._worker_disconnect_delay = worker_disconnect_delay\n        self._active_workers = {}  # map from id to a Worker object\n        self._task_history = task_history or history.NopHistory()\n\n    def dump(self):\n        state = (self._tasks, self._active_workers)\n        try:\n            with open(self._state_path, 'w') as fobj:\n                pickle.dump(state, fobj)\n        except IOError:\n            logger.warning(\"Failed saving scheduler state\", exc_info=1)\n        else:\n            logger.info(\"Saved state in %s\", self._state_path)\n\n    # prone to lead to crashes when old state is unpickled with updated code. TODO some kind of version control?\n    def load(self):\n        if os.path.exists(self._state_path):\n            logger.info(\"Attempting to load state from %s\", self._state_path)\n            with open(self._state_path) as fobj:\n                state = pickle.load(fobj)\n            self._tasks, self._active_workers = state\n\n            # Convert from old format\n            # TODO: this is really ugly, we need something more future-proof\n            # Every time we add an attribute to the Worker class, this code needs to be updated\n            for k, v in self._active_workers.iteritems():\n                if isinstance(v, float):\n                    self._active_workers[k] = Worker(id=k, last_active=v)\n        else:\n            logger.info(\"No prior state file exists at %s. Starting with clean slate\", self._state_path)\n\n    def prune(self):\n        logger.info(\"Starting pruning of task graph\")\n        # Delete workers that haven't said anything for a while (probably killed)\n        delete_workers = []\n        for worker in self._active_workers.values():\n            if worker.last_active < time.time() - self._worker_disconnect_delay:\n                logger.info(\"Worker %s timed out (no contact for >=%ss)\", worker, self._worker_disconnect_delay)\n                delete_workers.append(worker.id)\n\n        for worker in delete_workers:\n            self._active_workers.pop(worker)\n\n        remaining_workers = set(self._active_workers.keys())\n\n        # Mark tasks with no remaining active stakeholders for deletion\n        for task_id, task in self._tasks.iteritems():\n            if not task.stakeholders.intersection(remaining_workers):\n                if task.remove is None:\n                    logger.info(\"Task %r has stakeholders %r but none remain connected -> will remove task in %s seconds\", task_id, task.stakeholders, self._remove_delay)\n                    task.remove = time.time() + self._remove_delay\n\n            if task.status == RUNNING and task.worker_running and task.worker_running not in remaining_workers:\n                # If a running worker disconnects, tag all its jobs as FAILED and subject it to the same retry logic\n                logger.info(\"Task %r is marked as running by disconnected worker %r -> marking as FAILED with retry delay of %rs\", task_id, task.worker_running, self._retry_delay)\n                task.worker_running = None\n                task.status = FAILED\n                task.retry = time.time() + self._retry_delay\n\n        # Remove tasks that have no stakeholders\n        remove_tasks = []\n        for task_id, task in self._tasks.iteritems():\n            if task.remove and time.time() > task.remove:\n                logger.info(\"Removing task %r (no connected stakeholders)\", task_id)\n                remove_tasks.append(task_id)\n\n        for task_id in remove_tasks:\n            self._tasks.pop(task_id)\n\n        # Reset FAILED tasks to PENDING if max timeout is reached, and retry delay is >= 0\n        for task in self._tasks.values():\n            if task.status == FAILED and self._retry_delay >= 0 and task.retry < time.time():\n                task.status = PENDING\n        logger.info(\"Done pruning task graph\")\n\n    def update(self, worker_id, worker_reference=None):\n        \"\"\" Keep track of whenever the worker was last active \"\"\"\n        worker = self._active_workers.setdefault(worker_id, Worker(worker_id))\n        if worker_reference:\n            worker.reference = worker_reference\n        worker.last_active = time.time()\n\n    def add_task(self, worker, task_id, status=PENDING, runnable=True, deps=None, expl=None, priority=0):\n        \"\"\"\n        * Add task identified by task_id if it doesn't exist\n        * If deps is not None, update dependency list\n        * Update status of task\n        * Add additional workers/stakeholders\n        \"\"\"\n        self.update(worker)\n\n        task = self._tasks.setdefault(task_id, Task(status=PENDING, deps=deps, priority=priority))\n\n        if task.remove is not None:\n            task.remove = None  # unmark task for removal so it isn't removed after being added\n\n        if not (task.status == RUNNING and status == PENDING):\n            # don't allow re-scheduling of task while it is running, it must either fail or succeed first\n            if status == PENDING or status != task.status:\n                # Update the DB only if there was a acctual change, to prevent noise.\n                # We also check for status == PENDING b/c that's the default value\n                # (so checking for status != task.status woule lie)\n                self._update_task_history(task_id, status)\n            task.status = status\n            if status == FAILED:\n                task.retry = time.time() + self._retry_delay\n\n        if deps is not None:\n            task.deps = set(deps)\n\n        task.stakeholders.add(worker)\n\n        if runnable:\n            task.workers.add(worker)\n\n        if expl is not None:\n            task.expl = expl\n\n    def get_work(self, worker, host=None):\n        # TODO: remove any expired nodes\n\n        # Algo: iterate over all nodes, find the first node with no dependencies and highest priority\n\n        # TODO: remove tasks that can't be done, figure out if the worker has absolutely\n        # nothing it can wait for\n\n        # Return remaining tasks that have no FAILED descendents\n        self.update(worker, {'host': host})\n        best_t = float('inf')\n        best_priority = float('-inf')\n        best_task = None\n        locally_pending_tasks = 0\n        running_tasks = []\n\n        for task_id, task in self._tasks.iteritems():\n            if worker not in task.workers:\n                continue\n\n            if task.status == RUNNING:\n                running_tasks.append({'task_id': task_id, 'worker': str(self._active_workers.get(task.worker_running))})\n\n            if task.status != PENDING:\n                continue\n\n            locally_pending_tasks += 1\n            ok = True\n            for dep in task.deps:\n                if dep not in self._tasks:\n                    ok = False\n                elif self._tasks[dep].status != DONE:\n                    ok = False\n\n            if ok:\n                if (-task.priority, task.time) < (-best_priority, best_t):\n                    best_t = task.time\n                    best_priority = task.priority\n                    best_task = task_id\n\n        if best_task:\n            t = self._tasks[best_task]\n            t.status = RUNNING\n            t.worker_running = worker\n            t.time_running = time.time()\n            self._update_task_history(best_task, RUNNING, host=host)\n\n        return {'n_pending_tasks': locally_pending_tasks,\n                'task_id': best_task,\n                'running_tasks': running_tasks}\n\n    def ping(self, worker):\n        self.update(worker)\n\n    def _upstream_status(self, task_id, upstream_status_table):\n        if task_id in upstream_status_table:\n            return upstream_status_table[task_id]\n        elif task_id in self._tasks:\n            task_stack = [task_id]\n\n            while task_stack:\n                dep_id = task_stack.pop()\n                if dep_id in self._tasks:\n                    dep = self._tasks[dep_id]\n                    if dep_id not in upstream_status_table:\n                        if dep.status == PENDING and dep.deps:\n                            task_stack = task_stack + [dep_id] + list(dep.deps)\n                            upstream_status_table[dep_id] = ''  # will be updated postorder\n                        else:\n                            dep_status = STATUS_TO_UPSTREAM_MAP.get(dep.status, '')\n                            upstream_status_table[dep_id] = dep_status\n                    elif upstream_status_table[dep_id] == '' and dep.deps:\n                        # This is the postorder update step when we set the\n                        # status based on the previously calculated child elements\n                        upstream_status = [upstream_status_table.get(id, '') for id in dep.deps]\n                        upstream_status.append('')  # to handle empty list\n                        status = max(upstream_status, key=UPSTREAM_SEVERITY_KEY)\n                        upstream_status_table[dep_id] = status\n            return upstream_status_table[dep_id]\n\n    def _serialize_task(self, task_id):\n        task = self._tasks[task_id]\n        return {\n            'deps': list(task.deps),\n            'status': task.status,\n            'workers': list(task.workers),\n            'worker_running': task.worker_running,\n            'time_running': getattr(task, \"time_running\", None),\n            'start_time': task.time,\n            'params': self._get_task_params(task_id),\n            'name': self._get_task_name(task_id)\n        }\n\n    def _get_task_params(self, task_id):\n        params = {}\n        params_part = task_id.split('(')[1].strip(')')\n        params_strings = params_part.split(\", \")\n\n        for param in params_strings:\n            if not param:\n                continue\n            split_param = param.split('=')\n            if len(split_param) != 2:\n                return {'<complex parameters>': params_part}\n            params[split_param[0]] = split_param[1]\n        return params\n\n    def _get_task_name(self, task_id):\n        return task_id.split('(')[0]\n\n    def graph(self):\n        self.prune()\n        serialized = {}\n        for task_id, task in self._tasks.iteritems():\n            serialized[task_id] = self._serialize_task(task_id)\n        return serialized\n\n    def _recurse_deps(self, task_id, serialized):\n        if task_id not in serialized:\n            task = self._tasks.get(task_id)\n            if task is None:\n                logger.warn('Missing task for id [%s]', task_id)\n                serialized[task_id] = {\n                    'deps': [],\n                    'status': UNKNOWN,\n                    'workers': [],\n                    'start_time': UNKNOWN,\n                    'params': self._get_task_params(task_id),\n                    'name': self._get_task_name(task_id)\n                }\n            else:\n                serialized[task_id] = self._serialize_task(task_id)\n                for dep in task.deps:\n                    self._recurse_deps(dep, serialized)\n\n    def dep_graph(self, task_id):\n        self.prune()\n        serialized = {}\n        if task_id in self._tasks:\n            self._recurse_deps(task_id, serialized)\n        return serialized\n\n    def task_list(self, status, upstream_status):\n        ''' query for a subset of tasks by status '''\n        self.prune()\n        result = {}\n        upstream_status_table = {}  # used to memoize upstream status\n        for task_id, task in self._tasks.iteritems():\n            if not status or task.status == status:\n                if (task.status != PENDING or not upstream_status or\n                    upstream_status == self._upstream_status(task_id, upstream_status_table)):\n                    serialized = self._serialize_task(task_id)\n                    result[task_id] = serialized\n        return result\n\n    def inverse_dependencies(self, task_id):\n        self.prune()\n        serialized = {}\n        if task_id in self._tasks:\n            self._traverse_inverse_deps(task_id, serialized)\n        return serialized\n\n    def _traverse_inverse_deps(self, task_id, serialized):\n        stack = [task_id]\n        serialized[task_id] = self._serialize_task(task_id)\n        while len(stack) > 0:\n            curr_id = stack.pop()\n            for id, task in self._tasks.iteritems():\n                if curr_id in task.deps:\n                    serialized[curr_id][\"deps\"].append(id)\n                    if id not in serialized:\n                        serialized[id] = self._serialize_task(id)\n                        serialized[id][\"deps\"] = []\n                        stack.append(id)\n\n    def task_search(self, task_str):\n        ''' query for a subset of tasks by task_id '''\n        self.prune()\n        result = collections.defaultdict(dict)\n        for task_id, task in self._tasks.iteritems():\n            if task_id.find(task_str) != -1:\n                serialized = self._serialize_task(task_id)\n                result[task.status][task_id] = serialized\n        return result\n\n    def fetch_error(self, task_id):\n        if self._tasks[task_id].expl is not None:\n            return {\"taskId\": task_id, \"error\": self._tasks[task_id].expl}\n        else:\n            return {\"taskId\": task_id, \"error\": \"\"}\n\n    def _update_task_history(self, task_id, status, host=None):\n        try:\n            if status == DONE or status == FAILED:\n                successful = (status == DONE)\n                self._task_history.task_finished(task_id, successful)\n            elif status == PENDING:\n                self._task_history.task_scheduled(task_id)\n            elif status == RUNNING:\n                self._task_history.task_started(task_id, host)\n        except:\n            logger.warning(\"Error saving Task history\", exc_info=1)\n\n    @property\n    def task_history(self):\n        # Used by server.py to expose the calls\n        return self._task_history\n",
          "file_after": "# Copyright (c) 2012 Spotify AB\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"); you may not\n# use this file except in compliance with the License. You may obtain a copy of\n# the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n# License for the specific language governing permissions and limitations under\n# the License.\n\nimport collections\nimport datetime\nimport os\nimport logging\nimport time\nimport cPickle as pickle\nimport task_history as history\nlogger = logging.getLogger(\"luigi.server\")\n\nfrom task_status import PENDING, FAILED, DONE, RUNNING, UNKNOWN\n\n\nclass Scheduler(object):\n    ''' Abstract base class\n\n    Note that the methods all take string arguments, not Task objects...\n    '''\n    add_task = NotImplemented\n    get_work = NotImplemented\n    ping = NotImplemented\n\nUPSTREAM_RUNNING = 'UPSTREAM_RUNNING'\nUPSTREAM_MISSING_INPUT = 'UPSTREAM_MISSING_INPUT'\nUPSTREAM_FAILED = 'UPSTREAM_FAILED'\n\nUPSTREAM_SEVERITY_ORDER = ('', UPSTREAM_RUNNING, UPSTREAM_MISSING_INPUT, UPSTREAM_FAILED)\nUPSTREAM_SEVERITY_KEY = lambda st: UPSTREAM_SEVERITY_ORDER.index(st)\nSTATUS_TO_UPSTREAM_MAP = {FAILED: UPSTREAM_FAILED, RUNNING: UPSTREAM_RUNNING, PENDING: UPSTREAM_MISSING_INPUT}\n\n\nclass Task(object):\n    def __init__(self, status, deps, priority=0):\n        self.stakeholders = set()  # workers ids that are somehow related to this task (i.e. don't prune while any of these workers are still active)\n        self.workers = set()  # workers ids that can perform task - task is 'BROKEN' if none of these workers are active\n        if deps is None:\n            self.deps = set()\n        else:\n            self.deps = set(deps)\n        self.status = status  # PENDING, RUNNING, FAILED or DONE\n        self.time = time.time()  # Timestamp when task was first added\n        self.retry = None\n        self.remove = None\n        self.worker_running = None  # the worker id that is currently running the task or None\n        self.time_running = None  # Timestamp when picked up by worker\n        self.expl = None\n        self.priority = priority\n\n    def __repr__(self):\n        return \"Task(%r)\" % vars(self)\n\n\nclass Worker(object):\n    \"\"\" Structure for tracking worker activity and keeping their references \"\"\"\n    def __init__(self, id, last_active=None):\n        self.id = id\n        self.reference = None  # reference to the worker in the real world. (Currently a dict containing just the host)\n        self.last_active = last_active  # seconds since epoch\n        self.info = {}\n\n    def add_info(self, info):\n        self.info.update(info)\n\n    def __str__(self):\n        return self.id\n\n\nclass CentralPlannerScheduler(Scheduler):\n    ''' Async scheduler that can handle multiple workers etc\n\n    Can be run locally or on a server (using RemoteScheduler + server.Server).\n    '''\n\n    def __init__(self, retry_delay=900.0, remove_delay=600.0, worker_disconnect_delay=60.0,\n                 state_path='/var/lib/luigi-server/state.pickle', task_history=None):\n        '''\n        (all arguments are in seconds)\n        Keyword Arguments:\n        retry_delay -- How long after a Task fails to try it again, or -1 to never retry\n        remove_delay -- How long after a Task finishes to remove it from the scheduler\n        state_path -- Path to state file (tasks and active workers)\n        worker_disconnect_delay -- If a worker hasn't communicated for this long, remove it from active workers\n        '''\n        self._state_path = state_path\n        self._tasks = {}  # map from id to a Task object\n        self._retry_delay = retry_delay\n        self._remove_delay = remove_delay\n        self._worker_disconnect_delay = worker_disconnect_delay\n        self._active_workers = {}  # map from id to a Worker object\n        self._task_history = task_history or history.NopHistory()\n\n    def dump(self):\n        state = (self._tasks, self._active_workers)\n        try:\n            with open(self._state_path, 'w') as fobj:\n                pickle.dump(state, fobj)\n        except IOError:\n            logger.warning(\"Failed saving scheduler state\", exc_info=1)\n        else:\n            logger.info(\"Saved state in %s\", self._state_path)\n\n    # prone to lead to crashes when old state is unpickled with updated code. TODO some kind of version control?\n    def load(self):\n        if os.path.exists(self._state_path):\n            logger.info(\"Attempting to load state from %s\", self._state_path)\n            with open(self._state_path) as fobj:\n                state = pickle.load(fobj)\n            self._tasks, self._active_workers = state\n\n            # Convert from old format\n            # TODO: this is really ugly, we need something more future-proof\n            # Every time we add an attribute to the Worker class, this code needs to be updated\n            for k, v in self._active_workers.iteritems():\n                if isinstance(v, float):\n                    self._active_workers[k] = Worker(id=k, last_active=v)\n        else:\n            logger.info(\"No prior state file exists at %s. Starting with clean slate\", self._state_path)\n\n    def prune(self):\n        logger.info(\"Starting pruning of task graph\")\n        # Delete workers that haven't said anything for a while (probably killed)\n        delete_workers = []\n        for worker in self._active_workers.values():\n            if worker.last_active < time.time() - self._worker_disconnect_delay:\n                logger.info(\"Worker %s timed out (no contact for >=%ss)\", worker, self._worker_disconnect_delay)\n                delete_workers.append(worker.id)\n\n        for worker in delete_workers:\n            self._active_workers.pop(worker)\n\n        remaining_workers = set(self._active_workers.keys())\n\n        # Mark tasks with no remaining active stakeholders for deletion\n        for task_id, task in self._tasks.iteritems():\n            if not task.stakeholders.intersection(remaining_workers):\n                if task.remove is None:\n                    logger.info(\"Task %r has stakeholders %r but none remain connected -> will remove task in %s seconds\", task_id, task.stakeholders, self._remove_delay)\n                    task.remove = time.time() + self._remove_delay\n\n            if task.status == RUNNING and task.worker_running and task.worker_running not in remaining_workers:\n                # If a running worker disconnects, tag all its jobs as FAILED and subject it to the same retry logic\n                logger.info(\"Task %r is marked as running by disconnected worker %r -> marking as FAILED with retry delay of %rs\", task_id, task.worker_running, self._retry_delay)\n                task.worker_running = None\n                task.status = FAILED\n                task.retry = time.time() + self._retry_delay\n\n        # Remove tasks that have no stakeholders\n        remove_tasks = []\n        for task_id, task in self._tasks.iteritems():\n            if task.remove and time.time() > task.remove:\n                logger.info(\"Removing task %r (no connected stakeholders)\", task_id)\n                remove_tasks.append(task_id)\n\n        for task_id in remove_tasks:\n            self._tasks.pop(task_id)\n\n        # Reset FAILED tasks to PENDING if max timeout is reached, and retry delay is >= 0\n        for task in self._tasks.values():\n            if task.status == FAILED and self._retry_delay >= 0 and task.retry < time.time():\n                task.status = PENDING\n        logger.info(\"Done pruning task graph\")\n\n    def update(self, worker_id, worker_reference=None):\n        \"\"\" Keep track of whenever the worker was last active \"\"\"\n        worker = self._active_workers.setdefault(worker_id, Worker(worker_id))\n        if worker_reference:\n            worker.reference = worker_reference\n        worker.last_active = time.time()\n\n    def add_task(self, worker, task_id, status=PENDING, runnable=True, deps=None, expl=None, priority=0):\n        \"\"\"\n        * Add task identified by task_id if it doesn't exist\n        * If deps is not None, update dependency list\n        * Update status of task\n        * Add additional workers/stakeholders\n        \"\"\"\n        self.update(worker)\n\n        task = self._tasks.setdefault(task_id, Task(status=PENDING, deps=deps, priority=priority))\n\n        if task.remove is not None:\n            task.remove = None  # unmark task for removal so it isn't removed after being added\n\n        if not (task.status == RUNNING and status == PENDING):\n            # don't allow re-scheduling of task while it is running, it must either fail or succeed first\n            if status == PENDING or status != task.status:\n                # Update the DB only if there was a acctual change, to prevent noise.\n                # We also check for status == PENDING b/c that's the default value\n                # (so checking for status != task.status woule lie)\n                self._update_task_history(task_id, status)\n            task.status = status\n            if status == FAILED:\n                task.retry = time.time() + self._retry_delay\n\n        if deps is not None:\n            task.deps = set(deps)\n\n        task.stakeholders.add(worker)\n\n        if runnable:\n            task.workers.add(worker)\n\n        if expl is not None:\n            task.expl = expl\n\n    def add_worker(self, worker, info):\n        self._active_workers[worker].add_info(info)\n\n    def get_work(self, worker, host=None):\n        # TODO: remove any expired nodes\n\n        # Algo: iterate over all nodes, find the first node with no dependencies and highest priority\n\n        # TODO: remove tasks that can't be done, figure out if the worker has absolutely\n        # nothing it can wait for\n\n        # Return remaining tasks that have no FAILED descendents\n        self.update(worker, {'host': host})\n        best_t = float('inf')\n        best_priority = float('-inf')\n        best_task = None\n        locally_pending_tasks = 0\n        running_tasks = []\n\n        for task_id, task in self._tasks.iteritems():\n            if worker not in task.workers:\n                continue\n\n            if task.status == RUNNING:\n                # Return a list of currently running tasks to the client,\n                # makes it easier to troubleshoot\n                other_worker = self._active_workers[task.worker_running]\n                more_info = {'task_id': task_id, 'worker': str(other_worker)}\n                if other_worker is not None:\n                    more_info.update(other_worker.info)\n                running_tasks.append(more_info)\n\n            if task.status != PENDING:\n                continue\n\n            locally_pending_tasks += 1\n            ok = True\n            for dep in task.deps:\n                if dep not in self._tasks:\n                    ok = False\n                elif self._tasks[dep].status != DONE:\n                    ok = False\n\n            if ok:\n                if (-task.priority, task.time) < (-best_priority, best_t):\n                    best_t = task.time\n                    best_priority = task.priority\n                    best_task = task_id\n\n        if best_task:\n            t = self._tasks[best_task]\n            t.status = RUNNING\n            t.worker_running = worker\n            t.time_running = time.time()\n            self._update_task_history(best_task, RUNNING, host=host)\n\n        return {'n_pending_tasks': locally_pending_tasks,\n                'task_id': best_task,\n                'running_tasks': running_tasks}\n\n    def ping(self, worker):\n        self.update(worker)\n\n    def _upstream_status(self, task_id, upstream_status_table):\n        if task_id in upstream_status_table:\n            return upstream_status_table[task_id]\n        elif task_id in self._tasks:\n            task_stack = [task_id]\n\n            while task_stack:\n                dep_id = task_stack.pop()\n                if dep_id in self._tasks:\n                    dep = self._tasks[dep_id]\n                    if dep_id not in upstream_status_table:\n                        if dep.status == PENDING and dep.deps:\n                            task_stack = task_stack + [dep_id] + list(dep.deps)\n                            upstream_status_table[dep_id] = ''  # will be updated postorder\n                        else:\n                            dep_status = STATUS_TO_UPSTREAM_MAP.get(dep.status, '')\n                            upstream_status_table[dep_id] = dep_status\n                    elif upstream_status_table[dep_id] == '' and dep.deps:\n                        # This is the postorder update step when we set the\n                        # status based on the previously calculated child elements\n                        upstream_status = [upstream_status_table.get(id, '') for id in dep.deps]\n                        upstream_status.append('')  # to handle empty list\n                        status = max(upstream_status, key=UPSTREAM_SEVERITY_KEY)\n                        upstream_status_table[dep_id] = status\n            return upstream_status_table[dep_id]\n\n    def _serialize_task(self, task_id):\n        task = self._tasks[task_id]\n        return {\n            'deps': list(task.deps),\n            'status': task.status,\n            'workers': list(task.workers),\n            'worker_running': task.worker_running,\n            'time_running': getattr(task, \"time_running\", None),\n            'start_time': task.time,\n            'params': self._get_task_params(task_id),\n            'name': self._get_task_name(task_id)\n        }\n\n    def _get_task_params(self, task_id):\n        params = {}\n        params_part = task_id.split('(')[1].strip(')')\n        params_strings = params_part.split(\", \")\n\n        for param in params_strings:\n            if not param:\n                continue\n            split_param = param.split('=')\n            if len(split_param) != 2:\n                return {'<complex parameters>': params_part}\n            params[split_param[0]] = split_param[1]\n        return params\n\n    def _get_task_name(self, task_id):\n        return task_id.split('(')[0]\n\n    def graph(self):\n        self.prune()\n        serialized = {}\n        for task_id, task in self._tasks.iteritems():\n            serialized[task_id] = self._serialize_task(task_id)\n        return serialized\n\n    def _recurse_deps(self, task_id, serialized):\n        if task_id not in serialized:\n            task = self._tasks.get(task_id)\n            if task is None:\n                logger.warn('Missing task for id [%s]', task_id)\n                serialized[task_id] = {\n                    'deps': [],\n                    'status': UNKNOWN,\n                    'workers': [],\n                    'start_time': UNKNOWN,\n                    'params': self._get_task_params(task_id),\n                    'name': self._get_task_name(task_id)\n                }\n            else:\n                serialized[task_id] = self._serialize_task(task_id)\n                for dep in task.deps:\n                    self._recurse_deps(dep, serialized)\n\n    def dep_graph(self, task_id):\n        self.prune()\n        serialized = {}\n        if task_id in self._tasks:\n            self._recurse_deps(task_id, serialized)\n        return serialized\n\n    def task_list(self, status, upstream_status):\n        ''' query for a subset of tasks by status '''\n        self.prune()\n        result = {}\n        upstream_status_table = {}  # used to memoize upstream status\n        for task_id, task in self._tasks.iteritems():\n            if not status or task.status == status:\n                if (task.status != PENDING or not upstream_status or\n                    upstream_status == self._upstream_status(task_id, upstream_status_table)):\n                    serialized = self._serialize_task(task_id)\n                    result[task_id] = serialized\n        return result\n\n    def inverse_dependencies(self, task_id):\n        self.prune()\n        serialized = {}\n        if task_id in self._tasks:\n            self._traverse_inverse_deps(task_id, serialized)\n        return serialized\n\n    def _traverse_inverse_deps(self, task_id, serialized):\n        stack = [task_id]\n        serialized[task_id] = self._serialize_task(task_id)\n        while len(stack) > 0:\n            curr_id = stack.pop()\n            for id, task in self._tasks.iteritems():\n                if curr_id in task.deps:\n                    serialized[curr_id][\"deps\"].append(id)\n                    if id not in serialized:\n                        serialized[id] = self._serialize_task(id)\n                        serialized[id][\"deps\"] = []\n                        stack.append(id)\n\n    def task_search(self, task_str):\n        ''' query for a subset of tasks by task_id '''\n        self.prune()\n        result = collections.defaultdict(dict)\n        for task_id, task in self._tasks.iteritems():\n            if task_id.find(task_str) != -1:\n                serialized = self._serialize_task(task_id)\n                result[task.status][task_id] = serialized\n        return result\n\n    def fetch_error(self, task_id):\n        if self._tasks[task_id].expl is not None:\n            return {\"taskId\": task_id, \"error\": self._tasks[task_id].expl}\n        else:\n            return {\"taskId\": task_id, \"error\": \"\"}\n\n    def _update_task_history(self, task_id, status, host=None):\n        try:\n            if status == DONE or status == FAILED:\n                successful = (status == DONE)\n                self._task_history.task_finished(task_id, successful)\n            elif status == PENDING:\n                self._task_history.task_scheduled(task_id)\n            elif status == RUNNING:\n                self._task_history.task_started(task_id, host)\n        except:\n            logger.warning(\"Error saving Task history\", exc_info=1)\n\n    @property\n    def task_history(self):\n        # Used by server.py to expose the calls\n        return self._task_history\n",
          "file_patch": "@@ -69,9 +69,13 @@ class Worker(object):\n         self.id = id\n         self.reference = None  # reference to the worker in the real world. (Currently a dict containing just the host)\n         self.last_active = last_active  # seconds since epoch\n+        self.info = {}\n+\n+    def add_info(self, info):\n+        self.info.update(info)\n \n     def __str__(self):\n-        return \"%s on %s, last active %s\" % (self.id, self.reference, datetime.datetime.utcfromtimestamp(self.last_active).isoformat())\n+        return self.id\n \n \n class CentralPlannerScheduler(Scheduler):\n@@ -212,6 +216,9 @@ class CentralPlannerScheduler(Scheduler):\n         if expl is not None:\n             task.expl = expl\n \n+    def add_worker(self, worker, info):\n+        self._active_workers[worker].add_info(info)\n+\n     def get_work(self, worker, host=None):\n         # TODO: remove any expired nodes\n \n@@ -233,7 +240,13 @@ class CentralPlannerScheduler(Scheduler):\n                 continue\n \n             if task.status == RUNNING:\n-                running_tasks.append({'task_id': task_id, 'worker': str(self._active_workers.get(task.worker_running))})\n+                # Return a list of currently running tasks to the client,\n+                # makes it easier to troubleshoot\n+                other_worker = self._active_workers[task.worker_running]\n+                more_info = {'task_id': task_id, 'worker': str(other_worker)}\n+                if other_worker is not None:\n+                    more_info.update(other_worker.info)\n+                running_tasks.append(more_info)\n \n             if task.status != PENDING:\n                 continue\n",
          "files_name_in_blame_commit": [
            "rpc.py",
            "worker.py",
            "central_planner_test.py",
            "scheduler.py"
          ]
        }
      },
      "ea8476655ba625d0e2d51cf86a179603bf49c707": {
        "commit": {
          "commit_id": "ea8476655ba625d0e2d51cf86a179603bf49c707",
          "commit_message": "Fixed issue #341 and added unit test",
          "commit_author": "Erik Bernhardsson",
          "commit_date": "2014-06-02 08:28:56",
          "commit_parent": "21735765ba318719bccb3502c49516e2d7635451"
        },
        "function": {
          "function_name": "__init__",
          "function_code_before": "def __init__(self, id):\n    self.id = id\n    self.reference = None\n    self.last_active = None",
          "function_code_after": "def __init__(self, id, last_active=None):\n    self.id = id\n    self.reference = None\n    self.last_active = last_active",
          "function_before_start_line": 65,
          "function_before_end_line": 68,
          "function_after_start_line": 65,
          "function_after_end_line": 68,
          "function_before_token_count": 22,
          "function_after_token_count": 0,
          "functions_name_modified_file": [
            "_update_task_history",
            "prune",
            "_upstream_status",
            "ping",
            "_traverse_inverse_deps",
            "inverse_dependencies",
            "load",
            "_recurse_deps",
            "add_task",
            "task_list",
            "_serialize_task",
            "task_history",
            "dep_graph",
            "__str__",
            "graph",
            "dump",
            "__repr__",
            "__init__",
            "update",
            "_get_task_params",
            "get_work",
            "_get_task_name",
            "fetch_error"
          ],
          "functions_name_all_files": [
            "_update_task_history",
            "prune",
            "_upstream_status",
            "ping",
            "_traverse_inverse_deps",
            "inverse_dependencies",
            "load",
            "_recurse_deps",
            "test_load_old_state",
            "add_task",
            "task_list",
            "_serialize_task",
            "task_history",
            "dep_graph",
            "__str__",
            "graph",
            "dump",
            "__repr__",
            "__init__",
            "update",
            "_get_task_params",
            "get_work",
            "_get_task_name",
            "fetch_error"
          ],
          "functions_name_co_evolved_modified_file": [
            "load"
          ],
          "functions_name_co_evolved_all_files": [
            "load",
            "test_load_old_state"
          ]
        },
        "file": {
          "file_name": "scheduler.py",
          "file_nloc": 290,
          "file_complexity": 101,
          "file_token_count": 2137,
          "file_before": "# Copyright (c) 2012 Spotify AB\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"); you may not\n# use this file except in compliance with the License. You may obtain a copy of\n# the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n# License for the specific language governing permissions and limitations under\n# the License.\n\nimport datetime\nimport os\nimport logging\nimport time\nimport cPickle as pickle\nimport task_history as history\nlogger = logging.getLogger(\"luigi.server\")\n\nfrom task_status import PENDING, FAILED, DONE, RUNNING, UNKNOWN\n\n\nclass Scheduler(object):\n    ''' Abstract base class\n\n    Note that the methods all take string arguments, not Task objects...\n    '''\n    add_task = NotImplemented\n    get_work = NotImplemented\n    ping = NotImplemented\n\nUPSTREAM_RUNNING = 'UPSTREAM_RUNNING'\nUPSTREAM_MISSING_INPUT = 'UPSTREAM_MISSING_INPUT'\nUPSTREAM_FAILED = 'UPSTREAM_FAILED'\n\nUPSTREAM_SEVERITY_ORDER = ('', UPSTREAM_RUNNING, UPSTREAM_MISSING_INPUT, UPSTREAM_FAILED)\nUPSTREAM_SEVERITY_KEY = lambda st: UPSTREAM_SEVERITY_ORDER.index(st)\nSTATUS_TO_UPSTREAM_MAP = {FAILED: UPSTREAM_FAILED, RUNNING: UPSTREAM_RUNNING, PENDING: UPSTREAM_MISSING_INPUT}\n\n\nclass Task(object):\n    def __init__(self, status, deps):\n        self.stakeholders = set()  # workers ids that are somehow related to this task (i.e. don't prune while any of these workers are still active)\n        self.workers = set()  # workers ids that can perform task - task is 'BROKEN' if none of these workers are active\n        if deps is None:\n            self.deps = set()\n        else:\n            self.deps = set(deps)\n        self.status = status  # PENDING, RUNNING, FAILED or DONE\n        self.time = time.time()  # Timestamp when task was first added\n        self.retry = None\n        self.remove = None\n        self.worker_running = None  # the worker id that is currently running the task or None\n        self.expl = None\n\n    def __repr__(self):\n        return \"Task(%r)\" % vars(self)\n\n\nclass Worker(object):\n    \"\"\" Structure for tracking worker activity and keeping their references \"\"\"\n    def __init__(self, id):\n        self.id = id\n        self.reference = None  # reference to the worker in the real world. (Currently a dict containing just the host)\n        self.last_active = None  # seconds since epoch\n\n    def __str__(self):\n        return \"%s on %s, last active %s\" % (self.id, self.reference, datetime.datetime.utcfromtimestamp(self.last_active).isoformat())\n\n\nclass CentralPlannerScheduler(Scheduler):\n    ''' Async scheduler that can handle multiple workers etc\n\n    Can be run locally or on a server (using RemoteScheduler + server.Server).\n    '''\n\n    def __init__(self, retry_delay=900.0, remove_delay=600.0, worker_disconnect_delay=60.0,\n                 state_path='/var/lib/luigi-server/state.pickle', task_history=None):\n        '''\n        (all arguments are in seconds)\n        Keyword Arguments:\n        retry_delay -- How long after a Task fails to try it again, or -1 to never retry\n        remove_delay -- How long after a Task finishes to remove it from the scheduler\n        state_path -- Path to state file (tasks and active workers)\n        worker_disconnect_delay -- If a worker hasn't communicated for this long, remove it from active workers\n        '''\n        self._state_path = state_path\n        self._tasks = {}  # map from id to a Task object\n        self._retry_delay = retry_delay\n        self._remove_delay = remove_delay\n        self._worker_disconnect_delay = worker_disconnect_delay\n        self._active_workers = {}  # map from id to a Worker object\n        self._task_history = task_history or history.NopHistory()\n\n    def dump(self):\n        state = (self._tasks, self._active_workers)\n        try:\n            with open(self._state_path, 'w') as fobj:\n                pickle.dump(state, fobj)\n        except IOError:\n            logger.warning(\"Failed saving scheduler state\", exc_info=1)\n        else:\n            logger.info(\"Saved state in %s\", self._state_path)\n\n    # prone to lead to crashes when old state is unpickled with updated code. TODO some kind of version control?\n    def load(self):\n        if os.path.exists(self._state_path):\n            logger.info(\"Attempting to load state from %s\", self._state_path)\n            with open(self._state_path) as fobj:\n                state = pickle.load(fobj)\n            self._tasks, self._active_workers = state\n        else:\n            logger.info(\"No prior state file exists at %s. Starting with clean slate\", self._state_path)\n\n    def prune(self):\n        logger.info(\"Starting pruning of task graph\")\n        # Delete workers that haven't said anything for a while (probably killed)\n        delete_workers = []\n        for worker in self._active_workers.values():\n            if worker.last_active < time.time() - self._worker_disconnect_delay:\n                logger.info(\"Worker %s timed out (no contact for >=%ss)\", worker, self._worker_disconnect_delay)\n                delete_workers.append(worker.id)\n\n        for worker in delete_workers:\n            self._active_workers.pop(worker)\n\n        remaining_workers = set(self._active_workers.keys())\n\n        # Mark tasks with no remaining active stakeholders for deletion\n        for task_id, task in self._tasks.iteritems():\n            if not task.stakeholders.intersection(remaining_workers):\n                if task.remove is None:\n                    logger.info(\"Task %r has stakeholders %r but none remain connected -> will remove task in %s seconds\", task_id, task.stakeholders, self._remove_delay)\n                    task.remove = time.time() + self._remove_delay\n\n            if task.status == RUNNING and task.worker_running and task.worker_running not in remaining_workers:\n                # If a running worker disconnects, tag all its jobs as FAILED and subject it to the same retry logic\n                logger.info(\"Task %r is marked as running by disconnected worker %r -> marking as FAILED with retry delay of %rs\", task_id, task.worker_running, self._retry_delay)\n                task.worker_running = None\n                task.status = FAILED\n                task.retry = time.time() + self._retry_delay\n\n        # Remove tasks that have no stakeholders\n        remove_tasks = []\n        for task_id, task in self._tasks.iteritems():\n            if task.remove and time.time() > task.remove:\n                logger.info(\"Removing task %r (no connected stakeholders)\", task_id)\n                remove_tasks.append(task_id)\n\n        for task_id in remove_tasks:\n            self._tasks.pop(task_id)\n\n        # Reset FAILED tasks to PENDING if max timeout is reached, and retry delay is >= 0\n        for task in self._tasks.values():\n            if task.status == FAILED and self._retry_delay >= 0 and task.retry < time.time():\n                task.status = PENDING\n        logger.info(\"Done pruning task graph\")\n\n    def update(self, worker_id, worker_reference=None):\n        \"\"\" Keep track of whenever the worker was last active \"\"\"\n        worker = self._active_workers.setdefault(worker_id, Worker(worker_id))\n        if worker_reference:\n            worker.reference = worker_reference\n        worker.last_active = time.time()\n\n    def add_task(self, worker, task_id, status=PENDING, runnable=True, deps=None, expl=None):\n        \"\"\"\n        * Add task identified by task_id if it doesn't exist\n        * If deps is not None, update dependency list\n        * Update status of task\n        * Add additional workers/stakeholders\n        \"\"\"\n        self.update(worker)\n\n        task = self._tasks.setdefault(task_id, Task(status=PENDING, deps=deps))\n\n        if task.remove is not None:\n            task.remove = None  # unmark task for removal so it isn't removed after being added\n\n        if not (task.status == RUNNING and status == PENDING):\n            # don't allow re-scheduling of task while it is running, it must either fail or succeed first\n            task.status = status\n            if status == FAILED:\n                task.retry = time.time() + self._retry_delay\n\n        if deps is not None:\n            task.deps = set(deps)\n\n        task.stakeholders.add(worker)\n\n        if runnable:\n            task.workers.add(worker)\n\n        if expl is not None:\n            task.expl = expl\n        self._update_task_history(task_id, status)\n\n    def get_work(self, worker, host=None):\n        # TODO: remove any expired nodes\n\n        # Algo: iterate over all nodes, find first node with no dependencies\n\n        # TODO: remove tasks that can't be done, figure out if the worker has absolutely\n        # nothing it can wait for\n\n        # Return remaining tasks that have no FAILED descendents\n        self.update(worker, {'host': host})\n        best_t = float('inf')\n        best_task = None\n        locally_pending_tasks = 0\n        running_tasks = []\n\n        for task_id, task in self._tasks.iteritems():\n            if worker not in task.workers:\n                continue\n\n            if task.status == RUNNING:\n                running_tasks.append({'task_id': task_id, 'worker': str(self._active_workers.get(task.worker_running))})\n\n            if task.status != PENDING:\n                continue\n\n            locally_pending_tasks += 1\n            ok = True\n            for dep in task.deps:\n                if dep not in self._tasks:\n                    ok = False\n                elif self._tasks[dep].status != DONE:\n                    ok = False\n\n            if ok:\n                if task.time < best_t:\n                    best_t = task.time\n                    best_task = task_id\n\n        if best_task:\n            t = self._tasks[best_task]\n            t.status = RUNNING\n            t.worker_running = worker\n            self._update_task_history(best_task, RUNNING, host=host)\n\n        return {'n_pending_tasks': locally_pending_tasks,\n                'task_id': best_task,\n                'running_tasks': running_tasks}\n\n    def ping(self, worker):\n        self.update(worker)\n\n    def _upstream_status(self, task_id, upstream_status_table):\n        if task_id in upstream_status_table:\n            return upstream_status_table[task_id]\n        elif task_id in self._tasks:\n            task_stack = [task_id]\n\n            while task_stack:\n                dep_id = task_stack.pop()\n                if dep_id in self._tasks:\n                    dep = self._tasks[dep_id]\n                    if dep_id not in upstream_status_table:\n                        if dep.status == PENDING and dep.deps:\n                            task_stack = task_stack + [dep_id] + list(dep.deps)\n                            upstream_status_table[dep_id] = ''  # will be updated postorder\n                        else:\n                            dep_status = STATUS_TO_UPSTREAM_MAP.get(dep.status, '')\n                            upstream_status_table[dep_id] = dep_status\n                    elif upstream_status_table[dep_id] == '' and dep.deps:\n                        # This is the postorder update step when we set the\n                        # status based on the previously calculated child elements\n                        upstream_status = [upstream_status_table.get(id, '') for id in dep.deps]\n                        upstream_status.append('')  # to handle empty list\n                        status = max(upstream_status, key=UPSTREAM_SEVERITY_KEY)\n                        upstream_status_table[dep_id] = status\n            return upstream_status_table[dep_id]\n\n    def _serialize_task(self, task_id):\n        task = self._tasks[task_id]\n        return {\n            'deps': list(task.deps),\n            'status': task.status,\n            'workers': list(task.workers),\n            'start_time': task.time,\n            'params': self._get_task_params(task_id),\n            'name': self._get_task_name(task_id)\n        }\n\n    def _get_task_params(self, task_id):\n        params = {}\n        params_part = task_id.split('(')[1].strip(')')\n        params_strings = params_part.split(\", \")\n\n        for param in params_strings:\n            if not param:\n                continue\n            split_param = param.split('=')\n            if len(split_param) != 2:\n                return {'<complex parameters>': params_part}\n            params[split_param[0]] = split_param[1]\n        return params\n\n    def _get_task_name(self, task_id):\n        return task_id.split('(')[0]\n\n    def graph(self):\n        self.prune()\n        serialized = {}\n        for task_id, task in self._tasks.iteritems():\n            serialized[task_id] = self._serialize_task(task_id)\n        return serialized\n\n    def _recurse_deps(self, task_id, serialized):\n        if task_id not in serialized:\n            task = self._tasks.get(task_id)\n            if task is None:\n                logger.warn('Missing task for id [%s]', task_id)\n                serialized[task_id] = {\n                    'deps': [],\n                    'status': UNKNOWN,\n                    'workers': [],\n                    'start_time': UNKNOWN,\n                    'params': self._get_task_params(task_id),\n                    'name': self._get_task_name(task_id)\n                }\n            else:\n                serialized[task_id] = self._serialize_task(task_id)\n                for dep in task.deps:\n                    self._recurse_deps(dep, serialized)\n\n    def dep_graph(self, task_id):\n        self.prune()\n        serialized = {}\n        if task_id in self._tasks:\n            self._recurse_deps(task_id, serialized)\n        return serialized\n\n    def task_list(self, status, upstream_status):\n        ''' query for a subset of tasks by status '''\n        self.prune()\n        result = {}\n        upstream_status_table = {}  # used to memoize upstream status\n        for task_id, task in self._tasks.iteritems():\n            if not status or task.status == status:\n                if (task.status != PENDING or not upstream_status or\n                    upstream_status == self._upstream_status(task_id, upstream_status_table)):\n                    serialized = self._serialize_task(task_id)\n                    result[task_id] = serialized\n        return result\n\n    def inverse_dependencies(self, task_id):\n        self.prune()\n        serialized = {}\n        if task_id in self._tasks:\n            self._traverse_inverse_deps(task_id, serialized)\n        return serialized\n\n    def _traverse_inverse_deps(self, task_id, serialized):\n        stack = [task_id]\n        serialized[task_id] = self._serialize_task(task_id)\n        while len(stack) > 0:\n            curr_id = stack.pop()\n            for id, task in self._tasks.iteritems():\n                if curr_id in task.deps:\n                    serialized[curr_id][\"deps\"].append(id)\n                    if id not in serialized:\n                        serialized[id] = self._serialize_task(id)\n                        serialized[id][\"deps\"] = []\n                        stack.append(id)\n\n    def fetch_error(self, task_id):\n        if self._tasks[task_id].expl is not None:\n            return {\"taskId\": task_id, \"error\": self._tasks[task_id].expl}\n        else:\n            return {\"taskId\": task_id, \"error\": \"\"}\n\n    def _update_task_history(self, task_id, status, host=None):\n        try:\n            if status == DONE or status == FAILED:\n                successful = (status == DONE)\n                self._task_history.task_finished(task_id, successful)\n            elif status == PENDING:\n                self._task_history.task_scheduled(task_id)\n            elif status == RUNNING:\n                self._task_history.task_started(task_id, host)\n        except:\n            logger.warning(\"Error saving Task history\", exc_info=1)\n\n    @property\n    def task_history(self):\n        # Used by server.py to expose the calls\n        return self._task_history\n",
          "file_after": "# Copyright (c) 2012 Spotify AB\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"); you may not\n# use this file except in compliance with the License. You may obtain a copy of\n# the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n# License for the specific language governing permissions and limitations under\n# the License.\n\nimport datetime\nimport os\nimport logging\nimport time\nimport cPickle as pickle\nimport task_history as history\nlogger = logging.getLogger(\"luigi.server\")\n\nfrom task_status import PENDING, FAILED, DONE, RUNNING, UNKNOWN\n\n\nclass Scheduler(object):\n    ''' Abstract base class\n\n    Note that the methods all take string arguments, not Task objects...\n    '''\n    add_task = NotImplemented\n    get_work = NotImplemented\n    ping = NotImplemented\n\nUPSTREAM_RUNNING = 'UPSTREAM_RUNNING'\nUPSTREAM_MISSING_INPUT = 'UPSTREAM_MISSING_INPUT'\nUPSTREAM_FAILED = 'UPSTREAM_FAILED'\n\nUPSTREAM_SEVERITY_ORDER = ('', UPSTREAM_RUNNING, UPSTREAM_MISSING_INPUT, UPSTREAM_FAILED)\nUPSTREAM_SEVERITY_KEY = lambda st: UPSTREAM_SEVERITY_ORDER.index(st)\nSTATUS_TO_UPSTREAM_MAP = {FAILED: UPSTREAM_FAILED, RUNNING: UPSTREAM_RUNNING, PENDING: UPSTREAM_MISSING_INPUT}\n\n\nclass Task(object):\n    def __init__(self, status, deps):\n        self.stakeholders = set()  # workers ids that are somehow related to this task (i.e. don't prune while any of these workers are still active)\n        self.workers = set()  # workers ids that can perform task - task is 'BROKEN' if none of these workers are active\n        if deps is None:\n            self.deps = set()\n        else:\n            self.deps = set(deps)\n        self.status = status  # PENDING, RUNNING, FAILED or DONE\n        self.time = time.time()  # Timestamp when task was first added\n        self.retry = None\n        self.remove = None\n        self.worker_running = None  # the worker id that is currently running the task or None\n        self.expl = None\n\n    def __repr__(self):\n        return \"Task(%r)\" % vars(self)\n\n\nclass Worker(object):\n    \"\"\" Structure for tracking worker activity and keeping their references \"\"\"\n    def __init__(self, id, last_active=None):\n        self.id = id\n        self.reference = None  # reference to the worker in the real world. (Currently a dict containing just the host)\n        self.last_active = last_active  # seconds since epoch\n\n    def __str__(self):\n        return \"%s on %s, last active %s\" % (self.id, self.reference, datetime.datetime.utcfromtimestamp(self.last_active).isoformat())\n\n\nclass CentralPlannerScheduler(Scheduler):\n    ''' Async scheduler that can handle multiple workers etc\n\n    Can be run locally or on a server (using RemoteScheduler + server.Server).\n    '''\n\n    def __init__(self, retry_delay=900.0, remove_delay=600.0, worker_disconnect_delay=60.0,\n                 state_path='/var/lib/luigi-server/state.pickle', task_history=None):\n        '''\n        (all arguments are in seconds)\n        Keyword Arguments:\n        retry_delay -- How long after a Task fails to try it again, or -1 to never retry\n        remove_delay -- How long after a Task finishes to remove it from the scheduler\n        state_path -- Path to state file (tasks and active workers)\n        worker_disconnect_delay -- If a worker hasn't communicated for this long, remove it from active workers\n        '''\n        self._state_path = state_path\n        self._tasks = {}  # map from id to a Task object\n        self._retry_delay = retry_delay\n        self._remove_delay = remove_delay\n        self._worker_disconnect_delay = worker_disconnect_delay\n        self._active_workers = {}  # map from id to a Worker object\n        self._task_history = task_history or history.NopHistory()\n\n    def dump(self):\n        state = (self._tasks, self._active_workers)\n        try:\n            with open(self._state_path, 'w') as fobj:\n                pickle.dump(state, fobj)\n        except IOError:\n            logger.warning(\"Failed saving scheduler state\", exc_info=1)\n        else:\n            logger.info(\"Saved state in %s\", self._state_path)\n\n    # prone to lead to crashes when old state is unpickled with updated code. TODO some kind of version control?\n    def load(self):\n        if os.path.exists(self._state_path):\n            logger.info(\"Attempting to load state from %s\", self._state_path)\n            with open(self._state_path) as fobj:\n                state = pickle.load(fobj)\n            self._tasks, self._active_workers = state\n\n            # Convert from old format\n            # TODO: this is really ugly, we need something more future-proof\n            # Every time we add an attribute to the Worker class, this code needs to be updated\n            for k, v in self._active_workers.iteritems():\n                if isinstance(v, float):\n                    self._active_workers[k] = Worker(id=k, last_active=v)\n        else:\n            logger.info(\"No prior state file exists at %s. Starting with clean slate\", self._state_path)\n\n    def prune(self):\n        logger.info(\"Starting pruning of task graph\")\n        # Delete workers that haven't said anything for a while (probably killed)\n        delete_workers = []\n        for worker in self._active_workers.values():\n            if worker.last_active < time.time() - self._worker_disconnect_delay:\n                logger.info(\"Worker %s timed out (no contact for >=%ss)\", worker, self._worker_disconnect_delay)\n                delete_workers.append(worker.id)\n\n        for worker in delete_workers:\n            self._active_workers.pop(worker)\n\n        remaining_workers = set(self._active_workers.keys())\n\n        # Mark tasks with no remaining active stakeholders for deletion\n        for task_id, task in self._tasks.iteritems():\n            if not task.stakeholders.intersection(remaining_workers):\n                if task.remove is None:\n                    logger.info(\"Task %r has stakeholders %r but none remain connected -> will remove task in %s seconds\", task_id, task.stakeholders, self._remove_delay)\n                    task.remove = time.time() + self._remove_delay\n\n            if task.status == RUNNING and task.worker_running and task.worker_running not in remaining_workers:\n                # If a running worker disconnects, tag all its jobs as FAILED and subject it to the same retry logic\n                logger.info(\"Task %r is marked as running by disconnected worker %r -> marking as FAILED with retry delay of %rs\", task_id, task.worker_running, self._retry_delay)\n                task.worker_running = None\n                task.status = FAILED\n                task.retry = time.time() + self._retry_delay\n\n        # Remove tasks that have no stakeholders\n        remove_tasks = []\n        for task_id, task in self._tasks.iteritems():\n            if task.remove and time.time() > task.remove:\n                logger.info(\"Removing task %r (no connected stakeholders)\", task_id)\n                remove_tasks.append(task_id)\n\n        for task_id in remove_tasks:\n            self._tasks.pop(task_id)\n\n        # Reset FAILED tasks to PENDING if max timeout is reached, and retry delay is >= 0\n        for task in self._tasks.values():\n            if task.status == FAILED and self._retry_delay >= 0 and task.retry < time.time():\n                task.status = PENDING\n        logger.info(\"Done pruning task graph\")\n\n    def update(self, worker_id, worker_reference=None):\n        \"\"\" Keep track of whenever the worker was last active \"\"\"\n        worker = self._active_workers.setdefault(worker_id, Worker(worker_id))\n        if worker_reference:\n            worker.reference = worker_reference\n        worker.last_active = time.time()\n\n    def add_task(self, worker, task_id, status=PENDING, runnable=True, deps=None, expl=None):\n        \"\"\"\n        * Add task identified by task_id if it doesn't exist\n        * If deps is not None, update dependency list\n        * Update status of task\n        * Add additional workers/stakeholders\n        \"\"\"\n        self.update(worker)\n\n        task = self._tasks.setdefault(task_id, Task(status=PENDING, deps=deps))\n\n        if task.remove is not None:\n            task.remove = None  # unmark task for removal so it isn't removed after being added\n\n        if not (task.status == RUNNING and status == PENDING):\n            # don't allow re-scheduling of task while it is running, it must either fail or succeed first\n            task.status = status\n            if status == FAILED:\n                task.retry = time.time() + self._retry_delay\n\n        if deps is not None:\n            task.deps = set(deps)\n\n        task.stakeholders.add(worker)\n\n        if runnable:\n            task.workers.add(worker)\n\n        if expl is not None:\n            task.expl = expl\n        self._update_task_history(task_id, status)\n\n    def get_work(self, worker, host=None):\n        # TODO: remove any expired nodes\n\n        # Algo: iterate over all nodes, find first node with no dependencies\n\n        # TODO: remove tasks that can't be done, figure out if the worker has absolutely\n        # nothing it can wait for\n\n        # Return remaining tasks that have no FAILED descendents\n        self.update(worker, {'host': host})\n        best_t = float('inf')\n        best_task = None\n        locally_pending_tasks = 0\n        running_tasks = []\n\n        for task_id, task in self._tasks.iteritems():\n            if worker not in task.workers:\n                continue\n\n            if task.status == RUNNING:\n                running_tasks.append({'task_id': task_id, 'worker': str(self._active_workers.get(task.worker_running))})\n\n            if task.status != PENDING:\n                continue\n\n            locally_pending_tasks += 1\n            ok = True\n            for dep in task.deps:\n                if dep not in self._tasks:\n                    ok = False\n                elif self._tasks[dep].status != DONE:\n                    ok = False\n\n            if ok:\n                if task.time < best_t:\n                    best_t = task.time\n                    best_task = task_id\n\n        if best_task:\n            t = self._tasks[best_task]\n            t.status = RUNNING\n            t.worker_running = worker\n            self._update_task_history(best_task, RUNNING, host=host)\n\n        return {'n_pending_tasks': locally_pending_tasks,\n                'task_id': best_task,\n                'running_tasks': running_tasks}\n\n    def ping(self, worker):\n        self.update(worker)\n\n    def _upstream_status(self, task_id, upstream_status_table):\n        if task_id in upstream_status_table:\n            return upstream_status_table[task_id]\n        elif task_id in self._tasks:\n            task_stack = [task_id]\n\n            while task_stack:\n                dep_id = task_stack.pop()\n                if dep_id in self._tasks:\n                    dep = self._tasks[dep_id]\n                    if dep_id not in upstream_status_table:\n                        if dep.status == PENDING and dep.deps:\n                            task_stack = task_stack + [dep_id] + list(dep.deps)\n                            upstream_status_table[dep_id] = ''  # will be updated postorder\n                        else:\n                            dep_status = STATUS_TO_UPSTREAM_MAP.get(dep.status, '')\n                            upstream_status_table[dep_id] = dep_status\n                    elif upstream_status_table[dep_id] == '' and dep.deps:\n                        # This is the postorder update step when we set the\n                        # status based on the previously calculated child elements\n                        upstream_status = [upstream_status_table.get(id, '') for id in dep.deps]\n                        upstream_status.append('')  # to handle empty list\n                        status = max(upstream_status, key=UPSTREAM_SEVERITY_KEY)\n                        upstream_status_table[dep_id] = status\n            return upstream_status_table[dep_id]\n\n    def _serialize_task(self, task_id):\n        task = self._tasks[task_id]\n        return {\n            'deps': list(task.deps),\n            'status': task.status,\n            'workers': list(task.workers),\n            'start_time': task.time,\n            'params': self._get_task_params(task_id),\n            'name': self._get_task_name(task_id)\n        }\n\n    def _get_task_params(self, task_id):\n        params = {}\n        params_part = task_id.split('(')[1].strip(')')\n        params_strings = params_part.split(\", \")\n\n        for param in params_strings:\n            if not param:\n                continue\n            split_param = param.split('=')\n            if len(split_param) != 2:\n                return {'<complex parameters>': params_part}\n            params[split_param[0]] = split_param[1]\n        return params\n\n    def _get_task_name(self, task_id):\n        return task_id.split('(')[0]\n\n    def graph(self):\n        self.prune()\n        serialized = {}\n        for task_id, task in self._tasks.iteritems():\n            serialized[task_id] = self._serialize_task(task_id)\n        return serialized\n\n    def _recurse_deps(self, task_id, serialized):\n        if task_id not in serialized:\n            task = self._tasks.get(task_id)\n            if task is None:\n                logger.warn('Missing task for id [%s]', task_id)\n                serialized[task_id] = {\n                    'deps': [],\n                    'status': UNKNOWN,\n                    'workers': [],\n                    'start_time': UNKNOWN,\n                    'params': self._get_task_params(task_id),\n                    'name': self._get_task_name(task_id)\n                }\n            else:\n                serialized[task_id] = self._serialize_task(task_id)\n                for dep in task.deps:\n                    self._recurse_deps(dep, serialized)\n\n    def dep_graph(self, task_id):\n        self.prune()\n        serialized = {}\n        if task_id in self._tasks:\n            self._recurse_deps(task_id, serialized)\n        return serialized\n\n    def task_list(self, status, upstream_status):\n        ''' query for a subset of tasks by status '''\n        self.prune()\n        result = {}\n        upstream_status_table = {}  # used to memoize upstream status\n        for task_id, task in self._tasks.iteritems():\n            if not status or task.status == status:\n                if (task.status != PENDING or not upstream_status or\n                    upstream_status == self._upstream_status(task_id, upstream_status_table)):\n                    serialized = self._serialize_task(task_id)\n                    result[task_id] = serialized\n        return result\n\n    def inverse_dependencies(self, task_id):\n        self.prune()\n        serialized = {}\n        if task_id in self._tasks:\n            self._traverse_inverse_deps(task_id, serialized)\n        return serialized\n\n    def _traverse_inverse_deps(self, task_id, serialized):\n        stack = [task_id]\n        serialized[task_id] = self._serialize_task(task_id)\n        while len(stack) > 0:\n            curr_id = stack.pop()\n            for id, task in self._tasks.iteritems():\n                if curr_id in task.deps:\n                    serialized[curr_id][\"deps\"].append(id)\n                    if id not in serialized:\n                        serialized[id] = self._serialize_task(id)\n                        serialized[id][\"deps\"] = []\n                        stack.append(id)\n\n    def fetch_error(self, task_id):\n        if self._tasks[task_id].expl is not None:\n            return {\"taskId\": task_id, \"error\": self._tasks[task_id].expl}\n        else:\n            return {\"taskId\": task_id, \"error\": \"\"}\n\n    def _update_task_history(self, task_id, status, host=None):\n        try:\n            if status == DONE or status == FAILED:\n                successful = (status == DONE)\n                self._task_history.task_finished(task_id, successful)\n            elif status == PENDING:\n                self._task_history.task_scheduled(task_id)\n            elif status == RUNNING:\n                self._task_history.task_started(task_id, host)\n        except:\n            logger.warning(\"Error saving Task history\", exc_info=1)\n\n    @property\n    def task_history(self):\n        # Used by server.py to expose the calls\n        return self._task_history\n",
          "file_patch": "@@ -62,10 +62,10 @@ class Task(object):\n \n class Worker(object):\n     \"\"\" Structure for tracking worker activity and keeping their references \"\"\"\n-    def __init__(self, id):\n+    def __init__(self, id, last_active=None):\n         self.id = id\n         self.reference = None  # reference to the worker in the real world. (Currently a dict containing just the host)\n-        self.last_active = None  # seconds since epoch\n+        self.last_active = last_active  # seconds since epoch\n \n     def __str__(self):\n         return \"%s on %s, last active %s\" % (self.id, self.reference, datetime.datetime.utcfromtimestamp(self.last_active).isoformat())\n@@ -112,6 +112,13 @@ class CentralPlannerScheduler(Scheduler):\n             with open(self._state_path) as fobj:\n                 state = pickle.load(fobj)\n             self._tasks, self._active_workers = state\n+\n+            # Convert from old format\n+            # TODO: this is really ugly, we need something more future-proof\n+            # Every time we add an attribute to the Worker class, this code needs to be updated\n+            for k, v in self._active_workers.iteritems():\n+                if isinstance(v, float):\n+                    self._active_workers[k] = Worker(id=k, last_active=v)\n         else:\n             logger.info(\"No prior state file exists at %s. Starting with clean slate\", self._state_path)\n \n",
          "files_name_in_blame_commit": [
            "scheduler.py",
            "scheduler_test.py"
          ]
        }
      },
      "c75085b3c812c8054ef76d39676ab473e45b9600": {
        "commit": {
          "commit_id": "c75085b3c812c8054ef76d39676ab473e45b9600",
          "commit_message": "include worker id and host in log output\n\nMakes log output more usable for troubleshooting when somewhere a worker process is hung. The \"is currently run by worker-RANDOM_NUMBER\" error message will now include the host, and logs on that host will let associate it with the pid.",
          "commit_author": "Uldis Barbans",
          "commit_date": "2014-05-08 16:30:46",
          "commit_parent": "7ae59768d1d7cdfce8cf1b0a5717fa69d89d4ce1"
        },
        "function": {
          "function_name": "__init__",
          "function_code_before": "def __init__(self, status, deps):\n    self.stakeholders = set()\n    self.workers = set()\n    if deps is None:\n        self.deps = set()\n    else:\n        self.deps = set(deps)\n    self.status = status\n    self.time = time.time()\n    self.retry = None\n    self.remove = None\n    self.worker_running = None\n    self.expl = None",
          "function_code_after": "def __init__(self, id):\n    self.id = id\n    self.reference = None\n    self.last_active = None",
          "function_before_start_line": 44,
          "function_before_end_line": 56,
          "function_after_start_line": 65,
          "function_after_end_line": 68,
          "function_before_token_count": 79,
          "function_after_token_count": 22,
          "functions_name_modified_file": [
            "_update_task_history",
            "prune",
            "_upstream_status",
            "ping",
            "_traverse_inverse_deps",
            "inverse_dependencies",
            "load",
            "_recurse_deps",
            "add_task",
            "task_list",
            "_serialize_task",
            "task_history",
            "dep_graph",
            "__str__",
            "graph",
            "dump",
            "__repr__",
            "__init__",
            "update",
            "_get_task_params",
            "get_work",
            "_get_task_name",
            "fetch_error"
          ],
          "functions_name_all_files": [
            "_update_task_history",
            "test_remove_dep",
            "prune",
            "tearDown",
            "_upstream_status",
            "test_two_workers",
            "test_parameter_split",
            "_email_complete_error",
            "ping",
            "_fork_task",
            "test_retry",
            "test_dep",
            "_traverse_inverse_deps",
            "inverse_dependencies",
            "load",
            "_log_unexpected_error",
            "_email_unexpected_error",
            "_log_remote_tasks",
            "_add_task_and_deps",
            "_recurse_deps",
            "test_timeout",
            "test_two_worker_info",
            "add_task",
            "_validate_dependency",
            "task_list",
            "add",
            "run",
            "_serialize_task",
            "test_disallowed_state_changes",
            "task_history",
            "dep_graph",
            "_run_task",
            "_get_work",
            "test_disconnect_running",
            "_check_complete",
            "__str__",
            "graph",
            "_sleeper",
            "dump",
            "__repr__",
            "setUp",
            "__init__",
            "update",
            "test_failed_dep",
            "_validate_task",
            "_check_complete_value",
            "test_broken_dep",
            "_get_task_params",
            "stop",
            "_log_complete_error",
            "_add",
            "get_work",
            "_get_task_name",
            "_add_external",
            "_reap_children",
            "setTime",
            "fetch_error"
          ],
          "functions_name_co_evolved_modified_file": [
            "update",
            "prune",
            "get_work",
            "__str__"
          ],
          "functions_name_co_evolved_all_files": [
            "update",
            "prune",
            "_run_task",
            "test_two_worker_info",
            "get_work",
            "__str__",
            "run"
          ]
        },
        "file": {
          "file_name": "scheduler.py",
          "file_nloc": 287,
          "file_complexity": 99,
          "file_token_count": 2095,
          "file_before": "# Copyright (c) 2012 Spotify AB\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"); you may not\n# use this file except in compliance with the License. You may obtain a copy of\n# the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n# License for the specific language governing permissions and limitations under\n# the License.\n\nimport os\nimport logging\nimport time\nimport cPickle as pickle\nimport task_history as history\nlogger = logging.getLogger(\"luigi.server\")\n\nfrom task_status import PENDING, FAILED, DONE, RUNNING, UNKNOWN\n\n\nclass Scheduler(object):\n    ''' Abstract base class\n\n    Note that the methods all take string arguments, not Task objects...\n    '''\n    add_task = NotImplemented\n    get_work = NotImplemented\n    ping = NotImplemented\n\nUPSTREAM_RUNNING = 'UPSTREAM_RUNNING'\nUPSTREAM_MISSING_INPUT = 'UPSTREAM_MISSING_INPUT'\nUPSTREAM_FAILED = 'UPSTREAM_FAILED'\n\nUPSTREAM_SEVERITY_ORDER = ('', UPSTREAM_RUNNING, UPSTREAM_MISSING_INPUT, UPSTREAM_FAILED)\nUPSTREAM_SEVERITY_KEY = lambda st: UPSTREAM_SEVERITY_ORDER.index(st)\nSTATUS_TO_UPSTREAM_MAP = {FAILED: UPSTREAM_FAILED, RUNNING: UPSTREAM_RUNNING, PENDING: UPSTREAM_MISSING_INPUT}\n\n\nclass Task(object):\n    def __init__(self, status, deps):\n        self.stakeholders = set()  # workers that are somehow related to this task (i.e. don't prune while any of these workers are still active)\n        self.workers = set()  # workers that can perform task - task is 'BROKEN' if none of these workers are active\n        if deps is None:\n            self.deps = set()\n        else:\n            self.deps = set(deps)\n        self.status = status  # PENDING, RUNNING, FAILED or DONE\n        self.time = time.time()  # Timestamp when task was first added\n        self.retry = None\n        self.remove = None\n        self.worker_running = None  # the worker that is currently running the task or None\n        self.expl = None\n\n    def __repr__(self):\n        return \"Task(%r)\" % vars(self)\n\n\nclass CentralPlannerScheduler(Scheduler):\n    ''' Async scheduler that can handle multiple workers etc\n\n    Can be run locally or on a server (using RemoteScheduler + server.Server).\n    '''\n\n    def __init__(self, retry_delay=900.0, remove_delay=600.0, worker_disconnect_delay=60.0,\n                 state_path='/var/lib/luigi-server/state.pickle', task_history=None):\n        '''\n        (all arguments are in seconds)\n        Keyword Arguments:\n        retry_delay -- How long after a Task fails to try it again, or -1 to never retry\n        remove_delay -- How long after a Task finishes to remove it from the scheduler\n        state_path -- Path to state file (tasks and active workers)\n        worker_disconnect_delay -- If a worker hasn't communicated for this long, remove it from active workers\n        '''\n        self._state_path = state_path\n        self._tasks = {}\n        self._retry_delay = retry_delay\n        self._remove_delay = remove_delay\n        self._worker_disconnect_delay = worker_disconnect_delay\n        self._active_workers = {}  # map from id to timestamp (last updated)\n        self._task_history = task_history or history.NopHistory()\n        # TODO: have a Worker object instead, add more data to it\n\n    def dump(self):\n        state = (self._tasks, self._active_workers)\n        try:\n            with open(self._state_path, 'w') as fobj:\n                pickle.dump(state, fobj)\n        except IOError:\n            logger.warning(\"Failed saving scheduler state\", exc_info=1)\n        else:\n            logger.info(\"Saved state in %s\", self._state_path)\n\n    def load(self):\n        if os.path.exists(self._state_path):\n            logger.info(\"Attempting to load state from %s\", self._state_path)\n            with open(self._state_path) as fobj:\n                state = pickle.load(fobj)\n            self._tasks, self._active_workers = state\n        else:\n            logger.info(\"No prior state file exists at %s. Starting with clean slate\", self._state_path)\n\n    def prune(self):\n        logger.info(\"Starting pruning of task graph\")\n        # Delete workers that haven't said anything for a while (probably killed)\n        delete_workers = []\n        for worker in self._active_workers:\n            if self._active_workers[worker] < time.time() - self._worker_disconnect_delay:\n                logger.info(\"worker %r updated at %s timed out (no contact for >=%ss)\", worker, self._active_workers[worker], self._worker_disconnect_delay)\n                delete_workers.append(worker)\n\n        for worker in delete_workers:\n            self._active_workers.pop(worker)\n\n        remaining_workers = set(self._active_workers.keys())\n\n        # Mark tasks with no remaining active stakeholders for deletion\n        for task_id, task in self._tasks.iteritems():\n            if not task.stakeholders.intersection(remaining_workers):\n                if task.remove is None:\n                    logger.info(\"Task %r has stakeholders %r but none remain connected -> will remove task in %s seconds\", task_id, task.stakeholders, self._remove_delay)\n                    task.remove = time.time() + self._remove_delay\n\n            if task.status == RUNNING and task.worker_running and task.worker_running not in remaining_workers:\n                # If a running worker disconnects, tag all its jobs as FAILED and subject it to the same retry logic\n                logger.info(\"Task %r is marked as running by disconnected worker %r -> marking as FAILED with retry delay of %rs\", task_id, task.worker_running, self._retry_delay)\n                task.worker_running = None\n                task.status = FAILED\n                task.retry = time.time() + self._retry_delay\n\n        # Remove tasks that have no stakeholders\n        remove_tasks = []\n        for task_id, task in self._tasks.iteritems():\n            if task.remove and time.time() > task.remove:\n                logger.info(\"Removing task %r (no connected stakeholders)\", task_id)\n                remove_tasks.append(task_id)\n\n        for task_id in remove_tasks:\n            self._tasks.pop(task_id)\n\n        # Reset FAILED tasks to PENDING if max timeout is reached, and retry delay is >= 0\n        for task in self._tasks.values():\n            if task.status == FAILED and self._retry_delay >= 0 and task.retry < time.time():\n                task.status = PENDING\n        logger.info(\"Done pruning task graph\")\n\n    def update(self, worker):\n        # update timestamp so that we keep track\n        # of whenever the worker was last active\n        self._active_workers[worker] = time.time()\n\n    def add_task(self, worker, task_id, status=PENDING, runnable=True, deps=None, expl=None):\n        \"\"\"\n        * Add task identified by task_id if it doesn't exist\n        * If deps is not None, update dependency list\n        * Update status of task\n        * Add additional workers/stakeholders\n        \"\"\"\n        self.update(worker)\n\n        task = self._tasks.setdefault(task_id, Task(status=PENDING, deps=deps))\n\n        if task.remove is not None:\n            task.remove = None  # unmark task for removal so it isn't removed after being added\n\n        if not (task.status == RUNNING and status == PENDING):\n            # don't allow re-scheduling of task while it is running, it must either fail or succeed first\n            task.status = status\n            if status == FAILED:\n                task.retry = time.time() + self._retry_delay\n\n        if deps is not None:\n            task.deps = set(deps)\n\n        task.stakeholders.add(worker)\n\n        if runnable:\n            task.workers.add(worker)\n\n        if expl is not None:\n            task.expl = expl\n        self._update_task_history(task_id, status)\n\n    def get_work(self, worker, host=None):\n        # TODO: remove any expired nodes\n\n        # Algo: iterate over all nodes, find first node with no dependencies\n\n        # TODO: remove tasks that can't be done, figure out if the worker has absolutely\n        # nothing it can wait for\n\n        # Return remaining tasks that have no FAILED descendents\n        self.update(worker)\n        best_t = float('inf')\n        best_task = None\n        locally_pending_tasks = 0\n        running_tasks = []\n\n        for task_id, task in self._tasks.iteritems():\n            if worker not in task.workers:\n                continue\n\n            if task.status == RUNNING:\n                running_tasks.append({'task_id': task_id, 'worker': task.worker_running})\n\n            if task.status != PENDING:\n                continue\n\n            locally_pending_tasks += 1\n            ok = True\n            for dep in task.deps:\n                if dep not in self._tasks:\n                    ok = False\n                elif self._tasks[dep].status != DONE:\n                    ok = False\n\n            if ok:\n                if task.time < best_t:\n                    best_t = task.time\n                    best_task = task_id\n\n        if best_task:\n            t = self._tasks[best_task]\n            t.status = RUNNING\n            t.worker_running = worker\n            self._update_task_history(best_task, RUNNING, host=host)\n\n        return {'n_pending_tasks': locally_pending_tasks,\n                'task_id': best_task,\n                'running_tasks': running_tasks}\n\n    def ping(self, worker):\n        self.update(worker)\n\n    def _upstream_status(self, task_id, upstream_status_table):\n        if task_id in upstream_status_table:\n            return upstream_status_table[task_id]\n        elif task_id in self._tasks:\n            task_stack = [task_id]\n\n            while task_stack:\n                dep_id = task_stack.pop()\n                if dep_id in self._tasks:\n                    dep = self._tasks[dep_id]\n                    if dep_id not in upstream_status_table:\n                        if dep.status == PENDING and dep.deps:\n                            task_stack = task_stack + [dep_id] + list(dep.deps)\n                            upstream_status_table[dep_id] = ''  # will be updated postorder\n                        else:\n                            dep_status = STATUS_TO_UPSTREAM_MAP.get(dep.status, '')\n                            upstream_status_table[dep_id] = dep_status\n                    elif upstream_status_table[dep_id] == '' and dep.deps:\n                        # This is the postorder update step when we set the\n                        # status based on the previously calculated child elements\n                        upstream_status = [upstream_status_table.get(id, '') for id in dep.deps]\n                        upstream_status.append('')  # to handle empty list\n                        status = max(upstream_status, key=UPSTREAM_SEVERITY_KEY)\n                        upstream_status_table[dep_id] = status\n            return upstream_status_table[dep_id]\n\n    def _serialize_task(self, task_id):\n        task = self._tasks[task_id]\n        return {\n            'deps': list(task.deps),\n            'status': task.status,\n            'workers': list(task.workers),\n            'start_time': task.time,\n            'params': self._get_task_params(task_id),\n            'name': self._get_task_name(task_id)\n        }\n\n    def _get_task_params(self, task_id):\n        params = {}\n        params_part = task_id.split('(')[1].strip(')')\n        params_strings = params_part.split(\", \")\n\n        for param in params_strings:\n            if not param:\n                continue\n            split_param = param.split('=')\n            if len(split_param) != 2:\n                return {'<complex parameters>': params_part}\n            params[split_param[0]] = split_param[1]\n        return params\n\n    def _get_task_name(self, task_id):\n        return task_id.split('(')[0]\n\n    def graph(self):\n        self.prune()\n        serialized = {}\n        for task_id, task in self._tasks.iteritems():\n            serialized[task_id] = self._serialize_task(task_id)\n        return serialized\n\n    def _recurse_deps(self, task_id, serialized):\n        if task_id not in serialized:\n            task = self._tasks.get(task_id)\n            if task is None:\n                logger.warn('Missing task for id [%s]', task_id)\n                serialized[task_id] = {\n                    'deps': [],\n                    'status': UNKNOWN,\n                    'workers': [],\n                    'start_time': UNKNOWN,\n                    'params': self._get_task_params(task_id),\n                    'name': self._get_task_name(task_id)\n                }\n            else:\n                serialized[task_id] = self._serialize_task(task_id)\n                for dep in task.deps:\n                    self._recurse_deps(dep, serialized)\n\n    def dep_graph(self, task_id):\n        self.prune()\n        serialized = {}\n        if task_id in self._tasks:\n            self._recurse_deps(task_id, serialized)\n        return serialized\n\n    def task_list(self, status, upstream_status):\n        ''' query for a subset of tasks by status '''\n        self.prune()\n        result = {}\n        upstream_status_table = {}  # used to memoize upstream status\n        for task_id, task in self._tasks.iteritems():\n            if not status or task.status == status:\n                if (task.status != PENDING or not upstream_status or\n                    upstream_status == self._upstream_status(task_id, upstream_status_table)):\n                    serialized = self._serialize_task(task_id)\n                    result[task_id] = serialized\n        return result\n\n    def inverse_dependencies(self, task_id):\n        self.prune()\n        serialized = {}\n        if task_id in self._tasks:\n            self._traverse_inverse_deps(task_id, serialized)\n        return serialized\n\n    def _traverse_inverse_deps(self, task_id, serialized):\n        stack = [task_id]\n        serialized[task_id] = self._serialize_task(task_id)\n        while len(stack) > 0:\n            curr_id = stack.pop()\n            for id, task in self._tasks.iteritems():\n                if curr_id in task.deps:\n                    serialized[curr_id][\"deps\"].append(id)\n                    if id not in serialized:\n                        serialized[id] = self._serialize_task(id)\n                        serialized[id][\"deps\"] = []\n                        stack.append(id)\n\n    def fetch_error(self, task_id):\n        if self._tasks[task_id].expl is not None:\n            return {\"taskId\": task_id, \"error\": self._tasks[task_id].expl}\n        else:\n            return {\"taskId\": task_id, \"error\": \"\"}\n\n    def _update_task_history(self, task_id, status, host=None):\n        try:\n            if status == DONE or status == FAILED:\n                successful = (status == DONE)\n                self._task_history.task_finished(task_id, successful)\n            elif status == PENDING:\n                self._task_history.task_scheduled(task_id)\n            elif status == RUNNING:\n                self._task_history.task_started(task_id, host)\n        except:\n            logger.warning(\"Error saving Task history\", exc_info=1)\n\n    @property\n    def task_history(self):\n        # Used by server.py to expose the calls\n        return self._task_history\n",
          "file_after": "# Copyright (c) 2012 Spotify AB\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"); you may not\n# use this file except in compliance with the License. You may obtain a copy of\n# the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n# License for the specific language governing permissions and limitations under\n# the License.\n\nimport datetime\nimport os\nimport logging\nimport time\nimport cPickle as pickle\nimport task_history as history\nlogger = logging.getLogger(\"luigi.server\")\n\nfrom task_status import PENDING, FAILED, DONE, RUNNING, UNKNOWN\n\n\nclass Scheduler(object):\n    ''' Abstract base class\n\n    Note that the methods all take string arguments, not Task objects...\n    '''\n    add_task = NotImplemented\n    get_work = NotImplemented\n    ping = NotImplemented\n\nUPSTREAM_RUNNING = 'UPSTREAM_RUNNING'\nUPSTREAM_MISSING_INPUT = 'UPSTREAM_MISSING_INPUT'\nUPSTREAM_FAILED = 'UPSTREAM_FAILED'\n\nUPSTREAM_SEVERITY_ORDER = ('', UPSTREAM_RUNNING, UPSTREAM_MISSING_INPUT, UPSTREAM_FAILED)\nUPSTREAM_SEVERITY_KEY = lambda st: UPSTREAM_SEVERITY_ORDER.index(st)\nSTATUS_TO_UPSTREAM_MAP = {FAILED: UPSTREAM_FAILED, RUNNING: UPSTREAM_RUNNING, PENDING: UPSTREAM_MISSING_INPUT}\n\n\nclass Task(object):\n    def __init__(self, status, deps):\n        self.stakeholders = set()  # workers ids that are somehow related to this task (i.e. don't prune while any of these workers are still active)\n        self.workers = set()  # workers ids that can perform task - task is 'BROKEN' if none of these workers are active\n        if deps is None:\n            self.deps = set()\n        else:\n            self.deps = set(deps)\n        self.status = status  # PENDING, RUNNING, FAILED or DONE\n        self.time = time.time()  # Timestamp when task was first added\n        self.retry = None\n        self.remove = None\n        self.worker_running = None  # the worker id that is currently running the task or None\n        self.expl = None\n\n    def __repr__(self):\n        return \"Task(%r)\" % vars(self)\n\n\nclass Worker(object):\n    \"\"\" Structure for tracking worker activity and keeping their references \"\"\"\n    def __init__(self, id):\n        self.id = id\n        self.reference = None  # reference to the worker in the real world. (Currently a dict containing just the host)\n        self.last_active = None  # seconds since epoch\n\n    def __str__(self):\n        return \"%s on %s, last active %s\" % (self.id, self.reference, datetime.datetime.utcfromtimestamp(self.last_active).isoformat())\n\n\nclass CentralPlannerScheduler(Scheduler):\n    ''' Async scheduler that can handle multiple workers etc\n\n    Can be run locally or on a server (using RemoteScheduler + server.Server).\n    '''\n\n    def __init__(self, retry_delay=900.0, remove_delay=600.0, worker_disconnect_delay=60.0,\n                 state_path='/var/lib/luigi-server/state.pickle', task_history=None):\n        '''\n        (all arguments are in seconds)\n        Keyword Arguments:\n        retry_delay -- How long after a Task fails to try it again, or -1 to never retry\n        remove_delay -- How long after a Task finishes to remove it from the scheduler\n        state_path -- Path to state file (tasks and active workers)\n        worker_disconnect_delay -- If a worker hasn't communicated for this long, remove it from active workers\n        '''\n        self._state_path = state_path\n        self._tasks = {}  # map from id to a Task object\n        self._retry_delay = retry_delay\n        self._remove_delay = remove_delay\n        self._worker_disconnect_delay = worker_disconnect_delay\n        self._active_workers = {}  # map from id to a Worker object\n        self._task_history = task_history or history.NopHistory()\n\n    def dump(self):\n        state = (self._tasks, self._active_workers)\n        try:\n            with open(self._state_path, 'w') as fobj:\n                pickle.dump(state, fobj)\n        except IOError:\n            logger.warning(\"Failed saving scheduler state\", exc_info=1)\n        else:\n            logger.info(\"Saved state in %s\", self._state_path)\n\n    # prone to lead to crashes when old state is unpickled with updated code. TODO some kind of version control?\n    def load(self):\n        if os.path.exists(self._state_path):\n            logger.info(\"Attempting to load state from %s\", self._state_path)\n            with open(self._state_path) as fobj:\n                state = pickle.load(fobj)\n            self._tasks, self._active_workers = state\n        else:\n            logger.info(\"No prior state file exists at %s. Starting with clean slate\", self._state_path)\n\n    def prune(self):\n        logger.info(\"Starting pruning of task graph\")\n        # Delete workers that haven't said anything for a while (probably killed)\n        delete_workers = []\n        for worker in self._active_workers.values():\n            if worker.last_active < time.time() - self._worker_disconnect_delay:\n                logger.info(\"Worker %s timed out (no contact for >=%ss)\", worker, self._worker_disconnect_delay)\n                delete_workers.append(worker.id)\n\n        for worker in delete_workers:\n            self._active_workers.pop(worker)\n\n        remaining_workers = set(self._active_workers.keys())\n\n        # Mark tasks with no remaining active stakeholders for deletion\n        for task_id, task in self._tasks.iteritems():\n            if not task.stakeholders.intersection(remaining_workers):\n                if task.remove is None:\n                    logger.info(\"Task %r has stakeholders %r but none remain connected -> will remove task in %s seconds\", task_id, task.stakeholders, self._remove_delay)\n                    task.remove = time.time() + self._remove_delay\n\n            if task.status == RUNNING and task.worker_running and task.worker_running not in remaining_workers:\n                # If a running worker disconnects, tag all its jobs as FAILED and subject it to the same retry logic\n                logger.info(\"Task %r is marked as running by disconnected worker %r -> marking as FAILED with retry delay of %rs\", task_id, task.worker_running, self._retry_delay)\n                task.worker_running = None\n                task.status = FAILED\n                task.retry = time.time() + self._retry_delay\n\n        # Remove tasks that have no stakeholders\n        remove_tasks = []\n        for task_id, task in self._tasks.iteritems():\n            if task.remove and time.time() > task.remove:\n                logger.info(\"Removing task %r (no connected stakeholders)\", task_id)\n                remove_tasks.append(task_id)\n\n        for task_id in remove_tasks:\n            self._tasks.pop(task_id)\n\n        # Reset FAILED tasks to PENDING if max timeout is reached, and retry delay is >= 0\n        for task in self._tasks.values():\n            if task.status == FAILED and self._retry_delay >= 0 and task.retry < time.time():\n                task.status = PENDING\n        logger.info(\"Done pruning task graph\")\n\n    def update(self, worker_id, worker_reference=None):\n        \"\"\" Keep track of whenever the worker was last active \"\"\"\n        worker = self._active_workers.setdefault(worker_id, Worker(worker_id))\n        if worker_reference:\n            worker.reference = worker_reference\n        worker.last_active = time.time()\n\n    def add_task(self, worker, task_id, status=PENDING, runnable=True, deps=None, expl=None):\n        \"\"\"\n        * Add task identified by task_id if it doesn't exist\n        * If deps is not None, update dependency list\n        * Update status of task\n        * Add additional workers/stakeholders\n        \"\"\"\n        self.update(worker)\n\n        task = self._tasks.setdefault(task_id, Task(status=PENDING, deps=deps))\n\n        if task.remove is not None:\n            task.remove = None  # unmark task for removal so it isn't removed after being added\n\n        if not (task.status == RUNNING and status == PENDING):\n            # don't allow re-scheduling of task while it is running, it must either fail or succeed first\n            task.status = status\n            if status == FAILED:\n                task.retry = time.time() + self._retry_delay\n\n        if deps is not None:\n            task.deps = set(deps)\n\n        task.stakeholders.add(worker)\n\n        if runnable:\n            task.workers.add(worker)\n\n        if expl is not None:\n            task.expl = expl\n        self._update_task_history(task_id, status)\n\n    def get_work(self, worker, host=None):\n        # TODO: remove any expired nodes\n\n        # Algo: iterate over all nodes, find first node with no dependencies\n\n        # TODO: remove tasks that can't be done, figure out if the worker has absolutely\n        # nothing it can wait for\n\n        # Return remaining tasks that have no FAILED descendents\n        self.update(worker, {'host': host})\n        best_t = float('inf')\n        best_task = None\n        locally_pending_tasks = 0\n        running_tasks = []\n\n        for task_id, task in self._tasks.iteritems():\n            if worker not in task.workers:\n                continue\n\n            if task.status == RUNNING:\n                running_tasks.append({'task_id': task_id, 'worker': str(self._active_workers.get(task.worker_running))})\n\n            if task.status != PENDING:\n                continue\n\n            locally_pending_tasks += 1\n            ok = True\n            for dep in task.deps:\n                if dep not in self._tasks:\n                    ok = False\n                elif self._tasks[dep].status != DONE:\n                    ok = False\n\n            if ok:\n                if task.time < best_t:\n                    best_t = task.time\n                    best_task = task_id\n\n        if best_task:\n            t = self._tasks[best_task]\n            t.status = RUNNING\n            t.worker_running = worker\n            self._update_task_history(best_task, RUNNING, host=host)\n\n        return {'n_pending_tasks': locally_pending_tasks,\n                'task_id': best_task,\n                'running_tasks': running_tasks}\n\n    def ping(self, worker):\n        self.update(worker)\n\n    def _upstream_status(self, task_id, upstream_status_table):\n        if task_id in upstream_status_table:\n            return upstream_status_table[task_id]\n        elif task_id in self._tasks:\n            task_stack = [task_id]\n\n            while task_stack:\n                dep_id = task_stack.pop()\n                if dep_id in self._tasks:\n                    dep = self._tasks[dep_id]\n                    if dep_id not in upstream_status_table:\n                        if dep.status == PENDING and dep.deps:\n                            task_stack = task_stack + [dep_id] + list(dep.deps)\n                            upstream_status_table[dep_id] = ''  # will be updated postorder\n                        else:\n                            dep_status = STATUS_TO_UPSTREAM_MAP.get(dep.status, '')\n                            upstream_status_table[dep_id] = dep_status\n                    elif upstream_status_table[dep_id] == '' and dep.deps:\n                        # This is the postorder update step when we set the\n                        # status based on the previously calculated child elements\n                        upstream_status = [upstream_status_table.get(id, '') for id in dep.deps]\n                        upstream_status.append('')  # to handle empty list\n                        status = max(upstream_status, key=UPSTREAM_SEVERITY_KEY)\n                        upstream_status_table[dep_id] = status\n            return upstream_status_table[dep_id]\n\n    def _serialize_task(self, task_id):\n        task = self._tasks[task_id]\n        return {\n            'deps': list(task.deps),\n            'status': task.status,\n            'workers': list(task.workers),\n            'start_time': task.time,\n            'params': self._get_task_params(task_id),\n            'name': self._get_task_name(task_id)\n        }\n\n    def _get_task_params(self, task_id):\n        params = {}\n        params_part = task_id.split('(')[1].strip(')')\n        params_strings = params_part.split(\", \")\n\n        for param in params_strings:\n            if not param:\n                continue\n            split_param = param.split('=')\n            if len(split_param) != 2:\n                return {'<complex parameters>': params_part}\n            params[split_param[0]] = split_param[1]\n        return params\n\n    def _get_task_name(self, task_id):\n        return task_id.split('(')[0]\n\n    def graph(self):\n        self.prune()\n        serialized = {}\n        for task_id, task in self._tasks.iteritems():\n            serialized[task_id] = self._serialize_task(task_id)\n        return serialized\n\n    def _recurse_deps(self, task_id, serialized):\n        if task_id not in serialized:\n            task = self._tasks.get(task_id)\n            if task is None:\n                logger.warn('Missing task for id [%s]', task_id)\n                serialized[task_id] = {\n                    'deps': [],\n                    'status': UNKNOWN,\n                    'workers': [],\n                    'start_time': UNKNOWN,\n                    'params': self._get_task_params(task_id),\n                    'name': self._get_task_name(task_id)\n                }\n            else:\n                serialized[task_id] = self._serialize_task(task_id)\n                for dep in task.deps:\n                    self._recurse_deps(dep, serialized)\n\n    def dep_graph(self, task_id):\n        self.prune()\n        serialized = {}\n        if task_id in self._tasks:\n            self._recurse_deps(task_id, serialized)\n        return serialized\n\n    def task_list(self, status, upstream_status):\n        ''' query for a subset of tasks by status '''\n        self.prune()\n        result = {}\n        upstream_status_table = {}  # used to memoize upstream status\n        for task_id, task in self._tasks.iteritems():\n            if not status or task.status == status:\n                if (task.status != PENDING or not upstream_status or\n                    upstream_status == self._upstream_status(task_id, upstream_status_table)):\n                    serialized = self._serialize_task(task_id)\n                    result[task_id] = serialized\n        return result\n\n    def inverse_dependencies(self, task_id):\n        self.prune()\n        serialized = {}\n        if task_id in self._tasks:\n            self._traverse_inverse_deps(task_id, serialized)\n        return serialized\n\n    def _traverse_inverse_deps(self, task_id, serialized):\n        stack = [task_id]\n        serialized[task_id] = self._serialize_task(task_id)\n        while len(stack) > 0:\n            curr_id = stack.pop()\n            for id, task in self._tasks.iteritems():\n                if curr_id in task.deps:\n                    serialized[curr_id][\"deps\"].append(id)\n                    if id not in serialized:\n                        serialized[id] = self._serialize_task(id)\n                        serialized[id][\"deps\"] = []\n                        stack.append(id)\n\n    def fetch_error(self, task_id):\n        if self._tasks[task_id].expl is not None:\n            return {\"taskId\": task_id, \"error\": self._tasks[task_id].expl}\n        else:\n            return {\"taskId\": task_id, \"error\": \"\"}\n\n    def _update_task_history(self, task_id, status, host=None):\n        try:\n            if status == DONE or status == FAILED:\n                successful = (status == DONE)\n                self._task_history.task_finished(task_id, successful)\n            elif status == PENDING:\n                self._task_history.task_scheduled(task_id)\n            elif status == RUNNING:\n                self._task_history.task_started(task_id, host)\n        except:\n            logger.warning(\"Error saving Task history\", exc_info=1)\n\n    @property\n    def task_history(self):\n        # Used by server.py to expose the calls\n        return self._task_history\n",
          "file_patch": "@@ -12,6 +12,7 @@\n # License for the specific language governing permissions and limitations under\n # the License.\n \n+import datetime\n import os\n import logging\n import time\n@@ -42,8 +43,8 @@ STATUS_TO_UPSTREAM_MAP = {FAILED: UPSTREAM_FAILED, RUNNING: UPSTREAM_RUNNING, PE\n \n class Task(object):\n     def __init__(self, status, deps):\n-        self.stakeholders = set()  # workers that are somehow related to this task (i.e. don't prune while any of these workers are still active)\n-        self.workers = set()  # workers that can perform task - task is 'BROKEN' if none of these workers are active\n+        self.stakeholders = set()  # workers ids that are somehow related to this task (i.e. don't prune while any of these workers are still active)\n+        self.workers = set()  # workers ids that can perform task - task is 'BROKEN' if none of these workers are active\n         if deps is None:\n             self.deps = set()\n         else:\n@@ -52,13 +53,24 @@ class Task(object):\n         self.time = time.time()  # Timestamp when task was first added\n         self.retry = None\n         self.remove = None\n-        self.worker_running = None  # the worker that is currently running the task or None\n+        self.worker_running = None  # the worker id that is currently running the task or None\n         self.expl = None\n \n     def __repr__(self):\n         return \"Task(%r)\" % vars(self)\n \n \n+class Worker(object):\n+    \"\"\" Structure for tracking worker activity and keeping their references \"\"\"\n+    def __init__(self, id):\n+        self.id = id\n+        self.reference = None  # reference to the worker in the real world. (Currently a dict containing just the host)\n+        self.last_active = None  # seconds since epoch\n+\n+    def __str__(self):\n+        return \"%s on %s, last active %s\" % (self.id, self.reference, datetime.datetime.utcfromtimestamp(self.last_active).isoformat())\n+\n+\n class CentralPlannerScheduler(Scheduler):\n     ''' Async scheduler that can handle multiple workers etc\n \n@@ -76,13 +88,12 @@ class CentralPlannerScheduler(Scheduler):\n         worker_disconnect_delay -- If a worker hasn't communicated for this long, remove it from active workers\n         '''\n         self._state_path = state_path\n-        self._tasks = {}\n+        self._tasks = {}  # map from id to a Task object\n         self._retry_delay = retry_delay\n         self._remove_delay = remove_delay\n         self._worker_disconnect_delay = worker_disconnect_delay\n-        self._active_workers = {}  # map from id to timestamp (last updated)\n+        self._active_workers = {}  # map from id to a Worker object\n         self._task_history = task_history or history.NopHistory()\n-        # TODO: have a Worker object instead, add more data to it\n \n     def dump(self):\n         state = (self._tasks, self._active_workers)\n@@ -94,6 +105,7 @@ class CentralPlannerScheduler(Scheduler):\n         else:\n             logger.info(\"Saved state in %s\", self._state_path)\n \n+    # prone to lead to crashes when old state is unpickled with updated code. TODO some kind of version control?\n     def load(self):\n         if os.path.exists(self._state_path):\n             logger.info(\"Attempting to load state from %s\", self._state_path)\n@@ -107,10 +119,10 @@ class CentralPlannerScheduler(Scheduler):\n         logger.info(\"Starting pruning of task graph\")\n         # Delete workers that haven't said anything for a while (probably killed)\n         delete_workers = []\n-        for worker in self._active_workers:\n-            if self._active_workers[worker] < time.time() - self._worker_disconnect_delay:\n-                logger.info(\"worker %r updated at %s timed out (no contact for >=%ss)\", worker, self._active_workers[worker], self._worker_disconnect_delay)\n-                delete_workers.append(worker)\n+        for worker in self._active_workers.values():\n+            if worker.last_active < time.time() - self._worker_disconnect_delay:\n+                logger.info(\"Worker %s timed out (no contact for >=%ss)\", worker, self._worker_disconnect_delay)\n+                delete_workers.append(worker.id)\n \n         for worker in delete_workers:\n             self._active_workers.pop(worker)\n@@ -147,10 +159,12 @@ class CentralPlannerScheduler(Scheduler):\n                 task.status = PENDING\n         logger.info(\"Done pruning task graph\")\n \n-    def update(self, worker):\n-        # update timestamp so that we keep track\n-        # of whenever the worker was last active\n-        self._active_workers[worker] = time.time()\n+    def update(self, worker_id, worker_reference=None):\n+        \"\"\" Keep track of whenever the worker was last active \"\"\"\n+        worker = self._active_workers.setdefault(worker_id, Worker(worker_id))\n+        if worker_reference:\n+            worker.reference = worker_reference\n+        worker.last_active = time.time()\n \n     def add_task(self, worker, task_id, status=PENDING, runnable=True, deps=None, expl=None):\n         \"\"\"\n@@ -193,7 +207,7 @@ class CentralPlannerScheduler(Scheduler):\n         # nothing it can wait for\n \n         # Return remaining tasks that have no FAILED descendents\n-        self.update(worker)\n+        self.update(worker, {'host': host})\n         best_t = float('inf')\n         best_task = None\n         locally_pending_tasks = 0\n@@ -204,7 +218,7 @@ class CentralPlannerScheduler(Scheduler):\n                 continue\n \n             if task.status == RUNNING:\n-                running_tasks.append({'task_id': task_id, 'worker': task.worker_running})\n+                running_tasks.append({'task_id': task_id, 'worker': str(self._active_workers.get(task.worker_running))})\n \n             if task.status != PENDING:\n                 continue\n",
          "files_name_in_blame_commit": [
            "scheduler.py",
            "central_planner_test.py",
            "worker.py"
          ]
        }
      }
    }
  }
}