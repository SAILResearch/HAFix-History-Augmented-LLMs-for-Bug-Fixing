{
  "id": "56",
  "blame_commit": {
    "commit": {
      "commit_id": "58fd1f0589d33aeb33c4129cfedfb7737495efc0",
      "commit_message": "Refactor training part of `engine` module. (#10029)\n\n* Refactor topological part of Keras engine.\r\n\r\n* Fix imports\r\n\r\n* Fix merge mixup.\r\n\r\n* Refactor training part of the Keras engine.\r\n\r\n* Fix unit tests.",
      "commit_author": "Fran\u00e7ois Chollet",
      "commit_date": "2018-04-24 21:06:54",
      "commit_parent": "b076e227da6beaf87d6c84eff1a92285e4662acf"
    },
    "function": {
      "function_name": "weighted_masked_objective",
      "function_code_before": "",
      "function_code_after": "def weighted_masked_objective(fn):\n    \"\"\"Adds support for masking and sample-weighting to an objective function.\n\n    It transforms an objective function `fn(y_true, y_pred)`\n    into a sample-weighted, cost-masked objective function\n    `fn(y_true, y_pred, weights, mask)`.\n\n    # Arguments\n        fn: The objective function to wrap,\n            with signature `fn(y_true, y_pred)`.\n\n    # Returns\n        A function with signature `fn(y_true, y_pred, weights, mask)`.\n    \"\"\"\n    if fn is None:\n        return None\n\n    def weighted(y_true, y_pred, weights, mask=None):\n        \"\"\"Wrapper function.\n\n        # Arguments\n            y_true: `y_true` argument of `fn`.\n            y_pred: `y_pred` argument of `fn`.\n            weights: Weights tensor.\n            mask: Mask tensor.\n\n        # Returns\n            Scalar tensor.\n        \"\"\"\n        score_array = fn(y_true, y_pred)\n        if mask is not None:\n            mask = K.cast(mask, K.floatx())\n            score_array *= mask\n            score_array /= K.mean(mask)\n        if weights is not None:\n            ndim = K.ndim(score_array)\n            weight_ndim = K.ndim(weights)\n            score_array = K.mean(score_array, axis=list(range(weight_ndim, ndim)))\n            score_array *= weights\n            score_array /= K.mean(K.cast(K.not_equal(weights, 0), K.floatx()))\n        return K.mean(score_array)\n    return weighted",
      "function_before_start_line": "",
      "function_before_end_line": "",
      "function_after_start_line": 352,
      "function_after_end_line": 402,
      "function_before_token_count": 0,
      "function_after_token_count": 17,
      "functions_name_modified_file": [
        "standardize_weights",
        "batch_shuffle",
        "make_batches",
        "collect_metrics",
        "standardize_input_data",
        "check_num_samples",
        "check_loss_and_target_compatibility",
        "standardize_class_weights",
        "standardize_sample_or_class_weights",
        "standardize_sample_weights",
        "weighted_masked_objective",
        "check_array_length_consistency"
      ],
      "functions_name_all_files": [
        "test_sequential_pop",
        "predict_loop",
        "has_arg",
        "test_loss_masking",
        "custom_object_scope",
        "test_sparse_placeholder_fit",
        "check_num_samples",
        "evaluate",
        "standardize_weights",
        "fit_loop",
        "is_all_none",
        "_check_trainable_weights_consistency",
        "test_nested_sequential_trainability",
        "test_with_list_as_targets",
        "__getitem__",
        "test_sequential",
        "__init__",
        "evaluate_generator",
        "slice_arrays",
        "collect_metrics",
        "_uses_dynamic_learning_phase",
        "test_pandas_dataframe",
        "check_loss_and_target_compatibility",
        "get_custom_objects",
        "weighted_masked_objective",
        "compile",
        "test_loop",
        "_get_test_data",
        "test_nested_sequential",
        "fit",
        "standardize_sample_or_class_weights",
        "func_dump",
        "_make_predict_function",
        "test_on_batch",
        "test_model_with_partial_loss",
        "test_trainable_argument",
        "predict",
        "standardize_input_data",
        "standardize_sample_weights",
        "deserialize_keras_object",
        "make_batches",
        "test_sequential_update_disabling",
        "fit_generator",
        "test_model_methods",
        "__len__",
        "standardize_class_weights",
        "_make_train_function",
        "check_array_length_consistency",
        "test_masking",
        "test_check_bad_shape",
        "test_weighted_masked_objective",
        "to_list",
        "test_sparse_inputs_targets",
        "test_target_tensors",
        "serialize_keras_object",
        "predict_generator",
        "object_list_uid",
        "__enter__",
        "test_check_array_length_consistency",
        "test_rebuild_model",
        "_standardize_user_data",
        "on_epoch_end",
        "test_model_with_external_loss",
        "test_warnings",
        "test_check_not_failing",
        "func_load",
        "_make_test_function",
        "test_check_last_is_one",
        "predict_on_batch",
        "test_clone_sequential_model",
        "in_tmpdir",
        "train_on_batch",
        "test_trainable_weights_count_consistency",
        "test_sequential_count_params",
        "add",
        "test_clone_functional_model",
        "testslice_arrays",
        "test_model_custom_target_tensors",
        "update",
        "batch_shuffle",
        "test_model_with_input_feed_tensor",
        "test_sequential_fit_generator",
        "__exit__"
      ],
      "functions_name_co_evolved_modified_file": [
        "standardize_weights",
        "batch_shuffle",
        "make_batches",
        "collect_metrics",
        "standardize_input_data",
        "check_loss_and_target_compatibility",
        "standardize_class_weights",
        "standardize_sample_or_class_weights",
        "standardize_sample_weights",
        "check_num_samples",
        "check_array_length_consistency"
      ],
      "functions_name_co_evolved_all_files": [
        "_batch_shuffle",
        "_check_array_lengths",
        "predict_loop",
        "test_check_bad_shape",
        "test_check_last_is_one",
        "test_weighted_masked_objective",
        "predict_on_batch",
        "test_sparse_inputs_targets",
        "_weighted_masked_objective",
        "test_loss_masking",
        "_standardize_sample_or_class_weights",
        "fit",
        "train_on_batch",
        "_standardize_class_weights",
        "_make_predict_function",
        "standardize_sample_or_class_weights",
        "evaluate",
        "test_on_batch",
        "check_num_samples",
        "test_sparse_placeholder_fit",
        "test_check_array_lengths",
        "standardize_weights",
        "_collect_metrics",
        "predict_generator",
        "_make_batches",
        "_standardize_input_data",
        "_standardize_weights",
        "predict",
        "_fit_loop",
        "fit_loop",
        "standardize_input_data",
        "test_check_array_length_consistency",
        "standardize_sample_weights",
        "_check_num_samples",
        "test_slice_arrays",
        "testslice_arrays",
        "_standardize_user_data",
        "make_batches",
        "fit_generator",
        "_test_loop",
        "_slice_arrays",
        "__getitem__",
        "test_model_methods",
        "test_loop",
        "test_sequential",
        "evaluate_generator",
        "batch_shuffle",
        "test_check_not_failing",
        "test_warnings",
        "slice_arrays",
        "collect_metrics",
        "_check_loss_and_target_compatibility",
        "_uses_dynamic_learning_phase",
        "check_loss_and_target_compatibility",
        "standardize_class_weights",
        "_make_train_function",
        "_predict_loop",
        "_standardize_sample_weights",
        "check_array_length_consistency",
        "_make_test_function",
        "compile"
      ]
    },
    "file": {
      "file_name": "training_utils.py",
      "file_nloc": 327,
      "file_complexity": 118,
      "file_token_count": 2140,
      "file_before": null,
      "file_after": "\"\"\"Training-related utilities.\n\"\"\"\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport copy\nimport numpy as np\n\nfrom .. import backend as K\nfrom .. import losses\n\n\ndef standardize_input_data(data,\n                           names,\n                           shapes=None,\n                           check_batch_axis=True,\n                           exception_prefix=''):\n    \"\"\"Normalizes inputs and targets provided by users.\n\n    Users may pass data as a list of arrays, dictionary of arrays,\n    or as a single array. We normalize this to an ordered list of\n    arrays (same order as `names`), while checking that the provided\n    arrays have shapes that match the network's expectations.\n\n    # Arguments\n        data: User-provided input data (polymorphic).\n        names: List of expected array names.\n        shapes: Optional list of expected array shapes.\n        check_batch_axis: Boolean; whether to check that\n            the batch axis of the arrays matches the expected\n            value found in `shapes`.\n        exception_prefix: String prefix used for exception formatting.\n\n    # Returns\n        List of standardized input arrays (one array per model input).\n\n    # Raises\n        ValueError: in case of improperly formatted user-provided data.\n    \"\"\"\n    if not names:\n        if data is not None and hasattr(data, '__len__') and len(data):\n            raise ValueError('Error when checking model ' +\n                             exception_prefix + ': '\n                             'expected no data, but got:', data)\n        return []\n    if data is None:\n        return [None for _ in range(len(names))]\n\n    if isinstance(data, dict):\n        try:\n            data = [data[x].values if data[x].__class__.__name__ == 'DataFrame'\n                    else data[x] for x in names]\n        except KeyError as e:\n            raise ValueError(\n                'No data provided for \"' + e.args[0] + '\". Need data '\n                'for each key in: ' + str(names))\n    elif isinstance(data, list):\n        if len(names) == 1 and data and isinstance(data[0], (float, int)):\n            data = [np.asarray(data)]\n        else:\n            data = [x.values if x.__class__.__name__ == 'DataFrame'\n                    else x for x in data]\n    else:\n        data = data.values if data.__class__.__name__ == 'DataFrame' else data\n        data = [data]\n    data = [np.expand_dims(x, 1) if x is not None and x.ndim == 1\n            else x for x in data]\n\n    if len(data) != len(names):\n        if data and hasattr(data[0], 'shape'):\n            raise ValueError(\n                'Error when checking model ' + exception_prefix +\n                ': the list of Numpy arrays that you are passing to '\n                'your model is not the size the model expected. '\n                'Expected to see ' + str(len(names)) + ' array(s), '\n                'but instead got the following list of ' +\n                str(len(data)) + ' arrays: ' + str(data)[:200] + '...')\n        elif len(names) > 1:\n            raise ValueError(\n                'Error when checking model ' + exception_prefix +\n                ': you are passing a list as input to your model, '\n                'but the model expects a list of ' + str(len(names)) +\n                ' Numpy arrays instead. The list you passed was: ' +\n                str(data)[:200])\n        elif len(data) == 1 and not hasattr(data[0], 'shape'):\n            raise TypeError(\n                'Error when checking model ' + exception_prefix +\n                ': data should be a Numpy array, or list/dict of '\n                'Numpy arrays. Found: ' + str(data)[:200] + '...')\n        elif len(names) == 1:\n            data = [np.asarray(data)]\n\n    # Check shapes compatibility.\n    if shapes:\n        for i in range(len(names)):\n            if shapes[i] is not None:\n                data_shape = data[i].shape\n                shape = shapes[i]\n                if data[i].ndim != len(shape):\n                    raise ValueError(\n                        'Error when checking ' + exception_prefix +\n                        ': expected ' + names[i] + ' to have ' +\n                        str(len(shape)) + ' dimensions, but got array '\n                        'with shape ' + str(data_shape))\n                if not check_batch_axis:\n                    data_shape = data_shape[1:]\n                    shape = shape[1:]\n                for dim, ref_dim in zip(data_shape, shape):\n                    if ref_dim != dim and ref_dim:\n                        raise ValueError(\n                            'Error when checking ' + exception_prefix +\n                            ': expected ' + names[i] + ' to have shape ' +\n                            str(shape) + ' but got array with shape ' +\n                            str(data_shape))\n    return data\n\n\ndef standardize_sample_or_class_weights(x_weight,\n                                        output_names,\n                                        weight_type):\n    \"\"\"Maps `sample_weight` or `class_weight` to model outputs.\n\n    # Arguments\n        x_weight: User-provided `sample_weight` or `class_weight` argument.\n        output_names: List of output names (strings) in the model.\n        weight_type: A string used purely for exception printing.\n\n    # Returns\n        A list of `sample_weight` or `class_weight` where there are exactly\n            one element per model output.\n\n    # Raises\n        ValueError: In case of invalid user-provided argument.\n    \"\"\"\n    if x_weight is None or len(x_weight) == 0:\n        return [None for _ in output_names]\n    if len(output_names) == 1:\n        if isinstance(x_weight, list) and len(x_weight) == 1:\n            return x_weight\n        if isinstance(x_weight, dict) and output_names[0] in x_weight:\n            return [x_weight[output_names[0]]]\n        else:\n            return [x_weight]\n    if isinstance(x_weight, list):\n        if len(x_weight) != len(output_names):\n            raise ValueError('Provided `' + weight_type + '` was a list of ' +\n                             str(len(x_weight)) +\n                             ' elements, but the model has ' +\n                             str(len(output_names)) + ' outputs. '\n                             'You should provide one `' + weight_type + '`'\n                             'array per model output.')\n        return x_weight\n    if isinstance(x_weight, dict):\n        x_weights = []\n        for name in output_names:\n            x_weights.append(x_weight.get(name))\n        return x_weights\n    else:\n        raise TypeError('The model has multiple outputs, so `' +\n                        weight_type + '` '\n                        'should be either a list or a dict. '\n                        'Provided `' + weight_type +\n                        '` type not understood: ' +\n                        str(x_weight))\n\n\ndef standardize_class_weights(class_weight, output_names):\n    return standardize_sample_or_class_weights(class_weight,\n                                               output_names,\n                                               'class_weight')\n\n\ndef standardize_sample_weights(sample_weight, output_names):\n    return standardize_sample_or_class_weights(sample_weight,\n                                               output_names,\n                                               'sample_weight')\n\n\ndef check_array_length_consistency(inputs, targets, weights=None):\n    \"\"\"Checks if batch axes are the same for numpy arrays.\n\n    # Arguments\n        inputs: list of Numpy arrays of inputs.\n        targets: list of Numpy arrays of targets.\n        weights: list of Numpy arrays of sample weights.\n\n    # Raises\n        ValueError: in case of incorrectly formatted data.\n    \"\"\"\n    def set_of_lengths(x):\n        # return a set with the variation between\n        # different shapes, with None => 0\n        if x is None:\n            return {0}\n        else:\n            return set([0 if y is None else y.shape[0] for y in x])\n\n    set_x = set_of_lengths(inputs)\n    set_y = set_of_lengths(targets)\n    set_w = set_of_lengths(weights)\n    if len(set_x) > 1:\n        raise ValueError('All input arrays (x) should have '\n                         'the same number of samples. Got array shapes: ' +\n                         str([x.shape for x in inputs]))\n    if len(set_y) > 1:\n        raise ValueError('All target arrays (y) should have '\n                         'the same number of samples. Got array shapes: ' +\n                         str([y.shape for y in targets]))\n    if set_x and set_y and list(set_x)[0] != list(set_y)[0]:\n        raise ValueError('Input arrays should have '\n                         'the same number of samples as target arrays. '\n                         'Found ' + str(list(set_x)[0]) + ' input samples '\n                         'and ' + str(list(set_y)[0]) + ' target samples.')\n    if len(set_w) > 1:\n        raise ValueError('All sample_weight arrays should have '\n                         'the same number of samples. Got array shapes: ' +\n                         str([w.shape for w in weights]))\n    if set_y and set_w and list(set_y)[0] != list(set_w)[0]:\n        raise ValueError('Sample_weight arrays should have '\n                         'the same number of samples as target arrays. Got ' +\n                         str(list(set_y)[0]) + ' input samples and ' +\n                         str(list(set_w)[0]) + ' target samples.')\n\n\ndef check_loss_and_target_compatibility(targets, loss_fns, output_shapes):\n    \"\"\"Does validation on the compatibility of targets and loss functions.\n\n    This helps prevent users from using loss functions incorrectly.\n\n    # Arguments\n        targets: list of Numpy arrays of targets.\n        loss_fns: list of loss functions.\n        output_shapes: list of shapes of model outputs.\n\n    # Raises\n        ValueError: if a loss function or target array\n            is incompatible with an output.\n    \"\"\"\n    key_losses = {losses.mean_squared_error,\n                  losses.binary_crossentropy,\n                  losses.categorical_crossentropy}\n    for y, loss, shape in zip(targets, loss_fns, output_shapes):\n        if y is None or loss is None:\n            continue\n        if loss is losses.categorical_crossentropy:\n            if y.shape[-1] == 1:\n                raise ValueError(\n                    'You are passing a target array of shape ' + str(y.shape) +\n                    ' while using as loss `categorical_crossentropy`. '\n                    '`categorical_crossentropy` expects '\n                    'targets to be binary matrices (1s and 0s) '\n                    'of shape (samples, classes). '\n                    'If your targets are integer classes, '\n                    'you can convert them to the expected format via:\\n'\n                    '```\\n'\n                    'from keras.utils import to_categorical\\n'\n                    'y_binary = to_categorical(y_int)\\n'\n                    '```\\n'\n                    '\\n'\n                    'Alternatively, you can use the loss function '\n                    '`sparse_categorical_crossentropy` instead, '\n                    'which does expect integer targets.')\n        if loss in key_losses:\n            for target_dim, out_dim in zip(y.shape[1:], shape[1:]):\n                if out_dim is not None and target_dim != out_dim:\n                    raise ValueError(\n                        'A target array with shape ' + str(y.shape) +\n                        ' was passed for an output of shape ' + str(shape) +\n                        ' while using as loss `' + loss.__name__ + '`. '\n                        'This loss expects '\n                        'targets to have the same shape '\n                        'as the output.')\n\n\ndef collect_metrics(metrics, output_names):\n    \"\"\"Maps metric functions to model outputs.\n\n    # Arguments\n        metrics: a list or dict of metric functions.\n        output_names: a list of the names (strings) of model outputs.\n\n    # Returns\n        A list (one entry per model output) of lists of metric functions.\n        For instance, if the model has 2 outputs, and for the first output\n        we want to compute \"binary_accuracy\" and \"binary_crossentropy\",\n        and just \"binary_accuracy\" for the second output,\n        the list would look like:\n            `[[binary_accuracy, binary_crossentropy], [binary_accuracy]]`\n\n    # Raises\n        TypeError: if an incorrect type is passed for the `metrics` argument.\n    \"\"\"\n    if not metrics:\n        return [[] for _ in output_names]\n    if isinstance(metrics, list):\n        # we then apply all metrics to all outputs.\n        return [copy.copy(metrics) for _ in output_names]\n    elif isinstance(metrics, dict):\n        nested_metrics = []\n        for name in output_names:\n            output_metrics = metrics.get(name, [])\n            if not isinstance(output_metrics, list):\n                output_metrics = [output_metrics]\n            nested_metrics.append(output_metrics)\n        return nested_metrics\n    else:\n        raise TypeError('Type of `metrics` argument not understood. '\n                        'Expected a list or dictionary, found: ' +\n                        str(metrics))\n\n\ndef batch_shuffle(index_array, batch_size):\n    \"\"\"Shuffles an array in a batch-wise fashion.\n\n    Useful for shuffling HDF5 arrays\n    (where one cannot access arbitrary indices).\n\n    # Arguments\n        index_array: array of indices to be shuffled.\n        batch_size: integer.\n\n    # Returns\n        The `index_array` array, shuffled in a batch-wise fashion.\n    \"\"\"\n    batch_count = int(len(index_array) / batch_size)\n    # to reshape we need to be cleanly divisible by batch size\n    # we stash extra items and reappend them after shuffling\n    last_batch = index_array[batch_count * batch_size:]\n    index_array = index_array[:batch_count * batch_size]\n    index_array = index_array.reshape((batch_count, batch_size))\n    np.random.shuffle(index_array)\n    index_array = index_array.flatten()\n    return np.append(index_array, last_batch)\n\n\ndef make_batches(size, batch_size):\n    \"\"\"Returns a list of batch indices (tuples of indices).\n\n    # Arguments\n        size: Integer, total size of the data to slice into batches.\n        batch_size: Integer, batch size.\n\n    # Returns\n        A list of tuples of array indices.\n    \"\"\"\n    num_batches = (size + batch_size - 1) // batch_size  # round up\n    return [(i * batch_size, min(size, (i + 1) * batch_size))\n            for i in range(num_batches)]\n\n\ndef weighted_masked_objective(fn):\n    \"\"\"Adds support for masking and sample-weighting to an objective function.\n\n    It transforms an objective function `fn(y_true, y_pred)`\n    into a sample-weighted, cost-masked objective function\n    `fn(y_true, y_pred, weights, mask)`.\n\n    # Arguments\n        fn: The objective function to wrap,\n            with signature `fn(y_true, y_pred)`.\n\n    # Returns\n        A function with signature `fn(y_true, y_pred, weights, mask)`.\n    \"\"\"\n    if fn is None:\n        return None\n\n    def weighted(y_true, y_pred, weights, mask=None):\n        \"\"\"Wrapper function.\n\n        # Arguments\n            y_true: `y_true` argument of `fn`.\n            y_pred: `y_pred` argument of `fn`.\n            weights: Weights tensor.\n            mask: Mask tensor.\n\n        # Returns\n            Scalar tensor.\n        \"\"\"\n        # score_array has ndim >= 2\n        score_array = fn(y_true, y_pred)\n        if mask is not None:\n            # Cast the mask to floatX to avoid float64 upcasting in Theano\n            mask = K.cast(mask, K.floatx())\n            # mask should have the same shape as score_array\n            score_array *= mask\n            #  the loss per batch should be proportional\n            #  to the number of unmasked samples.\n            score_array /= K.mean(mask)\n\n        # apply sample weighting\n        if weights is not None:\n            # reduce score_array to same ndim as weight array\n            ndim = K.ndim(score_array)\n            weight_ndim = K.ndim(weights)\n            score_array = K.mean(score_array,\n                                 axis=list(range(weight_ndim, ndim)))\n            score_array *= weights\n            score_array /= K.mean(K.cast(K.not_equal(weights, 0), K.floatx()))\n        return K.mean(score_array)\n    return weighted\n\n\ndef standardize_weights(y,\n                        sample_weight=None,\n                        class_weight=None,\n                        sample_weight_mode=None):\n    \"\"\"Performs sample weight validation and standardization.\n\n    Everything gets normalized to a single sample-wise (or timestep-wise)\n    weight array.\n\n    # Arguments\n        y: Numpy array of model targets to be weighted.\n        sample_weight: User-provided `sample_weight` argument.\n        class_weight: User-provided `class_weight` argument.\n        sample_weight_mode: One of `None` or `\"temporal\"`.\n            `\"temporal\"` indicated that we expect 2D weight data\n            that will be applied to the last 2 dimensions of\n            the targets (i.e. we are weighting timesteps, not samples).\n\n    # Returns\n        A numpy array of target weights, one entry per sample to weight.\n\n    # Raises\n        ValueError: In case of invalid user-provided arguments.\n    \"\"\"\n    if sample_weight_mode is not None:\n        if sample_weight_mode != 'temporal':\n            raise ValueError('\"sample_weight_mode '\n                             'should be None or \"temporal\". '\n                             'Found: ' + str(sample_weight_mode))\n        if len(y.shape) < 3:\n            raise ValueError('Found a sample_weight array for '\n                             'an input with shape ' +\n                             str(y.shape) + '. '\n                             'Timestep-wise sample weighting (use of '\n                             'sample_weight_mode=\"temporal\") is restricted to '\n                             'outputs that are at least 3D, i.e. that have '\n                             'a time dimension.')\n        if sample_weight is not None and len(sample_weight.shape) != 2:\n            raise ValueError('Found a sample_weight array with shape ' +\n                             str(sample_weight.shape) + '. '\n                             'In order to use timestep-wise sample weighting, '\n                             'you should pass a 2D sample_weight array.')\n    else:\n        if sample_weight is not None and len(sample_weight.shape) != 1:\n            raise ValueError('Found a sample_weight array with shape ' +\n                             str(sample_weight.shape) + '. '\n                             'In order to use timestep-wise sample weights, '\n                             'you should specify '\n                             'sample_weight_mode=\"temporal\" '\n                             'in compile(). If you just mean to use '\n                             'sample-wise weights, make sure your '\n                             'sample_weight array is 1D.')\n\n    if sample_weight is not None:\n        if len(sample_weight.shape) > len(y.shape):\n            raise ValueError('Found a sample_weight with shape' +\n                             str(sample_weight.shape) + '.'\n                             'Expected sample_weight with rank '\n                             'less than or equal to ' + str(len(y.shape)))\n\n        if y.shape[:sample_weight.ndim] != sample_weight.shape:\n            raise ValueError('Found a sample_weight array with shape ' +\n                             str(sample_weight.shape) +\n                             ' for an input with shape ' +\n                             str(y.shape) + '. '\n                             'sample_weight cannot be broadcast.')\n        return sample_weight\n    elif isinstance(class_weight, dict):\n        if len(y.shape) > 2:\n            raise ValueError('`class_weight` not supported for '\n                             '3+ dimensional targets.')\n        if y.shape[1] > 1:\n            y_classes = np.argmax(y, axis=1)\n        elif y.shape[1] == 1:\n            y_classes = np.reshape(y, y.shape[0])\n        else:\n            y_classes = y\n\n        weights = np.asarray([class_weight[cls] for cls in y_classes\n                              if cls in class_weight])\n\n        if len(weights) != len(y_classes):\n            # subtract the sets to pick all missing classes\n            existing_classes = set(y_classes)\n            existing_class_weight = set(class_weight.keys())\n            raise ValueError('`class_weight` must contain '\n                             'all classes in the data.'\n                             ' The classes %s exist in the data but not in '\n                             '`class_weight`.'\n                             % (existing_classes - existing_class_weight))\n        return weights\n    else:\n        if sample_weight_mode is None:\n            return np.ones((y.shape[0],), dtype=K.floatx())\n        else:\n            return np.ones((y.shape[0], y.shape[1]), dtype=K.floatx())\n\n\ndef check_num_samples(ins,\n                      batch_size=None,\n                      steps=None,\n                      steps_name='steps'):\n    \"\"\"Checks the number of samples provided for training and evaluation.\n\n    The number of samples is not defined when running with `steps`,\n    in which case the number of samples is set to `None`.\n\n    # Arguments\n        ins: List of tensors to be fed to the Keras function.\n        batch_size: Integer batch size or `None` if not defined.\n        steps: Total number of steps (batches of samples)\n            before declaring `predict_loop` finished.\n            Ignored with the default value of `None`.\n        steps_name: The public API's parameter name for `steps`.\n\n    # Raises\n        ValueError: when `steps` is `None` and the attribute `ins.shape`\n        does not exist. Also raises ValueError when `steps` is not `None`\n        and `batch_size` is not `None` because they are mutually\n        exclusive.\n\n    # Returns\n        When steps is `None`, returns the number of samples to be\n        processed based on the size of the first dimension of the\n        first input numpy array. When steps is not `None` and\n        `batch_size` is `None`, returns `None`.\n\n    # Raises\n        ValueError: In case of invalid arguments.\n    \"\"\"\n    if steps is not None:\n        num_samples = None\n        if batch_size is not None:\n            raise ValueError('If ' + steps_name +\n                             ' is set, the `batch_size` must be None.')\n    elif ins and hasattr(ins[0], 'shape'):\n        num_samples = ins[0].shape[0]\n    else:\n        raise ValueError('Either the input data should have '\n                         'a defined shape, or ' + steps_name +\n                         ' should be specified.')\n    return num_samples\n",
      "file_patch": "@@ -0,0 +1,546 @@\n+\"\"\"Training-related utilities.\n+\"\"\"\n+from __future__ import absolute_import\n+from __future__ import division\n+from __future__ import print_function\n+\n+import copy\n+import numpy as np\n+\n+from .. import backend as K\n+from .. import losses\n+\n+\n+def standardize_input_data(data,\n+                           names,\n+                           shapes=None,\n+                           check_batch_axis=True,\n+                           exception_prefix=''):\n+    \"\"\"Normalizes inputs and targets provided by users.\n+\n+    Users may pass data as a list of arrays, dictionary of arrays,\n+    or as a single array. We normalize this to an ordered list of\n+    arrays (same order as `names`), while checking that the provided\n+    arrays have shapes that match the network's expectations.\n+\n+    # Arguments\n+        data: User-provided input data (polymorphic).\n+        names: List of expected array names.\n+        shapes: Optional list of expected array shapes.\n+        check_batch_axis: Boolean; whether to check that\n+            the batch axis of the arrays matches the expected\n+            value found in `shapes`.\n+        exception_prefix: String prefix used for exception formatting.\n+\n+    # Returns\n+        List of standardized input arrays (one array per model input).\n+\n+    # Raises\n+        ValueError: in case of improperly formatted user-provided data.\n+    \"\"\"\n+    if not names:\n+        if data is not None and hasattr(data, '__len__') and len(data):\n+            raise ValueError('Error when checking model ' +\n+                             exception_prefix + ': '\n+                             'expected no data, but got:', data)\n+        return []\n+    if data is None:\n+        return [None for _ in range(len(names))]\n+\n+    if isinstance(data, dict):\n+        try:\n+            data = [data[x].values if data[x].__class__.__name__ == 'DataFrame'\n+                    else data[x] for x in names]\n+        except KeyError as e:\n+            raise ValueError(\n+                'No data provided for \"' + e.args[0] + '\". Need data '\n+                'for each key in: ' + str(names))\n+    elif isinstance(data, list):\n+        if len(names) == 1 and data and isinstance(data[0], (float, int)):\n+            data = [np.asarray(data)]\n+        else:\n+            data = [x.values if x.__class__.__name__ == 'DataFrame'\n+                    else x for x in data]\n+    else:\n+        data = data.values if data.__class__.__name__ == 'DataFrame' else data\n+        data = [data]\n+    data = [np.expand_dims(x, 1) if x is not None and x.ndim == 1\n+            else x for x in data]\n+\n+    if len(data) != len(names):\n+        if data and hasattr(data[0], 'shape'):\n+            raise ValueError(\n+                'Error when checking model ' + exception_prefix +\n+                ': the list of Numpy arrays that you are passing to '\n+                'your model is not the size the model expected. '\n+                'Expected to see ' + str(len(names)) + ' array(s), '\n+                'but instead got the following list of ' +\n+                str(len(data)) + ' arrays: ' + str(data)[:200] + '...')\n+        elif len(names) > 1:\n+            raise ValueError(\n+                'Error when checking model ' + exception_prefix +\n+                ': you are passing a list as input to your model, '\n+                'but the model expects a list of ' + str(len(names)) +\n+                ' Numpy arrays instead. The list you passed was: ' +\n+                str(data)[:200])\n+        elif len(data) == 1 and not hasattr(data[0], 'shape'):\n+            raise TypeError(\n+                'Error when checking model ' + exception_prefix +\n+                ': data should be a Numpy array, or list/dict of '\n+                'Numpy arrays. Found: ' + str(data)[:200] + '...')\n+        elif len(names) == 1:\n+            data = [np.asarray(data)]\n+\n+    # Check shapes compatibility.\n+    if shapes:\n+        for i in range(len(names)):\n+            if shapes[i] is not None:\n+                data_shape = data[i].shape\n+                shape = shapes[i]\n+                if data[i].ndim != len(shape):\n+                    raise ValueError(\n+                        'Error when checking ' + exception_prefix +\n+                        ': expected ' + names[i] + ' to have ' +\n+                        str(len(shape)) + ' dimensions, but got array '\n+                        'with shape ' + str(data_shape))\n+                if not check_batch_axis:\n+                    data_shape = data_shape[1:]\n+                    shape = shape[1:]\n+                for dim, ref_dim in zip(data_shape, shape):\n+                    if ref_dim != dim and ref_dim:\n+                        raise ValueError(\n+                            'Error when checking ' + exception_prefix +\n+                            ': expected ' + names[i] + ' to have shape ' +\n+                            str(shape) + ' but got array with shape ' +\n+                            str(data_shape))\n+    return data\n+\n+\n+def standardize_sample_or_class_weights(x_weight,\n+                                        output_names,\n+                                        weight_type):\n+    \"\"\"Maps `sample_weight` or `class_weight` to model outputs.\n+\n+    # Arguments\n+        x_weight: User-provided `sample_weight` or `class_weight` argument.\n+        output_names: List of output names (strings) in the model.\n+        weight_type: A string used purely for exception printing.\n+\n+    # Returns\n+        A list of `sample_weight` or `class_weight` where there are exactly\n+            one element per model output.\n+\n+    # Raises\n+        ValueError: In case of invalid user-provided argument.\n+    \"\"\"\n+    if x_weight is None or len(x_weight) == 0:\n+        return [None for _ in output_names]\n+    if len(output_names) == 1:\n+        if isinstance(x_weight, list) and len(x_weight) == 1:\n+            return x_weight\n+        if isinstance(x_weight, dict) and output_names[0] in x_weight:\n+            return [x_weight[output_names[0]]]\n+        else:\n+            return [x_weight]\n+    if isinstance(x_weight, list):\n+        if len(x_weight) != len(output_names):\n+            raise ValueError('Provided `' + weight_type + '` was a list of ' +\n+                             str(len(x_weight)) +\n+                             ' elements, but the model has ' +\n+                             str(len(output_names)) + ' outputs. '\n+                             'You should provide one `' + weight_type + '`'\n+                             'array per model output.')\n+        return x_weight\n+    if isinstance(x_weight, dict):\n+        x_weights = []\n+        for name in output_names:\n+            x_weights.append(x_weight.get(name))\n+        return x_weights\n+    else:\n+        raise TypeError('The model has multiple outputs, so `' +\n+                        weight_type + '` '\n+                        'should be either a list or a dict. '\n+                        'Provided `' + weight_type +\n+                        '` type not understood: ' +\n+                        str(x_weight))\n+\n+\n+def standardize_class_weights(class_weight, output_names):\n+    return standardize_sample_or_class_weights(class_weight,\n+                                               output_names,\n+                                               'class_weight')\n+\n+\n+def standardize_sample_weights(sample_weight, output_names):\n+    return standardize_sample_or_class_weights(sample_weight,\n+                                               output_names,\n+                                               'sample_weight')\n+\n+\n+def check_array_length_consistency(inputs, targets, weights=None):\n+    \"\"\"Checks if batch axes are the same for numpy arrays.\n+\n+    # Arguments\n+        inputs: list of Numpy arrays of inputs.\n+        targets: list of Numpy arrays of targets.\n+        weights: list of Numpy arrays of sample weights.\n+\n+    # Raises\n+        ValueError: in case of incorrectly formatted data.\n+    \"\"\"\n+    def set_of_lengths(x):\n+        # return a set with the variation between\n+        # different shapes, with None => 0\n+        if x is None:\n+            return {0}\n+        else:\n+            return set([0 if y is None else y.shape[0] for y in x])\n+\n+    set_x = set_of_lengths(inputs)\n+    set_y = set_of_lengths(targets)\n+    set_w = set_of_lengths(weights)\n+    if len(set_x) > 1:\n+        raise ValueError('All input arrays (x) should have '\n+                         'the same number of samples. Got array shapes: ' +\n+                         str([x.shape for x in inputs]))\n+    if len(set_y) > 1:\n+        raise ValueError('All target arrays (y) should have '\n+                         'the same number of samples. Got array shapes: ' +\n+                         str([y.shape for y in targets]))\n+    if set_x and set_y and list(set_x)[0] != list(set_y)[0]:\n+        raise ValueError('Input arrays should have '\n+                         'the same number of samples as target arrays. '\n+                         'Found ' + str(list(set_x)[0]) + ' input samples '\n+                         'and ' + str(list(set_y)[0]) + ' target samples.')\n+    if len(set_w) > 1:\n+        raise ValueError('All sample_weight arrays should have '\n+                         'the same number of samples. Got array shapes: ' +\n+                         str([w.shape for w in weights]))\n+    if set_y and set_w and list(set_y)[0] != list(set_w)[0]:\n+        raise ValueError('Sample_weight arrays should have '\n+                         'the same number of samples as target arrays. Got ' +\n+                         str(list(set_y)[0]) + ' input samples and ' +\n+                         str(list(set_w)[0]) + ' target samples.')\n+\n+\n+def check_loss_and_target_compatibility(targets, loss_fns, output_shapes):\n+    \"\"\"Does validation on the compatibility of targets and loss functions.\n+\n+    This helps prevent users from using loss functions incorrectly.\n+\n+    # Arguments\n+        targets: list of Numpy arrays of targets.\n+        loss_fns: list of loss functions.\n+        output_shapes: list of shapes of model outputs.\n+\n+    # Raises\n+        ValueError: if a loss function or target array\n+            is incompatible with an output.\n+    \"\"\"\n+    key_losses = {losses.mean_squared_error,\n+                  losses.binary_crossentropy,\n+                  losses.categorical_crossentropy}\n+    for y, loss, shape in zip(targets, loss_fns, output_shapes):\n+        if y is None or loss is None:\n+            continue\n+        if loss is losses.categorical_crossentropy:\n+            if y.shape[-1] == 1:\n+                raise ValueError(\n+                    'You are passing a target array of shape ' + str(y.shape) +\n+                    ' while using as loss `categorical_crossentropy`. '\n+                    '`categorical_crossentropy` expects '\n+                    'targets to be binary matrices (1s and 0s) '\n+                    'of shape (samples, classes). '\n+                    'If your targets are integer classes, '\n+                    'you can convert them to the expected format via:\\n'\n+                    '```\\n'\n+                    'from keras.utils import to_categorical\\n'\n+                    'y_binary = to_categorical(y_int)\\n'\n+                    '```\\n'\n+                    '\\n'\n+                    'Alternatively, you can use the loss function '\n+                    '`sparse_categorical_crossentropy` instead, '\n+                    'which does expect integer targets.')\n+        if loss in key_losses:\n+            for target_dim, out_dim in zip(y.shape[1:], shape[1:]):\n+                if out_dim is not None and target_dim != out_dim:\n+                    raise ValueError(\n+                        'A target array with shape ' + str(y.shape) +\n+                        ' was passed for an output of shape ' + str(shape) +\n+                        ' while using as loss `' + loss.__name__ + '`. '\n+                        'This loss expects '\n+                        'targets to have the same shape '\n+                        'as the output.')\n+\n+\n+def collect_metrics(metrics, output_names):\n+    \"\"\"Maps metric functions to model outputs.\n+\n+    # Arguments\n+        metrics: a list or dict of metric functions.\n+        output_names: a list of the names (strings) of model outputs.\n+\n+    # Returns\n+        A list (one entry per model output) of lists of metric functions.\n+        For instance, if the model has 2 outputs, and for the first output\n+        we want to compute \"binary_accuracy\" and \"binary_crossentropy\",\n+        and just \"binary_accuracy\" for the second output,\n+        the list would look like:\n+            `[[binary_accuracy, binary_crossentropy], [binary_accuracy]]`\n+\n+    # Raises\n+        TypeError: if an incorrect type is passed for the `metrics` argument.\n+    \"\"\"\n+    if not metrics:\n+        return [[] for _ in output_names]\n+    if isinstance(metrics, list):\n+        # we then apply all metrics to all outputs.\n+        return [copy.copy(metrics) for _ in output_names]\n+    elif isinstance(metrics, dict):\n+        nested_metrics = []\n+        for name in output_names:\n+            output_metrics = metrics.get(name, [])\n+            if not isinstance(output_metrics, list):\n+                output_metrics = [output_metrics]\n+            nested_metrics.append(output_metrics)\n+        return nested_metrics\n+    else:\n+        raise TypeError('Type of `metrics` argument not understood. '\n+                        'Expected a list or dictionary, found: ' +\n+                        str(metrics))\n+\n+\n+def batch_shuffle(index_array, batch_size):\n+    \"\"\"Shuffles an array in a batch-wise fashion.\n+\n+    Useful for shuffling HDF5 arrays\n+    (where one cannot access arbitrary indices).\n+\n+    # Arguments\n+        index_array: array of indices to be shuffled.\n+        batch_size: integer.\n+\n+    # Returns\n+        The `index_array` array, shuffled in a batch-wise fashion.\n+    \"\"\"\n+    batch_count = int(len(index_array) / batch_size)\n+    # to reshape we need to be cleanly divisible by batch size\n+    # we stash extra items and reappend them after shuffling\n+    last_batch = index_array[batch_count * batch_size:]\n+    index_array = index_array[:batch_count * batch_size]\n+    index_array = index_array.reshape((batch_count, batch_size))\n+    np.random.shuffle(index_array)\n+    index_array = index_array.flatten()\n+    return np.append(index_array, last_batch)\n+\n+\n+def make_batches(size, batch_size):\n+    \"\"\"Returns a list of batch indices (tuples of indices).\n+\n+    # Arguments\n+        size: Integer, total size of the data to slice into batches.\n+        batch_size: Integer, batch size.\n+\n+    # Returns\n+        A list of tuples of array indices.\n+    \"\"\"\n+    num_batches = (size + batch_size - 1) // batch_size  # round up\n+    return [(i * batch_size, min(size, (i + 1) * batch_size))\n+            for i in range(num_batches)]\n+\n+\n+def weighted_masked_objective(fn):\n+    \"\"\"Adds support for masking and sample-weighting to an objective function.\n+\n+    It transforms an objective function `fn(y_true, y_pred)`\n+    into a sample-weighted, cost-masked objective function\n+    `fn(y_true, y_pred, weights, mask)`.\n+\n+    # Arguments\n+        fn: The objective function to wrap,\n+            with signature `fn(y_true, y_pred)`.\n+\n+    # Returns\n+        A function with signature `fn(y_true, y_pred, weights, mask)`.\n+    \"\"\"\n+    if fn is None:\n+        return None\n+\n+    def weighted(y_true, y_pred, weights, mask=None):\n+        \"\"\"Wrapper function.\n+\n+        # Arguments\n+            y_true: `y_true` argument of `fn`.\n+            y_pred: `y_pred` argument of `fn`.\n+            weights: Weights tensor.\n+            mask: Mask tensor.\n+\n+        # Returns\n+            Scalar tensor.\n+        \"\"\"\n+        # score_array has ndim >= 2\n+        score_array = fn(y_true, y_pred)\n+        if mask is not None:\n+            # Cast the mask to floatX to avoid float64 upcasting in Theano\n+            mask = K.cast(mask, K.floatx())\n+            # mask should have the same shape as score_array\n+            score_array *= mask\n+            #  the loss per batch should be proportional\n+            #  to the number of unmasked samples.\n+            score_array /= K.mean(mask)\n+\n+        # apply sample weighting\n+        if weights is not None:\n+            # reduce score_array to same ndim as weight array\n+            ndim = K.ndim(score_array)\n+            weight_ndim = K.ndim(weights)\n+            score_array = K.mean(score_array,\n+                                 axis=list(range(weight_ndim, ndim)))\n+            score_array *= weights\n+            score_array /= K.mean(K.cast(K.not_equal(weights, 0), K.floatx()))\n+        return K.mean(score_array)\n+    return weighted\n+\n+\n+def standardize_weights(y,\n+                        sample_weight=None,\n+                        class_weight=None,\n+                        sample_weight_mode=None):\n+    \"\"\"Performs sample weight validation and standardization.\n+\n+    Everything gets normalized to a single sample-wise (or timestep-wise)\n+    weight array.\n+\n+    # Arguments\n+        y: Numpy array of model targets to be weighted.\n+        sample_weight: User-provided `sample_weight` argument.\n+        class_weight: User-provided `class_weight` argument.\n+        sample_weight_mode: One of `None` or `\"temporal\"`.\n+            `\"temporal\"` indicated that we expect 2D weight data\n+            that will be applied to the last 2 dimensions of\n+            the targets (i.e. we are weighting timesteps, not samples).\n+\n+    # Returns\n+        A numpy array of target weights, one entry per sample to weight.\n+\n+    # Raises\n+        ValueError: In case of invalid user-provided arguments.\n+    \"\"\"\n+    if sample_weight_mode is not None:\n+        if sample_weight_mode != 'temporal':\n+            raise ValueError('\"sample_weight_mode '\n+                             'should be None or \"temporal\". '\n+                             'Found: ' + str(sample_weight_mode))\n+        if len(y.shape) < 3:\n+            raise ValueError('Found a sample_weight array for '\n+                             'an input with shape ' +\n+                             str(y.shape) + '. '\n+                             'Timestep-wise sample weighting (use of '\n+                             'sample_weight_mode=\"temporal\") is restricted to '\n+                             'outputs that are at least 3D, i.e. that have '\n+                             'a time dimension.')\n+        if sample_weight is not None and len(sample_weight.shape) != 2:\n+            raise ValueError('Found a sample_weight array with shape ' +\n+                             str(sample_weight.shape) + '. '\n+                             'In order to use timestep-wise sample weighting, '\n+                             'you should pass a 2D sample_weight array.')\n+    else:\n+        if sample_weight is not None and len(sample_weight.shape) != 1:\n+            raise ValueError('Found a sample_weight array with shape ' +\n+                             str(sample_weight.shape) + '. '\n+                             'In order to use timestep-wise sample weights, '\n+                             'you should specify '\n+                             'sample_weight_mode=\"temporal\" '\n+                             'in compile(). If you just mean to use '\n+                             'sample-wise weights, make sure your '\n+                             'sample_weight array is 1D.')\n+\n+    if sample_weight is not None:\n+        if len(sample_weight.shape) > len(y.shape):\n+            raise ValueError('Found a sample_weight with shape' +\n+                             str(sample_weight.shape) + '.'\n+                             'Expected sample_weight with rank '\n+                             'less than or equal to ' + str(len(y.shape)))\n+\n+        if y.shape[:sample_weight.ndim] != sample_weight.shape:\n+            raise ValueError('Found a sample_weight array with shape ' +\n+                             str(sample_weight.shape) +\n+                             ' for an input with shape ' +\n+                             str(y.shape) + '. '\n+                             'sample_weight cannot be broadcast.')\n+        return sample_weight\n+    elif isinstance(class_weight, dict):\n+        if len(y.shape) > 2:\n+            raise ValueError('`class_weight` not supported for '\n+                             '3+ dimensional targets.')\n+        if y.shape[1] > 1:\n+            y_classes = np.argmax(y, axis=1)\n+        elif y.shape[1] == 1:\n+            y_classes = np.reshape(y, y.shape[0])\n+        else:\n+            y_classes = y\n+\n+        weights = np.asarray([class_weight[cls] for cls in y_classes\n+                              if cls in class_weight])\n+\n+        if len(weights) != len(y_classes):\n+            # subtract the sets to pick all missing classes\n+            existing_classes = set(y_classes)\n+            existing_class_weight = set(class_weight.keys())\n+            raise ValueError('`class_weight` must contain '\n+                             'all classes in the data.'\n+                             ' The classes %s exist in the data but not in '\n+                             '`class_weight`.'\n+                             % (existing_classes - existing_class_weight))\n+        return weights\n+    else:\n+        if sample_weight_mode is None:\n+            return np.ones((y.shape[0],), dtype=K.floatx())\n+        else:\n+            return np.ones((y.shape[0], y.shape[1]), dtype=K.floatx())\n+\n+\n+def check_num_samples(ins,\n+                      batch_size=None,\n+                      steps=None,\n+                      steps_name='steps'):\n+    \"\"\"Checks the number of samples provided for training and evaluation.\n+\n+    The number of samples is not defined when running with `steps`,\n+    in which case the number of samples is set to `None`.\n+\n+    # Arguments\n+        ins: List of tensors to be fed to the Keras function.\n+        batch_size: Integer batch size or `None` if not defined.\n+        steps: Total number of steps (batches of samples)\n+            before declaring `predict_loop` finished.\n+            Ignored with the default value of `None`.\n+        steps_name: The public API's parameter name for `steps`.\n+\n+    # Raises\n+        ValueError: when `steps` is `None` and the attribute `ins.shape`\n+        does not exist. Also raises ValueError when `steps` is not `None`\n+        and `batch_size` is not `None` because they are mutually\n+        exclusive.\n+\n+    # Returns\n+        When steps is `None`, returns the number of samples to be\n+        processed based on the size of the first dimension of the\n+        first input numpy array. When steps is not `None` and\n+        `batch_size` is `None`, returns `None`.\n+\n+    # Raises\n+        ValueError: In case of invalid arguments.\n+    \"\"\"\n+    if steps is not None:\n+        num_samples = None\n+        if batch_size is not None:\n+            raise ValueError('If ' + steps_name +\n+                             ' is set, the `batch_size` must be None.')\n+    elif ins and hasattr(ins[0], 'shape'):\n+        num_samples = ins[0].shape[0]\n+    else:\n+        raise ValueError('Either the input data should have '\n+                         'a defined shape, or ' + steps_name +\n+                         ' should be specified.')\n+    return num_samples\n",
      "files_name_in_blame_commit": [
        "test_sequential_model.py",
        "training_utils.py",
        "training_generator.py",
        "training.py",
        "test_training.py",
        "generic_utils.py",
        "test_loss_masking.py",
        "training_arrays.py"
      ]
    }
  },
  "commits_modify_file_before_fix": {
    "size": 9
  },
  "recursive_blame_commits": {}
}