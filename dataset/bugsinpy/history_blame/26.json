{
  "id": "26",
  "blame_commit": {
    "commit": {
      "commit_id": "57c43fdce6809382b360e1dff61b1289d62ce679",
      "commit_message": "added SitemapSpider, with tests and doc",
      "commit_author": "Pablo Hoffman",
      "commit_date": "2011-06-15 11:54:34",
      "commit_parent": "91dc46539fbcfd0df553d632676254d46d97f9e7"
    },
    "function": {
      "function_name": "_parse_sitemap",
      "function_code_before": "",
      "function_code_after": "",
      "function_before_start_line": "",
      "function_before_end_line": "",
      "function_after_start_line": "",
      "function_after_end_line": "",
      "function_before_token_count": 0,
      "function_after_token_count": 0,
      "functions_name_modified_file": [
        "__init__",
        "_parse_sitemap",
        "start_requests"
      ],
      "functions_name_all_files": [
        "__init__",
        "test_sitemap_index",
        "__iter__",
        "start_requests",
        "sitemap_urls_from_robots",
        "_parse_sitemap",
        "test_sitemap",
        "test_sitemap_urls_from_robots"
      ],
      "functions_name_co_evolved_modified_file": [
        "__init__",
        "start_requests"
      ],
      "functions_name_co_evolved_all_files": [
        "__init__",
        "test_sitemap_index",
        "__iter__",
        "start_requests",
        "sitemap_urls_from_robots",
        "test_sitemap",
        "test_sitemap_urls_from_robots"
      ]
    },
    "file": {
      "file_name": "sitemap.py",
      "file_nloc": 35,
      "file_complexity": 15,
      "file_token_count": 278,
      "file_before": null,
      "file_after": "import re\n\nfrom scrapy.spider import BaseSpider\nfrom scrapy.http import Request\nfrom scrapy.utils.sitemap import Sitemap, sitemap_urls_from_robots\n\nclass SitemapSpider(BaseSpider):\n\n    sitemap_urls = ()\n    sitemap_rules = [('', 'parse')]\n\n    def __init__(self, *a, **kw):\n        super(SitemapSpider, self).__init__(*a, **kw)\n        self._cbs = []\n        for r, c in self.sitemap_rules:\n            if isinstance(r, basestring):\n                r = re.compile(r)\n            if isinstance(c, basestring):\n                c = getattr(self, c)\n            self._cbs.append((r, c))\n            print self._cbs\n\n    def start_requests(self):\n        return [Request(x, callback=self._parse_sitemap) for x in self.sitemap_urls]\n\n    def _parse_sitemap(self, response):\n        if response.url.endswith('/robots.txt'):\n            for url in sitemap_urls_from_robots(response.body):\n                yield Request(url, callback=self._parse_sitemap)\n        else:\n            s = Sitemap(response.body)\n            if s.type == 'sitemapindex':\n                for sitemap in s:\n                    yield Request(sitemap['loc'], callback=self._parse_sitemap)\n            elif s.type == 'urlset':\n                for url in s:\n                    loc = url['loc']\n                    for r, c in self._cbs:\n                        if r.search(loc):\n                            yield Request(loc, callback=c)\n                            break\n",
      "file_patch": "@@ -0,0 +1,41 @@\n+import re\n+\n+from scrapy.spider import BaseSpider\n+from scrapy.http import Request\n+from scrapy.utils.sitemap import Sitemap, sitemap_urls_from_robots\n+\n+class SitemapSpider(BaseSpider):\n+\n+    sitemap_urls = ()\n+    sitemap_rules = [('', 'parse')]\n+\n+    def __init__(self, *a, **kw):\n+        super(SitemapSpider, self).__init__(*a, **kw)\n+        self._cbs = []\n+        for r, c in self.sitemap_rules:\n+            if isinstance(r, basestring):\n+                r = re.compile(r)\n+            if isinstance(c, basestring):\n+                c = getattr(self, c)\n+            self._cbs.append((r, c))\n+            print self._cbs\n+\n+    def start_requests(self):\n+        return [Request(x, callback=self._parse_sitemap) for x in self.sitemap_urls]\n+\n+    def _parse_sitemap(self, response):\n+        if response.url.endswith('/robots.txt'):\n+            for url in sitemap_urls_from_robots(response.body):\n+                yield Request(url, callback=self._parse_sitemap)\n+        else:\n+            s = Sitemap(response.body)\n+            if s.type == 'sitemapindex':\n+                for sitemap in s:\n+                    yield Request(sitemap['loc'], callback=self._parse_sitemap)\n+            elif s.type == 'urlset':\n+                for url in s:\n+                    loc = url['loc']\n+                    for r, c in self._cbs:\n+                        if r.search(loc):\n+                            yield Request(loc, callback=c)\n+                            break\n",
      "files_name_in_blame_commit": [
        "test_utils_sitemap.py",
        "__init__.py",
        "sitemap.py"
      ]
    }
  },
  "commits_modify_file_before_fix": {
    "size": 19
  },
  "recursive_blame_commits": {}
}